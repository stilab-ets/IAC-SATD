Repo URL,Satd Comment Id,File Path Of First Occurence,File Path Of Last Occurence,renamed,Keyword,SATD Comment,context,bloc of first occurrence,bloc type of first occurrence,bloc of last occurrence,bloc type of last occurrence,SATD Comment Line Of First Occurence,SATD Comment Line Of Last Occurence,first Commit Hash,last Commit Hash,Link To The File Of First Occurence,Link To The File Of Last Occurence/When Adressed,Introduction Time,Last Occurence (even solved or not),number of commits,adressed ?
https://github.com/terraform-aws-modules/terraform-aws-eks,2,main.tf,main.tf,0,fix,"/** # terraform-aws-eks  * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). * Instructions on [this post](https://aws.amazon.com/blogs/aws/amazon-eks-now-generally-available/) * can help guide you through connecting to the cluster via `kubectl`.  * | Branch | Build status                                                                                                                                                      | * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) |  * ## Assumptions  ** You want to create a set of resources around an EKS cluster: namely an autoscaling group of workers and a security group for them. ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put this EKS.  * ## Usage example  * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry:  * ```hcl * module ""eks"" { *   source                = ""terraform-aws-modules/eks/aws"" *   version               = ""0.1.0"" *   cluster_name          = ""test-eks-cluster"" *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] *   tags                  = ""${map(""Environment"", ""test"")}"" *   vpc_id                = ""vpc-abcde012"" * } * ```  * ## Dependencies  * The `configure_kubectl_session` variable requires that both `[kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`heptio-authenticator-aws`](https://github.com/heptio/authenticator#4-set-up-kubectl-to-use-heptio-authenticator-for-aws-tokens) are installed and on your shell's PATH.  * ## Testing  * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them:  * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). * 2. Install bundler and the gems from our Gemfile: * *     ```bash *     gem install bundler && bundle install *     ``` * * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. * 4. Test using `bundle exec kitchen test` from the root of the repo.  * For now, connectivity to the kubernetes cluster is not tested but will be in the * future. If `configure_kubectl_session` is set `true`, once the test fixture has * converged, you can query the test cluster with `kubectl get nodes --watch --kubeconfig kubeconfig`.  * ## Doc generation  * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). * Generate them like so:  * ```bash * go get github.com/segmentio/terraform-docs * terraform-docs md ./ | cat -s | ghead -n -1 > README.md * ```  * ## Contributing  * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section.  * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md).  * ## IAM Permissions  * Testing and using this repo requires a minimum set of IAM permissions. Test permissions * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md).  * ## Change log  * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes.  * ## Authors  * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)!  * ## License  * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details.*/","/** 
 # terraform-aws-eks 
  
 * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available 
 * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). 
 * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) 
 * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). 
 * Instructions on [this post](https://aws.amazon.com/blogs/aws/amazon-eks-now-generally-available/) 
 * can help guide you through connecting to the cluster via `kubectl`. 
  
 * | Branch | Build status                                                                                                                                                      | 
 * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | 
 * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) | 
  
 * ## Assumptions 
  
 ** You want to create a set of resources around an EKS cluster: namely an autoscaling group of workers and a security group for them. 
 ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put this EKS. 
  
 * ## Usage example 
  
 * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry: 
  
 * ```hcl 
 * module ""eks"" { 
 *   source                = ""terraform-aws-modules/eks/aws"" 
 *   version               = ""0.1.0"" 
 *   cluster_name          = ""test-eks-cluster"" 
 *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] 
 *   tags                  = ""${map(""Environment"", ""test"")}"" 
 *   vpc_id                = ""vpc-abcde012"" 
 * } 
 * ``` 
  
 * ## Dependencies 
  
 * The `configure_kubectl_session` variable requires that both `[kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) 
 (>=1.10) and [`heptio-authenticator-aws`](https://github.com/heptio/authenticator#4-set-up-kubectl-to-use-heptio-authenticator-for-aws-tokens) 
 are installed and on your shell's PATH. 
  
 * ## Testing 
  
 * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them: 
  
 * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). 
 * 2. Install bundler and the gems from our Gemfile: 
 * 
 *     ```bash 
 *     gem install bundler && bundle install 
 *     ``` 
 * 
 * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. 
 * 4. Test using `bundle exec kitchen test` from the root of the repo. 
  
 * For now, connectivity to the kubernetes cluster is not tested but will be in the 
 * future. If `configure_kubectl_session` is set `true`, once the test fixture has 
 * converged, you can query the test cluster with `kubectl get nodes --watch --kubeconfig kubeconfig`. 
  
 * ## Doc generation 
  
 * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). 
 * Generate them like so: 
  
 * ```bash 
 * go get github.com/segmentio/terraform-docs 
 * terraform-docs md ./ | cat -s | ghead -n -1 > README.md 
 * ``` 
  
 * ## Contributing 
  
 * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section. 
  
 * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md). 
  
 * ## IAM Permissions 
  
 * Testing and using this repo requires a minimum set of IAM permissions. Test permissions 
 * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md). 
  
 * ## Change log 
  
 * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes. 
  
 * ## Authors 
  
 * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. 
 * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)! 
  
 * ## License 
  
 * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details. 
 */ ","provider ""null"" {}
",provider,"provider ""null"" {}
",provider,1,,6bda7ee97d0d83c1e7d45f77df8b30a4cda409cb,210e92d8219811fea3588328e633e534c1bd38d3,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/6bda7ee97d0d83c1e7d45f77df8b30a4cda409cb/main.tf#L1,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/210e92d8219811fea3588328e633e534c1bd38d3/main.tf,2018-06-11 03:34:13-07:00,2018-06-11 12:07:46-07:00,2,1
https://github.com/apache/beam,6,playground/terraform/infrastructure/setup/iam.tf,playground/terraform/infrastructure/setup/iam.tf,0,// todo,"#    ""user:${var.developer_account_email}"" // TODO: add variable","# 
 # Licensed to the Apache Software Foundation (ASF) under one 
 # or more contributor license agreements.  See the NOTICE file 
 # distributed with this work for additional information 
 # regarding copyright ownership.  The ASF licenses this file 
 # to you under the Apache License, Version 2.0 (the 
 # ""License""); you may not use this file except in compliance 
 # with the License.  You may obtain a copy of the License at 
 # 
 #   http://www.apache.org/licenses/LICENSE-2.0 
 # 
 # Unless required by applicable law or agreed to in writing, 
 # software distributed under the License is distributed on an 
 # ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY 
 # KIND, either express or implied.  See the License for the 
 # specific language governing permissions and limitations 
 # under the License. 
 #  
 #resource ""google_service_account"" ""terraform_service_account"" { 
 #  account_id   = ""terraform"" 
 #  display_name = ""terraform"" 
 #} 
 # 
 #resource ""google_project_iam_member"" ""terraform_service_account_roles"" { 
 #  for_each = toset([ 
 #    // TODO: add the required roles to provision resources (not OWNER :-)!) 
 #  ]) 
 #  role    = each.key 
 #  member  = ""serviceAccount:${google_service_account.terraform_service_account.email}"" 
 #  project = var.project_id 
 #} 
 # 
 #resource ""google_service_account_iam_binding"" ""terraform_service_account_token_permissions"" { 
 #  service_account_id = google_service_account.terraform_service_account.id 
 #  members = [ 
 #    ""user:${var.developer_account_email}"" // TODO: add variable 
 #  ] 
 #  role    = ""roles/iam.serviceAccountTokenCreator"" 
 #}  
 #resource ""google_service_account_iam_binding"" ""application_service_account_binding"" { 
 #  members            = [ 
 #    ""serviceAccount:${google_service_account.terraform_service_account.email}"" 
 #  ] 
 #  role               = ""roles/iam.serviceAccountUser"" 
 #  service_account_id = google_service_account.playground_service_account.id 
 #} ","resource ""google_service_account"" ""playground_service_account"" {
  account_id   = var.service_account_id
  display_name = var.service_account_id
}
",resource,"resource ""google_service_account"" ""playground_service_account"" {
  account_id   = var.service_account_id
  display_name = var.service_account_id
}
",resource,37,,675c0bc10f813ea593702f5e6a0fd2ce38caf720,ea4411f0082a65b7a974222b5954a44bd8dc8d31,https://github.com/apache/beam/blob/675c0bc10f813ea593702f5e6a0fd2ce38caf720/playground/terraform/infrastructure/setup/iam.tf#L37,https://github.com/apache/beam/blob/ea4411f0082a65b7a974222b5954a44bd8dc8d31/playground/terraform/infrastructure/setup/iam.tf,2022-02-22 10:04:20-08:00,2023-05-08 15:38:25-04:00,4,1
https://github.com/terraform-aws-modules/terraform-aws-eks,34,main.tf,main.tf,0,fix,"/** # terraform-aws-eks  * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html).  * | Branch | Build status                                                                                                                                                      | * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) |  * ## Assumptions  ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources. ** If using the default Updating Variable Value (`true`) for `configure_kubectl_session`, it's required that both [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`aws-iam-authenticator`](https://github.com/kubernetes-sigs/aws-iam-authenticator#4-set-up-kubectl-to-use-authentication-tokens-provided-by-aws-iam-authenticator-for-kubernetes) are installed and on your shell's PATH.  * ## Usage example  * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry:  * ```hcl * module ""eks"" { *   source                = ""terraform-aws-modules/eks/aws"" *   cluster_name          = ""test-eks-cluster"" *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] *   tags                  = ""${map(""Environment"", ""test"")}"" *   vpc_id                = ""vpc-abcde012"" * } * ```  * ## Other Documentation * * - [Autoscaling](docs/autoscaling.md): How to enabled worker node autoscaling.  * ## Release schedule  * Generally the maintainers will try to release the module once every 2 weeks to * keep up with PR additions. If particularly pressing changes are added or maintainers * come up with the spare time (hah!), release may happen more often on occasion.  * ## Testing  * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them:  * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). * 2. Install bundler and the gems from our Gemfile: * *     ```bash *     gem install bundler && bundle install *     ``` * * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. * 4. Test using `bundle exec kitchen test` from the root of the repo.  * For now, connectivity to the kubernetes cluster is not tested but will be in the * future. If `configure_kubectl_session` is set `true`, once the test fixture has * converged, you can query the test cluster from that terminal session with * `kubectl get nodes --watch --kubeconfig kubeconfig`.  * ## Doc generation  * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). * Generate them like so:  * ```bash * go get github.com/segmentio/terraform-docs * terraform-docs md ./ | cat -s | tail -r | tail -n +2 | tail -r > README.md * ```  * ## Contributing  * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section.  * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md).  * ## IAM Permissions  * Testing and using this repo requires a minimum set of IAM permissions. Test permissions * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md).  * ## Change log  * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes.  * ## Authors  * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)!  * ## License  * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details.*/","/** 
 # terraform-aws-eks 
  
 * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available 
 * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). 
 * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) 
 * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). 
 * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html). 
  
 * | Branch | Build status                                                                                                                                                      | 
 * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | 
 * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) | 
  
 * ## Assumptions 
  
 ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. 
 ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. 
 ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources. 
 ** If using the default Updating Variable Value (`true`) for `configure_kubectl_session`, it's required that both [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`aws-iam-authenticator`](https://github.com/kubernetes-sigs/aws-iam-authenticator#4-set-up-kubectl-to-use-authentication-tokens-provided-by-aws-iam-authenticator-for-kubernetes) are installed and on your shell's PATH. 
  
 * ## Usage example 
  
 * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry: 
  
 * ```hcl 
 * module ""eks"" { 
 *   source                = ""terraform-aws-modules/eks/aws"" 
 *   cluster_name          = ""test-eks-cluster"" 
 *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] 
 *   tags                  = ""${map(""Environment"", ""test"")}"" 
 *   vpc_id                = ""vpc-abcde012"" 
 * } 
 * ``` 
  
 * ## Other Documentation 
 * 
 * - [Autoscaling](docs/autoscaling.md): How to enabled worker node autoscaling. 
  
 * ## Release schedule 
  
 * Generally the maintainers will try to release the module once every 2 weeks to 
 * keep up with PR additions. If particularly pressing changes are added or maintainers 
 * come up with the spare time (hah!), release may happen more often on occasion. 
  
 * ## Testing 
  
 * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them: 
  
 * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). 
 * 2. Install bundler and the gems from our Gemfile: 
 * 
 *     ```bash 
 *     gem install bundler && bundle install 
 *     ``` 
 * 
 * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. 
 * 4. Test using `bundle exec kitchen test` from the root of the repo. 
  
 * For now, connectivity to the kubernetes cluster is not tested but will be in the 
 * future. If `configure_kubectl_session` is set `true`, once the test fixture has 
 * converged, you can query the test cluster from that terminal session with 
 * `kubectl get nodes --watch --kubeconfig kubeconfig`. 
  
 * ## Doc generation 
  
 * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). 
 * Generate them like so: 
  
 * ```bash 
 * go get github.com/segmentio/terraform-docs 
 * terraform-docs md ./ | cat -s | tail -r | tail -n +2 | tail -r > README.md 
 * ``` 
  
 * ## Contributing 
  
 * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section. 
  
 * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md). 
  
 * ## IAM Permissions 
  
 * Testing and using this repo requires a minimum set of IAM permissions. Test permissions 
 * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md). 
  
 * ## Change log 
  
 * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes. 
  
 * ## Authors 
  
 * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. 
 * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)! 
  
 * ## License 
  
 * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details. 
 */ ","provider ""null"" {}
",provider,"provider ""null"" {}
",provider,1,,28f7e9dd41579820684896ee624ff59963875f56,fdd44c8f385fa8af83ff737827dcb3404fe3db59,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/28f7e9dd41579820684896ee624ff59963875f56/main.tf#L1,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/fdd44c8f385fa8af83ff737827dcb3404fe3db59/main.tf,2018-08-27 17:17:32+02:00,2018-09-25 12:30:56+02:00,2,1
https://github.com/terraform-aws-modules/terraform-aws-eks,3,main.tf,main.tf,0,fix,"/** # terraform-aws-eks  * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started).  * | Branch | Build status                                                                                                                                                      | * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) |  * ## Assumptions ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources.  * ## Usage example  * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry:  * ```hcl * module ""eks"" { *   source                = ""terraform-aws-modules/eks/aws"" *   version               = ""0.1.0"" *   cluster_name          = ""test-eks-cluster"" *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] *   tags                  = ""${map(""Environment"", ""test"")}"" *   vpc_id                = ""vpc-abcde012"" * } * ```  * ## Dependencies  * The `configure_kubectl_session` variable requires that both `[kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`heptio-authenticator-aws`](https://github.com/heptio/authenticator#4-set-up-kubectl-to-use-heptio-authenticator-for-aws-tokens) are installed and on your shell's PATH.  * ## Testing  * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them:  * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). * 2. Install bundler and the gems from our Gemfile: * *     ```bash *     gem install bundler && bundle install *     ``` * * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. * 4. Test using `bundle exec kitchen test` from the root of the repo.  * For now, connectivity to the kubernetes cluster is not tested but will be in the * future. If `configure_kubectl_session` is set `true`, once the test fixture has * converged, you can query the test cluster from that terminal session with * `kubectl get nodes --watch --kubeconfig kubeconfig`.  * ## Doc generation  * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). * Generate them like so:  * ```bash * go get github.com/segmentio/terraform-docs * terraform-docs md ./ | cat -s | ghead -n -1 > README.md * ```  * ## Contributing  * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section.  * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md).  * ## IAM Permissions  * Testing and using this repo requires a minimum set of IAM permissions. Test permissions * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md).  * ## Change log  * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes.  * ## Authors  * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)!  * ## License  * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details.*/","/** 
 # terraform-aws-eks 
  
 * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available 
 * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). 
 * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) 
 * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). 
  
 * | Branch | Build status                                                                                                                                                      | 
 * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | 
 * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) | 
  
 * ## Assumptions 
 ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. 
 ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. 
 ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources. 
  
 * ## Usage example 
  
 * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry: 
  
 * ```hcl 
 * module ""eks"" { 
 *   source                = ""terraform-aws-modules/eks/aws"" 
 *   version               = ""0.1.0"" 
 *   cluster_name          = ""test-eks-cluster"" 
 *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] 
 *   tags                  = ""${map(""Environment"", ""test"")}"" 
 *   vpc_id                = ""vpc-abcde012"" 
 * } 
 * ``` 
  
 * ## Dependencies 
  
 * The `configure_kubectl_session` variable requires that both `[kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) 
 (>=1.10) and [`heptio-authenticator-aws`](https://github.com/heptio/authenticator#4-set-up-kubectl-to-use-heptio-authenticator-for-aws-tokens) 
 are installed and on your shell's PATH. 
  
 * ## Testing 
  
 * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them: 
  
 * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). 
 * 2. Install bundler and the gems from our Gemfile: 
 * 
 *     ```bash 
 *     gem install bundler && bundle install 
 *     ``` 
 * 
 * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. 
 * 4. Test using `bundle exec kitchen test` from the root of the repo. 
  
 * For now, connectivity to the kubernetes cluster is not tested but will be in the 
 * future. If `configure_kubectl_session` is set `true`, once the test fixture has 
 * converged, you can query the test cluster from that terminal session with 
 * `kubectl get nodes --watch --kubeconfig kubeconfig`. 
  
 * ## Doc generation 
  
 * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). 
 * Generate them like so: 
  
 * ```bash 
 * go get github.com/segmentio/terraform-docs 
 * terraform-docs md ./ | cat -s | ghead -n -1 > README.md 
 * ``` 
  
 * ## Contributing 
  
 * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section. 
  
 * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md). 
  
 * ## IAM Permissions 
  
 * Testing and using this repo requires a minimum set of IAM permissions. Test permissions 
 * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md). 
  
 * ## Change log 
  
 * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes. 
  
 * ## Authors 
  
 * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. 
 * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)! 
  
 * ## License 
  
 * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details. 
 */ ","provider ""null"" {}
",provider,"provider ""null"" {}
",provider,1,,210e92d8219811fea3588328e633e534c1bd38d3,c8997a5cf6f2707462ef8206360f8004f99724bd,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/210e92d8219811fea3588328e633e534c1bd38d3/main.tf#L1,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/c8997a5cf6f2707462ef8206360f8004f99724bd/main.tf,2018-06-11 12:07:46-07:00,2018-06-11 15:54:19-07:00,2,1
https://github.com/terraform-aws-modules/terraform-aws-eks,9,main.tf,main.tf,0,fix,"/** # terraform-aws-eks  * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html).  * | Branch | Build status                                                                                                                                                      | * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) |  * ## Assumptions  ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources. ** If using the default Updating Updating Variable Value (`true`) for `configure_kubectl_session`, it's required that both [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`heptio-authenticator-aws`](https://github.com/heptio/authenticator#4-set-up-kubectl-to-use-heptio-authenticator-for-aws-tokens) are installed and on your shell's PATH.  * ## Usage example  * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry:  * ```hcl * module ""eks"" { *   source                = ""terraform-aws-modules/eks/aws"" *   version               = ""0.1.0"" *   cluster_name          = ""test-eks-cluster"" *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] *   tags                  = ""${map(""Environment"", ""test"")}"" *   vpc_id                = ""vpc-abcde012"" * } * ```  * ## Release schedule  * Generally the maintainers will try to release the module once every 2 weeks to * keep up with PR additions. If particularly pressing changes are added or maintainers * come up with the spare time (hah!), release may happen more often on occasion.  * ## Testing  * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them:  * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). * 2. Install bundler and the gems from our Gemfile: * *     ```bash *     gem install bundler && bundle install *     ``` * * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. * 4. Test using `bundle exec kitchen test` from the root of the repo.  * For now, connectivity to the kubernetes cluster is not tested but will be in the * future. If `configure_kubectl_session` is set `true`, once the test fixture has * converged, you can query the test cluster from that terminal session with * `kubectl get nodes --watch --kubeconfig kubeconfig`.  * ## Doc generation  * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). * Generate them like so:  * ```bash * go get github.com/segmentio/terraform-docs * terraform-docs md ./ | cat -s | ghead -n -1 > README.md * ```  * ## Contributing  * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section.  * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md).  * ## IAM Permissions  * Testing and using this repo requires a minimum set of IAM permissions. Test permissions * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md).  * ## Change log  * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes.  * ## Authors  * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)!  * ## License  * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details.*/","/** 
 # terraform-aws-eks 
  
 * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available 
 * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). 
 * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) 
 * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). 
 * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html). 
  
 * | Branch | Build status                                                                                                                                                      | 
 * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | 
 * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) | 
  
 * ## Assumptions 
  
 ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. 
 ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. 
 ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources. 
 ** If using the default Updating Updating Variable Value (`true`) for `configure_kubectl_session`, it's required that both [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`heptio-authenticator-aws`](https://github.com/heptio/authenticator#4-set-up-kubectl-to-use-heptio-authenticator-for-aws-tokens) are installed and on your shell's PATH. 
  
 * ## Usage example 
  
 * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry: 
  
 * ```hcl 
 * module ""eks"" { 
 *   source                = ""terraform-aws-modules/eks/aws"" 
 *   version               = ""0.1.0"" 
 *   cluster_name          = ""test-eks-cluster"" 
 *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] 
 *   tags                  = ""${map(""Environment"", ""test"")}"" 
 *   vpc_id                = ""vpc-abcde012"" 
 * } 
 * ``` 
  
 * ## Release schedule 
  
 * Generally the maintainers will try to release the module once every 2 weeks to 
 * keep up with PR additions. If particularly pressing changes are added or maintainers 
 * come up with the spare time (hah!), release may happen more often on occasion. 
  
 * ## Testing 
  
 * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them: 
  
 * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). 
 * 2. Install bundler and the gems from our Gemfile: 
 * 
 *     ```bash 
 *     gem install bundler && bundle install 
 *     ``` 
 * 
 * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. 
 * 4. Test using `bundle exec kitchen test` from the root of the repo. 
  
 * For now, connectivity to the kubernetes cluster is not tested but will be in the 
 * future. If `configure_kubectl_session` is set `true`, once the test fixture has 
 * converged, you can query the test cluster from that terminal session with 
 * `kubectl get nodes --watch --kubeconfig kubeconfig`. 
  
 * ## Doc generation 
  
 * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). 
 * Generate them like so: 
  
 * ```bash 
 * go get github.com/segmentio/terraform-docs 
 * terraform-docs md ./ | cat -s | ghead -n -1 > README.md 
 * ``` 
  
 * ## Contributing 
  
 * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section. 
  
 * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md). 
  
 * ## IAM Permissions 
  
 * Testing and using this repo requires a minimum set of IAM permissions. Test permissions 
 * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md). 
  
 * ## Change log 
  
 * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes. 
  
 * ## Authors 
  
 * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. 
 * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)! 
  
 * ## License 
  
 * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details. 
 */ ","provider ""null"" {}
",provider,"provider ""null"" {}
",provider,1,,abe72915f3cf5e0bc8090f1d14bd01627a19fcca,f754fe45ee2d566651725e1f1a48ff657fb014b3,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/abe72915f3cf5e0bc8090f1d14bd01627a19fcca/main.tf#L1,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/f754fe45ee2d566651725e1f1a48ff657fb014b3/main.tf,2018-06-25 01:24:58-07:00,2018-07-01 01:51:07-07:00,2,1
https://github.com/Optum/dce,12,modules/reset_codebuild.tf,modules/reset_codebuild.tf,0,nuke,"/**  * Configure CodePipeline resources to  * execute our Account Reset process.  * - Run aws-nuke in each user account  *  * We are currently configured a unique CodePipeline  * resource for every user account.  * The Lambda whicxh triggers the CodePipeline refer to them  * by name, eg. `AccountReset_<AccountId>`*/","/** 
 * Configure CodePipeline resources to 
 * execute our Account Reset process. 
 * - Run aws-nuke in each user account 
 * 
 * We are currently configured a unique CodePipeline 
 * resource for every user account. 
 * The Lambda whicxh triggers the CodePipeline refer to them 
 * by name, eg. `AccountReset_<AccountId>` 
 */ ","locals {
  # https://stackoverflow.com/a/47243622
  isPr = replace(var.namespace, ""pr-"", """") != var.namespace
}
",locals,"locals {
  # https://stackoverflow.com/a/47243622
  isPr = replace(var.namespace, ""pr-"", """") != var.namespace
}
",locals,1,1.0,672ed294b124e4349a4057f5d35f01b7babdd123,d58e0e41033426cd444afaa3489ed2bef7d7c0c4,https://github.com/Optum/dce/blob/672ed294b124e4349a4057f5d35f01b7babdd123/modules/reset_codebuild.tf#L1,https://github.com/Optum/dce/blob/d58e0e41033426cd444afaa3489ed2bef7d7c0c4/modules/reset_codebuild.tf#L1,2019-11-05 16:22:03-06:00,2023-01-26 08:37:15-06:00,5,0
https://github.com/terraform-aws-modules/terraform-aws-eks,26,main.tf,main.tf,0,fix,"/** # terraform-aws-eks  * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html).  * | Branch | Build status                                                                                                                                                      | * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) |  * ## Assumptions  ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources. ** If using the default Updating Updating Variable Value (`true`) for `configure_kubectl_session`, it's required that both [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`aws-iam-authenticator`](https://github.com/kubernetes-sigs/aws-iam-authenticator#4-set-up-kubectl-to-use-authentication-tokens-provided-by-aws-iam-authenticator-for-kubernetes) are installed and on your shell's PATH.  * ## Usage example  * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry:  * ```hcl * module ""eks"" { *   source                = ""terraform-aws-modules/eks/aws"" *   cluster_name          = ""test-eks-cluster"" *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] *   tags                  = ""${map(""Environment"", ""test"")}"" *   vpc_id                = ""vpc-abcde012"" * } * ```  * ## Release schedule  * Generally the maintainers will try to release the module once every 2 weeks to * keep up with PR additions. If particularly pressing changes are added or maintainers * come up with the spare time (hah!), release may happen more often on occasion.  * ## Testing  * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them:  * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). * 2. Install bundler and the gems from our Gemfile: * *     ```bash *     gem install bundler && bundle install *     ``` * * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. * 4. Test using `bundle exec kitchen test` from the root of the repo.  * For now, connectivity to the kubernetes cluster is not tested but will be in the * future. If `configure_kubectl_session` is set `true`, once the test fixture has * converged, you can query the test cluster from that terminal session with * `kubectl get nodes --watch --kubeconfig kubeconfig`.  * ## Doc generation  * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). * Generate them like so:  * ```bash * go get github.com/segmentio/terraform-docs * terraform-docs md ./ | cat -s | tail -r | tail -n +2 | tail -r > README.md * ```  * ## Contributing  * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section.  * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md).  * ## IAM Permissions  * Testing and using this repo requires a minimum set of IAM permissions. Test permissions * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md).  * ## Change log  * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes.  * ## Authors  * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)!  * ## License  * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details.*/","/** 
 # terraform-aws-eks 
  
 * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available 
 * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). 
 * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) 
 * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). 
 * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html). 
  
 * | Branch | Build status                                                                                                                                                      | 
 * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | 
 * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) | 
  
 * ## Assumptions 
  
 ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. 
 ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. 
 ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources. 
 ** If using the default Updating Updating Variable Value (`true`) for `configure_kubectl_session`, it's required that both [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`aws-iam-authenticator`](https://github.com/kubernetes-sigs/aws-iam-authenticator#4-set-up-kubectl-to-use-authentication-tokens-provided-by-aws-iam-authenticator-for-kubernetes) are installed and on your shell's PATH. 
  
 * ## Usage example 
  
 * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry: 
  
 * ```hcl 
 * module ""eks"" { 
 *   source                = ""terraform-aws-modules/eks/aws"" 
 *   cluster_name          = ""test-eks-cluster"" 
 *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] 
 *   tags                  = ""${map(""Environment"", ""test"")}"" 
 *   vpc_id                = ""vpc-abcde012"" 
 * } 
 * ``` 
  
 * ## Release schedule 
  
 * Generally the maintainers will try to release the module once every 2 weeks to 
 * keep up with PR additions. If particularly pressing changes are added or maintainers 
 * come up with the spare time (hah!), release may happen more often on occasion. 
  
 * ## Testing 
  
 * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them: 
  
 * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). 
 * 2. Install bundler and the gems from our Gemfile: 
 * 
 *     ```bash 
 *     gem install bundler && bundle install 
 *     ``` 
 * 
 * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. 
 * 4. Test using `bundle exec kitchen test` from the root of the repo. 
  
 * For now, connectivity to the kubernetes cluster is not tested but will be in the 
 * future. If `configure_kubectl_session` is set `true`, once the test fixture has 
 * converged, you can query the test cluster from that terminal session with 
 * `kubectl get nodes --watch --kubeconfig kubeconfig`. 
  
 * ## Doc generation 
  
 * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). 
 * Generate them like so: 
  
 * ```bash 
 * go get github.com/segmentio/terraform-docs 
 * terraform-docs md ./ | cat -s | tail -r | tail -n +2 | tail -r > README.md 
 * ``` 
  
 * ## Contributing 
  
 * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section. 
  
 * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md). 
  
 * ## IAM Permissions 
  
 * Testing and using this repo requires a minimum set of IAM permissions. Test permissions 
 * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md). 
  
 * ## Change log 
  
 * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes. 
  
 * ## Authors 
  
 * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. 
 * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)! 
  
 * ## License 
  
 * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details. 
 */ ","provider ""null"" {}
",provider,"provider ""null"" {}
",provider,1,,69d7a3ce4a5d51c91569b078947efdbd10e3b1eb,28f7e9dd41579820684896ee624ff59963875f56,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/69d7a3ce4a5d51c91569b078947efdbd10e3b1eb/main.tf#L1,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/28f7e9dd41579820684896ee624ff59963875f56/main.tf,2018-07-11 23:47:35-07:00,2018-08-27 17:17:32+02:00,3,1
https://github.com/nebari-dev/nebari,7,qhub/template/{{ cookiecutter.repo_directory }}/infrastructure/modules/kubernetes/services/monitoring/main.tf,qhub/template/{{ cookiecutter.repo_directory }}/infrastructure/modules/kubernetes/services/monitoring/main.tf,0,crap,# This job will scrape from any service with the label app.kubernetes.io/component=traefik-internal-service,"# This job will scrape from any service with the label app.kubernetes.io/component=traefik-internal-service 
 # and the annotation app.kubernetes.io/scrape=true","resource ""helm_release"" ""kube-prometheus-stack-helm-deployment"" {
  name       = ""kube-prometheus-stack""
  namespace  = var.namespace
  repository = ""https://prometheus-community.github.io/helm-charts""
  chart      = ""kube-prometheus-stack""
  version    = ""16.12.0""

  values = [<<EOT
prometheus:    
  prometheusSpec:    
    additionalScrapeConfigs:    
    
    # This job will scrape from any service with the label app.kubernetes.io/component=traefik-internal-service
    # and the annotation app.kubernetes.io/scrape=true 
    - job_name: 'traefik'    
    
      kubernetes_sd_configs:    
        - role: service    
          
      relabel_configs:    
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]    
        action: keep    
        regex: true    
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]    
        action: replace    
        target_label: __metrics_path__    
        regex: (.+)    
      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
        action: replace    
        regex: ([^:]+)(?::\d+)?;(\d+)    
        replacement: $1:$2    
        target_label: __address__
EOT
  ]

  set {
    name  = ""grafana.grafana\\.ini.server.domain""
    value = var.external-url
  }

  set {
    name  = ""grafana.grafana\\.ini.server.root_url""
    value = ""%(protocol)s://%(domain)s/monitoring""
  }

  set {
    name  = ""grafana.grafana\\.ini.server.server_from_sub_path""
    value = ""true""
  }
}
",resource,,,13,0.0,8d1ae18da01ec173c05970f466058a048c6f83ac,e65621ed9fc3e374626cc3929742df6ba94fc8d7,https://github.com/nebari-dev/nebari/blob/8d1ae18da01ec173c05970f466058a048c6f83ac/qhub/template/{{ cookiecutter.repo_directory }}/infrastructure/modules/kubernetes/services/monitoring/main.tf#L13,https://github.com/nebari-dev/nebari/blob/e65621ed9fc3e374626cc3929742df6ba94fc8d7/qhub/template/{{ cookiecutter.repo_directory }}/infrastructure/modules/kubernetes/services/monitoring/main.tf#L0,2021-09-13 17:39:56-05:00,2022-02-03 11:12:40-05:00,2,2
https://github.com/ManagedKube/kubernetes-ops,9,terraform-modules/aws/msk_1.0.9/module/security-group-variables.tf,terraform-modules/aws/msk_1.0.9/module/security-group-variables.tf,0,implement,# Terraform module that create AWS Security Groups should implement.,"# security-group-variables Version: 3 
 # 
 # Copy this file from https://github.com/cloudposse/terraform-aws-security-group/blob/master/exports/security-group-variables.tf 
 # and EDIT IT TO SUIT YOUR PROJECT. Update the version number above if you update this file from a later version. 
 # Unlike null-label context.tf, this file cannot be automatically updated 
 # because of the tight integration with the module using it. 
 ## 
 # Delete this top comment block, except for the first line (version number), 
 # REMOVE COMMENTS below that are intended for the initial implementor and not maintainers or end users. 
 # 
 # This file provides the standard inputs that all Cloud Posse Open Source 
 # Terraform module that create AWS Security Groups should implement. 
 # This file does NOT provide implementation of the inputs, as that 
 # of course varies with each module. 
 # 
 # This file declares some standard outputs modules should create, 
 # but the declarations should be moved to `outputs.tf` and of course 
 # may need to be modified based on the module's use of security-group. 
 #  ","variable ""create_security_group"" {
  type        = bool
  description = ""Set `true` to create and configure a new security group. If false, `associated_security_group_ids` must be provided.""
  default     = true
}
",variable,"variable ""create_security_group"" {
  type        = bool
  description = ""Set `true` to create and configure a new security group. If false, `associated_security_group_ids` must be provided.""
  default     = true
}
",variable,12,12.0,c8193c7d74e2f7c624f0867337294cb66a2b9469,c8193c7d74e2f7c624f0867337294cb66a2b9469,https://github.com/ManagedKube/kubernetes-ops/blob/c8193c7d74e2f7c624f0867337294cb66a2b9469/terraform-modules/aws/msk_1.0.9/module/security-group-variables.tf#L12,https://github.com/ManagedKube/kubernetes-ops/blob/c8193c7d74e2f7c624f0867337294cb66a2b9469/terraform-modules/aws/msk_1.0.9/module/security-group-variables.tf#L12,2023-12-14 10:29:30-08:00,2023-12-14 10:29:30-08:00,1,0
https://github.com/magma/magma,55,orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf,orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf,0,crap,"# Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator","# Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator 
 # labels: 
 # Set timeout for scrape","resource ""helm_release"" ""yace_exporter"" {
  count = var.cloudwatch_exporter_enabled ? 1 : 0

  name       = ""yace-exporter""
  namespace  = kubernetes_namespace.monitoring.metadata[0].name
  repository = ""https://mogaal.github.io/helm-charts/""
  chart      = ""prometheus-yace-exporter""
  timeout    = 600
  values = [<<EOT
# Default values for prometheus-yace-exporter.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: quay.io/invisionag/yet-another-cloudwatch-exporter
  tag: v0.25.0-alpha
  pullPolicy: IfNotPresent

nameOverride: """"
fullnameOverride: """"

service:
  type: ClusterIP
  port: 80
  annotations:
    prometheus.io/scrape: ""true""
    prometheus.io/port: ""5000""
  labels: {}

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: ""true""
  hosts:
    - host: chart-example.local
      paths: []

  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}

podAnnoatations: {}

podLabels: {}

aws:
  role:

  # The name of a pre-created secret in which AWS credentials are stored. When
  # set, aws_access_key_id is assumed to be in a field called access_key,
  # aws_secret_access_key is assumed to be in a field called secret_key, and the
  # session token, if it exists, is assumed to be in a field called
  # security_token
  secret:
    name:
    includesSessionToken: false

  # Note: Do not specify the aws_access_key_id and aws_secret_access_key if you specified role or secret.name before
  aws_access_key_id:
  aws_secret_access_key:

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  annotations: {}
  labels: {}
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

rbac:
  # Specifies whether RBAC resources should be created
  create: true

serviceMonitor:
  # When set true then use a ServiceMonitor to configure scraping
  enabled: true
  # Set the namespace the ServiceMonitor should be deployed
  namespace: ${var.orc8r_kubernetes_namespace}
  # Set how frequently Prometheus should scrape
  interval: 30s
  # Set targetPort for serviceMonitor
  port: http
  # Set path to cloudwatch-exporter telemtery-path
  telemetryPath: /metrics
  # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator
  # labels:
  # Set timeout for scrape
  timeout: 10s


config: |-
  discovery:
    exportedTagsOnMetrics:
      ec2:
        - Name
      ebs:
        - VolumeId
    jobs:
    - regions:
        - ${var.region}
      type: ""es""
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: FreeStorageSpace
          statistics:
          - 'Sum'
          period: 600
          length: 60
        - name: ClusterStatus.green
          statistics:
          - 'Minimum'
          period: 600
          length: 60
        - name: ClusterStatus.yellow
          statistics:
          - 'Maximum'
          period: 600
          length: 60
        - name: ClusterStatus.red
          statistics:
          - 'Maximum'
          period: 600
          length: 60
    - type: ""elb""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: HealthyHostCount
          statistics:
          - 'Minimum'
          period: 600
          length: 600
        - name: HTTPCode_Backend_4XX
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
    - type: ""ec2""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: NetworkIn
          statistics:
          - 'Sum'
          period: 600
          length: 600
        - name: NetworkOut
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
  EOT
  ]
  set_sensitive {
    name  = ""aws.aws_access_key_id""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].id : """"
  }
  set_sensitive {
    name  = ""aws.aws_secret_access_key""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].secret : """"
  }
}",resource,"resource ""helm_release"" ""yace_exporter"" {
  count = var.cloudwatch_exporter_enabled ? 1 : 0

  name       = ""yace-exporter""
  namespace  = kubernetes_namespace.monitoring[0].metadata[0].name
  repository = ""https://mogaal.github.io/helm-charts/""
  chart      = ""prometheus-yace-exporter""
  timeout    = 600
  values = [<<EOT
# Default values for prometheus-yace-exporter.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: quay.io/invisionag/yet-another-cloudwatch-exporter
  tag: v0.25.0-alpha
  pullPolicy: IfNotPresent

nameOverride: """"
fullnameOverride: """"

service:
  type: ClusterIP
  port: 80
  annotations:
    prometheus.io/scrape: ""true""
    prometheus.io/port: ""5000""
  labels: {}

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: ""true""
  hosts:
    - host: chart-example.local
      paths: []

  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}

podAnnoatations: {}

podLabels: {}

aws:
  role:

  # The name of a pre-created secret in which AWS credentials are stored. When
  # set, aws_access_key_id is assumed to be in a field called access_key,
  # aws_secret_access_key is assumed to be in a field called secret_key, and the
  # session token, if it exists, is assumed to be in a field called
  # security_token
  secret:
    name:
    includesSessionToken: false

  # Note: Do not specify the aws_access_key_id and aws_secret_access_key if you specified role or secret.name before
  aws_access_key_id:
  aws_secret_access_key:

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  annotations: {}
  labels: {}
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

rbac:
  # Specifies whether RBAC resources should be created
  create: true

serviceMonitor:
  # When set true then use a ServiceMonitor to configure scraping
  enabled: true
  # Set the namespace the ServiceMonitor should be deployed
  namespace: ${var.orc8r_kubernetes_namespace}
  # Set how frequently Prometheus should scrape
  interval: 30s
  # Set targetPort for serviceMonitor
  port: http
  # Set path to cloudwatch-exporter telemtery-path
  telemetryPath: /metrics
  # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator
  # labels:
  # Set timeout for scrape
  timeout: 10s


config: |-
  discovery:
    exportedTagsOnMetrics:
      ec2:
        - Name
      ebs:
        - VolumeId
    jobs:
    - regions:
        - ${var.region}
      type: ""es""
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: FreeStorageSpace
          statistics:
          - 'Sum'
          period: 600
          length: 60
        - name: ClusterStatus.green
          statistics:
          - 'Minimum'
          period: 600
          length: 60
        - name: ClusterStatus.yellow
          statistics:
          - 'Maximum'
          period: 600
          length: 60
        - name: ClusterStatus.red
          statistics:
          - 'Maximum'
          period: 600
          length: 60
    - type: ""elb""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: HealthyHostCount
          statistics:
          - 'Minimum'
          period: 600
          length: 600
        - name: HTTPCode_Backend_4XX
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
    - type: ""ec2""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: NetworkIn
          statistics:
          - 'Sum'
          period: 600
          length: 600
        - name: NetworkOut
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
  EOT
  ]
  set_sensitive {
    name  = ""aws.aws_access_key_id""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].id : """"
  }
  set_sensitive {
    name  = ""aws.aws_secret_access_key""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].secret : """"
  }
}",resource,160,160.0,cbee95c6b1cdad7ba277e02b920f715e21c97df6,b72801aa1cccc468273bbb99e5ec4fafa3c052c9,https://github.com/magma/magma/blob/cbee95c6b1cdad7ba277e02b920f715e21c97df6/orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf#L160,https://github.com/magma/magma/blob/b72801aa1cccc468273bbb99e5ec4fafa3c052c9/orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf#L160,2021-06-04 09:12:15+03:00,2022-03-10 13:24:52+00:00,3,0
https://github.com/terraform-aws-modules/terraform-aws-eks,5,main.tf,main.tf,0,fix,"/** # terraform-aws-eks  * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html).  * | Branch | Build status                                                                                                                                                      | * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) |  * ## Assumptions  ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources.  * ## Usage example  * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry:  * ```hcl * module ""eks"" { *   source                = ""terraform-aws-modules/eks/aws"" *   version               = ""0.1.0"" *   cluster_name          = ""test-eks-cluster"" *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] *   tags                  = ""${map(""Environment"", ""test"")}"" *   vpc_id                = ""vpc-abcde012"" * } * ```  * ## Dependencies  * The `configure_kubectl_session` variable requires that both `[kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`heptio-authenticator-aws`](https://github.com/heptio/authenticator#4-set-up-kubectl-to-use-heptio-authenticator-for-aws-tokens) are installed and on your shell's PATH.  * ## Testing  * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them:  * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). * 2. Install bundler and the gems from our Gemfile: * *     ```bash *     gem install bundler && bundle install *     ``` * * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. * 4. Test using `bundle exec kitchen test` from the root of the repo.  * For now, connectivity to the kubernetes cluster is not tested but will be in the * future. If `configure_kubectl_session` is set `true`, once the test fixture has * converged, you can query the test cluster from that terminal session with * `kubectl get nodes --watch --kubeconfig kubeconfig`.  * ## Doc generation  * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). * Generate them like so:  * ```bash * go get github.com/segmentio/terraform-docs * terraform-docs md ./ | cat -s | ghead -n -1 > README.md * ```  * ## Contributing  * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section.  * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md).  * ## IAM Permissions  * Testing and using this repo requires a minimum set of IAM permissions. Test permissions * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md).  * ## Change log  * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes.  * ## Authors  * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)!  * ## License  * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details.*/","/** 
 # terraform-aws-eks 
  
 * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available 
 * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). 
 * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) 
 * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). 
 * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html). 
  
 * | Branch | Build status                                                                                                                                                      | 
 * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | 
 * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) | 
  
 * ## Assumptions 
  
 ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. 
 ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. 
 ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources. 
  
 * ## Usage example 
  
 * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry: 
  
 * ```hcl 
 * module ""eks"" { 
 *   source                = ""terraform-aws-modules/eks/aws"" 
 *   version               = ""0.1.0"" 
 *   cluster_name          = ""test-eks-cluster"" 
 *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] 
 *   tags                  = ""${map(""Environment"", ""test"")}"" 
 *   vpc_id                = ""vpc-abcde012"" 
 * } 
 * ``` 
  
 * ## Dependencies 
  
 * The `configure_kubectl_session` variable requires that both `[kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) 
 (>=1.10) and [`heptio-authenticator-aws`](https://github.com/heptio/authenticator#4-set-up-kubectl-to-use-heptio-authenticator-for-aws-tokens) 
 are installed and on your shell's PATH. 
  
 * ## Testing 
  
 * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them: 
  
 * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). 
 * 2. Install bundler and the gems from our Gemfile: 
 * 
 *     ```bash 
 *     gem install bundler && bundle install 
 *     ``` 
 * 
 * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. 
 * 4. Test using `bundle exec kitchen test` from the root of the repo. 
  
 * For now, connectivity to the kubernetes cluster is not tested but will be in the 
 * future. If `configure_kubectl_session` is set `true`, once the test fixture has 
 * converged, you can query the test cluster from that terminal session with 
 * `kubectl get nodes --watch --kubeconfig kubeconfig`. 
  
 * ## Doc generation 
  
 * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). 
 * Generate them like so: 
  
 * ```bash 
 * go get github.com/segmentio/terraform-docs 
 * terraform-docs md ./ | cat -s | ghead -n -1 > README.md 
 * ``` 
  
 * ## Contributing 
  
 * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section. 
  
 * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md). 
  
 * ## IAM Permissions 
  
 * Testing and using this repo requires a minimum set of IAM permissions. Test permissions 
 * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md). 
  
 * ## Change log 
  
 * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes. 
  
 * ## Authors 
  
 * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. 
 * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)! 
  
 * ## License 
  
 * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details. 
 */ ","provider ""null"" {}
",provider,"provider ""null"" {}
",provider,1,,c8997a5cf6f2707462ef8206360f8004f99724bd,9d3b5caff4681f9d33a08ebf73ff55d42e3236cb,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/c8997a5cf6f2707462ef8206360f8004f99724bd/main.tf#L1,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/9d3b5caff4681f9d33a08ebf73ff55d42e3236cb/main.tf,2018-06-11 15:54:19-07:00,2018-06-11 16:15:11-07:00,2,1
https://github.com/Optum/dce,6,modules/reset_codebuild.tf,modules/reset_codebuild.tf,0,nuke,"/**  * Configure CodePipeline resources to  * execute our Redbox Account Reset process.  * - Run aws-nuke in each user account  *  * We are currently configured a unique CodePipeline  * resource for every user account.  * The Lambda whicxh triggers the CodePipeline refer to them  * by name, eg. `AccountReset_<AccountId>`*/","/** 
 * Configure CodePipeline resources to 
 * execute our Redbox Account Reset process. 
 * - Run aws-nuke in each user account 
 * 
 * We are currently configured a unique CodePipeline 
 * resource for every user account. 
 * The Lambda whicxh triggers the CodePipeline refer to them 
 * by name, eg. `AccountReset_<AccountId>` 
 */ ","locals {
  # https://stackoverflow.com/a/47243622
  isPr = replace(var.namespace, ""pr-"", """") != var.namespace
}
",locals,"locals {
  # https://stackoverflow.com/a/47243622
  isPr = replace(var.namespace, ""pr-"", """") != var.namespace
}
",locals,1,,6ceb2a7535e54b8047d38580de4764584faae4b3,672ed294b124e4349a4057f5d35f01b7babdd123,https://github.com/Optum/dce/blob/6ceb2a7535e54b8047d38580de4764584faae4b3/modules/reset_codebuild.tf#L1,https://github.com/Optum/dce/blob/672ed294b124e4349a4057f5d35f01b7babdd123/modules/reset_codebuild.tf,2019-09-18 11:09:34-05:00,2019-11-05 16:22:03-06:00,4,1
https://github.com/magma/magma,53,orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf,orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf,0,crap,# When set true then use a ServiceMonitor to configure scraping,# When set true then use a ServiceMonitor to configure scraping,"resource ""helm_release"" ""yace_exporter"" {
  count = var.cloudwatch_exporter_enabled ? 1 : 0

  name       = ""yace-exporter""
  namespace  = kubernetes_namespace.monitoring.metadata[0].name
  repository = ""https://mogaal.github.io/helm-charts/""
  chart      = ""prometheus-yace-exporter""
  timeout    = 600
  values = [<<EOT
# Default values for prometheus-yace-exporter.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: quay.io/invisionag/yet-another-cloudwatch-exporter
  tag: v0.25.0-alpha
  pullPolicy: IfNotPresent

nameOverride: """"
fullnameOverride: """"

service:
  type: ClusterIP
  port: 80
  annotations:
    prometheus.io/scrape: ""true""
    prometheus.io/port: ""5000""
  labels: {}

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: ""true""
  hosts:
    - host: chart-example.local
      paths: []

  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}

podAnnoatations: {}

podLabels: {}

aws:
  role:

  # The name of a pre-created secret in which AWS credentials are stored. When
  # set, aws_access_key_id is assumed to be in a field called access_key,
  # aws_secret_access_key is assumed to be in a field called secret_key, and the
  # session token, if it exists, is assumed to be in a field called
  # security_token
  secret:
    name:
    includesSessionToken: false

  # Note: Do not specify the aws_access_key_id and aws_secret_access_key if you specified role or secret.name before
  aws_access_key_id:
  aws_secret_access_key:

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  annotations: {}
  labels: {}
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

rbac:
  # Specifies whether RBAC resources should be created
  create: true

serviceMonitor:
  # When set true then use a ServiceMonitor to configure scraping
  enabled: true
  # Set the namespace the ServiceMonitor should be deployed
  namespace: ${var.orc8r_kubernetes_namespace}
  # Set how frequently Prometheus should scrape
  interval: 30s
  # Set targetPort for serviceMonitor
  port: http
  # Set path to cloudwatch-exporter telemtery-path
  telemetryPath: /metrics
  # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator
  # labels:
  # Set timeout for scrape
  timeout: 10s


config: |-
  discovery:
    exportedTagsOnMetrics:
      ec2:
        - Name
      ebs:
        - VolumeId
    jobs:
    - regions:
        - ${var.region}
      type: ""es""
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: FreeStorageSpace
          statistics:
          - 'Sum'
          period: 600
          length: 60
        - name: ClusterStatus.green
          statistics:
          - 'Minimum'
          period: 600
          length: 60
        - name: ClusterStatus.yellow
          statistics:
          - 'Maximum'
          period: 600
          length: 60
        - name: ClusterStatus.red
          statistics:
          - 'Maximum'
          period: 600
          length: 60
    - type: ""elb""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: HealthyHostCount
          statistics:
          - 'Minimum'
          period: 600
          length: 600
        - name: HTTPCode_Backend_4XX
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
    - type: ""ec2""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: NetworkIn
          statistics:
          - 'Sum'
          period: 600
          length: 600
        - name: NetworkOut
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
  EOT
  ]
  set_sensitive {
    name  = ""aws.aws_access_key_id""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].id : """"
  }
  set_sensitive {
    name  = ""aws.aws_secret_access_key""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].secret : """"
  }
}",resource,"resource ""helm_release"" ""yace_exporter"" {
  count = var.cloudwatch_exporter_enabled ? 1 : 0

  name       = ""yace-exporter""
  namespace  = kubernetes_namespace.monitoring[0].metadata[0].name
  repository = ""https://mogaal.github.io/helm-charts/""
  chart      = ""prometheus-yace-exporter""
  timeout    = 600
  values = [<<EOT
# Default values for prometheus-yace-exporter.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: quay.io/invisionag/yet-another-cloudwatch-exporter
  tag: v0.25.0-alpha
  pullPolicy: IfNotPresent

nameOverride: """"
fullnameOverride: """"

service:
  type: ClusterIP
  port: 80
  annotations:
    prometheus.io/scrape: ""true""
    prometheus.io/port: ""5000""
  labels: {}

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: ""true""
  hosts:
    - host: chart-example.local
      paths: []

  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}

podAnnoatations: {}

podLabels: {}

aws:
  role:

  # The name of a pre-created secret in which AWS credentials are stored. When
  # set, aws_access_key_id is assumed to be in a field called access_key,
  # aws_secret_access_key is assumed to be in a field called secret_key, and the
  # session token, if it exists, is assumed to be in a field called
  # security_token
  secret:
    name:
    includesSessionToken: false

  # Note: Do not specify the aws_access_key_id and aws_secret_access_key if you specified role or secret.name before
  aws_access_key_id:
  aws_secret_access_key:

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  annotations: {}
  labels: {}
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

rbac:
  # Specifies whether RBAC resources should be created
  create: true

serviceMonitor:
  # When set true then use a ServiceMonitor to configure scraping
  enabled: true
  # Set the namespace the ServiceMonitor should be deployed
  namespace: ${var.orc8r_kubernetes_namespace}
  # Set how frequently Prometheus should scrape
  interval: 30s
  # Set targetPort for serviceMonitor
  port: http
  # Set path to cloudwatch-exporter telemtery-path
  telemetryPath: /metrics
  # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator
  # labels:
  # Set timeout for scrape
  timeout: 10s


config: |-
  discovery:
    exportedTagsOnMetrics:
      ec2:
        - Name
      ebs:
        - VolumeId
    jobs:
    - regions:
        - ${var.region}
      type: ""es""
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: FreeStorageSpace
          statistics:
          - 'Sum'
          period: 600
          length: 60
        - name: ClusterStatus.green
          statistics:
          - 'Minimum'
          period: 600
          length: 60
        - name: ClusterStatus.yellow
          statistics:
          - 'Maximum'
          period: 600
          length: 60
        - name: ClusterStatus.red
          statistics:
          - 'Maximum'
          period: 600
          length: 60
    - type: ""elb""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: HealthyHostCount
          statistics:
          - 'Minimum'
          period: 600
          length: 600
        - name: HTTPCode_Backend_4XX
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
    - type: ""ec2""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: NetworkIn
          statistics:
          - 'Sum'
          period: 600
          length: 600
        - name: NetworkOut
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
  EOT
  ]
  set_sensitive {
    name  = ""aws.aws_access_key_id""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].id : """"
  }
  set_sensitive {
    name  = ""aws.aws_secret_access_key""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].secret : """"
  }
}",resource,150,150.0,cbee95c6b1cdad7ba277e02b920f715e21c97df6,b72801aa1cccc468273bbb99e5ec4fafa3c052c9,https://github.com/magma/magma/blob/cbee95c6b1cdad7ba277e02b920f715e21c97df6/orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf#L150,https://github.com/magma/magma/blob/b72801aa1cccc468273bbb99e5ec4fafa3c052c9/orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf#L150,2021-06-04 09:12:15+03:00,2022-03-10 13:24:52+00:00,3,0
https://github.com/Optum/dce,1,modules/reset_codebuild.tf,modules/reset_codebuild.tf,0,nuke,"/**  * Configure CodePipeline resources to  * execute our Redbox Account Reset process.  * - Run aws-nuke in each user account  * - Re-apply Launchpad to each user account  *  * We are currently configured a unique CodePipeline  * resource for every user account.  * The Lambda whicxh triggers the CodePipeline refer to them  * by name, eg. `AccountReset_<AccountId>`*/","/** 
 * Configure CodePipeline resources to 
 * execute our Redbox Account Reset process. 
 * - Run aws-nuke in each user account 
 * - Re-apply Launchpad to each user account 
 * 
 * We are currently configured a unique CodePipeline 
 * resource for every user account. 
 * The Lambda whicxh triggers the CodePipeline refer to them 
 * by name, eg. `AccountReset_<AccountId>` 
 */ ","locals {
  # https://stackoverflow.com/a/47243622
  isPr = replace(var.namespace, ""pr-"", """") != var.namespace
}
",locals,"locals {
  # https://stackoverflow.com/a/47243622
  isPr = replace(var.namespace, ""pr-"", """") != var.namespace
}
",locals,1,,7afd40b03674d04e65c06a000585c13a0d3be257,6ceb2a7535e54b8047d38580de4764584faae4b3,https://github.com/Optum/dce/blob/7afd40b03674d04e65c06a000585c13a0d3be257/modules/reset_codebuild.tf#L1,https://github.com/Optum/dce/blob/6ceb2a7535e54b8047d38580de4764584faae4b3/modules/reset_codebuild.tf,2019-06-17 15:07:38-05:00,2019-09-18 11:09:34-05:00,4,1
https://github.com/compiler-explorer/infra,16,terraform/asg.tf,terraform/asg.tf,0,// todo,"// TODO: consider a new scaling policy. e.g. ""target tracking""","// TODO: consider a new scaling policy. e.g. ""target tracking"" 
 //resource ""aws_autoscaling_policy"" ""compiler-explorer-nonspot-prod-scale-up"" { 
 //  autoscaling_group_name = ""${aws_autoscaling_group.nonspot-prod.name}"" 
 //  name = ""ce-increase-nonspot"" 
 //  scaling_adjustment = 1 
 //  adjustment_type = ""SimpleScaling"" 
 //  estimated_instance_warmup = 1000 
 //  cooldown = 500 
 //} 
 // 
 //resource ""aws_autoscaling_policy"" ""compiler-explorer-nonspot-prod-scale-down"" { 
 //  autoscaling_group_name = ""${aws_autoscaling_group.nonspot-prod.name}"" 
 //  name = ""ce-increase-nonspot"" 
 //  scaling_adjustment = -1 
 //  adjustment_type = ""SimpleScaling"" 
 //  estimated_instance_warmup = 1000 
 //  cooldown = 500 
 //} 
 // 
 //resource ""aws_cloudwatch_metric_alarm"" ""compiler-explorer-cpu-load-high"" { 
 //  alarm_name = ""compiler-explorer-cpu-load-high"" 
 //  comparison_operator = ""GreaterThanOrEqualToThreshold"" 
 //  threshold = 40 
 //  evaluation_periods = 4 
 //  metric_name = ""CPUUtilization"" 
 //  namespace = ""AWS/EC2"" 
 //  period = 300 
 //  statistic = ""Average"" 
 // 
 //  dimensions { 
 //    AutoScalingGroupName = ""${aws_autoscaling_group.nonspot-prod.arn}"" 
 //  } 
 // 
 //  alarm_description = ""Scale up Compiler Explorer when CPU load is high"" 
 //  alarm_actions = [ 
 //    ""${aws_autoscaling_policy.compiler-explorer-nonspot-prod-scale-down.arn}""] 
 //} 
 // 
 //// This is actually an alarm on >= 10%, with an ""if it's ok"" setting to reduce the group size. 
 //// This gives us some hysteresis between scaling up and scaling down. 
 //resource ""aws_cloudwatch_metric_alarm"" ""compiler-explorer-cpu-load-low"" { 
 //  alarm_name = ""compiler-explorer-cpu-load-low"" 
 //  comparison_operator = ""GreaterThanOrEqualToThreshold"" 
 //  threshold = 10 
 //  evaluation_periods = 1 
 //  metric_name = ""CPUUtilization"" 
 //  namespace = ""AWS/EC2"" 
 //  period = 900 
 //  statistic = ""Average"" 
 // 
 //  dimensions { 
 //    AutoScalingGroupName = ""${aws_autoscaling_group.nonspot-prod.arn}"" 
 //  } 
 // 
 //  alarm_description = ""Scale down Compiler Explorer when load returns to normal"" 
 //  ok_actions = [ 
 //    ""${aws_autoscaling_policy.compiler-explorer-nonspot-prod-scale-down.arn}""] 
 //} ","resource ""aws_autoscaling_group"" ""spot-beta"" {
  desired_capacity = 1
  health_check_grace_period = 500
  health_check_type = ""EC2""
  launch_configuration = ""${aws_launch_configuration.CompilerExplorer-beta-c5.id}""
  max_size = 4
  min_size = 0
  name = ""spot-beta""
  vpc_zone_identifier = [
    ""${local.subnets}""
  ]
  tag {
    key = ""Environment""
    value = ""Beta""
    propagate_at_launch = true
  }

  tag {
    key = ""Name""
    value = ""Beta""
    propagate_at_launch = true
  }

  tag {
    key = ""Site""
    value = ""CompilerExplorer""
    propagate_at_launch = true
  }
  target_group_arns = [
    ""${aws_alb_target_group.beta.arn}""
  ]
}
",resource,"resource ""aws_autoscaling_group"" ""spot-beta"" {
  desired_capacity = 1
  health_check_grace_period = 500
  health_check_type = ""EC2""
  launch_configuration = ""${aws_launch_configuration.CompilerExplorer-beta-c5.id}""
  max_size = 4
  min_size = 0
  name = ""spot-beta""
  vpc_zone_identifier = [
    ""${local.subnets}""
  ]
  tag {
    key = ""Environment""
    value = ""Beta""
    propagate_at_launch = true
  }

  tag {
    key = ""Name""
    value = ""Beta""
    propagate_at_launch = true
  }

  tag {
    key = ""Site""
    value = ""CompilerExplorer""
    propagate_at_launch = true
  }
  target_group_arns = [
    ""${aws_alb_target_group.beta.arn}""
  ]
}
",resource,45,,52b74b9730d4cb1537f8ff48e00f9cde7727eaf5,d46317e464ebb2911dced315139a063d62489c9f,https://github.com/compiler-explorer/infra/blob/52b74b9730d4cb1537f8ff48e00f9cde7727eaf5/terraform/asg.tf#L45,https://github.com/compiler-explorer/infra/blob/d46317e464ebb2911dced315139a063d62489c9f/terraform/asg.tf,2018-11-05 17:43:45-06:00,2018-11-05 20:02:54-06:00,3,1
https://github.com/Worklytics/psoxy,302,infra/modules/source-token-external-todo/main.tf,infra/modules/source-token-external-todo/main.tf,0,todo,# TODO - Create User-Managed Token for ${var.source_id},# TODO - Create User-Managed Token for ${var.source_id},"resource ""local_file"" ""source_connection_instructions"" {
  filename = ""TODO - ${var.source_id}.md""
  content  = <<EOT
# TODO - Create User-Managed Token for ${var.source_id}

Follow the following steps:

${var.connector_specific_external_steps}

Then:
   1. ensure that you have sufficient permissions to set the value of the `${var.token_secret_id}`
      in ${var.host_cloud == ""aws"" ? ""AWS Systems Manager Parameter"" : ""GCP Secret Manager Secret""};
      if not, send these instructions (and the value you created previously) to someone who does via
      a secure means
   2. use ${upper(var.host_cloud)} console or CLI, fill the token value you created above as the
      value of `${var.token_secret_id}`

AWS example:
```shell
aws ssm put-parameter \
--name ""${var.token_secret_id}"" \
--type ""SecureString "" \
--value ""YOUR_VALUE_HERE"" \
--overwrite
```

GCP example:
(passing `-` as `--data-file` will let you paste you token value to `stdin`)
```shell
gcloud secrets versions add ${var.secret_id} --project=YOUR_PROJECT_ID --data-file=-
```

EOT
}
",resource,"locals {
  todo_content = <<EOT
# TODO - Create User-Managed Token for ${var.source_id}

Follow the following steps:

${var.connector_specific_external_steps}

${join(""\n"", var.additional_steps)}
EOT
}
",locals,11,9.0,6a9ec1ef24d076ed7b38f93edc34210a8fceee2e,5937e6a45055a94ff2b493f96f21863d91699825,https://github.com/Worklytics/psoxy/blob/6a9ec1ef24d076ed7b38f93edc34210a8fceee2e/infra/modules/source-token-external-todo/main.tf#L11,https://github.com/Worklytics/psoxy/blob/5937e6a45055a94ff2b493f96f21863d91699825/infra/modules/source-token-external-todo/main.tf#L9,2022-07-22 14:58:31-07:00,2024-03-06 18:11:21+00:00,9,0
https://github.com/terraform-aws-modules/terraform-aws-eks,19,main.tf,main.tf,0,fix,"/** # terraform-aws-eks  * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html).  * | Branch | Build status                                                                                                                                                      | * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) |  * ## Assumptions  ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources. ** If using the default Updating Variable Value (`true`) for `configure_kubectl_session`, it's required that both [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`heptio-authenticator-aws`](https://github.com/heptio/authenticator#4-set-up-kubectl-to-use-heptio-authenticator-for-aws-tokens) are installed and on your shell's PATH.  * ## Usage example  * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry:  * ```hcl * module ""eks"" { *   source                = ""terraform-aws-modules/eks/aws"" *   cluster_name          = ""test-eks-cluster"" *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] *   tags                  = ""${map(""Environment"", ""test"")}"" *   vpc_id                = ""vpc-abcde012"" * } * ```  * ## Release schedule  * Generally the maintainers will try to release the module once every 2 weeks to * keep up with PR additions. If particularly pressing changes are added or maintainers * come up with the spare time (hah!), release may happen more often on occasion.  * ## Testing  * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them:  * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). * 2. Install bundler and the gems from our Gemfile: * *     ```bash *     gem install bundler && bundle install *     ``` * * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. * 4. Test using `bundle exec kitchen test` from the root of the repo.  * For now, connectivity to the kubernetes cluster is not tested but will be in the * future. If `configure_kubectl_session` is set `true`, once the test fixture has * converged, you can query the test cluster from that terminal session with * `kubectl get nodes --watch --kubeconfig kubeconfig`.  * ## Doc generation  * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). * Generate them like so:  * ```bash * go get github.com/segmentio/terraform-docs * terraform-docs md ./ | cat -s | ghead -n -1 > README.md * ```  * ## Contributing  * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section.  * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md).  * ## IAM Permissions  * Testing and using this repo requires a minimum set of IAM permissions. Test permissions * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md).  * ## Change log  * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes.  * ## Authors  * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)!  * ## License  * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details.*/","/** 
 # terraform-aws-eks 
  
 * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available 
 * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). 
 * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) 
 * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). 
 * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html). 
  
 * | Branch | Build status                                                                                                                                                      | 
 * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | 
 * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) | 
  
 * ## Assumptions 
  
 ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. 
 ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. 
 ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources. 
 ** If using the default Updating Variable Value (`true`) for `configure_kubectl_session`, it's required that both [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`heptio-authenticator-aws`](https://github.com/heptio/authenticator#4-set-up-kubectl-to-use-heptio-authenticator-for-aws-tokens) are installed and on your shell's PATH. 
  
 * ## Usage example 
  
 * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry: 
  
 * ```hcl 
 * module ""eks"" { 
 *   source                = ""terraform-aws-modules/eks/aws"" 
 *   cluster_name          = ""test-eks-cluster"" 
 *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] 
 *   tags                  = ""${map(""Environment"", ""test"")}"" 
 *   vpc_id                = ""vpc-abcde012"" 
 * } 
 * ``` 
  
 * ## Release schedule 
  
 * Generally the maintainers will try to release the module once every 2 weeks to 
 * keep up with PR additions. If particularly pressing changes are added or maintainers 
 * come up with the spare time (hah!), release may happen more often on occasion. 
  
 * ## Testing 
  
 * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them: 
  
 * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). 
 * 2. Install bundler and the gems from our Gemfile: 
 * 
 *     ```bash 
 *     gem install bundler && bundle install 
 *     ``` 
 * 
 * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. 
 * 4. Test using `bundle exec kitchen test` from the root of the repo. 
  
 * For now, connectivity to the kubernetes cluster is not tested but will be in the 
 * future. If `configure_kubectl_session` is set `true`, once the test fixture has 
 * converged, you can query the test cluster from that terminal session with 
 * `kubectl get nodes --watch --kubeconfig kubeconfig`. 
  
 * ## Doc generation 
  
 * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). 
 * Generate them like so: 
  
 * ```bash 
 * go get github.com/segmentio/terraform-docs 
 * terraform-docs md ./ | cat -s | ghead -n -1 > README.md 
 * ``` 
  
 * ## Contributing 
  
 * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section. 
  
 * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md). 
  
 * ## IAM Permissions 
  
 * Testing and using this repo requires a minimum set of IAM permissions. Test permissions 
 * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md). 
  
 * ## Change log 
  
 * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes. 
  
 * ## Authors 
  
 * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. 
 * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)! 
  
 * ## License 
  
 * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details. 
 */ ","provider ""null"" {}
",provider,"provider ""null"" {}
",provider,1,,f754fe45ee2d566651725e1f1a48ff657fb014b3,b4756a57f7af7c6c8acb9404c640ef5946eb0585,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/f754fe45ee2d566651725e1f1a48ff657fb014b3/main.tf#L1,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/b4756a57f7af7c6c8acb9404c640ef5946eb0585/main.tf,2018-07-01 01:51:07-07:00,2018-07-10 10:40:49-04:00,2,1
https://github.com/Worklytics/psoxy,201,infra/modules/aws-psoxy-instance/main.tf,infra/modules/aws-psoxy-instance/main.tf,0,# todo,# TODO: set meaningful paths per integration,# TODO: set meaningful paths per integration,"resource ""local_file"" ""todo"" {
  filename = ""test ${var.function_name}.md""
  content  = <<EOT
# Testing

## Prereqs
Requests to AWS API need to be [signed](https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html).
One tool to do it easily is [awscurl](https://github.com/okigan/awscurl). Install it:

```shell
pip install awscurl
```

## From Terminal
Call the API using a role with api execution grants.
```shell
export PSOXY_HOST=${var.api_gateway.api_endpoint}/live/${var.function_name}
aws sts assume-role --role-arn ${var.api_caller_role_arn} --duration 900 --role-session-name ${var.function_name}_local_test --output json > token.json
export CALLER_ACCESS_KEY_ID=`cat token.json| jq -r '.Credentials.AccessKeyId'`
export CALLER_SECRET_ACCESS_KEY=`cat token.json| jq -r '.Credentials.SecretAccessKey'`
export CALLER_SESSION_TOKEN=`cat token.json| jq -r '.Credentials.SessionToken'`
rm token.json
# TODO: set meaningful paths per integration
export PSOXY_PATH=/admin/directory/v1/customer/my_customer/domains

awscurl --service execute-api --access_key $CALLER_ACCESS_KEY_ID --secret_key $CALLER_SECRET_ACCESS_KEY --security_token $CALLER_SESSION_TOKEN $PSOXY_HOST$PSOXY_PATH
# Remove env variables
unset CALLER_ACCESS_KEY_ID CALLER_SECRET_ACCESS_KEY CALLER_SESSION_TOKEN
```

NOTE: if you want to customize the rule set used by Psoxy for your source, you can add a
`rules.yaml` file into the deployment directory (`target/deployment`) before invoking the command
above. The rules you define in the YAML file will override the ruleset specified in the codebase for
the source.

EOT
}
",resource,"resource ""local_file"" ""todo"" {
  filename = ""${local.outdir}/build-deploy-test-${var.function_name}.md""
  content  = <<EOT

## Testing

Review the deployed function in AWS console:

- https://console.aws.amazon.com/lambda/home?region=${var.region}#/functions/${var.function_name}?tab=monitoring

### Prereqs
Requests to AWS API need to be [signed](https://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html).
One tool to do it easily is [awscurl](https://github.com/okigan/awscurl). Install it:

```shell
pip install awscurl
```

### From Terminal

```shell
./${local_file.test-call.filename} ""${var.aws_assume_role_arn}"" ""${var.api_gateway.api_endpoint}/live/${var.function_name}/admin/directory/v1/customer/my_customer/domains""
```

NOTE: if you want to customize the rule set used by Psoxy for your source, you can add a
`rules.yaml` file into the deployment directory (`target/deployment`) before invoking the command
above. The rules you define in the YAML file will override the ruleset specified in the codebase for
the source.

EOT
}
",resource,143,,450629d23c2f1ec4eb34e057d6d2c3ae5215abf2,7a89585cef810c9febca2de63aa87fccb084ab50,https://github.com/Worklytics/psoxy/blob/450629d23c2f1ec4eb34e057d6d2c3ae5215abf2/infra/modules/aws-psoxy-instance/main.tf#L143,https://github.com/Worklytics/psoxy/blob/7a89585cef810c9febca2de63aa87fccb084ab50/infra/modules/aws-psoxy-instance/main.tf,2022-01-19 08:22:58-08:00,2022-01-19 10:09:49-08:00,2,1
https://github.com/terraform-aws-modules/terraform-aws-eks,50,main.tf,main.tf,0,fix,"/** # terraform-aws-eks  * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html).  * | Branch | Build status                                                                                                                                                      | * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) |  * ## Assumptions  ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources. ** If using the default Updating Updating Variable Value (`true`) for `configure_kubectl_session`, it's required that both [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`aws-iam-authenticator`](https://github.com/kubernetes-sigs/aws-iam-authenticator#4-set-up-kubectl-to-use-authentication-tokens-provided-by-aws-iam-authenticator-for-kubernetes) are installed and on your shell's PATH.  * ## Usage example  * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry:  * ```hcl * module ""eks"" { *   source                = ""terraform-aws-modules/eks/aws"" *   cluster_name          = ""test-eks-cluster"" *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] *   tags                  = {Environment = ""test""} *   vpc_id                = ""vpc-abcde012"" * } * ```  * ## Other Documentation * * - [Autoscaling](docs/autoscaling.md): How to enabled worker node autoscaling.  * ## Release schedule  * Generally the maintainers will try to release the module once every 2 weeks to * keep up with PR additions. If particularly pressing changes are added or maintainers * come up with the spare time (hah!), release may happen more often on occasion.  * ## Testing  * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them:  * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). * 2. Install bundler and the gems from our Gemfile: * *     ```bash *     gem install bundler && bundle install *     ``` * * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. * 4. Test using `bundle exec kitchen test` from the root of the repo.  * For now, connectivity to the kubernetes cluster is not tested but will be in the * future. If `configure_kubectl_session` is set `true`, once the test fixture has * converged, you can query the test cluster from that terminal session with * `kubectl get nodes --watch --kubeconfig kubeconfig`.  * ## Doc generation  * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). * Generate them like so:  * ```bash * go get github.com/segmentio/terraform-docs * terraform-docs md ./ | cat -s | tail -r | tail -n +2 | tail -r > README.md * ```  * ## Contributing  * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section.  * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md).  * ## IAM Permissions  * Testing and using this repo requires a minimum set of IAM permissions. Test permissions * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md).  * ## Change log  * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes.  * ## Authors  * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)!  * ## License  * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details.*/","/** 
 # terraform-aws-eks 
  
 * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available 
 * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). 
 * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) 
 * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). 
 * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html). 
  
 * | Branch | Build status                                                                                                                                                      | 
 * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | 
 * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) | 
  
 * ## Assumptions 
  
 ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. 
 ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. 
 ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources. 
 ** If using the default Updating Updating Variable Value (`true`) for `configure_kubectl_session`, it's required that both [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`aws-iam-authenticator`](https://github.com/kubernetes-sigs/aws-iam-authenticator#4-set-up-kubectl-to-use-authentication-tokens-provided-by-aws-iam-authenticator-for-kubernetes) are installed and on your shell's PATH. 
  
 * ## Usage example 
  
 * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry: 
  
 * ```hcl 
 * module ""eks"" { 
 *   source                = ""terraform-aws-modules/eks/aws"" 
 *   cluster_name          = ""test-eks-cluster"" 
 *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] 
 *   tags                  = {Environment = ""test""} 
 *   vpc_id                = ""vpc-abcde012"" 
 * } 
 * ``` 
  
 * ## Other Documentation 
 * 
 * - [Autoscaling](docs/autoscaling.md): How to enabled worker node autoscaling. 
  
 * ## Release schedule 
  
 * Generally the maintainers will try to release the module once every 2 weeks to 
 * keep up with PR additions. If particularly pressing changes are added or maintainers 
 * come up with the spare time (hah!), release may happen more often on occasion. 
  
 * ## Testing 
  
 * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them: 
  
 * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). 
 * 2. Install bundler and the gems from our Gemfile: 
 * 
 *     ```bash 
 *     gem install bundler && bundle install 
 *     ``` 
 * 
 * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. 
 * 4. Test using `bundle exec kitchen test` from the root of the repo. 
  
 * For now, connectivity to the kubernetes cluster is not tested but will be in the 
 * future. If `configure_kubectl_session` is set `true`, once the test fixture has 
 * converged, you can query the test cluster from that terminal session with 
 * `kubectl get nodes --watch --kubeconfig kubeconfig`. 
  
 * ## Doc generation 
  
 * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). 
 * Generate them like so: 
  
 * ```bash 
 * go get github.com/segmentio/terraform-docs 
 * terraform-docs md ./ | cat -s | tail -r | tail -n +2 | tail -r > README.md 
 * ``` 
  
 * ## Contributing 
  
 * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section. 
  
 * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md). 
  
 * ## IAM Permissions 
  
 * Testing and using this repo requires a minimum set of IAM permissions. Test permissions 
 * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md). 
  
 * ## Change log 
  
 * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes. 
  
 * ## Authors 
  
 * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. 
 * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)! 
  
 * ## License 
  
 * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details. 
 */ ","provider ""null"" {}
",provider,the block associated got renamed or deleted,,1,,fdd44c8f385fa8af83ff737827dcb3404fe3db59,31ec0f7db32d0b8b94cf1a7d3c0812db6731bd3b,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/fdd44c8f385fa8af83ff737827dcb3404fe3db59/main.tf#L1,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/31ec0f7db32d0b8b94cf1a7d3c0812db6731bd3b/main.tf,2018-09-25 12:30:56+02:00,2018-10-25 17:46:32+02:00,3,1
https://github.com/terraform-aws-modules/terraform-aws-eks,22,main.tf,main.tf,0,fix,"/** # terraform-aws-eks  * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html).  * | Branch | Build status                                                                                                                                                      | * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) |  * ## Assumptions  ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources. ** If using the default Updating Updating Variable Value (`true`) for `configure_kubectl_session`, it's required that both [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`aws-iam-authenticator`](https://github.com/kubernetes-sigs/aws-iam-authenticator#4-set-up-kubectl-to-use-authentication-tokens-provided-by-aws-iam-authenticator-for-kubernetes) are installed and on your shell's PATH.  * ## Usage example  * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry:  * ```hcl * module ""eks"" { *   source                = ""terraform-aws-modules/eks/aws"" *   cluster_name          = ""test-eks-cluster"" *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] *   tags                  = ""${map(""Environment"", ""test"")}"" *   vpc_id                = ""vpc-abcde012"" * } * ```  * ## Release schedule  * Generally the maintainers will try to release the module once every 2 weeks to * keep up with PR additions. If particularly pressing changes are added or maintainers * come up with the spare time (hah!), release may happen more often on occasion.  * ## Testing  * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them:  * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). * 2. Install bundler and the gems from our Gemfile: * *     ```bash *     gem install bundler && bundle install *     ``` * * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. * 4. Test using `bundle exec kitchen test` from the root of the repo.  * For now, connectivity to the kubernetes cluster is not tested but will be in the * future. If `configure_kubectl_session` is set `true`, once the test fixture has * converged, you can query the test cluster from that terminal session with * `kubectl get nodes --watch --kubeconfig kubeconfig`.  * ## Doc generation  * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). * Generate them like so:  * ```bash * go get github.com/segmentio/terraform-docs * terraform-docs md ./ | cat -s | ghead -n -1 > README.md * ```  * ## Contributing  * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section.  * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md).  * ## IAM Permissions  * Testing and using this repo requires a minimum set of IAM permissions. Test permissions * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md).  * ## Change log  * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes.  * ## Authors  * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)!  * ## License  * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details.*/","/** 
 # terraform-aws-eks 
  
 * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available 
 * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). 
 * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) 
 * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). 
 * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html). 
  
 * | Branch | Build status                                                                                                                                                      | 
 * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | 
 * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) | 
  
 * ## Assumptions 
  
 ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. 
 ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. 
 ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources. 
 ** If using the default Updating Updating Variable Value (`true`) for `configure_kubectl_session`, it's required that both [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`aws-iam-authenticator`](https://github.com/kubernetes-sigs/aws-iam-authenticator#4-set-up-kubectl-to-use-authentication-tokens-provided-by-aws-iam-authenticator-for-kubernetes) are installed and on your shell's PATH. 
  
 * ## Usage example 
  
 * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry: 
  
 * ```hcl 
 * module ""eks"" { 
 *   source                = ""terraform-aws-modules/eks/aws"" 
 *   cluster_name          = ""test-eks-cluster"" 
 *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] 
 *   tags                  = ""${map(""Environment"", ""test"")}"" 
 *   vpc_id                = ""vpc-abcde012"" 
 * } 
 * ``` 
  
 * ## Release schedule 
  
 * Generally the maintainers will try to release the module once every 2 weeks to 
 * keep up with PR additions. If particularly pressing changes are added or maintainers 
 * come up with the spare time (hah!), release may happen more often on occasion. 
  
 * ## Testing 
  
 * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them: 
  
 * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). 
 * 2. Install bundler and the gems from our Gemfile: 
 * 
 *     ```bash 
 *     gem install bundler && bundle install 
 *     ``` 
 * 
 * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. 
 * 4. Test using `bundle exec kitchen test` from the root of the repo. 
  
 * For now, connectivity to the kubernetes cluster is not tested but will be in the 
 * future. If `configure_kubectl_session` is set `true`, once the test fixture has 
 * converged, you can query the test cluster from that terminal session with 
 * `kubectl get nodes --watch --kubeconfig kubeconfig`. 
  
 * ## Doc generation 
  
 * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). 
 * Generate them like so: 
  
 * ```bash 
 * go get github.com/segmentio/terraform-docs 
 * terraform-docs md ./ | cat -s | ghead -n -1 > README.md 
 * ``` 
  
 * ## Contributing 
  
 * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section. 
  
 * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md). 
  
 * ## IAM Permissions 
  
 * Testing and using this repo requires a minimum set of IAM permissions. Test permissions 
 * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md). 
  
 * ## Change log 
  
 * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes. 
  
 * ## Authors 
  
 * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. 
 * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)! 
  
 * ## License 
  
 * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details. 
 */ ","provider ""null"" {}
",provider,"provider ""null"" {}
",provider,1,,b4756a57f7af7c6c8acb9404c640ef5946eb0585,69d7a3ce4a5d51c91569b078947efdbd10e3b1eb,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/b4756a57f7af7c6c8acb9404c640ef5946eb0585/main.tf#L1,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/69d7a3ce4a5d51c91569b078947efdbd10e3b1eb/main.tf,2018-07-10 10:40:49-04:00,2018-07-11 23:47:35-07:00,2,1
https://github.com/oracle-terraform-modules/terraform-oci-oke,197,workerpools.tf,workerpools.tf,0,implementation,# Default worker_pools sub-module implementation for OKE cluster,"# Copyright (c) 2022, 2023 Oracle Corporation and/or its affiliates. 
 # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl  
 # Default worker_pools sub-module implementation for OKE cluster","module ""worker_pools"" {
  source                          = ""./modules/workerpools""
  config_file_profile             = var.config_file_profile
  worker_pools                    = var.worker_pools
  tenancy_id                      = local.tenancy_id
  compartment_id                  = local.worker_compartment_id
  region                          = var.region
  cluster_id                      = coalesce(var.cluster_id, module.oke.cluster_id)
  apiserver_private_host          = try(split("":"", module.oke.endpoints[0].private_endpoint)[0], """")
  apiserver_public_host           = try(split("":"", module.oke.endpoints[0].public_endpoint)[0], """")
  image_id                        = var.worker_image_id
  image_type                      = var.worker_image_type
  os                              = var.worker_image_os
  os_version                      = var.worker_image_os_version
  enabled                         = var.worker_pool_enabled
  mode                            = var.worker_pool_mode
  boot_volume_size                = var.worker_pool_boot_volume_size
  memory                          = var.worker_pool_memory
  ocpus                           = var.worker_pool_ocpus
  shape                           = var.worker_pool_shape
  size                            = var.worker_pool_size
  cloudinit                       = var.cloudinit_nodepool_common
  cluster_ca_cert                 = var.cluster_ca_cert
  kubernetes_version              = var.kubernetes_version
  pod_nsg_ids                     = try(split("","", lookup(module.network.nsg_ids, ""pods"", """")), [])
  worker_nsg_ids                  = coalescelist(var.worker_nsgs, try(split("","", lookup(module.network.nsg_ids, ""workers"", """")), []))
  assign_public_ip                = var.worker_type == ""public""
  subnet_id                       = coalesce(var.worker_pool_subnet_id, lookup(module.network.subnet_ids, ""workers"", """"))
  enable_pv_encryption_in_transit = var.enable_pv_encryption_in_transit
  sriov_num_vfs                   = var.sriov_num_vfs
  ssh_public_key                  = var.ssh_public_key
  ssh_public_key_path             = var.ssh_public_key_path
  timezone                        = var.node_pool_timezone
  volume_kms_key_id               = var.node_pool_volume_kms_key_id
  defined_tags                    = lookup(lookup(var.defined_tags, ""oke"", {}), ""node"", {})
  freeform_tags                   = lookup(lookup(var.freeform_tags, ""oke"", {}), ""node"", {})
  providers = {
    oci.home = oci.home
  }
}
",module,,,4,0.0,897bae1fd6cdbd22478066e6f93643a8f7482757,6c867cd8e9cbf559742f56658989bcded0d1fd89,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/897bae1fd6cdbd22478066e6f93643a8f7482757/workerpools.tf#L4,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/workerpools.tf#L0,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,2,2
https://github.com/Optum/dce,18,modules/reset_codebuild.tf,modules/reset_codebuild.tf,0,nuke,"// ""false"" to disable aws-nuke","value = var.reset_nuke_toggle // ""false"" to disable aws-nuke","resource ""aws_codebuild_project"" ""reset_build"" {
  name          = ""account-reset-${var.namespace}""
  description   = ""Reset AWS child accounts""
  build_timeout = ""480""
  service_role  = aws_iam_role.codebuild_reset.arn

  source {
    type     = ""S3""
    location = ""${aws_s3_bucket.artifacts.id}/codebuild/reset.zip""
  }

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type                = var.reset_compute_type
    image                       = var.reset_build_image
    type                        = var.reset_build_type
    image_pull_credentials_type = var.reset_image_pull_creds

    environment_variable {
      name  = ""RESET_ACCOUNT""
      value = ""STUB""
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name = ""RESET_ACCOUNT_ADMIN_ROLE_NAME""
      // This value will be passed in by the process_reset_queue
      // lambda, which pulls it from the Accounts DB table
      value = ""STUB""
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name = ""RESET_ACCOUNT_PRINCIPAL_ROLE_NAME""
      // This value will be passed in by the process_reset_queue
      // lambda, which pulls it from the Accounts DB table
      value = ""STUB""
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""RESET_ACCOUNT_PRINCIPAL_POLICY_NAME""
      value = local.principal_policy_name
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""RESET_NUKE_TEMPLATE_DEFAULT""
      value = ""default-nuke-config-template.yml""
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""RESET_NUKE_TEMPLATE_BUCKET""
      value = var.reset_nuke_template_bucket
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""RESET_NUKE_TEMPLATE_KEY""
      value = var.reset_nuke_template_key
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""ACCOUNT_DB""
      value = aws_dynamodb_table.accounts.id
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""LEASE_DB""
      value = aws_dynamodb_table.leases.id
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""AWS_CURRENT_REGION""
      value = var.aws_region
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""RESET_NUKE_TOGGLE""
      value = var.reset_nuke_toggle // ""false"" to disable aws-nuke
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""RESET_NUKE_REGIONS""
      value = join("","", var.allowed_regions)
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""RESET_COMPLETE_TOPIC_ARN""
      value = aws_sns_topic.reset_complete.arn
      type  = ""PLAINTEXT""
    }
  }

  tags = var.global_tags
}
",resource,"resource ""aws_codebuild_project"" ""reset_build"" {
  name          = ""account-reset-${var.namespace}""
  description   = ""Reset AWS child accounts""
  build_timeout = ""480""
  service_role  = aws_iam_role.codebuild_reset.arn

  source {
    type     = ""S3""
    location = ""${aws_s3_bucket.artifacts.id}/codebuild/reset.zip""
  }

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type                = var.reset_compute_type
    image                       = var.reset_build_image
    type                        = var.reset_build_type
    image_pull_credentials_type = var.reset_image_pull_creds

    environment_variable {
      name  = ""RESET_ACCOUNT""
      value = ""STUB""
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name = ""RESET_ACCOUNT_ADMIN_ROLE_NAME""
      // This value will be passed in by the process_reset_queue
      // lambda, which pulls it from the Accounts DB table
      value = ""STUB""
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name = ""RESET_ACCOUNT_PRINCIPAL_ROLE_NAME""
      // This value will be passed in by the process_reset_queue
      // lambda, which pulls it from the Accounts DB table
      value = ""STUB""
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""RESET_ACCOUNT_PRINCIPAL_POLICY_NAME""
      value = local.principal_policy_name
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""RESET_NUKE_TEMPLATE_DEFAULT""
      value = ""default-nuke-config-template.yml""
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""RESET_NUKE_TEMPLATE_BUCKET""
      value = var.reset_nuke_template_bucket
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""RESET_NUKE_TEMPLATE_KEY""
      value = var.reset_nuke_template_key
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""ACCOUNT_DB""
      value = aws_dynamodb_table.accounts.id
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""LEASE_DB""
      value = aws_dynamodb_table.leases.id
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""AWS_CURRENT_REGION""
      value = var.aws_region
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""RESET_NUKE_TOGGLE""
      value = var.reset_nuke_toggle // ""false"" to disable aws-nuke
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""RESET_NUKE_REGIONS""
      value = join("","", var.allowed_regions)
      type  = ""PLAINTEXT""
    }

    environment_variable {
      name  = ""RESET_COMPLETE_TOPIC_ARN""
      value = aws_sns_topic.reset_complete.arn
      type  = ""PLAINTEXT""
    }
  }

  tags = var.global_tags
}
",resource,106,106.0,fabfd72f9281efaf0325977c835ac3954a0ca994,d58e0e41033426cd444afaa3489ed2bef7d7c0c4,https://github.com/Optum/dce/blob/fabfd72f9281efaf0325977c835ac3954a0ca994/modules/reset_codebuild.tf#L106,https://github.com/Optum/dce/blob/d58e0e41033426cd444afaa3489ed2bef7d7c0c4/modules/reset_codebuild.tf#L106,2020-02-27 11:55:40-06:00,2023-01-26 08:37:15-06:00,2,0
https://github.com/awslabs/data-on-eks,1,ai-ml/ray/main.tf,ai-ml/terraform/ray/main.tf,1,implemented,#   Upstream module implemented Security groups based on the best practices doc https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html.,"#----------------------------------------------------------------------------------------------------------# 
 # Security groups used in this module created by the upstream modules terraform-aws-eks (https://github.com/terraform-aws-modules/terraform-aws-eks). 
 #   Upstream module implemented Security groups based on the best practices doc https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html. 
 #   So, by default the security groups are restrictive. Users needs to enable rules for specific ports required for App requirement or Add-ons 
 #   See the notes below for each rule used in these examples 
 #----------------------------------------------------------------------------------------------------------#","module ""eks_blueprints"" {
  source = ""github.com/aws-ia/terraform-aws-eks-blueprints""

  cluster_name    = local.name
  cluster_version = var.eks_cluster_version

  vpc_id             = module.vpc.vpc_id
  private_subnet_ids = module.vpc.private_subnets

  #----------------------------------------------------------------------------------------------------------#
  # Security groups used in this module created by the upstream modules terraform-aws-eks (https://github.com/terraform-aws-modules/terraform-aws-eks).
  #   Upstream module implemented Security groups based on the best practices doc https://docs.aws.amazon.com/eks/latest/userguide/sec-group-reqs.html.
  #   So, by default the security groups are restrictive. Users needs to enable rules for specific ports required for App requirement or Add-ons
  #   See the notes below for each rule used in these examples
  #----------------------------------------------------------------------------------------------------------#
  node_security_group_additional_rules = {
    # Extend node-to-node security group rules. Recommended and required for the Add-ons
    ingress_self_all = {
      description = ""Node to node all ports/protocols""
      protocol    = ""-1""
      from_port   = 0
      to_port     = 0
      type        = ""ingress""
      self        = true
    }
    # Recommended outbound traffic for Node groups
    egress_all = {
      description      = ""Node all egress""
      protocol         = ""-1""
      from_port        = 0
      to_port          = 0
      type             = ""egress""
      cidr_blocks      = [""0.0.0.0/0""]
      ipv6_cidr_blocks = [""::/0""]
    }
    # Allows Control Plane Nodes to talk to Worker nodes on all ports. Added this to simplify the example and further avoid issues with Add-ons communication with Control plane.
    # This can be restricted further to specific port based on the requirement for each Add-on e.g., metrics-server 4443, analytics-operator 8080, karpenter 8443 etc.
    # Change this according to your security requirements if needed
    ingress_cluster_to_node_all_traffic = {
      description                   = ""Cluster API to Nodegroup all traffic""
      protocol                      = ""-1""
      from_port                     = 0
      to_port                       = 0
      type                          = ""ingress""
      source_cluster_security_group = true
    }
  }

  managed_node_groups = {
    mg_5 = {
      node_group_name = ""managed-ondemand""
      instance_types  = [""m5.8xlarge""]
      min_size        = 3
      subnet_ids      = module.vpc.private_subnets
    }
  }

  tags = local.tags
}
",module,,,30,0.0,0eed8e06b46254808b00b0bb4144ccfaf58241e2,84d14e8fee669fb5db87dd5f4d0e40554ee3a0d3,https://github.com/awslabs/data-on-eks/blob/0eed8e06b46254808b00b0bb4144ccfaf58241e2/ai-ml/ray/main.tf#L30,https://github.com/awslabs/data-on-eks/blob/84d14e8fee669fb5db87dd5f4d0e40554ee3a0d3/ai-ml/terraform/ray/main.tf#L0,2022-09-26 12:12:52+01:00,2023-03-21 08:33:37+00:00,4,2
https://github.com/ministryofjustice/modernisation-platform,60,terraform/modernisation-platform-account/secrets.tf,terraform/modernisation-platform-account/secrets.tf,0,nuke,# Account IDs to be auto-nuked on weekly basis,"# Account IDs to be auto-nuked on weekly basis 
 # Tfsec ignore 
 # - AWS095: No requirement currently to encrypt this secret with customer-managed KMS key 
 #tfsec:ignore:AWS095","resource ""aws_secretsmanager_secret"" ""nuke_account_ids"" {
  # checkov:skip=CKV_AWS_149:No requirement currently to encrypt this secret with customer-managed KMS key
  name        = ""nuke_account_ids""
  description = ""Account IDs to be auto-nuked on weekly basis. CAUTION: Any account ID you add here will be automatically nuked! This secret is used by GitHub actions job nuke.yml inside the environments repo, to find the Account IDs to be nuked.""
  tags        = local.tags
}
",resource,"resource ""aws_secretsmanager_secret"" ""nuke_account_ids"" {
  # checkov:skip=CKV2_AWS_57:Auto rotation not possible
  name        = ""nuke_account_ids""
  description = ""Account IDs to be auto-nuked on weekly basis. CAUTION: Any account ID you add here will be automatically nuked! This secret is used by GitHub actions job nuke.yml inside the environments repo, to find the Account IDs to be nuked.""
  kms_key_id  = aws_kms_key.secrets_key.id
  tags        = local.tags
  replica {
    region = local.replica_region
  }
}
",resource,62,127.0,205c5cd21c3a6af4aa65ef7c9e2189dce40320f7,f5df80ab6ca1c06741f9457065b2bed2f0e04125,https://github.com/ministryofjustice/modernisation-platform/blob/205c5cd21c3a6af4aa65ef7c9e2189dce40320f7/terraform/modernisation-platform-account/secrets.tf#L62,https://github.com/ministryofjustice/modernisation-platform/blob/f5df80ab6ca1c06741f9457065b2bed2f0e04125/terraform/modernisation-platform-account/secrets.tf#L127,2022-04-26 11:51:29+01:00,2024-03-22 04:50:35+00:00,23,0
https://github.com/zenml-io/mlstacks,75,src/mlstacks/terraform/k3d-modular/output_test_harness_cfg.tf,src/mlstacks/terraform/k3d-modular/output_test_harness_cfg.tf,0,workaround,"# is forked. As a workaround, the deployment can be started separately,","# IMPORTANT: don't use this with pytest auto-provisioning. Running forked 
 # daemons in pytest leads to serious issues because the whole test process 
 # is forked. As a workaround, the deployment can be started separately, 
 # before pytest is invoked.","resource ""local_file"" ""test_framework_cfg_file"" {
  content  = <<-ADD
requirements:

%{if var.enable_container_registry}
  - name: k3d-container-registry-${random_string.cluster_id.result}
    description: >-
      Local K3D container registry.
    system_tools:
      - docker
    stacks:
      - name: k3d-${random_string.cluster_id.result}
        type: container_registry
        flavor: default
        configuration:
          uri: ""k3d-${local.k3d_registry.name}-${random_string.cluster_id.result}.localhost:${local.k3d_registry.port}""
%{endif}

%{if var.enable_orchestrator_kubernetes}
  - name: k3d-kubernetes-${random_string.cluster_id.result}
    description: >-
      K3D cluster that can be used as a kubernetes orchestrator.
    system_tools:
      - docker
      - kubectl
    capabilities:
      synchronized: true
    stacks:
      - name: k3d-kubernetes-${random_string.cluster_id.result}
        type: orchestrator
        flavor: kubernetes
        containerized: true
        configuration:
          kubernetes_context: ""k3d-${k3d_cluster.zenml-cluster[0].name}""
          synchronous: true
          kubernetes_namespace: ""${local.k3d.workloads_namespace}""
          local: true
%{endif}

%{if var.enable_orchestrator_kubeflow}
  - name: k3d-kubeflow-${random_string.cluster_id.result}
    description: >-
      Kubeflow running in a local K3D cluster.
    system_tools:
      - docker
      - kubectl
    capabilities:
      synchronized: true
    stacks:
      - name: k3d-kubeflow-${random_string.cluster_id.result}
        type: orchestrator
        flavor: kubeflow
        containerized: true
        configuration:
          kubernetes_context: ""k3d-${k3d_cluster.zenml-cluster[0].name}""
          synchronous: true
          local: true
%{endif}

%{if var.enable_orchestrator_tekton}
  - name: k3d-tekton-${random_string.cluster_id.result}
    description: >-
      Tekton running in a local K3D cluster.
    system_tools:
      - docker
      - kubectl
    capabilities:
      synchronized: true
    stacks:
      - name: k3d-tekton-${random_string.cluster_id.result}
        type: orchestrator
        flavor: tekton
        containerized: true
        configuration:
          kubernetes_context: ""k3d-${k3d_cluster.zenml-cluster[0].name}""
          kubernetes_namespace: ""${local.tekton.workloads_namespace}""
          local: true
%{endif}

%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
  - name: k3d-minio-artifact-store-${random_string.cluster_id.result}
    description: >-
      Minio artifact store running in a local K3D cluster.
    stacks:
      - name: k3d-minio-${random_string.cluster_id.result}
        type: artifact_store
        flavor: s3
        configuration:
          path: ""s3://${local.minio.zenml_minio_store_bucket}""
          key: ""${var.zenml-minio-store-access-key}""
          secret: ""${var.zenml-minio-store-secret-key}""
          client_kwargs: '{""endpoint_url"":""${module.minio_server[0].artifact_S3_Endpoint_URL}"", ""region_name"":""us-east-1""}'
%{endif}

%{if var.enable_experiment_tracker_mlflow}
  - name: k3d-mlflow-${random_string.cluster_id.result}
    description: >-
      MLFlow deployed in a local K3D cluster.
    stacks: 
      - name: k3d-mlflow-${random_string.cluster_id.result}
        type: experiment_tracker
        flavor: mlflow
        configuration:
          tracking_uri: ""${module.mlflow[0].mlflow-tracking-URL}""
          tracking_username: ""${var.mlflow-username}""
          tracking_password: ""${var.mlflow-password}""
%{endif}

%{if var.enable_model_deployer_seldon}
  - name: k3d-seldon-${random_string.cluster_id.result}
    description: >-
      Seldon Core deployed in a local K3D cluster.
    system_tools:
      - kubectl
    stacks:
      - name: k3d-seldon-${random_string.cluster_id.result}
        type: model_deployer
        flavor: seldon
        configuration:
          kubernetes_context: ""k3d-${k3d_cluster.zenml-cluster[0].name}""
          kubernetes_namespace: ""${local.seldon.workloads_namespace}""
          base_url:  ""http://${var.enable_model_deployer_seldon ? module.istio[0].ingress-ip-address : """"}""
          kubernetes_secret_name: ""${var.seldon-secret-name}""

%{endif}

%{if var.enable_model_deployer_kserve}
  - name: k3d-kserve-${random_string.cluster_id.result}
    description: >-
      Kserve deployed in a local K3D cluster.
    system_tools:
      - kubectl
    stacks:
      - name: k3d-kserve-${random_string.cluster_id.result}
        type: model_deployer
        flavor: kserve
        configuration:
          kubernetes_context: ""k3d-${k3d_cluster.zenml-cluster[0].name}""
          kubernetes_namespace: ""${local.kserve.workloads_namespace}""
          base_url:  ""http://${var.enable_model_deployer_kserve ? module.istio[0].ingress-ip-address : """"}""
          kubernetes_secret_name: ""${var.kserve-secret-name}""
%{endif}

environments:

  - name: default-k3d-local-orchestrator
    description: >-
      Default deployment with local orchestrator and other
      K3D provided or local components.
    deployment: default
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
      - local-secrets-manager
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
%{if var.enable_model_deployer_kserve}
      - k3d-kserve-${random_string.cluster_id.result}
%{endif}
    mandatory_requirements:
%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
    capabilities:
      synchronized: true

%{if var.enable_orchestrator_kubernetes} 
  - name: default-k3d-kubernetes-orchestrator
    description: >-
      Default deployment with K3D kubernetes orchestrator and other
      K3D provided or local components.
    deployment: default
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
%{if var.enable_model_deployer_kserve}
      - k3d-kserve-${random_string.cluster_id.result}
%{endif}
      - local-secrets-manager
    mandatory_requirements:
      - k3d-kubernetes-${random_string.cluster_id.result}
      - k3d-container-registry-${random_string.cluster_id.result}
%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
%{endif}

%{if var.enable_orchestrator_kubeflow}
  - name: default-k3d-kubeflow-orchestrator
    description: >-
      Default deployment with K3D kubeflow orchestrator and other
      K3D provided or local components.
    deployment: default
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
%{if var.enable_model_deployer_kserve}
      - k3d-kserve-${random_string.cluster_id.result}
%{endif}
      - local-secrets-manager
    mandatory_requirements:
      - k3d-kubeflow-${random_string.cluster_id.result}
      - k3d-container-registry-${random_string.cluster_id.result}
%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
%{endif}


%{if var.enable_orchestrator_tekton}
  - name: default-k3d-tekton-orchestrator
    description: >-
      Default deployment with K3D Tekton orchestrator and other
      K3D provided or local components.
    deployment: default
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
%{if var.enable_model_deployer_kserve}
      - k3d-kserve-${random_string.cluster_id.result}
%{endif}
      - local-secrets-manager
    mandatory_requirements:
      - k3d-tekton-${random_string.cluster_id.result}
      - k3d-container-registry-${random_string.cluster_id.result}
%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
%{endif}

    # IMPORTANT: don't use this with pytest auto-provisioning. Running forked
    # daemons in pytest leads to serious issues because the whole test process
    # is forked. As a workaround, the deployment can be started separately,
    # before pytest is invoked.
  - name: local-server-k3d-local-orchestrator
    description: >-
      Local server deployment with local orchestrator and other
      K3D provided or local components.
    deployment: local-server
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
      - local-secrets-manager
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
%{if var.enable_model_deployer_kserve}
      - k3d-kserve-${random_string.cluster_id.result}
%{endif}
    mandatory_requirements:
%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
    capabilities:
      synchronized: true

    # IMPORTANT: don't use this with pytest auto-provisioning. Running forked
    # daemons in pytest leads to serious issues because the whole test process
    # is forked. As a workaround, the deployment can be started separately,
    # before pytest is invoked.

%{if var.enable_orchestrator_kubernetes}
  - name: local-server-k3d-kubernetes-orchestrator
    description: >-
      Local server deployment with K3D kubernetes orchestrator and other
      K3D provided or local components.
    deployment: local-server
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
%{if var.enable_model_deployer_kserve}
      - k3d-kserve-${random_string.cluster_id.result}
%{endif}
      - local-secrets-manager
    mandatory_requirements:
      - k3d-kubernetes-${random_string.cluster_id.result}
      - k3d-container-registry-${random_string.cluster_id.result}
%{if var.enable_artifact_store || var.enable_orchestrator_kubeflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
%{endif}

%{if var.enable_orchestrator_kubeflow}
    # IMPORTANT: don't use this with pytest auto-provisioning. Running forked
    # daemons in pytest leads to serious issues because the whole test process
    # is forked. As a workaround, the deployment can be started separately,
    # before pytest is invoked.
  - name: local-server-k3d-kubeflow-orchestrator
    description: >-
      Local server deployment with K3D kubeflow orchestrator and other
      K3D provided or local components.
    deployment: local-server
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
%{if var.enable_model_deployer_kserve}
      - k3d-kserve-${random_string.cluster_id.result}
%{endif}
      - local-secrets-manager
    mandatory_requirements:
      - k3d-kubeflow-${random_string.cluster_id.result}
      - k3d-container-registry-${random_string.cluster_id.result}
%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
%{endif}

%{if var.enable_orchestrator_tekton}
    # IMPORTANT: don't use this with pytest auto-provisioning. Running forked
    # daemons in pytest leads to serious issues because the whole test process
    # is forked. As a workaround, the deployment can be started separately,
    # before pytest is invoked.
  - name: local-server-k3d-tekton-orchestrator
    description: >-
      Local server deployment with K3D Tekton orchestrator and other
      K3D provided or local components.
    deployment: local-server
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
%{if var.enable_model_deployer_kserve}
      - k3d-kserve-${random_string.cluster_id.result}
%{endif}
      - local-secrets-manager
    mandatory_requirements:
      - k3d-tekton-${random_string.cluster_id.result}
      - k3d-container-registry-${random_string.cluster_id.result}
%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
%{endif}
    ADD
  filename = ""./k3d_test_framework_cfg.yaml""
}",resource,"resource ""local_file"" ""test_framework_cfg_file"" {
  content  = <<-ADD
requirements:

%{if var.enable_container_registry}
  - name: k3d-container-registry-${random_string.cluster_id.result}
    description: >-
      Local K3D container registry.
    system_tools:
      - docker
    stacks:
      - name: k3d-${random_string.cluster_id.result}
        type: container_registry
        flavor: default
        configuration:
          uri: ""k3d-${local.k3d_registry.name}-${random_string.cluster_id.result}.localhost:${local.k3d_registry.port}""
%{endif}

%{if var.enable_orchestrator_kubernetes}
  - name: k3d-kubernetes-${random_string.cluster_id.result}
    description: >-
      K3D cluster that can be used as a kubernetes orchestrator.
    system_tools:
      - docker
      - kubectl
    capabilities:
      synchronized: true
    stacks:
      - name: k3d-kubernetes-${random_string.cluster_id.result}
        type: orchestrator
        flavor: kubernetes
        containerized: true
        configuration:
          kubernetes_context: ""k3d-${k3d_cluster.zenml-cluster[0].name}""
          synchronous: true
          kubernetes_namespace: ""${local.k3d.workloads_namespace}""
          local: true
%{endif}

%{if var.enable_orchestrator_kubeflow}
  - name: k3d-kubeflow-${random_string.cluster_id.result}
    description: >-
      Kubeflow running in a local K3D cluster.
    system_tools:
      - docker
      - kubectl
    capabilities:
      synchronized: true
    stacks:
      - name: k3d-kubeflow-${random_string.cluster_id.result}
        type: orchestrator
        flavor: kubeflow
        containerized: true
        configuration:
          kubernetes_context: ""k3d-${k3d_cluster.zenml-cluster[0].name}""
          synchronous: true
          local: true
%{endif}

%{if var.enable_orchestrator_tekton}
  - name: k3d-tekton-${random_string.cluster_id.result}
    description: >-
      Tekton running in a local K3D cluster.
    system_tools:
      - docker
      - kubectl
    capabilities:
      synchronized: true
    stacks:
      - name: k3d-tekton-${random_string.cluster_id.result}
        type: orchestrator
        flavor: tekton
        containerized: true
        configuration:
          kubernetes_context: ""k3d-${k3d_cluster.zenml-cluster[0].name}""
          kubernetes_namespace: ""${local.tekton.workloads_namespace}""
          local: true
%{endif}

%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
  - name: k3d-minio-artifact-store-${random_string.cluster_id.result}
    description: >-
      Minio artifact store running in a local K3D cluster.
    stacks:
      - name: k3d-minio-${random_string.cluster_id.result}
        type: artifact_store
        flavor: s3
        configuration:
          path: ""s3://${local.minio.zenml_minio_store_bucket}""
          key: ""${var.zenml-minio-store-access-key}""
          secret: ""${var.zenml-minio-store-secret-key}""
          client_kwargs: '{""endpoint_url"":""${module.minio_server[0].artifact_S3_Endpoint_URL}"", ""region_name"":""us-east-1""}'
%{endif}

%{if var.enable_experiment_tracker_mlflow}
  - name: k3d-mlflow-${random_string.cluster_id.result}
    description: >-
      MLFlow deployed in a local K3D cluster.
    stacks: 
      - name: k3d-mlflow-${random_string.cluster_id.result}
        type: experiment_tracker
        flavor: mlflow
        configuration:
          tracking_uri: ""${module.mlflow[0].mlflow-tracking-URL}""
          tracking_username: ""${var.mlflow-username}""
          tracking_password: ""${var.mlflow-password}""
%{endif}

%{if var.enable_model_deployer_seldon}
  - name: k3d-seldon-${random_string.cluster_id.result}
    description: >-
      Seldon Core deployed in a local K3D cluster.
    system_tools:
      - kubectl
    stacks:
      - name: k3d-seldon-${random_string.cluster_id.result}
        type: model_deployer
        flavor: seldon
        configuration:
          kubernetes_context: ""k3d-${k3d_cluster.zenml-cluster[0].name}""
          kubernetes_namespace: ""${local.seldon.workloads_namespace}""
          base_url:  ""http://${var.enable_model_deployer_seldon ? module.istio[0].ingress-ip-address : """"}""
          kubernetes_secret_name: ""${var.seldon-secret-name}""

%{endif}

environments:

  - name: default-k3d-local-orchestrator
    description: >-
      Default deployment with local orchestrator and other
      K3D provided or local components.
    deployment: default
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
    mandatory_requirements:
%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
    capabilities:
      synchronized: true

%{if var.enable_orchestrator_kubernetes} 
  - name: default-k3d-kubernetes-orchestrator
    description: >-
      Default deployment with K3D kubernetes orchestrator and other
      K3D provided or local components.
    deployment: default
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
      - local-secrets-manager
    mandatory_requirements:
      - k3d-kubernetes-${random_string.cluster_id.result}
      - k3d-container-registry-${random_string.cluster_id.result}
%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
%{endif}

%{if var.enable_orchestrator_kubeflow}
  - name: default-k3d-kubeflow-orchestrator
    description: >-
      Default deployment with K3D kubeflow orchestrator and other
      K3D provided or local components.
    deployment: default
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
      - local-secrets-manager
    mandatory_requirements:
      - k3d-kubeflow-${random_string.cluster_id.result}
      - k3d-container-registry-${random_string.cluster_id.result}
%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
%{endif}


%{if var.enable_orchestrator_tekton}
  - name: default-k3d-tekton-orchestrator
    description: >-
      Default deployment with K3D Tekton orchestrator and other
      K3D provided or local components.
    deployment: default
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
      - local-secrets-manager
    mandatory_requirements:
      - k3d-tekton-${random_string.cluster_id.result}
      - k3d-container-registry-${random_string.cluster_id.result}
%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
%{endif}

    # IMPORTANT: don't use this with pytest auto-provisioning. Running forked
    # daemons in pytest leads to serious issues because the whole test process
    # is forked. As a workaround, the deployment can be started separately,
    # before pytest is invoked.
  - name: local-server-k3d-local-orchestrator
    description: >-
      Local server deployment with local orchestrator and other
      K3D provided or local components.
    deployment: local-server
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
    mandatory_requirements:
%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
    capabilities:
      synchronized: true

    # IMPORTANT: don't use this with pytest auto-provisioning. Running forked
    # daemons in pytest leads to serious issues because the whole test process
    # is forked. As a workaround, the deployment can be started separately,
    # before pytest is invoked.

%{if var.enable_orchestrator_kubernetes}
  - name: local-server-k3d-kubernetes-orchestrator
    description: >-
      Local server deployment with K3D kubernetes orchestrator and other
      K3D provided or local components.
    deployment: local-server
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
      - local-secrets-manager
    mandatory_requirements:
      - k3d-kubernetes-${random_string.cluster_id.result}
      - k3d-container-registry-${random_string.cluster_id.result}
%{if var.enable_artifact_store || var.enable_orchestrator_kubeflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
%{endif}

%{if var.enable_orchestrator_kubeflow}
    # IMPORTANT: don't use this with pytest auto-provisioning. Running forked
    # daemons in pytest leads to serious issues because the whole test process
    # is forked. As a workaround, the deployment can be started separately,
    # before pytest is invoked.
  - name: local-server-k3d-kubeflow-orchestrator
    description: >-
      Local server deployment with K3D kubeflow orchestrator and other
      K3D provided or local components.
    deployment: local-server
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
      - local-secrets-manager
    mandatory_requirements:
      - k3d-kubeflow-${random_string.cluster_id.result}
      - k3d-container-registry-${random_string.cluster_id.result}
%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
%{endif}

%{if var.enable_orchestrator_tekton}
    # IMPORTANT: don't use this with pytest auto-provisioning. Running forked
    # daemons in pytest leads to serious issues because the whole test process
    # is forked. As a workaround, the deployment can be started separately,
    # before pytest is invoked.
  - name: local-server-k3d-tekton-orchestrator
    description: >-
      Local server deployment with K3D Tekton orchestrator and other
      K3D provided or local components.
    deployment: local-server
    requirements:
      - data-validators
%{if var.enable_experiment_tracker_mlflow}
      - k3d-mlflow-${random_string.cluster_id.result}
%{else}
      - mlflow-local-tracker
      - mlflow-local-deployer
%{endif}
%{if var.enable_model_deployer_seldon}
      - k3d-seldon-${random_string.cluster_id.result}
%{endif}
      - local-secrets-manager
    mandatory_requirements:
      - k3d-tekton-${random_string.cluster_id.result}
      - k3d-container-registry-${random_string.cluster_id.result}
%{if var.enable_artifact_store || var.enable_experiment_tracker_mlflow}
      - k3d-minio-artifact-store-${random_string.cluster_id.result}
%{endif}
%{endif}
    ADD
  filename = ""./k3d_test_framework_cfg.yaml""
}
",resource,266,236.0,513352b0abbf05afdd65e6b869f5619acd1d8c66,14973148b3d05063d333fd9d9ad523be79198c71,https://github.com/zenml-io/mlstacks/blob/513352b0abbf05afdd65e6b869f5619acd1d8c66/src/mlstacks/terraform/k3d-modular/output_test_harness_cfg.tf#L266,https://github.com/zenml-io/mlstacks/blob/14973148b3d05063d333fd9d9ad523be79198c71/src/mlstacks/terraform/k3d-modular/output_test_harness_cfg.tf#L236,2023-08-23 11:34:20+02:00,2023-12-22 09:15:10+01:00,3,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,403,locals.tf,locals.tf,0,implementation,"# Enable eBPF-based Masquerading (""The eBPF-based implementation is the most efficient implementation"")","# Enable eBPF-based Masquerading (""The eBPF-based implementation is the most efficient implementation"")","locals {
  # ssh_agent_identity is not set if the private key is passed directly, but if ssh agent is used, the public key tells ssh agent which private key to use.
  # For terraforms provisioner.connection.agent_identity, we need the public key as a string.
  ssh_agent_identity = var.ssh_private_key == null ? var.ssh_public_key : null

  # If passed, a key already registered within hetzner is used.
  # Otherwise, a new one will be created by the module.
  hcloud_ssh_key_id = var.hcloud_ssh_key_id == null ? hcloud_ssh_key.k3s[0].id : var.hcloud_ssh_key_id

  ccm_version    = var.hetzner_ccm_version != null ? var.hetzner_ccm_version : data.github_release.hetzner_ccm[0].release_tag
  csi_version    = length(data.github_release.hetzner_csi) == 0 ? var.hetzner_csi_version : data.github_release.hetzner_csi[0].release_tag
  kured_version  = var.kured_version != null ? var.kured_version : data.github_release.kured[0].release_tag
  calico_version = length(data.github_release.calico) == 0 ? var.calico_version : data.github_release.calico[0].release_tag

  cilium_ipv4_native_routing_cidr = coalesce(var.cilium_ipv4_native_routing_cidr, var.cluster_ipv4_cidr)

  additional_k3s_environment = join(""\n"",
    [
      for var_name, var_value in var.additional_k3s_environment :
      ""${var_name}=\""${var_value}\""""
    ]
  )
  install_additional_k3s_environment = <<-EOT
  cat >> /etc/environment <<EOF
  ${local.additional_k3s_environment}
  EOF
  set -a; source /etc/environment; set +a;
  EOT

  install_system_alias = <<-EOT
  cat > /etc/profile.d/00-alias.sh <<EOF
  alias k=kubectl
  EOF
  EOT

  install_kubectl_bash_completion = <<-EOT
  cat > /etc/bash_completion.d/kubectl <<EOF
  if command -v kubectl >/dev/null; then
    source <(kubectl completion bash)
    complete -o default -F __start_kubectl k
  fi
  EOF
  EOT

  common_pre_install_k3s_commands = concat(
    [
      ""set -ex"",
      # rename the private network interface to eth1
      ""/etc/cloud/rename_interface.sh"",
      # prepare the k3s config directory
      ""mkdir -p /etc/rancher/k3s"",
      # move the config file into place and adjust permissions
      ""[ -f /tmp/config.yaml ] && mv /tmp/config.yaml /etc/rancher/k3s/config.yaml"",
      ""chmod 0600 /etc/rancher/k3s/config.yaml"",
      # if the server has already been initialized just stop here
      ""[ -e /etc/rancher/k3s/k3s.yaml ] && exit 0"",
      local.install_additional_k3s_environment,
      local.install_system_alias,
      local.install_kubectl_bash_completion,
    ],
    # User-defined commands to execute just before installing k3s.
    var.preinstall_exec,
    # Wait for a successful connection to the internet.
    [""timeout 180s /bin/sh -c 'while ! ping -c 1 ${var.address_for_connectivity_test} >/dev/null 2>&1; do echo \""Ready for k3s installation, waiting for a successful connection to the internet...\""; sleep 5; done; echo Connected'""]
  )

  kustomization_backup_yaml = yamlencode({
    apiVersion = ""kustomize.config.k8s.io/v1beta1""
    kind       = ""Kustomization""

    resources = concat(
      [
        ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",
        ""https://github.com/kubereboot/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",
        ""https://raw.githubusercontent.com/rancher/system-upgrade-controller/master/manifests/system-upgrade-controller.yaml"",
      ],
      var.disable_hetzner_csi ? [] : [
        ""hcloud-csi.yml""
      ],
      lookup(local.ingress_controller_install_resources, local.ingress_controller, []),
      lookup(local.cni_install_resources, var.cni_plugin, []),
      var.enable_longhorn ? [""longhorn.yaml""] : [],
      var.enable_csi_driver_smb ? [""csi-driver-smb.yaml""] : [],
      var.enable_cert_manager || var.enable_rancher ? [""cert_manager.yaml""] : [],
      var.enable_rancher ? [""rancher.yaml""] : [],
      var.rancher_registration_manifest_url != """" ? [var.rancher_registration_manifest_url] : []
    ),
    patchesStrategicMerge = concat(
      [
        file(""${path.module}/kustomize/system-upgrade-controller.yaml""),
        ""kured.yaml"",
        ""ccm.yaml"",
      ],
      lookup(local.cni_install_resource_patches, var.cni_plugin, [])
    )
  })

  apply_k3s_selinux = [""/sbin/semodule -v -i /usr/share/selinux/packages/k3s.pp""]

  install_k3s_server = concat(local.common_pre_install_k3s_commands, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC='server ${var.k3s_exec_server_args}' sh -""
  ], local.apply_k3s_selinux)
  install_k3s_agent = concat(local.common_pre_install_k3s_commands, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC='agent ${var.k3s_exec_agent_args}' sh -""
  ], local.apply_k3s_selinux)

  control_plane_nodes = merge([
    for pool_index, nodepool_obj in var.control_plane_nodepools : {
      for node_index in range(nodepool_obj.count) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        location : nodepool_obj.location,
        labels : concat(local.default_control_plane_labels, nodepool_obj.labels),
        taints : concat(local.default_control_plane_taints, nodepool_obj.taints),
        backups : nodepool_obj.backups,
        index : node_index
      }
    }
  ]...)

  agent_nodes = merge([
    for pool_index, nodepool_obj in var.agent_nodepools : {
      for node_index in range(nodepool_obj.count) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        longhorn_volume_size : coalesce(nodepool_obj.longhorn_volume_size, 0),
        floating_ip : lookup(nodepool_obj, ""floating_ip"", false),
        location : nodepool_obj.location,
        labels : concat(local.default_agent_labels, nodepool_obj.labels),
        taints : concat(local.default_agent_taints, nodepool_obj.taints),
        backups : lookup(nodepool_obj, ""backups"", false),
        index : node_index
      }
    }
  ]...)

  # The first two subnets are respectively the default subnet 10.0.0.0/16 use for potientially anything and 10.1.0.0/16 used for control plane nodes.
  # the rest of the subnets are for agent nodes in each nodepools.
  network_ipv4_subnets = [for index in range(256) : cidrsubnet(var.network_ipv4_cidr, 8, index)]

  # if we are in a single cluster config, we use the default klipper lb instead of Hetzner LB
  control_plane_count    = sum([for v in var.control_plane_nodepools : v.count])
  agent_count            = sum([for v in var.agent_nodepools : v.count])
  is_single_node_cluster = (local.control_plane_count + local.agent_count) == 1

  using_klipper_lb = var.enable_klipper_metal_lb || local.is_single_node_cluster

  has_external_load_balancer = local.using_klipper_lb || local.ingress_controller == ""none""

  ingress_replica_count     = (var.ingress_replica_count > 0) ? var.ingress_replica_count : (local.agent_count > 2) ? 3 : (local.agent_count == 2) ? 2 : 1
  ingress_max_replica_count = (var.ingress_max_replica_count > local.ingress_replica_count) ? var.ingress_max_replica_count : local.ingress_replica_count

  # disable k3s extras
  disable_extras = concat([""local-storage""], local.using_klipper_lb ? [] : [""servicelb""], [""traefik""], var.enable_metrics_server ? [] : [""metrics-server""])

  # Determine if scheduling should be allowed on control plane nodes, which will be always true for single node clusters and clusters or if scheduling is allowed on control plane nodes
  allow_scheduling_on_control_plane = local.is_single_node_cluster ? true : var.allow_scheduling_on_control_plane
  # Determine if loadbalancer target should be allowed on control plane nodes, which will be always true for single node clusters or if scheduling is allowed on control plane nodes
  allow_loadbalancer_target_on_control_plane = local.is_single_node_cluster ? true : var.allow_scheduling_on_control_plane

  # Default k3s node labels
  default_agent_labels         = concat([], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])
  default_control_plane_labels = concat(local.allow_loadbalancer_target_on_control_plane ? [] : [""node.kubernetes.io/exclude-from-external-load-balancers=true""], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])

  # Default k3s node taints
  default_control_plane_taints = concat([], local.allow_scheduling_on_control_plane ? [] : [""node-role.kubernetes.io/control-plane:NoSchedule""])
  default_agent_taints         = concat([], var.cni_plugin == ""cilium"" ? [""node.cilium.io/agent-not-ready:NoExecute""] : [])

  base_firewall_rules = concat(
    var.firewall_ssh_source == null ? [] : [
      # Allow all traffic to the ssh port
      {
        description = ""Allow Incoming SSH Traffic""
        direction   = ""in""
        protocol    = ""tcp""
        port        = var.ssh_port
        source_ips  = var.firewall_ssh_source
      },
    ],
    var.firewall_kube_api_source == null ? [] : [
      {
        description = ""Allow Incoming Requests to Kube API Server""
        direction   = ""in""
        protocol    = ""tcp""
        port        = ""6443""
        source_ips  = var.firewall_kube_api_source
      }
    ],
    !var.restrict_outbound_traffic ? [] : [
      # Allow basic out traffic
      # ICMP to ping outside services
      {
        description     = ""Allow Outbound ICMP Ping Requests""
        direction       = ""out""
        protocol        = ""icmp""
        port            = """"
        destination_ips = [""0.0.0.0/0"", ""::/0""]
      },

      # DNS
      {
        description     = ""Allow Outbound TCP DNS Requests""
        direction       = ""out""
        protocol        = ""tcp""
        port            = ""53""
        destination_ips = [""0.0.0.0/0"", ""::/0""]
      },
      {
        description     = ""Allow Outbound UDP DNS Requests""
        direction       = ""out""
        protocol        = ""udp""
        port            = ""53""
        destination_ips = [""0.0.0.0/0"", ""::/0""]
      },

      # HTTP(s)
      {
        description     = ""Allow Outbound HTTP Requests""
        direction       = ""out""
        protocol        = ""tcp""
        port            = ""80""
        destination_ips = [""0.0.0.0/0"", ""::/0""]
      },
      {
        description     = ""Allow Outbound HTTPS Requests""
        direction       = ""out""
        protocol        = ""tcp""
        port            = ""443""
        destination_ips = [""0.0.0.0/0"", ""::/0""]
      },

      #NTP
      {
        description     = ""Allow Outbound UDP NTP Requests""
        direction       = ""out""
        protocol        = ""udp""
        port            = ""123""
        destination_ips = [""0.0.0.0/0"", ""::/0""]
      }
    ],
    !local.using_klipper_lb ? [] : [
      # Allow incoming web traffic for single node clusters, because we are using k3s servicelb there,
      # not an external load-balancer.
      {
        description = ""Allow Incoming HTTP Connections""
        direction   = ""in""
        protocol    = ""tcp""
        port        = ""80""
        source_ips  = [""0.0.0.0/0"", ""::/0""]
      },
      {
        description = ""Allow Incoming HTTPS Connections""
        direction   = ""in""
        protocol    = ""tcp""
        port        = ""443""
        source_ips  = [""0.0.0.0/0"", ""::/0""]
      }
    ],
    var.block_icmp_ping_in ? [] : [
      {
        description = ""Allow Incoming ICMP Ping Requests""
        direction   = ""in""
        protocol    = ""icmp""
        port        = """"
        source_ips  = [""0.0.0.0/0"", ""::/0""]
      }
    ]
  )

  # create a new firewall list based on base_firewall_rules but with direction-protocol-port as key
  # this is needed to avoid duplicate rules
  firewall_rules = { for rule in local.base_firewall_rules : format(""%s-%s-%s"", lookup(rule, ""direction"", ""null""), lookup(rule, ""protocol"", ""null""), lookup(rule, ""port"", ""null"")) => rule }

  # do the same for var.extra_firewall_rules
  extra_firewall_rules = { for rule in var.extra_firewall_rules : format(""%s-%s-%s"", lookup(rule, ""direction"", ""null""), lookup(rule, ""protocol"", ""null""), lookup(rule, ""port"", ""null"")) => rule }

  # merge the two lists
  firewall_rules_merged = merge(local.firewall_rules, local.extra_firewall_rules)

  # convert the merged list back to a list
  firewall_rules_list = values(local.firewall_rules_merged)

  labels = {
    ""provisioner"" = ""terraform"",
    ""engine""      = ""k3s""
    ""cluster""     = var.cluster_name
  }

  labels_control_plane_node = {
    role = ""control_plane_node""
  }
  labels_control_plane_lb = {
    role = ""control_plane_lb""
  }

  labels_agent_node = {
    role = ""agent_node""
  }

  cni_install_resources = {
    ""calico"" = [""https://raw.githubusercontent.com/projectcalico/calico/${coalesce(local.calico_version, ""v3.25.1"")}/manifests/calico.yaml""]
    ""cilium"" = [""cilium.yaml""]
  }

  cni_install_resource_patches = {
    ""calico"" = [""calico.yaml""]
  }

  cni_k3s_settings = {
    ""flannel"" = {
      disable-network-policy = var.disable_network_policy
      flannel-backend        = var.enable_wireguard ? ""wireguard-native"" : ""vxlan""
    }
    ""calico"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
    ""cilium"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
  }

  etcd_s3_snapshots = length(keys(var.etcd_s3_backup)) > 0 ? merge(
    {
      ""etcd-s3"" = true
    },
  var.etcd_s3_backup) : {}

  kubelet_arg                 = [""cloud-provider=external"", ""volume-plugin-dir=/var/lib/kubelet/volumeplugins""]
  kube_controller_manager_arg = ""flex-volume-plugin-dir=/var/lib/kubelet/volumeplugins""
  flannel_iface               = ""eth1""

  ingress_controller = var.ingress_controller

  ingress_controller_service_names = {
    ""traefik"" = ""traefik""
    ""nginx""   = ""nginx-ingress-nginx-controller""
  }

  ingress_controller_namespace_names = {
    ""traefik"" = ""traefik""
    ""nginx""   = ""nginx""
  }

  ingress_controller_install_resources = {
    ""traefik"" = [""traefik_ingress.yaml""]
    ""nginx""   = [""nginx_ingress.yaml""]
  }

  cilium_values = var.cilium_values != """" ? var.cilium_values : <<EOT
# Enable Kubernetes host-scope IPAM mode (required for K3s + Hetzner CCM)
ipam:
  mode: kubernetes
k8s:
  requireIPv4PodCIDR: true

# Replace kube-proxy with Cilium
kubeProxyReplacement: true

# Set Tunnel Mode or Native Routing Mode (supported by Hetzner CCM Route Controller)
routingMode: ""${var.cilium_routing_mode}""
%{if var.cilium_routing_mode == ""native""~}
ipv4NativeRoutingCIDR: ""${local.cilium_ipv4_native_routing_cidr}""
%{endif~}

endpointRoutes:
  # Enable use of per endpoint routes instead of routing via the cilium_host interface.
  enabled: true

loadBalancer:
  # Enable LoadBalancer & NodePort XDP Acceleration (direct routing (routingMode=native) is recommended to achieve optimal performance)
  acceleration: native

bpf:
  # Enable eBPF-based Masquerading (""The eBPF-based implementation is the most efficient implementation"")
  masquerade: true
%{if var.enable_wireguard}
encryption:
  enabled: true
  type: wireguard
%{endif~}
%{if var.cilium_egress_gateway_enabled}
egressGateway:
  enabled: true
%{endif~}

MTU: 1450
  EOT

  # Not to be confused with the other helm values, this is used for the calico.yaml kustomize patch
  # It also serves as a stub for a potential future use via helm values
  calico_values = var.calico_values != """" ? var.calico_values : <<EOT
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: calico-node
  namespace: kube-system
  labels:
    k8s-app: calico-node
spec:
  template:
    spec:
      volumes:
        - name: flexvol-driver-host
          hostPath:
            type: DirectoryOrCreate
            path: /var/lib/kubelet/volumeplugins/nodeagent~uds
      containers:
        - name: calico-node
          env:
            - name: CALICO_IPV4POOL_CIDR
              value: ""${var.cluster_ipv4_cidr}""
            - name: FELIX_WIREGUARDENABLED
              value: ""${var.enable_wireguard}""

  EOT

  longhorn_values = var.longhorn_values != """" ? var.longhorn_values : <<EOT
defaultSettings:
%{if length(var.autoscaler_nodepools) != 0~}
  kubernetesClusterAutoscalerEnabled: true
%{endif~}
  defaultDataPath: /var/longhorn
persistence:
  defaultFsType: ${var.longhorn_fstype}
  defaultClassReplicaCount: ${var.longhorn_replica_count}
  %{if var.disable_hetzner_csi~}defaultClass: true%{else~}defaultClass: false%{endif~}
  EOT

  csi_driver_smb_values = var.csi_driver_smb_values != """" ? var.csi_driver_smb_values : <<EOT
  EOT

  nginx_values = var.nginx_values != """" ? var.nginx_values : <<EOT
controller:
  watchIngressWithoutClass: ""true""
  kind: ""Deployment""
  replicaCount: ${local.ingress_replica_count}
  config:
    ""use-forwarded-headers"": ""true""
    ""compute-full-forwarded-for"": ""true""
    ""use-proxy-protocol"": ""${!local.using_klipper_lb}""
%{if !local.using_klipper_lb~}
  service:
    annotations:
      ""load-balancer.hetzner.cloud/name"": ""${var.cluster_name}""
      ""load-balancer.hetzner.cloud/use-private-ip"": ""true""
      ""load-balancer.hetzner.cloud/disable-private-ingress"": ""true""
      ""load-balancer.hetzner.cloud/disable-public-network"": ""${var.load_balancer_disable_public_network}""
      ""load-balancer.hetzner.cloud/ipv6-disabled"": ""${var.load_balancer_disable_ipv6}""
      ""load-balancer.hetzner.cloud/location"": ""${var.load_balancer_location}""
      ""load-balancer.hetzner.cloud/type"": ""${var.load_balancer_type}""
      ""load-balancer.hetzner.cloud/uses-proxyprotocol"": ""${!local.using_klipper_lb}""
      ""load-balancer.hetzner.cloud/algorithm-type"": ""${var.load_balancer_algorithm_type}""
      ""load-balancer.hetzner.cloud/health-check-interval"": ""${var.load_balancer_health_check_interval}""
      ""load-balancer.hetzner.cloud/health-check-timeout"": ""${var.load_balancer_health_check_timeout}""
      ""load-balancer.hetzner.cloud/health-check-retries"": ""${var.load_balancer_health_check_retries}""
%{if var.lb_hostname != """"~}
      ""load-balancer.hetzner.cloud/hostname"": ""${var.lb_hostname}""
%{endif~}
%{endif~}
  EOT

  traefik_values = var.traefik_values != """" ? var.traefik_values : <<EOT
deployment:
  replicas: ${local.ingress_replica_count}
globalArguments: []
service:
  enabled: true
  type: LoadBalancer
%{if !local.using_klipper_lb~}
  annotations:
    ""load-balancer.hetzner.cloud/name"": ""${var.cluster_name}""
    ""load-balancer.hetzner.cloud/use-private-ip"": ""true""
    ""load-balancer.hetzner.cloud/disable-private-ingress"": ""true""
    ""load-balancer.hetzner.cloud/disable-public-network"": ""${var.load_balancer_disable_public_network}""
    ""load-balancer.hetzner.cloud/ipv6-disabled"": ""${var.load_balancer_disable_ipv6}""
    ""load-balancer.hetzner.cloud/location"": ""${var.load_balancer_location}""
    ""load-balancer.hetzner.cloud/type"": ""${var.load_balancer_type}""
    ""load-balancer.hetzner.cloud/uses-proxyprotocol"": ""${!local.using_klipper_lb}""
    ""load-balancer.hetzner.cloud/algorithm-type"": ""${var.load_balancer_algorithm_type}""
    ""load-balancer.hetzner.cloud/health-check-interval"": ""${var.load_balancer_health_check_interval}""
    ""load-balancer.hetzner.cloud/health-check-timeout"": ""${var.load_balancer_health_check_timeout}""
    ""load-balancer.hetzner.cloud/health-check-retries"": ""${var.load_balancer_health_check_retries}""
%{if var.lb_hostname != """"~}
    ""load-balancer.hetzner.cloud/hostname"": ""${var.lb_hostname}""
%{endif~}
%{endif~}
ports:
  web:
%{if var.traefik_redirect_to_https~}
    redirectTo: websecure
%{endif~}
%{if !local.using_klipper_lb~}
    proxyProtocol:
      trustedIPs:
        - 127.0.0.1/32
        - 10.0.0.0/8
    forwardedHeaders:
      trustedIPs:
        - 127.0.0.1/32
        - 10.0.0.0/8
  websecure:
    proxyProtocol:
      trustedIPs:
        - 127.0.0.1/32
        - 10.0.0.0/8
    forwardedHeaders:
      trustedIPs:
        - 127.0.0.1/32
        - 10.0.0.0/8
%{endif~}
%{if var.traefik_additional_ports != """"~}
%{for option in var.traefik_additional_ports~}
  ${option.name}:
    port: ${option.port}
    expose: true
    exposedPort: ${option.exposedPort}
    protocol: TCP
%{if !local.using_klipper_lb~}
    proxyProtocol:
      trustedIPs:
        - 127.0.0.1/32
        - 10.0.0.0/8
    forwardedHeaders:
      trustedIPs:
        - 127.0.0.1/32
        - 10.0.0.0/8
%{endif~}
%{endfor~}
%{endif~}
%{if var.traefik_pod_disruption_budget~}
podDisruptionBudget:
  enabled: true
  maxUnavailable: 33%
%{endif~}
additionalArguments:
  - ""--entrypoints.tcp=true""
%{if var.traefik_additional_options != """"~}
%{for option in var.traefik_additional_options~}
  - ""${option}""
%{endfor~}
%{endif~}
%{if var.traefik_resource_limits~}
resources:
  requests:
    cpu: ""100m""
    memory: ""50Mi""
  limits:
    cpu: ""300m""
    memory: ""150Mi""
%{endif~}
%{if var.traefik_autoscaling~}
autoscaling:
  enabled: true
  minReplicas: ${local.ingress_replica_count}
  maxReplicas: ${local.ingress_max_replica_count}
%{endif~}
  EOT

  rancher_values = var.rancher_values != """" ? var.rancher_values : <<EOT
hostname: ""${var.rancher_hostname != """" ? var.rancher_hostname : var.lb_hostname}""
replicas: ${length(local.control_plane_nodes)}
bootstrapPassword: ""${length(var.rancher_bootstrap_password) == 0 ? resource.random_password.rancher_bootstrap[0].result : var.rancher_bootstrap_password}""
global:
  cattle:
    psp:
      enabled: false
  EOT

  cert_manager_values = var.cert_manager_values != """" ? var.cert_manager_values : <<EOT
installCRDs: true
  EOT

  kured_options = merge({
    ""reboot-command"" : ""/usr/bin/systemctl reboot"",
    ""pre-reboot-node-labels"" : ""kured=rebooting"",
    ""post-reboot-node-labels"" : ""kured=done"",
    ""period"" : ""5m"",
  }, var.kured_options)

  k3s_registries_update_script = <<EOF
DATE=`date +%Y-%m-%d_%H-%M-%S`
if cmp -s /tmp/registries.yaml /etc/rancher/k3s/registries.yaml; then
  echo ""No update required to the registries.yaml file""
else
  echo ""Backing up /etc/rancher/k3s/registries.yaml to /tmp/registries_$DATE.yaml""
  cp /etc/rancher/k3s/registries.yaml /tmp/registries_$DATE.yaml
  echo ""Updated registries.yaml detected, restart of k3s service required""
  cp /tmp/registries.yaml /etc/rancher/k3s/registries.yaml
  if systemctl is-active --quiet k3s; then
    systemctl restart k3s || (echo ""Error: Failed to restart k3s. Restoring /etc/rancher/k3s/registries.yaml from backup"" && cp /tmp/registries_$DATE.yaml /etc/rancher/k3s/registries.yaml && systemctl restart k3s)
  elif systemctl is-active --quiet k3s-agent; then
    systemctl restart k3s-agent || (echo ""Error: Failed to restart k3s-agent. Restoring /etc/rancher/k3s/registries.yaml from backup"" && cp /tmp/registries_$DATE.yaml /etc/rancher/k3s/registries.yaml && systemctl restart k3s-agent)
  else
    echo ""No active k3s or k3s-agent service found""
  fi
  echo ""k3s service or k3s-agent service restarted successfully""
fi
EOF

  cloudinit_write_files_common = <<EOT
# Script to rename the private interface to eth1 and unify NetworkManager connection naming
- path: /etc/cloud/rename_interface.sh
  content: |
    #!/bin/bash
    set -euo pipefail

    sleep 11
    
    INTERFACE=$(ip link show | awk '/^3:/{print $2}' | sed 's/://g')
    MAC=$(cat /sys/class/net/$INTERFACE/address)
    
    cat <<EOF > /etc/udev/rules.d/70-persistent-net.rules
    SUBSYSTEM==""net"", ACTION==""add"", DRIVERS==""?*"", ATTR{address}==""$MAC"", NAME=""eth1""
    EOF

    ip link set $INTERFACE down
    ip link set $INTERFACE name eth1
    ip link set eth1 up

    eth0_connection=$(nmcli -g GENERAL.CONNECTION device show eth0)
    nmcli connection modify ""$eth0_connection"" \
      con-name eth0 \
      connection.interface-name eth0

    eth1_connection=$(nmcli -g GENERAL.CONNECTION device show eth1)
    nmcli connection modify ""$eth1_connection"" \
      con-name eth1 \
      connection.interface-name eth1

    systemctl restart NetworkManager
  permissions: ""0744""

# Disable ssh password authentication
- content: |
    Port ${var.ssh_port}
    PasswordAuthentication no
    X11Forwarding no
    MaxAuthTries ${var.ssh_max_auth_tries}
    AllowTcpForwarding no
    AllowAgentForwarding no
    AuthorizedKeysFile .ssh/authorized_keys
  path: /etc/ssh/sshd_config.d/kube-hetzner.conf

# Set reboot method as ""kured""
- content: |
    REBOOT_METHOD=kured
  path: /etc/transactional-update.conf

# Create Rancher repo config
- content: |
    [rancher-k3s-common-stable]
    name=Rancher K3s Common (stable)
    baseurl=https://rpm.rancher.io/k3s/stable/common/microos/noarch
    enabled=1
    gpgcheck=1
    repo_gpgcheck=0
    gpgkey=https://rpm.rancher.io/public.key
  path: /etc/zypp/repos.d/rancher-k3s-common.repo

# Create the kube_hetzner_selinux.te file, that allows in SELinux to not interfere with various needed services
- path: /root/kube_hetzner_selinux.te
  content: |
    module kube_hetzner_selinux 1.0;

    require {
      type kernel_t, bin_t, kernel_generic_helper_t, iscsid_t, iscsid_exec_t, var_run_t,
      init_t, unlabeled_t, systemd_logind_t, systemd_hostnamed_t, container_t,
      cert_t, container_var_lib_t, etc_t, usr_t, container_file_t, container_log_t,
      container_share_t, container_runtime_exec_t, container_runtime_t, var_log_t, proc_t;
      class key { read view };
      class file { open read execute execute_no_trans create link lock rename write append setattr unlink getattr watch };
      class sock_file { watch write create unlink };
      class unix_dgram_socket create;
      class unix_stream_socket { connectto read write };
      class dir { add_name create getattr link lock read rename remove_name reparent rmdir setattr unlink search write watch };
      class lnk_file { read create };
      class system module_request;
      class filesystem associate;
    }

    #============= kernel_generic_helper_t ==============
    allow kernel_generic_helper_t bin_t:file execute_no_trans;
    allow kernel_generic_helper_t kernel_t:key { read view };
    allow kernel_generic_helper_t self:unix_dgram_socket create;

    #============= iscsid_t ==============
    allow iscsid_t iscsid_exec_t:file execute;
    allow iscsid_t var_run_t:sock_file write;
    allow iscsid_t var_run_t:unix_stream_socket connectto;

    #============= init_t ==============
    allow init_t unlabeled_t:dir { add_name remove_name rmdir };
    allow init_t unlabeled_t:lnk_file create;
    allow init_t container_t:file { open read };

    #============= systemd_logind_t ==============
    allow systemd_logind_t unlabeled_t:dir search;

    #============= systemd_hostnamed_t ==============
    allow systemd_hostnamed_t unlabeled_t:dir search;

    #============= container_t ==============
    # Basic file and directory operations for specific types
    allow container_t cert_t:dir read;
    allow container_t cert_t:lnk_file read;
    allow container_t cert_t:file { read open };
    allow container_t container_var_lib_t:file { create open read write rename lock };
    allow container_t etc_t:dir { add_name remove_name write create setattr };
    allow container_t etc_t:sock_file { create unlink };
    allow container_t usr_t:dir { add_name create getattr link lock read rename remove_name reparent rmdir setattr unlink search write };
    allow container_t usr_t:file { append create execute getattr link lock read rename setattr unlink write };

    # Additional rules for container_t
    allow container_t container_file_t:file { open read write append getattr setattr };
    allow container_t container_file_t:sock_file watch;
    allow container_t container_log_t:file { open read write append getattr setattr };
    allow container_t container_share_t:dir { read write add_name remove_name };
    allow container_t container_share_t:file { read write create unlink };
    allow container_t container_runtime_exec_t:file { read execute execute_no_trans open };
    allow container_t container_runtime_t:unix_stream_socket { connectto read write };
    allow container_t kernel_t:system module_request;
    allow container_t container_log_t:dir { read watch };
    allow container_t container_log_t:file { open read watch };
    allow container_t container_log_t:lnk_file read;
    allow container_t var_log_t:dir { add_name write };
    allow container_t var_log_t:file { create lock open read setattr write };
    allow container_t var_log_t:dir remove_name;
    allow container_t var_log_t:file unlink;
    allow container_t proc_t:filesystem associate;

# Create the k3s registries file if needed
%{if var.k3s_registries != """"}
# Create k3s registries file
- content: ${base64encode(var.k3s_registries)}
  encoding: base64
  path: /etc/rancher/k3s/registries.yaml
%{endif}

# Apply new DNS config
%{if length(var.dns_servers) > 0}
# Set prepare for manual dns config
- content: |
    [main]
    dns=none
  path: /etc/NetworkManager/conf.d/dns.conf

- content: |
    %{for server in var.dns_servers~}
    nameserver ${server}
    %{endfor}
  path: /etc/resolv.conf
  permissions: '0644'
%{endif}
EOT

  cloudinit_runcmd_common = <<EOT
# ensure that /var uses full available disk size, thanks to btrfs this is easy
- [btrfs, 'filesystem', 'resize', 'max', '/var']

# SELinux permission for the SSH alternative port
%{if var.ssh_port != 22}
# SELinux permission for the SSH alternative port.
- [semanage, port, '-a', '-t', ssh_port_t, '-p', tcp, ${var.ssh_port}]
%{endif}

# Create and apply the necessary SELinux module for kube-hetzner
- [checkmodule, '-M', '-m', '-o', '/root/kube_hetzner_selinux.mod', '/root/kube_hetzner_selinux.te']
- ['semodule_package', '-o', '/root/kube_hetzner_selinux.pp', '-m', '/root/kube_hetzner_selinux.mod']
- [semodule, '-i', '/root/kube_hetzner_selinux.pp']
- [setsebool, '-P', 'virt_use_samba', '1']
- [setsebool, '-P', 'domain_kernel_load_modules', '1']

# Disable rebootmgr service as we use kured instead
- [systemctl, disable, '--now', 'rebootmgr.service']

%{if length(var.dns_servers) > 0}
# Set the dns manually
- [systemctl, 'reload', 'NetworkManager']
%{endif}

# Bounds the amount of logs that can survive on the system
- [sed, '-i', 's/#SystemMaxUse=/SystemMaxUse=3G/g', /etc/systemd/journald.conf]
- [sed, '-i', 's/#MaxRetentionSec=/MaxRetentionSec=1week/g', /etc/systemd/journald.conf]

# Reduces the default number of snapshots from 2-10 number limit, to 4 and from 4-10 number limit important, to 2
- [sed, '-i', 's/NUMBER_LIMIT=""2-10""/NUMBER_LIMIT=""4""/g', /etc/snapper/configs/root]
- [sed, '-i', 's/NUMBER_LIMIT_IMPORTANT=""4-10""/NUMBER_LIMIT_IMPORTANT=""3""/g', /etc/snapper/configs/root]

# Allow network interface
- [chmod, '+x', '/etc/cloud/rename_interface.sh']

# Restart the sshd service to apply the new config
- [systemctl, 'restart', 'sshd']

# Make sure the network is up
- [systemctl, restart, NetworkManager]
- [systemctl, status, NetworkManager]
- [ip, route, add, default, via, '172.31.1.1', dev, 'eth0']

# Cleanup some logs
- [truncate, '-s', '0', '/var/log/audit/audit.log']
EOT
}
",locals,"locals {
  # ssh_agent_identity is not set if the private key is passed directly, but if ssh agent is used, the public key tells ssh agent which private key to use.
  # For terraforms provisioner.connection.agent_identity, we need the public key as a string.
  ssh_agent_identity = var.ssh_private_key == null ? var.ssh_public_key : null

  # If passed, a key already registered within hetzner is used.
  # Otherwise, a new one will be created by the module.
  hcloud_ssh_key_id = var.hcloud_ssh_key_id == null ? hcloud_ssh_key.k3s[0].id : var.hcloud_ssh_key_id

  # if given as a variable, we want to use the given token. This is needed to restore the cluster
  k3s_token = var.k3s_token == null ? random_password.k3s_token.result : var.k3s_token

  ccm_version    = var.hetzner_ccm_version != null ? var.hetzner_ccm_version : data.github_release.hetzner_ccm[0].release_tag
  csi_version    = length(data.github_release.hetzner_csi) == 0 ? var.hetzner_csi_version : data.github_release.hetzner_csi[0].release_tag
  kured_version  = var.kured_version != null ? var.kured_version : data.github_release.kured[0].release_tag
  calico_version = length(data.github_release.calico) == 0 ? var.calico_version : data.github_release.calico[0].release_tag

  cilium_ipv4_native_routing_cidr = coalesce(var.cilium_ipv4_native_routing_cidr, var.cluster_ipv4_cidr)

  additional_k3s_environment = join(""\n"",
    [
      for var_name, var_value in var.additional_k3s_environment :
      ""${var_name}=\""${var_value}\""""
    ]
  )
  install_additional_k3s_environment = <<-EOT
  cat >> /etc/environment <<EOF
  ${local.additional_k3s_environment}
  EOF
  set -a; source /etc/environment; set +a;
  EOT

  install_system_alias = <<-EOT
  cat > /etc/profile.d/00-alias.sh <<EOF
  alias k=kubectl
  EOF
  EOT

  install_kubectl_bash_completion = <<-EOT
  cat > /etc/bash_completion.d/kubectl <<EOF
  if command -v kubectl >/dev/null; then
    source <(kubectl completion bash)
    complete -o default -F __start_kubectl k
  fi
  EOF
  EOT

  common_pre_install_k3s_commands = concat(
    [
      ""set -ex"",
      # rename the private network interface to eth1
      ""/etc/cloud/rename_interface.sh"",
      # prepare the k3s config directory
      ""mkdir -p /etc/rancher/k3s"",
      # move the config file into place and adjust permissions
      ""[ -f /tmp/config.yaml ] && mv /tmp/config.yaml /etc/rancher/k3s/config.yaml"",
      ""chmod 0600 /etc/rancher/k3s/config.yaml"",
      # if the server has already been initialized just stop here
      ""[ -e /etc/rancher/k3s/k3s.yaml ] && exit 0"",
      local.install_additional_k3s_environment,
      local.install_system_alias,
      local.install_kubectl_bash_completion,
    ],
    # User-defined commands to execute just before installing k3s.
    var.preinstall_exec,
    # Wait for a successful connection to the internet.
    [""timeout 180s /bin/sh -c 'while ! ping -c 1 ${var.address_for_connectivity_test} >/dev/null 2>&1; do echo \""Ready for k3s installation, waiting for a successful connection to the internet...\""; sleep 5; done; echo Connected'""]
  )

  common_post_install_k3s_commands = concat(var.postinstall_exec, [""restorecon -v /usr/local/bin/k3s""])

  kustomization_backup_yaml = yamlencode({
    apiVersion = ""kustomize.config.k8s.io/v1beta1""
    kind       = ""Kustomization""
    resources = concat(
      [
        ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",
        ""https://github.com/kubereboot/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",
        ""https://raw.githubusercontent.com/rancher/system-upgrade-controller/9e7e45c1bdd528093da36be1f1f32472469005e6/manifests/system-upgrade-controller.yaml"",
      ],
      var.disable_hetzner_csi ? [] : [""hcloud-csi.yml""],
      lookup(local.ingress_controller_install_resources, var.ingress_controller, []),
      lookup(local.cni_install_resources, var.cni_plugin, []),
      var.enable_longhorn ? [""longhorn.yaml""] : [],
      var.enable_csi_driver_smb ? [""csi-driver-smb.yaml""] : [],
      var.enable_cert_manager || var.enable_rancher ? [""cert_manager.yaml""] : [],
      var.enable_rancher ? [""rancher.yaml""] : [],
      var.rancher_registration_manifest_url != """" ? [var.rancher_registration_manifest_url] : []
    ),
    patches = [
      {
        target = {
          group     = ""apps""
          version   = ""v1""
          kind      = ""Deployment""
          name      = ""system-upgrade-controller""
          namespace = ""system-upgrade""
        }
        patch = file(""${path.module}/kustomize/system-upgrade-controller.yaml"")
      },
      {
        target = {
          group     = ""apps""
          version   = ""v1""
          kind      = ""Deployment""
          name      = ""system-upgrade-controller""
          namespace = ""system-upgrade""
        }
        patch = <<-EOF
          - op: replace
            path: /spec/template/spec/containers/0/image
            value: rancher/system-upgrade-controller:v0.13.4
        EOF
      },
      {
        path = ""kured.yaml""
      },
      {
        path = ""ccm.yaml""
      }
    ]
  })

  apply_k3s_selinux = [""/sbin/semodule -v -i /usr/share/selinux/packages/k3s.pp""]
  swap_node_label   = [""node.kubernetes.io/server-swap=enabled""]

  install_k3s_server = concat(local.common_pre_install_k3s_commands, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC='server ${var.k3s_exec_server_args}' sh -""
  ], (var.disable_selinux ? [] : local.apply_k3s_selinux), local.common_post_install_k3s_commands)
  install_k3s_agent = concat(local.common_pre_install_k3s_commands, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC='agent ${var.k3s_exec_agent_args}' sh -""
  ], (var.disable_selinux ? [] : local.apply_k3s_selinux), local.common_post_install_k3s_commands)

  control_plane_nodes = merge([
    for pool_index, nodepool_obj in var.control_plane_nodepools : {
      for node_index in range(nodepool_obj.count) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        location : nodepool_obj.location,
        labels : concat(local.default_control_plane_labels, nodepool_obj.swap_size != """" ? local.swap_node_label : [], nodepool_obj.labels),
        taints : concat(local.default_control_plane_taints, nodepool_obj.taints),
        kubelet_args : nodepool_obj.kubelet_args,
        backups : nodepool_obj.backups,
        swap_size : nodepool_obj.swap_size,
        zram_size : nodepool_obj.zram_size,
        index : node_index
        selinux : nodepool_obj.selinux
        placement_group_compat_idx : nodepool_obj.placement_group_compat_idx,
        placement_group : nodepool_obj.placement_group
      }
    }
  ]...)

  agent_nodes_from_integer_counts = merge([
    for pool_index, nodepool_obj in var.agent_nodepools : {
      # coalesce(nodepool_obj.count, 0) means we select those nodepools who's size is set by an integer count.
      for node_index in range(coalesce(nodepool_obj.count, 0)) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        longhorn_volume_size : coalesce(nodepool_obj.longhorn_volume_size, 0),
        floating_ip : lookup(nodepool_obj, ""floating_ip"", false),
        location : nodepool_obj.location,
        labels : concat(local.default_agent_labels, nodepool_obj.swap_size != """" ? local.swap_node_label : [], nodepool_obj.labels),
        taints : concat(local.default_agent_taints, nodepool_obj.taints),
        kubelet_args : nodepool_obj.kubelet_args,
        backups : lookup(nodepool_obj, ""backups"", false),
        swap_size : nodepool_obj.swap_size,
        zram_size : nodepool_obj.zram_size,
        index : node_index
        selinux : nodepool_obj.selinux
        placement_group_compat_idx : nodepool_obj.placement_group_compat_idx,
        placement_group : nodepool_obj.placement_group
      }
    }
  ]...)

  agent_nodes_from_maps_for_counts = merge([
    for pool_index, nodepool_obj in var.agent_nodepools : {
      # coalesce(nodepool_obj.nodes, {}) means we select those nodepools who's size is set by an integer count.
      for node_key, node_obj in coalesce(nodepool_obj.nodes, {}) :
      format(""%s-%s-%s"", pool_index, node_key, nodepool_obj.name) => merge(
        {
          nodepool_name : nodepool_obj.name,
          server_type : nodepool_obj.server_type,
          longhorn_volume_size : coalesce(nodepool_obj.longhorn_volume_size, 0),
          floating_ip : lookup(nodepool_obj, ""floating_ip"", false),
          location : nodepool_obj.location,
          labels : concat(local.default_agent_labels, nodepool_obj.swap_size != """" ? local.swap_node_label : [], nodepool_obj.labels),
          taints : concat(local.default_agent_taints, nodepool_obj.taints),
          kubelet_args : nodepool_obj.kubelet_args,
          backups : lookup(nodepool_obj, ""backups"", false),
          swap_size : nodepool_obj.swap_size,
          zram_size : nodepool_obj.zram_size,
          selinux : nodepool_obj.selinux,
          placement_group_compat_idx : nodepool_obj.placement_group_compat_idx,
          placement_group : nodepool_obj.placement_group,
          index : floor(tonumber(node_key)),
        },
        { for key, value in node_obj : key => value if value != null },
        {
          labels : concat(local.default_agent_labels, nodepool_obj.swap_size != """" ? local.swap_node_label : [], nodepool_obj.labels, coalesce(node_obj.labels, [])),
          taints : concat(local.default_agent_taints, nodepool_obj.taints, coalesce(node_obj.taints, [])),
        },
        (
          node_obj.append_index_to_node_name ? { node_name_suffix : ""-${floor(tonumber(node_key))}"" } : {}
        )
      )
    }
  ]...)


  agent_nodes = merge(
    local.agent_nodes_from_integer_counts,
    local.agent_nodes_from_maps_for_counts,
  )

  use_existing_network = length(var.existing_network_id) > 0

  # The first two subnets are respectively the default subnet 10.0.0.0/16 use for potientially anything and 10.1.0.0/16 used for control plane nodes.
  # the rest of the subnets are for agent nodes in each nodepools.
  network_ipv4_subnets = [for index in range(256) : cidrsubnet(var.network_ipv4_cidr, 8, index)]

  # if we are in a single cluster config, we use the default klipper lb instead of Hetzner LB
  control_plane_count    = sum([for v in var.control_plane_nodepools : v.count])
  agent_count            = sum([for v in var.agent_nodepools : length(coalesce(v.nodes, {})) + coalesce(v.count, 0)])
  is_single_node_cluster = (local.control_plane_count + local.agent_count) == 1

  using_klipper_lb = var.enable_klipper_metal_lb || local.is_single_node_cluster

  has_external_load_balancer = local.using_klipper_lb || var.ingress_controller == ""none""
  load_balancer_name         = ""${var.cluster_name}-${var.ingress_controller}""

  ingress_controller_service_names = {
    ""traefik"" = ""traefik""
    ""nginx""   = ""nginx-ingress-nginx-controller""
  }

  ingress_controller_install_resources = {
    ""traefik"" = [""traefik_ingress.yaml""]
    ""nginx""   = [""nginx_ingress.yaml""]
  }

  default_ingress_namespace_mapping = {
    ""traefik"" = ""traefik""
    ""nginx""   = ""nginx""
  }

  ingress_controller_namespace = var.ingress_target_namespace != """" ? var.ingress_target_namespace : lookup(local.default_ingress_namespace_mapping, var.ingress_controller, """")
  ingress_replica_count        = (var.ingress_replica_count > 0) ? var.ingress_replica_count : (local.agent_count > 2) ? 3 : (local.agent_count == 2) ? 2 : 1
  ingress_max_replica_count    = (var.ingress_max_replica_count > local.ingress_replica_count) ? var.ingress_max_replica_count : local.ingress_replica_count

  # disable k3s extras
  disable_extras = concat(var.enable_local_storage ? [] : [""local-storage""], local.using_klipper_lb ? [] : [""servicelb""], [""traefik""], var.enable_metrics_server ? [] : [""metrics-server""])

  # Determine if scheduling should be allowed on control plane nodes, which will be always true for single node clusters and clusters or if scheduling is allowed on control plane nodes
  allow_scheduling_on_control_plane = local.is_single_node_cluster ? true : var.allow_scheduling_on_control_plane
  # Determine if loadbalancer target should be allowed on control plane nodes, which will be always true for single node clusters or if scheduling is allowed on control plane nodes
  allow_loadbalancer_target_on_control_plane = local.is_single_node_cluster ? true : var.allow_scheduling_on_control_plane

  # Default k3s node labels
  default_agent_labels         = concat([], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])
  default_control_plane_labels = concat(local.allow_loadbalancer_target_on_control_plane ? [] : [""node.kubernetes.io/exclude-from-external-load-balancers=true""], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])

  # Default k3s node taints
  default_control_plane_taints = concat([], local.allow_scheduling_on_control_plane ? [] : [""node-role.kubernetes.io/control-plane:NoSchedule""])
  default_agent_taints         = concat([], var.cni_plugin == ""cilium"" ? [""node.cilium.io/agent-not-ready:NoExecute""] : [])

  base_firewall_rules = concat(
    var.firewall_ssh_source == null ? [] : [
      # Allow all traffic to the ssh port
      {
        description = ""Allow Incoming SSH Traffic""
        direction   = ""in""
        protocol    = ""tcp""
        port        = var.ssh_port
        source_ips  = var.firewall_ssh_source
      },
    ],
    var.firewall_kube_api_source == null ? [] : [
      {
        description = ""Allow Incoming Requests to Kube API Server""
        direction   = ""in""
        protocol    = ""tcp""
        port        = ""6443""
        source_ips  = var.firewall_kube_api_source
      }
    ],
    !var.restrict_outbound_traffic ? [] : [
      # Allow basic out traffic
      # ICMP to ping outside services
      {
        description     = ""Allow Outbound ICMP Ping Requests""
        direction       = ""out""
        protocol        = ""icmp""
        port            = """"
        destination_ips = [""0.0.0.0/0"", ""::/0""]
      },

      # DNS
      {
        description     = ""Allow Outbound TCP DNS Requests""
        direction       = ""out""
        protocol        = ""tcp""
        port            = ""53""
        destination_ips = [""0.0.0.0/0"", ""::/0""]
      },
      {
        description     = ""Allow Outbound UDP DNS Requests""
        direction       = ""out""
        protocol        = ""udp""
        port            = ""53""
        destination_ips = [""0.0.0.0/0"", ""::/0""]
      },

      # HTTP(s)
      {
        description     = ""Allow Outbound HTTP Requests""
        direction       = ""out""
        protocol        = ""tcp""
        port            = ""80""
        destination_ips = [""0.0.0.0/0"", ""::/0""]
      },
      {
        description     = ""Allow Outbound HTTPS Requests""
        direction       = ""out""
        protocol        = ""tcp""
        port            = ""443""
        destination_ips = [""0.0.0.0/0"", ""::/0""]
      },

      #NTP
      {
        description     = ""Allow Outbound UDP NTP Requests""
        direction       = ""out""
        protocol        = ""udp""
        port            = ""123""
        destination_ips = [""0.0.0.0/0"", ""::/0""]
      }
    ],
    !local.using_klipper_lb ? [] : [
      # Allow incoming web traffic for single node clusters, because we are using k3s servicelb there,
      # not an external load-balancer.
      {
        description = ""Allow Incoming HTTP Connections""
        direction   = ""in""
        protocol    = ""tcp""
        port        = ""80""
        source_ips  = [""0.0.0.0/0"", ""::/0""]
      },
      {
        description = ""Allow Incoming HTTPS Connections""
        direction   = ""in""
        protocol    = ""tcp""
        port        = ""443""
        source_ips  = [""0.0.0.0/0"", ""::/0""]
      }
    ],
    var.block_icmp_ping_in ? [] : [
      {
        description = ""Allow Incoming ICMP Ping Requests""
        direction   = ""in""
        protocol    = ""icmp""
        port        = """"
        source_ips  = [""0.0.0.0/0"", ""::/0""]
      }
    ]
  )

  # create a new firewall list based on base_firewall_rules but with direction-protocol-port as key
  # this is needed to avoid duplicate rules
  firewall_rules = { for rule in local.base_firewall_rules : format(""%s-%s-%s"", lookup(rule, ""direction"", ""null""), lookup(rule, ""protocol"", ""null""), lookup(rule, ""port"", ""null"")) => rule }

  # do the same for var.extra_firewall_rules
  extra_firewall_rules = { for rule in var.extra_firewall_rules : format(""%s-%s-%s"", lookup(rule, ""direction"", ""null""), lookup(rule, ""protocol"", ""null""), lookup(rule, ""port"", ""null"")) => rule }

  # merge the two lists
  firewall_rules_merged = merge(local.firewall_rules, local.extra_firewall_rules)

  # convert the merged list back to a list
  firewall_rules_list = values(local.firewall_rules_merged)

  labels = {
    ""provisioner"" = ""terraform"",
    ""engine""      = ""k3s""
    ""cluster""     = var.cluster_name
  }

  labels_control_plane_node = {
    role = ""control_plane_node""
  }
  labels_control_plane_lb = {
    role = ""control_plane_lb""
  }

  labels_agent_node = {
    role = ""agent_node""
  }

  cni_install_resources = {
    ""calico"" = [""https://raw.githubusercontent.com/projectcalico/calico/${coalesce(local.calico_version, ""v3.27.2"")}/manifests/calico.yaml""]
    ""cilium"" = [""cilium.yaml""]
  }

  cni_install_resource_patches = {
    ""calico"" = [""calico.yaml""]
  }

  cni_k3s_settings = {
    ""flannel"" = {
      disable-network-policy = var.disable_network_policy
      flannel-backend        = var.enable_wireguard ? ""wireguard-native"" : ""vxlan""
    }
    ""calico"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
    ""cilium"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
  }

  etcd_s3_snapshots = length(keys(var.etcd_s3_backup)) > 0 ? merge(
    {
      ""etcd-s3"" = true
    },
  var.etcd_s3_backup) : {}

  kubelet_arg                 = [""cloud-provider=external"", ""volume-plugin-dir=/var/lib/kubelet/volumeplugins""]
  kube_controller_manager_arg = ""flex-volume-plugin-dir=/var/lib/kubelet/volumeplugins""
  flannel_iface               = ""eth1""

  cilium_values = var.cilium_values != """" ? var.cilium_values : <<EOT
# Enable Kubernetes host-scope IPAM mode (required for K3s + Hetzner CCM)
ipam:
  mode: kubernetes
k8s:
  requireIPv4PodCIDR: true

# Replace kube-proxy with Cilium
kubeProxyReplacement: true
%{if var.disable_kube_proxy}
# Enable health check server (healthz) for the kube-proxy replacement
kubeProxyReplacementHealthzBindAddr: ""0.0.0.0:10256""
%{endif~}

# Access to Kube API Server (mandatory if kube-proxy is disabled)
k8sServiceHost: ""127.0.0.1""
k8sServicePort: ""6444""

# Set Tunnel Mode or Native Routing Mode (supported by Hetzner CCM Route Controller)
routingMode: ""${var.cilium_routing_mode}""
%{if var.cilium_routing_mode == ""native""~}
# Set the native routable CIDR
ipv4NativeRoutingCIDR: ""${local.cilium_ipv4_native_routing_cidr}""

# Bypass iptables Connection Tracking for Pod traffic (only works in Native Routing Mode)
installNoConntrackIptablesRules: true
%{endif~}

endpointRoutes:
  # Enable use of per endpoint routes instead of routing via the cilium_host interface.
  enabled: true

loadBalancer:
  # Enable LoadBalancer & NodePort XDP Acceleration (direct routing (routingMode=native) is recommended to achieve optimal performance)
  acceleration: native

bpf:
  # Enable eBPF-based Masquerading (""The eBPF-based implementation is the most efficient implementation"")
  masquerade: true
%{if var.enable_wireguard}
encryption:
  enabled: true
  type: wireguard
%{endif~}
%{if var.cilium_egress_gateway_enabled}
egressGateway:
  enabled: true
%{endif~}

%{if var.cilium_hubble_enabled}
hubble:
  relay:
    enabled: true
  ui:
    enabled: true
  metrics:
    enabled:
%{for metric in var.cilium_hubble_metrics_enabled~}
      - ""${metric}""
%{endfor~}
%{endif~}

MTU: 1450
  EOT

  # Not to be confused with the other helm values, this is used for the calico.yaml kustomize patch
  # It also serves as a stub for a potential future use via helm values
  calico_values = var.calico_values != """" ? var.calico_values : <<EOT
kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: calico-node
  namespace: kube-system
  labels:
    k8s-app: calico-node
spec:
  template:
    spec:
      volumes:
        - name: flexvol-driver-host
          hostPath:
            type: DirectoryOrCreate
            path: /var/lib/kubelet/volumeplugins/nodeagent~uds
      containers:
        - name: calico-node
          env:
            - name: CALICO_IPV4POOL_CIDR
              value: ""${var.cluster_ipv4_cidr}""
            - name: FELIX_WIREGUARDENABLED
              value: ""${var.enable_wireguard}""

  EOT

  longhorn_values = var.longhorn_values != """" ? var.longhorn_values : <<EOT
defaultSettings:
%{if length(var.autoscaler_nodepools) != 0~}
  kubernetesClusterAutoscalerEnabled: true
%{endif~}
  defaultDataPath: /var/longhorn
persistence:
  defaultFsType: ${var.longhorn_fstype}
  defaultClassReplicaCount: ${var.longhorn_replica_count}
  %{if var.disable_hetzner_csi~}defaultClass: true%{else~}defaultClass: false%{endif~}
  EOT

  csi_driver_smb_values = var.csi_driver_smb_values != """" ? var.csi_driver_smb_values : <<EOT
  EOT

  nginx_values = var.nginx_values != """" ? var.nginx_values : <<EOT
controller:
  watchIngressWithoutClass: ""true""
  kind: ""Deployment""
  replicaCount: ${local.ingress_replica_count}
  config:
    ""use-forwarded-headers"": ""true""
    ""compute-full-forwarded-for"": ""true""
    ""use-proxy-protocol"": ""${!local.using_klipper_lb}""
%{if !local.using_klipper_lb~}
  service:
    annotations:
      ""load-balancer.hetzner.cloud/name"": ""${local.load_balancer_name}""
      ""load-balancer.hetzner.cloud/use-private-ip"": ""true""
      ""load-balancer.hetzner.cloud/disable-private-ingress"": ""true""
      ""load-balancer.hetzner.cloud/disable-public-network"": ""${var.load_balancer_disable_public_network}""
      ""load-balancer.hetzner.cloud/ipv6-disabled"": ""${var.load_balancer_disable_ipv6}""
      ""load-balancer.hetzner.cloud/location"": ""${var.load_balancer_location}""
      ""load-balancer.hetzner.cloud/type"": ""${var.load_balancer_type}""
      ""load-balancer.hetzner.cloud/uses-proxyprotocol"": ""${!local.using_klipper_lb}""
      ""load-balancer.hetzner.cloud/algorithm-type"": ""${var.load_balancer_algorithm_type}""
      ""load-balancer.hetzner.cloud/health-check-interval"": ""${var.load_balancer_health_check_interval}""
      ""load-balancer.hetzner.cloud/health-check-timeout"": ""${var.load_balancer_health_check_timeout}""
      ""load-balancer.hetzner.cloud/health-check-retries"": ""${var.load_balancer_health_check_retries}""
%{if var.lb_hostname != """"~}
      ""load-balancer.hetzner.cloud/hostname"": ""${var.lb_hostname}""
%{endif~}
%{endif~}
  EOT

  traefik_values = var.traefik_values != """" ? var.traefik_values : <<EOT
image:
  tag: ${var.traefik_image_tag}
deployment:
  replicas: ${local.ingress_replica_count}
globalArguments: []
service:
  enabled: true
  type: LoadBalancer
%{if !local.using_klipper_lb~}
  annotations:
    ""load-balancer.hetzner.cloud/name"": ""${local.load_balancer_name}""
    ""load-balancer.hetzner.cloud/use-private-ip"": ""true""
    ""load-balancer.hetzner.cloud/disable-private-ingress"": ""true""
    ""load-balancer.hetzner.cloud/disable-public-network"": ""${var.load_balancer_disable_public_network}""
    ""load-balancer.hetzner.cloud/ipv6-disabled"": ""${var.load_balancer_disable_ipv6}""
    ""load-balancer.hetzner.cloud/location"": ""${var.load_balancer_location}""
    ""load-balancer.hetzner.cloud/type"": ""${var.load_balancer_type}""
    ""load-balancer.hetzner.cloud/uses-proxyprotocol"": ""${!local.using_klipper_lb}""
    ""load-balancer.hetzner.cloud/algorithm-type"": ""${var.load_balancer_algorithm_type}""
    ""load-balancer.hetzner.cloud/health-check-interval"": ""${var.load_balancer_health_check_interval}""
    ""load-balancer.hetzner.cloud/health-check-timeout"": ""${var.load_balancer_health_check_timeout}""
    ""load-balancer.hetzner.cloud/health-check-retries"": ""${var.load_balancer_health_check_retries}""
%{if var.lb_hostname != """"~}
    ""load-balancer.hetzner.cloud/hostname"": ""${var.lb_hostname}""
%{endif~}
%{endif~}
ports:
  web:
%{if var.traefik_redirect_to_https~}
    redirectTo:
      port: websecure
%{endif~}
%{if !local.using_klipper_lb~}
    proxyProtocol:
      trustedIPs:
        - 127.0.0.1/32
        - 10.0.0.0/8
%{for ip in var.traefik_additional_trusted_ips~}
        - ""${ip}""
%{endfor~}
    forwardedHeaders:
      trustedIPs:
        - 127.0.0.1/32
        - 10.0.0.0/8
%{for ip in var.traefik_additional_trusted_ips~}
        - ""${ip}""
%{endfor~}
  websecure:
    proxyProtocol:
      trustedIPs:
        - 127.0.0.1/32
        - 10.0.0.0/8
%{for ip in var.traefik_additional_trusted_ips~}
        - ""${ip}""
%{endfor~}
    forwardedHeaders:
      trustedIPs:
        - 127.0.0.1/32
        - 10.0.0.0/8
%{for ip in var.traefik_additional_trusted_ips~}
        - ""${ip}""
%{endfor~}
%{endif~}
%{if var.traefik_additional_ports != """"~}
%{for option in var.traefik_additional_ports~}
  ${option.name}:
    port: ${option.port}
    expose:
      default: true
    exposedPort: ${option.exposedPort}
    protocol: TCP
%{if !local.using_klipper_lb~}
    proxyProtocol:
      trustedIPs:
        - 127.0.0.1/32
        - 10.0.0.0/8
%{for ip in var.traefik_additional_trusted_ips~}
        - ""${ip}""
%{endfor~}
    forwardedHeaders:
      trustedIPs:
        - 127.0.0.1/32
        - 10.0.0.0/8
%{for ip in var.traefik_additional_trusted_ips~}
        - ""${ip}""
%{endfor~}
%{endif~}
%{endfor~}
%{endif~}
%{if var.traefik_pod_disruption_budget~}
podDisruptionBudget:
  enabled: true
  maxUnavailable: 33%
%{endif~}
additionalArguments:
  - ""--providers.kubernetesingress.ingressendpoint.publishedservice=${local.ingress_controller_namespace}/traefik""
%{for option in var.traefik_additional_options~}
  - ""${option}""
%{endfor~}
%{if var.traefik_resource_limits~}
resources:
  requests:
    cpu: ""100m""
    memory: ""50Mi""
  limits:
    cpu: ""300m""
    memory: ""150Mi""
%{endif~}
%{if var.traefik_autoscaling~}
autoscaling:
  enabled: true
  minReplicas: ${local.ingress_replica_count}
  maxReplicas: ${local.ingress_max_replica_count}
%{endif~}
  EOT

  rancher_values = var.rancher_values != """" ? var.rancher_values : <<EOT
hostname: ""${var.rancher_hostname != """" ? var.rancher_hostname : var.lb_hostname}""
replicas: ${length(local.control_plane_nodes)}
bootstrapPassword: ""${length(var.rancher_bootstrap_password) == 0 ? resource.random_password.rancher_bootstrap[0].result : var.rancher_bootstrap_password}""
global:
  cattle:
    psp:
      enabled: false
  EOT

  cert_manager_values = var.cert_manager_values != """" ? var.cert_manager_values : <<EOT
installCRDs: true
  EOT

  kured_options = merge({
    ""reboot-command"" : ""/usr/bin/systemctl reboot"",
    ""pre-reboot-node-labels"" : ""kured=rebooting"",
    ""post-reboot-node-labels"" : ""kured=done"",
    ""period"" : ""5m"",
    ""reboot-sentinel"" : ""/sentinel/reboot-required""
  }, var.kured_options)

  k3s_registries_update_script = <<EOF
DATE=`date +%Y-%m-%d_%H-%M-%S`
if cmp -s /tmp/registries.yaml /etc/rancher/k3s/registries.yaml; then
  echo ""No update required to the registries.yaml file""
else
  echo ""Backing up /etc/rancher/k3s/registries.yaml to /tmp/registries_$DATE.yaml""
  cp /etc/rancher/k3s/registries.yaml /tmp/registries_$DATE.yaml
  echo ""Updated registries.yaml detected, restart of k3s service required""
  cp /tmp/registries.yaml /etc/rancher/k3s/registries.yaml
  if systemctl is-active --quiet k3s; then
    systemctl restart k3s || (echo ""Error: Failed to restart k3s. Restoring /etc/rancher/k3s/registries.yaml from backup"" && cp /tmp/registries_$DATE.yaml /etc/rancher/k3s/registries.yaml && systemctl restart k3s)
  elif systemctl is-active --quiet k3s-agent; then
    systemctl restart k3s-agent || (echo ""Error: Failed to restart k3s-agent. Restoring /etc/rancher/k3s/registries.yaml from backup"" && cp /tmp/registries_$DATE.yaml /etc/rancher/k3s/registries.yaml && systemctl restart k3s-agent)
  else
    echo ""No active k3s or k3s-agent service found""
  fi
  echo ""k3s service or k3s-agent service restarted successfully""
fi
EOF

  k3s_config_update_script = <<EOF
DATE=`date +%Y-%m-%d_%H-%M-%S`
if cmp -s /tmp/config.yaml /etc/rancher/k3s/config.yaml; then
  echo ""No update required to the config.yaml file""
else
  if [ -f ""/etc/rancher/k3s/config.yaml"" ]; then
    echo ""Backing up /etc/rancher/k3s/config.yaml to /tmp/config_$DATE.yaml""
    cp /etc/rancher/k3s/config.yaml /tmp/config_$DATE.yaml
  fi
  echo ""Updated config.yaml detected, restart of k3s service required""
  cp /tmp/config.yaml /etc/rancher/k3s/config.yaml
  if systemctl is-active --quiet k3s; then
    systemctl restart k3s || (echo ""Error: Failed to restart k3s. Restoring /etc/rancher/k3s/config.yaml from backup"" && cp /tmp/config_$DATE.yaml /etc/rancher/k3s/config.yaml && systemctl restart k3s)
  elif systemctl is-active --quiet k3s-agent; then
    systemctl restart k3s-agent || (echo ""Error: Failed to restart k3s-agent. Restoring /etc/rancher/k3s/config.yaml from backup"" && cp /tmp/config_$DATE.yaml /etc/rancher/k3s/config.yaml && systemctl restart k3s-agent)
  else
    echo ""No active k3s or k3s-agent service found""
  fi
  echo ""k3s service or k3s-agent service (re)started successfully""
fi
EOF

  cloudinit_write_files_common = <<EOT
# Script to rename the private interface to eth1 and unify NetworkManager connection naming
- path: /etc/cloud/rename_interface.sh
  content: |
    #!/bin/bash
    set -euo pipefail

    sleep 11

    INTERFACE=$(ip link show | awk '/^3:/{print $2}' | sed 's/://g')
    MAC=$(cat /sys/class/net/$INTERFACE/address)

    cat <<EOF > /etc/udev/rules.d/70-persistent-net.rules
    SUBSYSTEM==""net"", ACTION==""add"", DRIVERS==""?*"", ATTR{address}==""$MAC"", NAME=""eth1""
    EOF

    ip link set $INTERFACE down
    ip link set $INTERFACE name eth1
    ip link set eth1 up

    eth0_connection=$(nmcli -g GENERAL.CONNECTION device show eth0)
    nmcli connection modify ""$eth0_connection"" \
      con-name eth0 \
      connection.interface-name eth0

    eth1_connection=$(nmcli -g GENERAL.CONNECTION device show eth1)
    nmcli connection modify ""$eth1_connection"" \
      con-name eth1 \
      connection.interface-name eth1

    systemctl restart NetworkManager
  permissions: ""0744""

# Disable ssh password authentication
- content: |
    Port ${var.ssh_port}
    PasswordAuthentication no
    X11Forwarding no
    MaxAuthTries ${var.ssh_max_auth_tries}
    AllowTcpForwarding no
    AllowAgentForwarding no
    AuthorizedKeysFile .ssh/authorized_keys
  path: /etc/ssh/sshd_config.d/kube-hetzner.conf

# Set reboot method as ""kured""
- content: |
    REBOOT_METHOD=kured
  path: /etc/transactional-update.conf

# Create Rancher repo config
- content: |
    [rancher-k3s-common-stable]
    name=Rancher K3s Common (stable)
    baseurl=https://rpm.rancher.io/k3s/stable/common/microos/noarch
    enabled=1
    gpgcheck=1
    repo_gpgcheck=0
    gpgkey=https://rpm.rancher.io/public.key
  path: /etc/zypp/repos.d/rancher-k3s-common.repo

# Create the kube_hetzner_selinux.te file, that allows in SELinux to not interfere with various needed services
- path: /root/kube_hetzner_selinux.te
  content: |
    module kube_hetzner_selinux 1.0;

    require {
        type kernel_t, bin_t, kernel_generic_helper_t, iscsid_t, iscsid_exec_t, var_run_t, var_lib_t,
            init_t, unlabeled_t, systemd_logind_t, systemd_hostnamed_t, container_t,
            cert_t, container_var_lib_t, etc_t, usr_t, container_file_t, container_log_t,
            container_share_t, container_runtime_exec_t, container_runtime_t, var_log_t, proc_t, io_uring_t, fuse_device_t, http_port_t,
            container_var_run_t;
        class key { read view };
        class file { open read execute execute_no_trans create link lock rename write append setattr unlink getattr watch };
        class sock_file { watch write create unlink };
        class unix_dgram_socket create;
        class unix_stream_socket { connectto read write };
        class dir { add_name create getattr link lock read rename remove_name reparent rmdir setattr unlink search write watch };
        class lnk_file { read create };
        class system module_request;
        class filesystem associate;
        class bpf map_create;
        class io_uring sqpoll;
        class anon_inode { create map read write };
        class tcp_socket name_connect;
        class chr_file { open read write };
    }

    #============= kernel_generic_helper_t ==============
    allow kernel_generic_helper_t bin_t:file execute_no_trans;
    allow kernel_generic_helper_t kernel_t:key { read view };
    allow kernel_generic_helper_t self:unix_dgram_socket create;

    #============= iscsid_t ==============
    allow iscsid_t iscsid_exec_t:file execute;
    allow iscsid_t var_run_t:sock_file write;
    allow iscsid_t var_run_t:unix_stream_socket connectto;

    #============= init_t ==============
    allow init_t unlabeled_t:dir { add_name remove_name rmdir search };
    allow init_t unlabeled_t:lnk_file create;
    allow init_t container_t:file { open read };
    allow init_t container_file_t:file { execute execute_no_trans };
    allow init_t fuse_device_t:chr_file { open read write };
    allow init_t http_port_t:tcp_socket name_connect;

    #============= systemd_logind_t ==============
    allow systemd_logind_t unlabeled_t:dir search;

    #============= systemd_hostnamed_t ==============
    allow systemd_hostnamed_t unlabeled_t:dir search;

    #============= container_t ==============
    allow container_t { cert_t container_log_t }:dir read;
    allow container_t { cert_t container_log_t }:lnk_file read;
    allow container_t cert_t:file { read open };
    allow container_t container_var_lib_t:file { create open read write rename lock setattr getattr unlink };
    allow container_t etc_t:dir { add_name remove_name write create setattr watch };
    allow container_t etc_t:file { create setattr unlink write };
    allow container_t etc_t:sock_file { create unlink };
    allow container_t usr_t:dir { add_name create getattr link lock read rename remove_name reparent rmdir setattr unlink search write };
    allow container_t usr_t:file { append create execute getattr link lock read rename setattr unlink write };
    allow container_t container_file_t:file { open read write append getattr setattr };
    allow container_t container_file_t:sock_file watch;
    allow container_t container_log_t:file { open read write append getattr setattr watch };
    allow container_t container_share_t:dir { read write add_name remove_name };
    allow container_t container_share_t:file { read write create unlink };
    allow container_t container_runtime_exec_t:file { read execute execute_no_trans open };
    allow container_t container_runtime_t:unix_stream_socket { connectto read write };
    allow container_t kernel_t:system module_request;
    allow container_t var_log_t:dir { add_name write remove_name watch read };
    allow container_t var_log_t:file { create lock open read setattr write unlink getattr };
    allow container_t var_lib_t:dir { add_name write read };
    allow container_t var_lib_t:file { create lock open read setattr write getattr };
    allow container_t proc_t:filesystem associate;
    allow container_t self:bpf map_create;
    allow container_t self:io_uring sqpoll;
    allow container_t io_uring_t:anon_inode { create map read write };
    allow container_t container_var_run_t:dir { add_name remove_name write };
    allow container_t container_var_run_t:file { create open read rename unlink write };

# Create the k3s registries file if needed
%{if var.k3s_registries != """"}
# Create k3s registries file
- content: ${base64encode(var.k3s_registries)}
  encoding: base64
  path: /etc/rancher/k3s/registries.yaml
%{endif}

# Apply new DNS config
%{if length(var.dns_servers) > 0}
# Set prepare for manual dns config
- content: |
    [main]
    dns=none
  path: /etc/NetworkManager/conf.d/dns.conf

- content: |
    %{for server in var.dns_servers~}
    nameserver ${server}
    %{endfor}
  path: /etc/resolv.conf
  permissions: '0644'
%{endif}
EOT

  cloudinit_runcmd_common = <<EOT
# ensure that /var uses full available disk size, thanks to btrfs this is easy
- [btrfs, 'filesystem', 'resize', 'max', '/var']

# SELinux permission for the SSH alternative port
%{if var.ssh_port != 22}
# SELinux permission for the SSH alternative port.
- [semanage, port, '-a', '-t', ssh_port_t, '-p', tcp, '${var.ssh_port}']
%{endif}

# Create and apply the necessary SELinux module for kube-hetzner
- [checkmodule, '-M', '-m', '-o', '/root/kube_hetzner_selinux.mod', '/root/kube_hetzner_selinux.te']
- ['semodule_package', '-o', '/root/kube_hetzner_selinux.pp', '-m', '/root/kube_hetzner_selinux.mod']
- [semodule, '-i', '/root/kube_hetzner_selinux.pp']
- [setsebool, '-P', 'virt_use_samba', '1']
- [setsebool, '-P', 'domain_kernel_load_modules', '1']

# Disable rebootmgr service as we use kured instead
- [systemctl, disable, '--now', 'rebootmgr.service']

%{if length(var.dns_servers) > 0}
# Set the dns manually
- [systemctl, 'reload', 'NetworkManager']
%{endif}

# Bounds the amount of logs that can survive on the system
- [sed, '-i', 's/#SystemMaxUse=/SystemMaxUse=3G/g', /etc/systemd/journald.conf]
- [sed, '-i', 's/#MaxRetentionSec=/MaxRetentionSec=1week/g', /etc/systemd/journald.conf]

# Reduces the default number of snapshots from 2-10 number limit, to 4 and from 4-10 number limit important, to 2
- [sed, '-i', 's/NUMBER_LIMIT=""2-10""/NUMBER_LIMIT=""4""/g', /etc/snapper/configs/root]
- [sed, '-i', 's/NUMBER_LIMIT_IMPORTANT=""4-10""/NUMBER_LIMIT_IMPORTANT=""3""/g', /etc/snapper/configs/root]

# Allow network interface
- [chmod, '+x', '/etc/cloud/rename_interface.sh']

# Restart the sshd service to apply the new config
- [systemctl, 'restart', 'sshd']

# Make sure the network is up
- [systemctl, restart, NetworkManager]
- [systemctl, status, NetworkManager]
- [ip, route, add, default, via, '172.31.1.1', dev, 'eth0']

# Cleanup some logs
- [truncate, '-s', '0', '/var/log/audit/audit.log']
EOT
}
",locals,378,472.0,bdf09db947d3104cbaa75b4aa42219d93bc699a2,339f69d330ef062b2cc4ea100bc63e485d833e15,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/bdf09db947d3104cbaa75b4aa42219d93bc699a2/locals.tf#L378,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/339f69d330ef062b2cc4ea100bc63e485d833e15/locals.tf#L472,2023-08-08 22:20:29+02:00,2024-05-20 16:25:07-03:00,56,0
https://github.com/magma/magma,54,orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf,orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf,0,crap,# Set how frequently Prometheus should scrape,# Set how frequently Prometheus should scrape,"resource ""helm_release"" ""yace_exporter"" {
  count = var.cloudwatch_exporter_enabled ? 1 : 0

  name       = ""yace-exporter""
  namespace  = kubernetes_namespace.monitoring.metadata[0].name
  repository = ""https://mogaal.github.io/helm-charts/""
  chart      = ""prometheus-yace-exporter""
  timeout    = 600
  values = [<<EOT
# Default values for prometheus-yace-exporter.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: quay.io/invisionag/yet-another-cloudwatch-exporter
  tag: v0.25.0-alpha
  pullPolicy: IfNotPresent

nameOverride: """"
fullnameOverride: """"

service:
  type: ClusterIP
  port: 80
  annotations:
    prometheus.io/scrape: ""true""
    prometheus.io/port: ""5000""
  labels: {}

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: ""true""
  hosts:
    - host: chart-example.local
      paths: []

  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}

podAnnoatations: {}

podLabels: {}

aws:
  role:

  # The name of a pre-created secret in which AWS credentials are stored. When
  # set, aws_access_key_id is assumed to be in a field called access_key,
  # aws_secret_access_key is assumed to be in a field called secret_key, and the
  # session token, if it exists, is assumed to be in a field called
  # security_token
  secret:
    name:
    includesSessionToken: false

  # Note: Do not specify the aws_access_key_id and aws_secret_access_key if you specified role or secret.name before
  aws_access_key_id:
  aws_secret_access_key:

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  annotations: {}
  labels: {}
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

rbac:
  # Specifies whether RBAC resources should be created
  create: true

serviceMonitor:
  # When set true then use a ServiceMonitor to configure scraping
  enabled: true
  # Set the namespace the ServiceMonitor should be deployed
  namespace: ${var.orc8r_kubernetes_namespace}
  # Set how frequently Prometheus should scrape
  interval: 30s
  # Set targetPort for serviceMonitor
  port: http
  # Set path to cloudwatch-exporter telemtery-path
  telemetryPath: /metrics
  # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator
  # labels:
  # Set timeout for scrape
  timeout: 10s


config: |-
  discovery:
    exportedTagsOnMetrics:
      ec2:
        - Name
      ebs:
        - VolumeId
    jobs:
    - regions:
        - ${var.region}
      type: ""es""
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: FreeStorageSpace
          statistics:
          - 'Sum'
          period: 600
          length: 60
        - name: ClusterStatus.green
          statistics:
          - 'Minimum'
          period: 600
          length: 60
        - name: ClusterStatus.yellow
          statistics:
          - 'Maximum'
          period: 600
          length: 60
        - name: ClusterStatus.red
          statistics:
          - 'Maximum'
          period: 600
          length: 60
    - type: ""elb""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: HealthyHostCount
          statistics:
          - 'Minimum'
          period: 600
          length: 600
        - name: HTTPCode_Backend_4XX
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
    - type: ""ec2""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: NetworkIn
          statistics:
          - 'Sum'
          period: 600
          length: 600
        - name: NetworkOut
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
  EOT
  ]
  set_sensitive {
    name  = ""aws.aws_access_key_id""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].id : """"
  }
  set_sensitive {
    name  = ""aws.aws_secret_access_key""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].secret : """"
  }
}",resource,"resource ""helm_release"" ""yace_exporter"" {
  count = var.cloudwatch_exporter_enabled ? 1 : 0

  name       = ""yace-exporter""
  namespace  = kubernetes_namespace.monitoring[0].metadata[0].name
  repository = ""https://mogaal.github.io/helm-charts/""
  chart      = ""prometheus-yace-exporter""
  timeout    = 600
  values = [<<EOT
# Default values for prometheus-yace-exporter.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: quay.io/invisionag/yet-another-cloudwatch-exporter
  tag: v0.25.0-alpha
  pullPolicy: IfNotPresent

nameOverride: """"
fullnameOverride: """"

service:
  type: ClusterIP
  port: 80
  annotations:
    prometheus.io/scrape: ""true""
    prometheus.io/port: ""5000""
  labels: {}

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: ""true""
  hosts:
    - host: chart-example.local
      paths: []

  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}

podAnnoatations: {}

podLabels: {}

aws:
  role:

  # The name of a pre-created secret in which AWS credentials are stored. When
  # set, aws_access_key_id is assumed to be in a field called access_key,
  # aws_secret_access_key is assumed to be in a field called secret_key, and the
  # session token, if it exists, is assumed to be in a field called
  # security_token
  secret:
    name:
    includesSessionToken: false

  # Note: Do not specify the aws_access_key_id and aws_secret_access_key if you specified role or secret.name before
  aws_access_key_id:
  aws_secret_access_key:

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  annotations: {}
  labels: {}
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

rbac:
  # Specifies whether RBAC resources should be created
  create: true

serviceMonitor:
  # When set true then use a ServiceMonitor to configure scraping
  enabled: true
  # Set the namespace the ServiceMonitor should be deployed
  namespace: ${var.orc8r_kubernetes_namespace}
  # Set how frequently Prometheus should scrape
  interval: 30s
  # Set targetPort for serviceMonitor
  port: http
  # Set path to cloudwatch-exporter telemtery-path
  telemetryPath: /metrics
  # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator
  # labels:
  # Set timeout for scrape
  timeout: 10s


config: |-
  discovery:
    exportedTagsOnMetrics:
      ec2:
        - Name
      ebs:
        - VolumeId
    jobs:
    - regions:
        - ${var.region}
      type: ""es""
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: FreeStorageSpace
          statistics:
          - 'Sum'
          period: 600
          length: 60
        - name: ClusterStatus.green
          statistics:
          - 'Minimum'
          period: 600
          length: 60
        - name: ClusterStatus.yellow
          statistics:
          - 'Maximum'
          period: 600
          length: 60
        - name: ClusterStatus.red
          statistics:
          - 'Maximum'
          period: 600
          length: 60
    - type: ""elb""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: HealthyHostCount
          statistics:
          - 'Minimum'
          period: 600
          length: 600
        - name: HTTPCode_Backend_4XX
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
    - type: ""ec2""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: NetworkIn
          statistics:
          - 'Sum'
          period: 600
          length: 600
        - name: NetworkOut
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
  EOT
  ]
  set_sensitive {
    name  = ""aws.aws_access_key_id""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].id : """"
  }
  set_sensitive {
    name  = ""aws.aws_secret_access_key""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].secret : """"
  }
}",resource,154,154.0,cbee95c6b1cdad7ba277e02b920f715e21c97df6,b72801aa1cccc468273bbb99e5ec4fafa3c052c9,https://github.com/magma/magma/blob/cbee95c6b1cdad7ba277e02b920f715e21c97df6/orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf#L154,https://github.com/magma/magma/blob/b72801aa1cccc468273bbb99e5ec4fafa3c052c9/orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf#L154,2021-06-04 09:12:15+03:00,2022-03-10 13:24:52+00:00,3,0
https://github.com/Azure/sap-automation,27,deploy/terraform/terraform-units/modules/sap_system/app_tier/disk_logic.tf,deploy/terraform/terraform-units/modules/sap_system/app_tier/disk_logic.tf,0,xxx,"// host: xxx, LUN: #, type: sapusr, size: #","//Disks for Ansible 
 // host: xxx, LUN: #, type: sapusr, size: # ","locals {
      base_app_data_disk_per_dbnode = (local.application_server_count > 0) ? flatten(
    [
      for storage_type in local.app_sizing.storage : [
        for idx, disk_count in range(storage_type.count) : {
          suffix               = format(""-%s%02d"", storage_type.name, disk_count + var.options.resource_offset)
          storage_account_type = storage_type.disk_type,
          disk_size_gb         = storage_type.size_gb,
          //The following two lines are for Ultradisks only
          disk_iops_read_write      = try(storage_type.disk-iops-read-write, null)
          disk_mbps_read_write      = try(storage_type.disk-mbps-read-write, null)
          caching                   = storage_type.caching,
          write_accelerator_enabled = storage_type.write_accelerator
          type                      = storage_type.name
          lun                       = try(storage_type.lun_start, 0) + idx
        }
      ]
      if storage_type.name != ""os"" && !try(storage_type.append, false)
    ]
  ) : []


  append_app_data_disk_per_dbnode = (local.application_server_count > 0) ? flatten(
    [
      for storage_type in local.app_sizing.storage : [
        for idx, disk_count in range(storage_type.count) : {
          suffix               = format(""-%s%02d"", storage_type.name, storage_type.lun_start + disk_count + var.options.resource_offset)
          storage_account_type = storage_type.disk_type,
          disk_size_gb         = storage_type.size_gb,
          //The following two lines are for Ultradisks only
          disk_iops_read_write      = try(storage_type.disk-iops-read-write, null)
          disk_mbps_read_write      = try(storage_type.disk-mbps-read-write, null)
          caching                   = storage_type.caching,
          write_accelerator_enabled = storage_type.write_accelerator
          type                      = storage_type.name
          lun                       = storage_type.lun_start + idx
        }
      ]
      if storage_type.name != ""os"" && try(storage_type.append, false)
    ]
  ) : []

  app_data_disk_per_dbnode = distinct(concat(local.base_app_data_disk_per_dbnode, local.append_app_data_disk_per_dbnode))

  app_data_disks = flatten([
    for idx, datadisk in local.app_data_disk_per_dbnode : [
      for vm_counter in range(local.application_server_count) : {
        suffix                    = datadisk.suffix
        vm_index                  = vm_counter
        caching                   = datadisk.caching
        storage_account_type      = datadisk.storage_account_type
        disk_size_gb              = datadisk.disk_size_gb
        write_accelerator_enabled = datadisk.write_accelerator_enabled
        disk_iops_read_write      = datadisk.disk_iops_read_write
        disk_mbps_read_write      = datadisk.disk_mbps_read_write
        lun                       = datadisk.lun
        type                      = datadisk.type
      }
    ]
  ])

  base_scs_data_disk_per_dbnode = (local.enable_deployment) ? flatten(
    [
      for storage_type in local.scs_sizing.storage : [
        for idx, disk_count in range(storage_type.count) : {
          suffix               = format(""-%s%02d"", storage_type.name, disk_count + var.options.resource_offset)
          storage_account_type = storage_type.disk_type,
          disk_size_gb         = storage_type.size_gb,
          //The following two lines are for Ultradisks only
          disk_iops_read_write      = try(storage_type.disk-iops-read-write, null)
          disk_mbps_read_write      = try(storage_type.disk-mbps-read-write, null)
          caching                   = storage_type.caching,
          write_accelerator_enabled = storage_type.write_accelerator
          type                      = storage_type.name
          lun                       = try(storage_type.lun_start, 0) + idx
        }
      ]
      if storage_type.name != ""os"" && !try(storage_type.append, false)
    ]
  ) : []

  append_scs_data_disk_per_dbnode = (local.enable_deployment) ? flatten(
    [
      for storage_type in local.scs_sizing.storage : [
        for idx, disk_count in range(storage_type.count) : {
          suffix               = format(""-%s%02d"", storage_type.name, disk_count + var.options.resource_offset)
          storage_account_type = storage_type.disk_type,
          disk_size_gb         = storage_type.size_gb,
          //The following two lines are for Ultradisks only
          disk_iops_read_write      = try(storage_type.disk-iops-read-write, null)
          disk_mbps_read_write      = try(storage_type.disk-mbps-read-write, null)
          caching                   = storage_type.caching,
          write_accelerator_enabled = storage_type.write_accelerator
          type                      = storage_type.name
          lun                       = storage_type.lun_start + idx
        }
      ]
      if storage_type.name != ""os"" && try(storage_type.append, false)
    ]
  ) : []

  scs_data_disk_per_dbnode = distinct(concat(local.base_scs_data_disk_per_dbnode, local.append_scs_data_disk_per_dbnode))

  scs_data_disks = flatten([
    for idx, datadisk in local.scs_data_disk_per_dbnode : [
      for vm_counter in range(local.scs_server_count) : {
        suffix                    = datadisk.suffix
        vm_index                  = vm_counter
        caching                   = datadisk.caching
        storage_account_type      = datadisk.storage_account_type
        disk_size_gb              = datadisk.disk_size_gb
        write_accelerator_enabled = datadisk.write_accelerator_enabled
        disk_iops_read_write      = datadisk.disk_iops_read_write
        disk_mbps_read_write      = datadisk.disk_mbps_read_write
        type                      = datadisk.type
        lun                       = datadisk.lun

      }
    ]
  ])

  base_web_data_disk_per_dbnode = (local.webdispatcher_count > 0) ? flatten(
    [
      for storage_type in local.web_sizing.storage : [
        for idx, disk_count in range(storage_type.count) : {
          suffix               = format(""-%s%02d"", storage_type.name, disk_count + var.options.resource_offset)
          storage_account_type = storage_type.disk_type,
          disk_size_gb         = storage_type.size_gb,
          //The following two lines are for Ultradisks only
          disk_iops_read_write      = try(storage_type.disk-iops-read-write, null)
          disk_mbps_read_write      = try(storage_type.disk-mbps-read-write, null)
          caching                   = storage_type.caching,
          write_accelerator_enabled = storage_type.write_accelerator
          type                      = storage_type.name
          lun                       = try(storage_type.lun_start, 0) + idx

        }
      ]
      if storage_type.name != ""os"" && !try(storage_type.append, false)
    ]
  ) : []

  append_web_data_disk_per_dbnode = (local.webdispatcher_count > 0) ? flatten(
    [
      for storage_type in local.web_sizing.storage : [
        for idx, disk_count in range(storage_type.count) : {
          suffix               = format(""-%s%02d"", storage_type.name, disk_count + var.options.resource_offset)
          storage_account_type = storage_type.disk_type,
          disk_size_gb         = storage_type.size_gb,
          //The following two lines are for Ultradisks only
          disk_iops_read_write      = try(storage_type.disk-iops-read-write, null)
          disk_mbps_read_write      = try(storage_type.disk-mbps-read-write, null)
          caching                   = storage_type.caching,
          write_accelerator_enabled = storage_type.write_accelerator
          type                      = storage_type.name
          lun                       = storage_type.lun_start + idx
        }
      ]
      if storage_type.name != ""os"" && try(storage_type.append, false)
    ]
  ) : []

  web_data_disk_per_dbnode = distinct(concat(local.base_web_data_disk_per_dbnode, local.append_web_data_disk_per_dbnode))
  web_data_disks = flatten([
    for idx, datadisk in local.web_data_disk_per_dbnode : [
      for vm_counter in range(local.webdispatcher_count) : {
        suffix                    = datadisk.suffix
        vm_index                  = vm_counter
        caching                   = datadisk.caching
        storage_account_type      = datadisk.storage_account_type
        disk_size_gb              = datadisk.disk_size_gb
        write_accelerator_enabled = datadisk.write_accelerator_enabled
        disk_iops_read_write      = datadisk.disk_iops_read_write
        disk_mbps_read_write      = datadisk.disk_mbps_read_write
        lun                       = datadisk.lun
        type                      = datadisk.type
      }
    ]
  ])

  full_appserver_names = distinct(flatten([for vm in var.naming.virtualmachine_names.APP_VMNAME :
    format(""%s%s%s%s"", local.prefix, var.naming.separator, vm, local.resource_suffixes.vm)]
  ))

  full_scsserver_names = distinct(flatten([for vm in var.naming.virtualmachine_names.SCS_VMNAME :
    format(""%s%s%s%s"", local.prefix, var.naming.separator, vm, local.resource_suffixes.vm)]
  ))

  full_webserver_names = distinct(flatten([for vm in var.naming.virtualmachine_names.WEB_VMNAME :
    format(""%s%s%s%s"", local.prefix, var.naming.separator, vm, local.resource_suffixes.vm)]
  ))

  //Disks for Ansible
  // host: xxx, LUN: #, type: sapusr, size: #

  app_disks_ansible = distinct(flatten([for vm in var.naming.virtualmachine_names.APP_COMPUTERNAME : [
    for idx, datadisk in local.app_data_disk_per_dbnode :
    format(""{ host: '%s', LUN: %d, type: '%s' }"", vm, idx, ""sap"")
  ]]))

  scs_disks_ansible = distinct(flatten([for vm in var.naming.virtualmachine_names.SCS_COMPUTERNAME : [
    for idx, datadisk in local.scs_data_disk_per_dbnode :
    format(""{ host: '%s', LUN: %d, type: '%s' }"", vm, idx, ""sap"")
  ]]))

  web_disks_ansible = distinct(flatten([for vm in var.naming.virtualmachine_names.WEB_COMPUTERNAME : [
    for idx, datadisk in local.web_data_disk_per_dbnode :
    format(""{ host: '%s', LUN: %d, type: '%s' }"", vm, idx, ""sap"")
  ]]))

}",locals,"locals {
  base_app_data_disk_per_node          = (local.application_server_count > 0) ? flatten(
                                           [
                                             for storage_type in local.app_sizing.storage : [
                                               for idx, disk_count in range(storage_type.count) : {
                                                 suffix               = format(""-%s%02d"", storage_type.name, disk_count + var.options.resource_offset)
                                                 storage_account_type = storage_type.disk_type,
                                                 disk_size_gb         = storage_type.size_gb,
                                                 //The following two lines are for Ultradisks only
                                                 disk_iops_read_write      = try(storage_type.disk_iops_read_write, null)
                                                 disk_mbps_read_write      = try(storage_type.disk_mbps_read_write, null)
                                                 caching                   = storage_type.caching,
                                                 write_accelerator_enabled = storage_type.write_accelerator
                                                 type                      = storage_type.name
                                                 lun                       = try(storage_type.lun_start, 0) + idx
                                               }
                                             ]
                                             if storage_type.name != ""os"" && !try(storage_type.append, false)
                                           ] ) : (
                                           []
                                         )


  append_app_data_disk_per_node        = (local.application_server_count > 0) ? flatten(
                                          [
                                            for storage_type in local.app_sizing.storage : [
                                              for idx, disk_count in range(storage_type.count) : {
                                                suffix = format(""-%s%02d"",
                                                  storage_type.name,
                                                  storage_type.name_offset + disk_count + var.options.resource_offset
                                                )
                                                storage_account_type = storage_type.disk_type,
                                                disk_size_gb         = storage_type.size_gb,
                                                //The following two lines are for Ultradisks only
                                                disk_iops_read_write      = try(storage_type.disk_iops_read_write, null)
                                                disk_mbps_read_write      = try(storage_type.disk_mbps_read_write, null)
                                                caching                   = storage_type.caching,
                                                write_accelerator_enabled = storage_type.write_accelerator
                                                type                      = storage_type.name
                                                lun                       = storage_type.lun_start + idx
                                              }
                                            ]
                                            if storage_type.name != ""os"" && try(storage_type.append, false)
                                          ] ) : (
                                          []
                                         )

  app_data_disk_per_node               = distinct(
                                           concat(
                                             local.base_app_data_disk_per_node,
                                             local.append_app_data_disk_per_node
                                           )
                                         )

  app_data_disks                       = flatten(
                                           [
                                               for idx, datadisk in local.app_data_disk_per_node : [
                                                 for vm_counter in range(local.application_server_count) : {
                                                   suffix                    = datadisk.suffix
                                                   vm_index                  = vm_counter
                                                   caching                   = datadisk.caching
                                                   storage_account_type      = datadisk.storage_account_type
                                                   disk_size_gb              = datadisk.disk_size_gb
                                                   write_accelerator_enabled = datadisk.write_accelerator_enabled
                                                   disk_iops_read_write      = datadisk.disk_iops_read_write
                                                   disk_mbps_read_write      = datadisk.disk_mbps_read_write
                                                   lun                       = datadisk.lun
                                                   type                      = datadisk.type
                                                 }
                                               ]
                                            ]
                                         )

  base_scs_data_disk_per_node          = (local.enable_deployment) ? flatten(
                                         [
                                           for storage_type in local.scs_sizing.storage : [
                                             for idx, disk_count in range(storage_type.count) : {
                                               suffix               = format(""-%s%02d"", storage_type.name, disk_count + var.options.resource_offset)
                                               storage_account_type = storage_type.disk_type,
                                               disk_size_gb         = storage_type.size_gb,
                                               //The following two lines are for Ultradisks only
                                               disk_iops_read_write      = try(storage_type.disk_iops_read_write, null)
                                               disk_mbps_read_write      = try(storage_type.disk_mbps_read_write, null)
                                               caching                   = storage_type.caching,
                                               write_accelerator_enabled = storage_type.write_accelerator
                                               type                      = storage_type.name
                                               lun                       = try(storage_type.lun_start, 0) + idx
                                             }
                                           ]
                                           if storage_type.name != ""os"" && !try(storage_type.append, false)
                                         ]) : (
                                         []
                                         )

  append_scs_data_disk_per_node        = (local.enable_deployment) ? flatten(
                                             [
                                               for storage_type in local.scs_sizing.storage : [
                                                 for idx, disk_count in range(storage_type.count) : {
                                                   suffix = format(""-%s%02d"",
                                                     storage_type.name,
                                                     storage_type.name_offset + disk_count + var.options.resource_offset
                                                   )
                                                   storage_account_type = storage_type.disk_type,
                                                   disk_size_gb         = storage_type.size_gb,
                                                   //The following two lines are for Ultradisks only
                                                   disk_iops_read_write      = try(storage_type.disk_iops_read_write, null)
                                                   disk_mbps_read_write      = try(storage_type.disk_mbps_read_write, null)
                                                   caching                   = storage_type.caching,
                                                   write_accelerator_enabled = storage_type.write_accelerator
                                                   type                      = storage_type.name
                                                   lun                       = storage_type.lun_start + idx
                                                 }
                                               ]
                                               if storage_type.name != ""os"" && try(storage_type.append, false)
                                             ] ) : (
                                             []
                                           )

  scs_data_disk_per_node               = distinct(
                                           concat(
                                             local.base_scs_data_disk_per_node,
                                             local.append_scs_data_disk_per_node
                                           )
                                         )

  scs_data_disks                       = flatten(
                                           [
                                             for idx, datadisk in local.scs_data_disk_per_node : [
                                               for vm_counter in range(local.scs_server_count) : {
                                                 suffix                    = datadisk.suffix
                                                 vm_index                  = vm_counter
                                                 caching                   = datadisk.caching
                                                 storage_account_type      = datadisk.storage_account_type
                                                 disk_size_gb              = datadisk.disk_size_gb
                                                 write_accelerator_enabled = datadisk.write_accelerator_enabled
                                                 disk_iops_read_write      = datadisk.disk_iops_read_write
                                                 disk_mbps_read_write      = datadisk.disk_mbps_read_write
                                                 type                      = datadisk.type
                                                 lun                       = datadisk.lun

                                               }
                                             ]
                                           ]
                                         )

  base_web_data_disk_per_node          = (local.webdispatcher_count > 0) ? flatten(
                                         [
                                           for storage_type in local.web_sizing.storage : [
                                             for idx, disk_count in range(storage_type.count) : {
                                               suffix               = format(""-%s%02d"", storage_type.name, disk_count + var.options.resource_offset)
                                               storage_account_type = storage_type.disk_type,
                                               disk_size_gb         = storage_type.size_gb,
                                               //The following two lines are for Ultradisks only
                                               disk_iops_read_write      = try(storage_type.disk_iops_read_write, null)
                                               disk_mbps_read_write      = try(storage_type.disk_mbps_read_write, null)
                                               caching                   = storage_type.caching,
                                               write_accelerator_enabled = storage_type.write_accelerator
                                               type                      = storage_type.name
                                               lun                       = try(storage_type.lun_start, 0) + idx

                                             }
                                           ]
                                           if storage_type.name != ""os"" && !try(storage_type.append, false)
                                         ] ) : (
                                         []
                                       )

  append_web_data_disk_per_node        = (local.webdispatcher_count > 0) ? flatten(
                                          [
                                            for storage_type in local.web_sizing.storage : [
                                              for idx, disk_count in range(storage_type.count) : {
                                                suffix = format(""-%s%02d"",
                                                  storage_type.name,
                                                  storage_type.name_offset + disk_count + var.options.resource_offset
                                                )
                                                storage_account_type = storage_type.disk_type,
                                                disk_size_gb         = storage_type.size_gb,
                                                //The following two lines are for Ultradisks only
                                                disk_iops_read_write      = try(storage_type.disk_iops_read_write, null)
                                                disk_mbps_read_write      = try(storage_type.disk_mbps_read_write, null)
                                                caching                   = storage_type.caching,
                                                write_accelerator_enabled = storage_type.write_accelerator
                                                type                      = storage_type.name
                                                lun                       = storage_type.lun_start + idx
                                              }
                                            ]
                                            if storage_type.name != ""os"" && try(storage_type.append, false)
                                          ] ) : (
                                          []
                                        )

  web_data_disk_per_node               = distinct(
                                           concat(
                                              local.base_web_data_disk_per_node,
                                              local.append_web_data_disk_per_node
                                           )
                                         )

  web_data_disks                       = flatten(
                                           [
                                             for idx, datadisk in local.web_data_disk_per_node : [
                                               for vm_counter in range(local.webdispatcher_count) : {
                                                 suffix                    = datadisk.suffix
                                                 vm_index                  = vm_counter
                                                 caching                   = datadisk.caching
                                                 storage_account_type      = datadisk.storage_account_type
                                                 disk_size_gb              = datadisk.disk_size_gb
                                                 write_accelerator_enabled = datadisk.write_accelerator_enabled
                                                 disk_iops_read_write      = datadisk.disk_iops_read_write
                                                 disk_mbps_read_write      = datadisk.disk_mbps_read_write
                                                 lun                       = datadisk.lun
                                                 type                      = datadisk.type
                                               }
                                             ]
                                           ]
                                         )

  full_appserver_names                 = distinct(flatten([for vm in var.naming.virtualmachine_names.APP_VMNAME :
                                           format(""%s%s%s%s"", local.prefix, var.naming.separator, vm, local.resource_suffixes.vm)]
                                         ))

  full_scsserver_names                 = distinct(flatten([for vm in var.naming.virtualmachine_names.SCS_VMNAME :
                                           format(""%s%s%s%s"", local.prefix, var.naming.separator, vm, local.resource_suffixes.vm)]
                                         ))

  full_webserver_names                 = distinct(flatten([for vm in var.naming.virtualmachine_names.WEB_VMNAME :
                                           format(""%s%s%s%s"", local.prefix, var.naming.separator, vm, local.resource_suffixes.vm)]
                                         ))

  //Disks for Ansible
  // host: xxx, LUN: #, type: sapusr, size: #

  app_disks_ansible                    = distinct(flatten([for vm in var.naming.virtualmachine_names.APP_COMPUTERNAME : [
                                           for idx, datadisk in local.app_data_disk_per_node :
                                           format(""{ host: '%s', LUN: %d, type: '%s' }"", vm, datadisk.lun, datadisk.type)
                                         ]]))

  scs_disks_ansible                    = distinct(flatten([for vm in var.naming.virtualmachine_names.SCS_COMPUTERNAME : [
                                           for idx, datadisk in local.scs_data_disk_per_node :
                                           format(""{ host: '%s', LUN: %d, type: '%s' }"", vm, datadisk.lun, datadisk.type)
                                         ]]))

  web_disks_ansible                    = distinct(flatten([for vm in var.naming.virtualmachine_names.WEB_COMPUTERNAME : [
                                           for idx, datadisk in local.web_data_disk_per_node :
                                           format(""{ host: '%s', LUN: %d, type: '%s' }"", vm, datadisk.lun, datadisk.type)
                                         ]]))

}
",locals,194,231.0,6ff0b891114c36d3aeccb850d830b698cd1fe52a,2b80acda1a988a8b1d5dea10945ead21d27fb01c,https://github.com/Azure/sap-automation/blob/6ff0b891114c36d3aeccb850d830b698cd1fe52a/deploy/terraform/terraform-units/modules/sap_system/app_tier/disk_logic.tf#L194,https://github.com/Azure/sap-automation/blob/2b80acda1a988a8b1d5dea10945ead21d27fb01c/deploy/terraform/terraform-units/modules/sap_system/app_tier/disk_logic.tf#L231,2021-11-17 19:29:07+02:00,2023-12-04 19:30:28+02:00,8,0
https://github.com/wireapp/wire-server-deploy,56,terraform/modules/sft/srv-announcer-iam.tf,terraform/modules/sft/srv-announcer-iam.tf,0,# todo,# TODO: Add a comment explaining this. Does this mean,"force_destroy = true # TODO: Add a comment explaining this. Does this mean 
 # changing this user will make existing srv announcements 
 # fail?","resource ""aws_iam_user"" ""srv-announcer"" {
  name          = ""${var.environment}-srv-announcer""
  force_destroy = true # TODO: Add a comment explaining this. Does this mean
                       # changing this user will make existing srv announcements
                       # fail?
}
",resource,"resource ""aws_iam_user"" ""srv-announcer"" {
  name          = ""${var.environment}-srv-announcer""
  force_destroy = true # TODO: Add a comment explaining this. Does this mean
                       # changing this user will make existing srv announcements
                       # fail?
}
",resource,3,3.0,ddf967a4a5d4f3e8f6ffbb9a93b47ee2089f60c0,ddf967a4a5d4f3e8f6ffbb9a93b47ee2089f60c0,https://github.com/wireapp/wire-server-deploy/blob/ddf967a4a5d4f3e8f6ffbb9a93b47ee2089f60c0/terraform/modules/sft/srv-announcer-iam.tf#L3,https://github.com/wireapp/wire-server-deploy/blob/ddf967a4a5d4f3e8f6ffbb9a93b47ee2089f60c0/terraform/modules/sft/srv-announcer-iam.tf#L3,2020-09-18 11:48:52+02:00,2020-09-18 11:48:52+02:00,1,0
https://github.com/Worklytics/psoxy,1448,infra/examples-dev/aws-all/main.tf,infra/examples-dev/aws-all/main.tf,0,todo,"#  as of June 2023, this just outputs TODO files, but would provision connections via future","## Worklytics connection configuration 
 #  as of June 2023, this just outputs TODO files, but would provision connections via future 
 #  Worklytics API / Terraform provider ","locals {
  all_connectors = merge(local.api_connectors, local.bulk_connectors)
  all_instances  = merge(module.psoxy.bulk_connector_instances, module.psoxy.api_connector_instances)
}
",locals,"locals {
  all_connectors = merge(local.api_connectors, local.bulk_connectors)
  all_instances  = merge(module.psoxy.bulk_connector_instances, module.psoxy.api_connector_instances)
}
",locals,110,146.0,f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e,8f6786b90a7f0fcb6120cc8f822fd07cab49697f,https://github.com/Worklytics/psoxy/blob/f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e/infra/examples-dev/aws-all/main.tf#L110,https://github.com/Worklytics/psoxy/blob/8f6786b90a7f0fcb6120cc8f822fd07cab49697f/infra/examples-dev/aws-all/main.tf#L146,2023-06-16 14:08:45-07:00,2024-04-23 19:32:11-07:00,87,0
https://github.com/google/go-cloud,79,internal/cmd/gocdk/internal/static/_assets/resource/pubsub/awssnssqs.tf,internal/cmd/gocdk/internal/static/_assets/resource/pubsub/awssnssqs.tf,0,# todo,# TODO(rvangent): Add comments explaining.,# TODO(rvangent): Add comments explaining.,"locals {
  awssns_topic_url        = ""awssns://${aws_sns_topic.topic.arn}?region=${local.aws_region}""
  awssqs_subscription_url = ""awssqs://${replace(aws_sqs_queue.queue.id, ""https://"", """")}?region=${local.aws_region}""
}
",locals,"locals {
  # The Go CDK URL for the topic: https://gocloud.dev/howto/pubsub/publish#sns.
  awssns_topic_url = ""awssns://${aws_sns_topic.topic.arn}?region=${local.aws_region}""
  # The Go CDK URL for the subscription: https://gocloud.dev/howto/pubsub/subscribe#sqs.
  awssqs_subscription_url = ""awssqs://${replace(aws_sqs_queue.queue.id, ""https://"", """")}?region=${local.aws_region}""
}
",locals,1,,6e0b8864bf31d410dbf465dd4f2acbdd49795c2c,be325655f0239f2338aa491e53a73f1fe479bd03,https://github.com/google/go-cloud/blob/6e0b8864bf31d410dbf465dd4f2acbdd49795c2c/internal/cmd/gocdk/internal/static/_assets/resource/pubsub/awssnssqs.tf#L1,https://github.com/google/go-cloud/blob/be325655f0239f2338aa491e53a73f1fe479bd03/internal/cmd/gocdk/internal/static/_assets/resource/pubsub/awssnssqs.tf,2019-07-01 10:27:24-07:00,2019-07-16 10:35:48-07:00,3,1
https://github.com/Azure/sap-automation,410,deploy/terraform/terraform-units/modules/sap_namegenerator/variables_global.tf,deploy/terraform/terraform-units/modules/sap_namegenerator/variables_global.tf,0,//todo,//Todo: Add to Documentation,//Todo: Add to Documentation,"variable ""azlimits"" {
  description = ""Name length for resources""
  default = {
    asr         = 50
    aaa         = 50
    acr         = 49
    afw         = 50
    rg          = 80
    kv          = 24
    stgaccnt    = 24
    vnet        = 38
    nsg         = 80
    snet        = 80
    nic         = 80
    vml         = 64
    vmw         = 15
    vm          = 80
    functionapp = 60
    lb          = 80
    lbrule      = 80
    evh         = 50
    la          = 63
    pip         = 80
    peer        = 80
    gen         = 24
  }
}
",variable,"variable ""azlimits"" {
  description = ""Name length for resources""
  default = {
    asr         = 50
    aaa         = 50
    acr         = 49
    afw         = 50
    rg          = 80
    kv          = 24
    stgaccnt    = 24
    vnet        = 38
    nsg         = 80
    snet        = 80
    nic         = 80
    vml         = 64
    vmw         = 15
    vm          = 80
    functionapp = 60
    lb          = 80
    lbrule      = 80
    evh         = 50
    la          = 63
    pip         = 80
    peer        = 80
    gen         = 24
  }
}
",variable,137,156.0,7b60debc6f454bd8ed6f8d77622ac31feb6cbea1,db20ac2a47d9d00329385330cb4af6b3c726c400,https://github.com/Azure/sap-automation/blob/7b60debc6f454bd8ed6f8d77622ac31feb6cbea1/deploy/terraform/terraform-units/modules/sap_namegenerator/variables_global.tf#L137,https://github.com/Azure/sap-automation/blob/db20ac2a47d9d00329385330cb4af6b3c726c400/deploy/terraform/terraform-units/modules/sap_namegenerator/variables_global.tf#L156,2022-06-01 12:40:16+05:30,2024-03-11 23:15:11+05:30,28,0
https://github.com/kubernetes/k8s.io,135,infra/gcp/clusters/projects/k8s-infra-public-pii/main.tf,infra/gcp/clusters/projects/k8s-infra-public-pii/main.tf,0,/*todo,"/*TODO(ameukam): This not working. possible conflict between both bindings data ""google_iam_policy"" ""storage_policy_objectadmin"" {   binding {     role = ""roles/storage.objectAdmin""     members = [       ""group:cloud-storage-analytics@google.com"",     ]   } }  resource ""google_storage_bucket_iam_policy"" ""analytics_objectadmin_policy"" {   bucket      = google_storage_bucket.audit-logs-gcs.name   policy_data = data.google_iam_policy.storage_policy_objectadmin.policy_data }    data ""google_iam_policy"" ""storage_policy_legacybucketwriter"" {   binding {     role = ""roles/storage.legacyBucketWriter""     members = [       ""group:cloud-storage-analytics@google.com"",     ]   } }  resource ""google_storage_bucket_iam_policy"" ""analytics_legacybucketwriter_policy"" {   bucket      = google_storage_bucket.audit-logs-gcs.name   policy_data = data.google_iam_policy.storage_policy_legacybucketwriter.policy_data }*/","/* TODO(ameukam): This not working. possible conflict between both bindings 
 data ""google_iam_policy"" ""storage_policy_objectadmin"" { 
 binding { 
 role = ""roles/storage.objectAdmin"" 
 members = [ 
 ""group:cloud-storage-analytics@google.com"", 
 ] 
 } 
 } 
  
 resource ""google_storage_bucket_iam_policy"" ""analytics_objectadmin_policy"" { 
 bucket      = google_storage_bucket.audit-logs-gcs.name 
 policy_data = data.google_iam_policy.storage_policy_objectadmin.policy_data 
 } 
  
  
  
 data ""google_iam_policy"" ""storage_policy_legacybucketwriter"" { 
 binding { 
 role = ""roles/storage.legacyBucketWriter"" 
 members = [ 
 ""group:cloud-storage-analytics@google.com"", 
 ] 
 } 
 } 
  
 resource ""google_storage_bucket_iam_policy"" ""analytics_legacybucketwriter_policy"" { 
 bucket      = google_storage_bucket.audit-logs-gcs.name 
 policy_data = data.google_iam_policy.storage_policy_legacybucketwriter.policy_data 
 } */  
 // Allow ready-only access to k8s-infra-gcs-access-logs@kubernetes.io","resource ""google_storage_bucket_iam_member"" ""artificats-gcs-logs"" {
  bucket = google_storage_bucket.audit-logs-gcs.name
  role   = ""roles/storage.objectViewer""
  member = ""group:k8s-infra-gcs-access-logs@kubernetes.io""
}
",resource,"resource ""google_storage_bucket_iam_member"" ""artificats-gcs-logs"" {
  bucket = google_storage_bucket.audit-logs-gcs.name
  role   = ""roles/storage.objectViewer""
  member = ""group:k8s-infra-gcs-access-logs@kubernetes.io""
}
",resource,91,,4f544947172ab6adcd01adf85da0731d72dd786f,9c644d502df226cbde56da64174b9bed3a267e6d,https://github.com/kubernetes/k8s.io/blob/4f544947172ab6adcd01adf85da0731d72dd786f/infra/gcp/clusters/projects/k8s-infra-public-pii/main.tf#L91,https://github.com/kubernetes/k8s.io/blob/9c644d502df226cbde56da64174b9bed3a267e6d/infra/gcp/clusters/projects/k8s-infra-public-pii/main.tf,2021-06-14 20:45:34+02:00,2021-06-21 21:21:33+02:00,2,1
https://github.com/cookpad/terraform-aws-eks,91,versions.tf,versions.tf,0,hack,# Run hack/versions k8sVersionNumber > versions.tf,"# Run hack/versions k8sVersionNumber > versions.tf 
 # to generate the latest values for this","locals {
  versions = {
    k8s                = ""1.25""
    vpc_cni            = ""v1.16.2-eksbuild.1""
    kube_proxy         = ""v1.25.9-eksbuild.1""
    coredns            = ""v1.9.3-eksbuild.9""
    aws_ebs_csi_driver = ""v1.27.0-eksbuild.1""
  }
}
",locals,"locals {
  versions = {
    k8s                = ""1.25""
    vpc_cni            = ""v1.16.2-eksbuild.1""
    kube_proxy         = ""v1.25.9-eksbuild.1""
    coredns            = ""v1.9.3-eksbuild.9""
    aws_ebs_csi_driver = ""v1.27.0-eksbuild.1""
  }
}
",locals,1,1.0,24dcbdb6ab6cae5024ba04e856990cb4a15a6976,24dcbdb6ab6cae5024ba04e856990cb4a15a6976,https://github.com/cookpad/terraform-aws-eks/blob/24dcbdb6ab6cae5024ba04e856990cb4a15a6976/versions.tf#L1,https://github.com/cookpad/terraform-aws-eks/blob/24dcbdb6ab6cae5024ba04e856990cb4a15a6976/versions.tf#L1,2024-02-16 15:23:52+00:00,2024-02-16 15:23:52+00:00,1,0
https://github.com/zenml-io/mlstacks,1,eks-s3-seldon-mlflow/eks.tf,eks-s3-seldon-mlflow/eks.tf,0,hack,# derived from our current zenhacks cluster,# derived from our current zenhacks cluster,"module ""eks"" {
  source  = ""terraform-aws-modules/eks/aws""
  version = ""17.23.0""

  cluster_name    = ""${local.prefix}-${local.eks.cluster_name}""
  cluster_version = ""1.22""
  subnets         = module.vpc.private_subnets
  enable_irsa     = true
  tags            = local.tags

  vpc_id = module.vpc.vpc_id

  node_groups_defaults = {
    ami_type  = ""AL2_x86_64""
    disk_size = 50
  }


  node_groups = {
    main = {
      desired_capacity = 1
      max_capacity     = 4
      min_capacity     = 1

      # derived from our current zenhacks cluster
      instance_types = [""t3.medium""]
      update_config = {
        max_unavailable_percentage = 50
      }
    }
  }

  # allowing worker nodes access to S3 buckets  
  workers_additional_policies = [
    ""arn:aws:iam::aws:policy/AmazonS3FullAccess"",
    ""arn:aws:iam::aws:policy/AutoScalingFullAccess"",
  ]
}
",module,"module ""eks"" {
  source  = ""terraform-aws-modules/eks/aws""
  version = ""17.23.0""

  cluster_name    = ""${local.prefix}-${local.eks.cluster_name}""
  cluster_version = ""1.22""
  subnets         = module.vpc.private_subnets
  enable_irsa     = true
  tags            = local.tags

  vpc_id = module.vpc.vpc_id

  node_groups_defaults = {
    ami_type  = ""AL2_x86_64""
    disk_size = 50
  }


  node_groups = {
    main = {
      desired_capacity = 1
      max_capacity     = 4
      min_capacity     = 1

      instance_types = [""t3.medium""]
      update_config = {
        max_unavailable_percentage = 50
      }
    }
  }

  # allowing worker nodes access to other resources  
  workers_additional_policies = [
    ""arn:aws:iam::aws:policy/AmazonS3FullAccess"",
    ""arn:aws:iam::aws:policy/AutoScalingFullAccess"",
    ""arn:aws:iam::aws:policy/SecretsManagerReadWrite"",
    ""arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"",
    ""arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"",
    ""arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"",
  ]
}
",module,27,,1cca458cd5abfbecc1beecabd75048f5cc8e0ac0,1e4b05bffab0bf21a85b4c7a9025de599f10fdd4,https://github.com/zenml-io/mlstacks/blob/1cca458cd5abfbecc1beecabd75048f5cc8e0ac0/eks-s3-seldon-mlflow/eks.tf#L27,https://github.com/zenml-io/mlstacks/blob/1e4b05bffab0bf21a85b4c7a9025de599f10fdd4/eks-s3-seldon-mlflow/eks.tf,2022-06-29 09:54:59+02:00,2022-07-12 17:39:44+05:30,2,1
https://github.com/ministryofjustice/aws-root-account,27,terraform/organizations-accounts-platforms-and-architecture-modernisation-platform.tf,terraform/organizations-accounts-platforms-and-architecture-modernisation-platform.tf,0,implementation,"# { id => ""account_id"", name => ""account name"" } for the GuardDuty implementation","# Below is a data source to get all Modernisation Platform-managed AWS accounts in a key => value 
 # format, where key is the account name and value is their ID; which is stored in AWS Secrets Manager 
 # on their side. We then store it in a local with the required map format: 
 # { id => ""account_id"", name => ""account name"" } for the GuardDuty implementation 
 # in this repository","data ""aws_secretsmanager_secret"" ""modernisation-platform-environment-management"" {
  provider = aws.modernisation-platform
  name     = ""environment_management""
}
",data,the block associated got renamed or deleted,,38,,a45940e5486c3236843e04fdf67e41e21838b6f8,d7b325af5b38460b9d14f63e6bdc9716285854f3,https://github.com/ministryofjustice/aws-root-account/blob/a45940e5486c3236843e04fdf67e41e21838b6f8/terraform/organizations-accounts-platforms-and-architecture-modernisation-platform.tf#L38,https://github.com/ministryofjustice/aws-root-account/blob/d7b325af5b38460b9d14f63e6bdc9716285854f3/terraform/organizations-accounts-platforms-and-architecture-modernisation-platform.tf,2020-12-21 16:24:16+00:00,2022-03-01 19:30:50+00:00,4,1
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,475,examples/data-solutions/gcs-to-bq-with-dataflow/main.tf,examples/data-solutions/gcs-to-bq-with-dataflow/main.tf,0,# todo,# TODO(jccb): doesn't work when project_create=false,"# TODO(jccb): doesn't work when project_create=false 
 # oslogin = true","module ""project-service"" {
  source          = ""../../../modules/project""
  name            = var.service_project_id
  parent          = var.root_node
  billing_account = var.billing_account
  project_create  = var.project_create
  services = [
    ""bigquery.googleapis.com"",
    ""bigqueryreservation.googleapis.com"",
    ""bigquerystorage.googleapis.com"",
    ""cloudkms.googleapis.com"",
    ""compute.googleapis.com"",
    ""dataflow.googleapis.com"",
    ""servicenetworking.googleapis.com"",
    ""storage.googleapis.com"",
  ]
  # TODO(jccb): doesn't work when project_create=false
  # oslogin = true
}
",module,the block associated got renamed or deleted,,43,,5cc6a62dec3ca1a778b1eb39eb02a89dcf894394,70d8d1f890f491867778829699cc54443000bc87,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/5cc6a62dec3ca1a778b1eb39eb02a89dcf894394/examples/data-solutions/gcs-to-bq-with-dataflow/main.tf#L43,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/70d8d1f890f491867778829699cc54443000bc87/examples/data-solutions/gcs-to-bq-with-dataflow/main.tf,2022-01-14 16:53:15+01:00,2022-01-14 16:53:15+01:00,2,1
https://github.com/mozilla/hubs-ops,3,terraform/modules/vpc/main.tf,terraform/modules/vpc/main.tf,0,crap,# Modified from https://charity.wtf/2016/04/14/scrapbag-of-useful-terraform-tips/,# Modified from https://charity.wtf/2016/04/14/scrapbag-of-useful-terraform-tips/,,,,,1,1.0,22dea8499c4d4edc7bacca86b1f134383c9b6e33,08503dd6b51b1528a4ce2d5c120a6b07864a350d,https://github.com/mozilla/hubs-ops/blob/22dea8499c4d4edc7bacca86b1f134383c9b6e33/terraform/modules/vpc/main.tf#L1,https://github.com/mozilla/hubs-ops/blob/08503dd6b51b1528a4ce2d5c120a6b07864a350d/terraform/modules/vpc/main.tf#L1,2017-09-26 18:49:42-07:00,2019-07-12 22:35:14+00:00,7,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,23,modules/iam-service-accounts/main.tf,modules/iam-service-accounts/main.tf,0,# todo,# TODO(ludoo): link from README,"# TODO(ludoo): link from README 
 # ref: https://cloud.google.com/vpc/docs/shared-vpc ",,,,,154,0.0,c486bfc66f9814e33b410602cb557a5e4d532912,13ed799a8b8353ae64a675ab1c5079b649f1c742,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c486bfc66f9814e33b410602cb557a5e4d532912/modules/iam-service-accounts/main.tf#L154,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/13ed799a8b8353ae64a675ab1c5079b649f1c742/modules/iam-service-accounts/main.tf#L0,2020-04-03 14:06:48+02:00,2020-10-20 22:36:03+02:00,4,2
https://github.com/oracle-terraform-modules/terraform-oci-oke,200,module-workers.tf,module-workers.tf,0,implementation,# Default workers sub-module implementation for OKE cluster,# Default workers sub-module implementation for OKE cluster,"module ""workers"" {
  count  = var.create_cluster ? 1 : 0
  source = ""./modules/workers""

  # Common
  compartment_id      = local.worker_compartment_id
  tenancy_id          = local.tenancy_id
  state_id            = random_id.state_id.id
  ad_numbers          = local.ad_numbers
  ad_numbers_to_names = local.ad_numbers_to_names

  # Cluster
  apiserver_private_host = try(split("":"", one(module.cluster[*].endpoints.private_endpoint))[0], """")
  cluster_ca_cert        = local.cluster_ca_cert
  cluster_dns            = var.cluster_dns
  cluster_id             = coalesce(var.cluster_id, one(module.cluster[*].cluster_id))
  kubernetes_version     = var.kubernetes_version

  # Worker pools
  worker_pool_enabled = var.worker_pool_enabled
  worker_pool_mode    = var.worker_pool_mode
  worker_pool_size    = var.worker_pool_size
  worker_pools        = var.worker_pools

  # Workers
  assign_dns            = var.assign_dns
  assign_public_ip      = var.worker_type == ""public""
  block_volume_type     = var.worker_block_volume_type
  cloudinit             = var.worker_cloudinit
  cni_type              = var.cni_type
  image_id              = var.worker_image_id
  image_ids             = local.image_ids
  image_os              = var.worker_image_os
  image_os_version      = var.worker_image_os_version
  image_type            = var.worker_image_type
  kubeproxy_mode        = var.kubeproxy_mode
  max_pods_per_node     = var.max_pods_per_node
  node_labels           = var.worker_node_labels
  pod_nsg_ids           = concat(var.pod_nsg_ids, var.cni_type == ""npn"" ? [module.network.pod_nsg_id] : [])
  pod_subnet_id         = lookup(module.network.subnet_ids, ""pods"", """")
  pv_transit_encryption = var.worker_pv_transit_encryption
  shape                 = var.worker_shape
  ssh_public_key        = local.ssh_public_key
  timezone              = var.timezone
  volume_kms_key_id     = var.worker_volume_kms_key_id
  worker_nsg_ids        = concat(var.worker_nsg_ids, [module.network.worker_nsg_id])
  worker_subnet_id      = lookup(module.network.subnet_ids, ""workers"")

  # FSS
  create_fss              = var.create_fss
  fss_availability_domain = coalesce(var.fss_availability_domain, local.ad_numbers_to_names[1])
  fss_subnet_id           = lookup(module.network.subnet_ids, ""fss"", lookup(module.network.subnet_ids, ""workers""))
  fss_nsg_ids             = var.fss_nsg_ids
  fss_mount_path          = var.fss_mount_path
  fss_max_fs_stat_bytes   = var.fss_max_fs_stat_bytes
  fss_max_fs_stat_files   = var.fss_max_fs_stat_files

  # Tagging
  tag_namespace    = var.tag_namespace
  defined_tags     = lookup(var.defined_tags, ""workers"", {})
  freeform_tags    = lookup(var.freeform_tags, ""workers"", {})
  use_defined_tags = var.use_defined_tags

  providers = {
    oci.home = oci.home
  }

  depends_on = [
    module.iam,
  ]
}
",module,"module ""workers"" {
  count  = local.cluster_enabled ? 1 : 0
  source = ""./modules/workers""

  # Common
  compartment_id      = local.worker_compartment_id
  tenancy_id          = local.tenancy_id
  state_id            = local.state_id
  ad_numbers          = local.ad_numbers
  ad_numbers_to_names = local.ad_numbers_to_names

  # Cluster
  apiserver_private_host = local.apiserver_private_host
  cluster_ca_cert        = local.cluster_ca_cert
  cluster_dns            = var.cluster_dns
  cluster_id             = coalesce(var.cluster_id, one(module.cluster[*].cluster_id))
  cluster_type           = var.cluster_type
  kubernetes_version     = var.kubernetes_version

  # Worker pools
  worker_pool_mode = var.worker_pool_mode
  worker_pool_size = var.worker_pool_size
  worker_pools     = var.worker_pools

  # Workers
  assign_dns                 = var.assign_dns
  assign_public_ip           = var.worker_is_public
  block_volume_type          = var.worker_block_volume_type
  capacity_reservation_id    = var.worker_capacity_reservation_id
  cloud_init                 = var.worker_cloud_init
  disable_default_cloud_init = var.worker_disable_default_cloud_init
  cni_type                   = var.cni_type
  image_id                   = var.worker_image_id
  image_ids                  = local.image_ids
  image_os                   = var.worker_image_os
  image_os_version           = var.worker_image_os_version
  image_type                 = var.worker_image_type
  kubeproxy_mode             = var.kubeproxy_mode
  max_pods_per_node          = var.max_pods_per_node
  node_labels                = var.worker_node_labels
  node_metadata              = var.worker_node_metadata
  agent_config               = var.agent_config
  platform_config            = var.platform_config
  pod_nsg_ids                = concat(var.pod_nsg_ids, var.cni_type == ""npn"" ? [try(module.network.pod_nsg_id, null)] : [])
  pod_subnet_id              = try(module.network.pod_subnet_id, """") # safe destroy; validated in submodule
  pv_transit_encryption      = var.worker_pv_transit_encryption
  shape                      = var.worker_shape
  ssh_public_key             = local.ssh_public_key
  timezone                   = var.timezone
  volume_kms_key_id          = var.worker_volume_kms_key_id
  worker_nsg_ids             = concat(var.worker_nsg_ids, [try(module.network.worker_nsg_id, null)])
  worker_subnet_id           = try(module.network.worker_subnet_id, """") # safe destroy; validated in submodule
  preemptible_config         = var.worker_preemptible_config

  # Tagging
  tag_namespace    = var.tag_namespace
  defined_tags     = local.workers_defined_tags
  freeform_tags    = local.workers_freeform_tags
  use_defined_tags = var.use_defined_tags

  depends_on = [
    module.iam,
  ]
}
",module,25,21.0,6c867cd8e9cbf559742f56658989bcded0d1fd89,aeb330eaee940bc25c1ad307a3e8db9dfae9fae8,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/module-workers.tf#L25,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/aeb330eaee940bc25c1ad307a3e8db9dfae9fae8/module-workers.tf#L21,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,23,0
https://github.com/Worklytics/psoxy,984,infra/modules/gcp-psoxy-bulk/main.tf,infra/modules/gcp-psoxy-bulk/main.tf,0,# todo,# TODO: added in v0.4.19,# TODO: added in v0.4.19,"moved {
  from = google_storage_bucket.output-bucket
  to   = module.output_bucket.google_storage_bucket.bucket
}
",moved,the block associated got renamed or deleted,,91,,4f41d480721ffa94079bbfb0cdcc181e22423300,bb41a6c349584a82517cb604a30196de7d807a75,https://github.com/Worklytics/psoxy/blob/4f41d480721ffa94079bbfb0cdcc181e22423300/infra/modules/gcp-psoxy-bulk/main.tf#L91,https://github.com/Worklytics/psoxy/blob/bb41a6c349584a82517cb604a30196de7d807a75/infra/modules/gcp-psoxy-bulk/main.tf,2023-04-18 16:24:15-07:00,2023-10-23 15:56:19-07:00,15,1
https://github.com/pluralsh/plural-artifacts,1,oauth2-proxy/terraform/aws/main.tf,oauth2-proxy/terraform/aws/main.tf,0,fix,#     # This zone_id is fixed,"# resource ""aws_cognito_user_pool"" ""pool"" { 
 #   name = ""${var.cognito_user_pool_name}"" 
 # }  
 # resource ""aws_cognito_user_pool_client"" ""client"" { 
 #   name = ""${var.cognito_user_pool_name}"" 
 #   user_pool_id = aws_cognito_user_pool.pool.id 
 #   callback_urls = [""https://${var.callback_domain}/oauth2/callback""] 
 #   allowed_oauth_flows = [""code""] 
 #   allowed_oauth_scopes = [""phone"", ""email"", ""openid"", ""profile""]  
 # }  
 # resource ""aws_route53_record"" ""auth-cognito-A"" { 
 #   name    = aws_cognito_user_pool_domain.main.domain 
 #   type    = ""A"" 
 #   zone_id = data.aws_route53_zone.kubeflow_aws.zone_id 
 #   alias { 
 #     evaluate_target_health = false 
 #     name                   = aws_cognito_user_pool_domain.main.cloudfront_distribution_arn 
 #     # This zone_id is fixed 
 #     zone_id = ""Z2FDTNDATAQYW2"" 
 #   } 
 # }  
 # resource ""aws_cognito_user_pool_domain"" ""main"" { 
 #   domain          = ""${var.cognito_user_pool_domain}"" 
 #   certificate_arn = aws_acm_certificate.cert.arn 
 #   user_pool_id    = aws_cognito_user_pool.pool.id 
 # }  
 # data ""aws_route53_zone"" ""kubeflow_aws"" { 
 #   name = ""kubeflow-aws.com"" 
 # }  
 # resource ""aws_acm_certificate"" ""cert"" { 
 #   domain_name       = ""${var.cognito_user_pool_domain}"" 
 #   validation_method = ""DNS""  
 #   tags = { 
 #     Environment = ""prod"" 
 #   }  
 #   lifecycle { 
 #     create_before_destroy = true 
 #   } 
 # } ",,,the block associated got renamed or deleted,,32,,a388a7e52b4a5c235fbe51ea4b3d9db2e29d94a4,369cb711c8d7e3829e0e480ab0eb7583ef7b99a8,https://github.com/pluralsh/plural-artifacts/blob/a388a7e52b4a5c235fbe51ea4b3d9db2e29d94a4/oauth2-proxy/terraform/aws/main.tf#L32,https://github.com/pluralsh/plural-artifacts/blob/369cb711c8d7e3829e0e480ab0eb7583ef7b99a8/oauth2-proxy/terraform/aws/main.tf,2021-08-02 19:44:02+00:00,2021-12-09 12:13:16+00:00,2,1
https://github.com/apache/beam,25,.github/gh-actions-self-hosted-runners/arc/locals.tf,.github/gh-actions-self-hosted-runners/arc/locals.tf,0,#fix,#fix dind issue,#fix dind issue,"locals {
        arc_values = {
            #fix dind issue
            ""image.dindSidecarRepositoryAndTag"" = ""docker:24.0.7-dind-alpine3.18""
            ""githubWebhookServer.enabled"" = ""${var.deploy_webhook}""
            ""authSecret.create"" = ""true""
            ""authSecret.github_app_id"" = data.google_secret_manager_secret_version.github_app_id.secret_data
            ""authSecret.github_app_installation_id"" = data.google_secret_manager_secret_version.github_app_install_id.secret_data
            ""authSecret.github_app_private_key"" = data.google_secret_manager_secret_version.github_private_key.secret_data
            ""githubWebhookServer.ingress.enabled"" = ""${var.deploy_webhook}""
            ""githubWebhookServer.ingress.hosts[0].host"" = var.ingress_domain
            ""githubWebhookServer.ingress.hosts[0].paths[0].path"" = ""/""
            ""githubWebhookServer.ingress.hosts[0].paths[0].pathType"" = ""ImplementationSpecific""
            ""githubWebhookServer.service.type"" = ""NodePort""
            ""githubWebhookServer.ingress.annotations.kubernetes\\.io/ingress\\.global-static-ip-name"" = var.deploy_webhook != ""false"" ? data.google_compute_global_address.actions-runner-ip[0].name : ""not-configured""
            ""githubWebhookServer.ingress.annotations.networking\\.gke\\.io/managed-certificates"" = ""managed-cert""
            ""githubWebhookServer.ingress.annotations.kubernetes\\.io/ingress\\.class"" = ""gce""
        }
}",locals,"locals {
        arc_values = {
            #fix dind issue
            ""image.dindSidecarRepositoryAndTag"" = ""docker:24.0.7-dind-alpine3.18""
            ""githubWebhookServer.enabled"" = ""${var.deploy_webhook}""
            ""authSecret.create"" = ""true""
            ""authSecret.github_app_id"" = data.google_secret_manager_secret_version.github_app_id.secret_data
            ""authSecret.github_app_installation_id"" = data.google_secret_manager_secret_version.github_app_install_id.secret_data
            ""authSecret.github_app_private_key"" = data.google_secret_manager_secret_version.github_private_key.secret_data
            ""githubWebhookServer.ingress.enabled"" = ""${var.deploy_webhook}""
            ""githubWebhookServer.ingress.hosts[0].host"" = var.ingress_domain
            ""githubWebhookServer.ingress.hosts[0].paths[0].path"" = ""/""
            ""githubWebhookServer.ingress.hosts[0].paths[0].pathType"" = ""ImplementationSpecific""
            ""githubWebhookServer.service.type"" = ""NodePort""
            ""githubWebhookServer.ingress.annotations.kubernetes\\.io/ingress\\.global-static-ip-name"" = var.deploy_webhook != ""false"" ? data.google_compute_global_address.actions-runner-ip[0].name : ""not-configured""
            ""githubWebhookServer.ingress.annotations.networking\\.gke\\.io/managed-certificates"" = ""managed-cert""
            ""githubWebhookServer.ingress.annotations.kubernetes\\.io/ingress\\.class"" = ""gce""
        }
}",locals,23,23.0,1033b71aff6a49509ef50e5c309bd95a19b6ff68,1033b71aff6a49509ef50e5c309bd95a19b6ff68,https://github.com/apache/beam/blob/1033b71aff6a49509ef50e5c309bd95a19b6ff68/.github/gh-actions-self-hosted-runners/arc/locals.tf#L23,https://github.com/apache/beam/blob/1033b71aff6a49509ef50e5c309bd95a19b6ff68/.github/gh-actions-self-hosted-runners/arc/locals.tf#L23,2023-12-18 09:23:26-05:00,2023-12-18 09:23:26-05:00,1,0
https://github.com/cookpad/terraform-aws-eks,90,versions.tf,versions.tf,0,hack,# Run hack/versions to generate the latest values for this,# Run hack/versions to generate the latest values for this,"locals {
  versions = {
    k8s                = ""1.25""
    vpc_cni            = ""v1.12.6-eksbuild.2""
    kube_proxy         = ""v1.25.9-eksbuild.1""
    coredns            = ""v1.9.3-eksbuild.3""
    aws_ebs_csi_driver = ""v1.19.0-eksbuild.2""
  }
}
",locals,"locals {
  versions = {
    k8s                = ""1.25""
    vpc_cni            = ""v1.16.2-eksbuild.1""
    kube_proxy         = ""v1.25.9-eksbuild.1""
    coredns            = ""v1.9.3-eksbuild.9""
    aws_ebs_csi_driver = ""v1.27.0-eksbuild.1""
  }
}
",locals,17,,f5918bacf2ee295f233c047cb71488a47c248e52,24dcbdb6ab6cae5024ba04e856990cb4a15a6976,https://github.com/cookpad/terraform-aws-eks/blob/f5918bacf2ee295f233c047cb71488a47c248e52/versions.tf#L17,https://github.com/cookpad/terraform-aws-eks/blob/24dcbdb6ab6cae5024ba04e856990cb4a15a6976/versions.tf,2023-06-21 09:58:23+02:00,2024-02-16 15:23:52+00:00,2,1
https://github.com/ministryofjustice/modernisation-platform,280,terraform/environments/core-shared-services/ad-fixngo.tf,terraform/environments/core-shared-services/ad-fixngo.tf,0,fix,# azure.noms.root and azure.hmpp.root AD DS infra migrated from Azure FixNGo,"# azure.noms.root and azure.hmpp.root AD DS infra migrated from Azure FixNGo 
 # For more details why this is here, please see: 
 # - https://github.com/ministryofjustice/modernisation-platform/issues/5970 
 # - https://dsdmoj.atlassian.net/wiki/x/3oCKGAE 
 # Managed by DSO team, slack: #ask-digital-studio-ops ","module ""ad_fixngo_ip_addresses"" {
  source = ""github.com/ministryofjustice/modernisation-platform-environments//terraform/modules/ip_addresses""
}
",module,"module ""ad_fixngo_ip_addresses"" {
  #checkov:skip=CKV_TF_1:Module registry does not support commit hashes for versions
  source = ""github.com/ministryofjustice/modernisation-platform-environments//terraform/modules/ip_addresses?ref=29c48e315aa5eeef5d604617169b2f6db953966e""
}
",module,1,1.0,4a4a151027aba403b4c260dd6716d7aadf3411a8,6fc3d7f041e9c3fe4f83d7367d41009d98b3d9a3,https://github.com/ministryofjustice/modernisation-platform/blob/4a4a151027aba403b4c260dd6716d7aadf3411a8/terraform/environments/core-shared-services/ad-fixngo.tf#L1,https://github.com/ministryofjustice/modernisation-platform/blob/6fc3d7f041e9c3fe4f83d7367d41009d98b3d9a3/terraform/environments/core-shared-services/ad-fixngo.tf#L1,2024-02-15 09:40:10+00:00,2024-05-23 11:38:09+01:00,26,0
