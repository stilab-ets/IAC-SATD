Repo URL,Satd Comment Id,File Path Of First Occurence,File Path Of Last Occurence,renamed,Keyword,SATD Comment,context,bloc of first occurrence,bloc type of first occurrence,bloc of last occurrence,bloc type of last occurrence,SATD Comment Line Of First Occurence,SATD Comment Line Of Last Occurence,first Commit Hash,last Commit Hash,Link To The File Of First Occurence,Link To The File Of Last Occurence/When Adressed,Introduction Time,Last Occurence (even solved or not),number of commits,adressed ?
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,405,modules/vpc-sc/access_levels.tf,modules/vpc-sc/access-levels.tf,1,implement,"# this code implements ""additive"" access levels, if ""authoritative""","/** 
 * Copyright 2021 Google LLC 
 * 
 * Licensed under the Apache License, Version 2.0 (the ""License""); 
 * you may not use this file except in compliance with the License. 
 * You may obtain a copy of the License at 
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, software 
 * distributed under the License is distributed on an ""AS IS"" BASIS, 
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 * See the License for the specific language governing permissions and 
 * limitations under the License. 
 */  
 # TODO(ludomagno): add a second variable and resource for custom access levels  
 # this code implements ""additive"" access levels, if ""authoritative"" 
 # access levels are needed, switch to the 
 # google_access_context_manager_access_levels resource ","resource ""google_access_context_manager_access_level"" ""basic"" {
  for_each = var.access_levels
  parent   = ""accessPolicies/${local.access_policy}""
  name     = ""accessPolicies/${local.access_policy}/accessLevels/${each.key}""
  title    = each.key
  basic {
    combining_function = each.value.combining_function
    dynamic ""conditions"" {
      for_each = toset(
        each.value.conditions == null ? [] : each.value.conditions
      )
      iterator = condition
      content {
        dynamic ""device_policy"" {
          for_each = toset(
            condition.key.device_policy == null ? [] : [condition.key.device_policy]
          )
          iterator = device_policy
          content {
            dynamic ""os_constraints"" {
              for_each = toset(
                device_policy.key.os_constraints == null ? [] : device_policy.key.os_constraints
              )
              iterator = os_constraint
              content {
                minimum_version            = os_constraint.key.minimum_version
                os_type                    = os_constraint.key.os_type
                require_verified_chrome_os = os_constraint.key.require_verified_chrome_os
              }
            }
            allowed_encryption_statuses      = device_policy.key.allowed_encryption_statuses
            allowed_device_management_levels = device_policy.key.allowed_device_management_levels
            require_admin_approval           = device_policy.key.require_admin_approval
            require_corp_owned               = device_policy.key.require_corp_owned
            require_screen_lock              = device_policy.key.require_screen_lock
          }
        }
        ip_subnetworks = (
          condition.key.ip_subnetworks == null ? [] : condition.key.ip_subnetworks
        )
        members = (
          condition.key.members == null ? [] : condition.key.members
        )
        negate = condition.key.negate
        regions = (
          condition.key.regions == null ? [] : condition.key.regions
        )
        required_access_levels = (
          condition.key.required_access_levels == null
          ? []
          : condition.key.required_access_levels
        )
      }
    }
  }
}
",resource,"resource ""google_access_context_manager_access_level"" ""basic"" {
  for_each    = merge(local.data.access_levels, var.access_levels)
  parent      = ""accessPolicies/${local.access_policy}""
  name        = ""accessPolicies/${local.access_policy}/accessLevels/${each.key}""
  title       = each.key
  description = each.value.description

  basic {
    combining_function = each.value.combining_function

    dynamic ""conditions"" {
      for_each = toset(each.value.conditions)
      iterator = c
      content {
        ip_subnetworks         = c.value.ip_subnetworks
        members                = c.value.members
        negate                 = c.value.negate
        regions                = c.value.regions
        required_access_levels = coalesce(c.value.required_access_levels, [])

        dynamic ""device_policy"" {
          for_each = c.value.device_policy == null ? [] : [c.value.device_policy]
          iterator = dp
          content {

            allowed_device_management_levels = (
              dp.value.allowed_device_management_levels
            )
            allowed_encryption_statuses = (
              dp.value.allowed_encryption_statuses
            )
            require_admin_approval = dp.value.key.require_admin_approval
            require_corp_owned     = dp.value.require_corp_owned
            require_screen_lock    = dp.value.require_screen_lock

            dynamic ""os_constraints"" {
              for_each = toset(
                dp.value.os_constraints == null
                ? []
                : dp.value.os_constraints
              )
              iterator = oc
              content {
                minimum_version            = oc.value.minimum_version
                os_type                    = oc.value.os_type
                require_verified_chrome_os = oc.value.require_verified_chrome_os
              }
            }

          }
        }

      }
    }

  }
}
",resource,19,19.0,2c7dab3bb2b572c4d60bc91ba48eb9e4ffbb8913,91615e014054ada63899da558f3b1c6cac5c8000,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2c7dab3bb2b572c4d60bc91ba48eb9e4ffbb8913/modules/vpc-sc/access_levels.tf#L19,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/91615e014054ada63899da558f3b1c6cac5c8000/modules/vpc-sc/access-levels.tf#L19,2021-12-31 13:29:22+01:00,2024-02-17 08:02:16+01:00,6,0
https://github.com/apache/beam,5,playground/terraform/infrastructure/setup/iam.tf,playground/terraform/infrastructure/setup/iam.tf,0,// todo,#    // TODO: add the required roles to provision resources (not OWNER :-)!),"# 
 # Licensed to the Apache Software Foundation (ASF) under one 
 # or more contributor license agreements.  See the NOTICE file 
 # distributed with this work for additional information 
 # regarding copyright ownership.  The ASF licenses this file 
 # to you under the Apache License, Version 2.0 (the 
 # ""License""); you may not use this file except in compliance 
 # with the License.  You may obtain a copy of the License at 
 # 
 #   http://www.apache.org/licenses/LICENSE-2.0 
 # 
 # Unless required by applicable law or agreed to in writing, 
 # software distributed under the License is distributed on an 
 # ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY 
 # KIND, either express or implied.  See the License for the 
 # specific language governing permissions and limitations 
 # under the License. 
 #  
 #resource ""google_service_account"" ""terraform_service_account"" { 
 #  account_id   = ""terraform"" 
 #  display_name = ""terraform"" 
 #} 
 # 
 #resource ""google_project_iam_member"" ""terraform_service_account_roles"" { 
 #  for_each = toset([ 
 #    // TODO: add the required roles to provision resources (not OWNER :-)!) 
 #  ]) 
 #  role    = each.key 
 #  member  = ""serviceAccount:${google_service_account.terraform_service_account.email}"" 
 #  project = var.project_id 
 #} 
 # 
 #resource ""google_service_account_iam_binding"" ""terraform_service_account_token_permissions"" { 
 #  service_account_id = google_service_account.terraform_service_account.id 
 #  members = [ 
 #    ""user:${var.developer_account_email}"" // TODO: add variable 
 #  ] 
 #  role    = ""roles/iam.serviceAccountTokenCreator"" 
 #}  
 #resource ""google_service_account_iam_binding"" ""application_service_account_binding"" { 
 #  members            = [ 
 #    ""serviceAccount:${google_service_account.terraform_service_account.email}"" 
 #  ] 
 #  role               = ""roles/iam.serviceAccountUser"" 
 #  service_account_id = google_service_account.playground_service_account.id 
 #} ","resource ""google_service_account"" ""playground_service_account"" {
  account_id   = var.service_account_id
  display_name = var.service_account_id
}
",resource,"resource ""google_service_account"" ""playground_service_account"" {
  account_id   = var.service_account_id
  display_name = var.service_account_id
}
",resource,27,,675c0bc10f813ea593702f5e6a0fd2ce38caf720,ea4411f0082a65b7a974222b5954a44bd8dc8d31,https://github.com/apache/beam/blob/675c0bc10f813ea593702f5e6a0fd2ce38caf720/playground/terraform/infrastructure/setup/iam.tf#L27,https://github.com/apache/beam/blob/ea4411f0082a65b7a974222b5954a44bd8dc8d31/playground/terraform/infrastructure/setup/iam.tf,2022-02-22 10:04:20-08:00,2023-05-08 15:38:25-04:00,4,1
https://github.com/mozilla/hubs-ops,105,terraform/modules/janus/main.tf,terraform/modules/janus/main.tf,0,todo,# TODO this causes large janus CPU spike,"# TODO this causes large janus CPU spike 
 # sudo sed -i ""s/#RateLimitBurst=1000/RateLimitBurst=5000/"" /etc/systemd/journald.conf 
 # sudo systemctl restart systemd-journald ",,,the block associated got renamed or deleted,,150,,1a131e1b5147ae2d598b54346cd12789434fdb75,c94b0b5d3b974715258e116f38727e608b15475f,https://github.com/mozilla/hubs-ops/blob/1a131e1b5147ae2d598b54346cd12789434fdb75/terraform/modules/janus/main.tf#L150,https://github.com/mozilla/hubs-ops/blob/c94b0b5d3b974715258e116f38727e608b15475f/terraform/modules/janus/main.tf,2018-06-27 14:37:39-07:00,2018-10-09 22:10:29+00:00,2,1
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1985,modules/net-vpc/subnets.tf,modules/net-vpc/subnets.tf,0,fix,#                  Revert to the following once fixed.,"# TODO(sruffilli): Provider 5.29.1 disabled reserved_internal_range because of a bug. 
 #                  Revert to the following once fixed. 
 # ip_cidr_range = ( 
 #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"") 
 #   ? null 
 #   : secondary_ip_range.value 
 # ) 
 # reserved_internal_range = ( 
 #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"") 
 #   ? secondary_ip_range.value 
 #   : null 
 # )","resource ""google_compute_subnetwork"" ""subnetwork"" {
  for_each      = local.subnets
  project       = var.project_id
  network       = local.network.name
  name          = each.value.name
  region        = each.value.region
  ip_cidr_range = each.value.ip_cidr_range
  description = (
    each.value.description == null
    ? ""Terraform-managed.""
    : each.value.description
  )
  private_ip_google_access = each.value.enable_private_access
  stack_type = (
    try(each.value.ipv6, null) != null ? ""IPV4_IPV6"" : null
  )
  ipv6_access_type = (
    try(each.value.ipv6, null) != null ? each.value.ipv6.access_type : null
  )
  # private_ipv6_google_access = try(each.value.ipv6.enable_private_access, null)
  dynamic ""secondary_ip_range"" {
    for_each = each.value.secondary_ip_ranges == null ? {} : each.value.secondary_ip_ranges
    content {
      range_name    = secondary_ip_range.key
      ip_cidr_range = secondary_ip_range.value
      # TODO(sruffilli): Provider 5.29.1 disabled reserved_internal_range because of a bug.
      #                  Revert to the following once fixed.
      # ip_cidr_range = (
      #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"")
      #   ? null
      #   : secondary_ip_range.value
      # )    
      # reserved_internal_range = (
      #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"")
      #   ? secondary_ip_range.value
      #   : null
      # )
    }
  }
  dynamic ""log_config"" {
    for_each = each.value.flow_logs_config != null ? [""""] : []
    content {
      aggregation_interval = each.value.flow_logs_config.aggregation_interval
      filter_expr          = each.value.flow_logs_config.filter_expression
      flow_sampling        = each.value.flow_logs_config.flow_sampling
      metadata             = each.value.flow_logs_config.metadata
      metadata_fields = (
        each.value.flow_logs_config.metadata == ""CUSTOM_METADATA""
        ? each.value.flow_logs_config.metadata_fields
        : null
      )
    }
  }
}
",resource,"resource ""google_compute_subnetwork"" ""subnetwork"" {
  for_each      = local.subnets
  project       = var.project_id
  network       = local.network.name
  name          = each.value.name
  region        = each.value.region
  ip_cidr_range = each.value.ip_cidr_range
  description = (
    each.value.description == null
    ? ""Terraform-managed.""
    : each.value.description
  )
  private_ip_google_access = each.value.enable_private_access
  stack_type = (
    try(each.value.ipv6, null) != null ? ""IPV4_IPV6"" : null
  )
  ipv6_access_type = (
    try(each.value.ipv6, null) != null ? each.value.ipv6.access_type : null
  )
  # private_ipv6_google_access = try(each.value.ipv6.enable_private_access, null)
  dynamic ""secondary_ip_range"" {
    for_each = each.value.secondary_ip_ranges == null ? {} : each.value.secondary_ip_ranges
    content {
      range_name    = secondary_ip_range.key
      ip_cidr_range = secondary_ip_range.value
      # TODO(sruffilli): Provider 5.29.1 disabled reserved_internal_range because of a bug.
      #                  Revert to the following once fixed.
      # ip_cidr_range = (
      #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"")
      #   ? null
      #   : secondary_ip_range.value
      # )    
      # reserved_internal_range = (
      #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"")
      #   ? secondary_ip_range.value
      #   : null
      # )
    }
  }
  dynamic ""log_config"" {
    for_each = each.value.flow_logs_config != null ? [""""] : []
    content {
      aggregation_interval = each.value.flow_logs_config.aggregation_interval
      filter_expr          = each.value.flow_logs_config.filter_expression
      flow_sampling        = each.value.flow_logs_config.flow_sampling
      metadata             = each.value.flow_logs_config.metadata
      metadata_fields = (
        each.value.flow_logs_config.metadata == ""CUSTOM_METADATA""
        ? each.value.flow_logs_config.metadata_fields
        : null
      )
    }
  }
}
",resource,163,163.0,d3ffcc2b1cf2f2ffc3fc480b90719116dee59563,d3ffcc2b1cf2f2ffc3fc480b90719116dee59563,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d3ffcc2b1cf2f2ffc3fc480b90719116dee59563/modules/net-vpc/subnets.tf#L163,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d3ffcc2b1cf2f2ffc3fc480b90719116dee59563/modules/net-vpc/subnets.tf#L163,2024-05-15 05:46:18+00:00,2024-05-15 05:46:18+00:00,1,0
https://github.com/terraform-google-modules/terraform-google-project-factory,125,examples/shared_vpc/main.tf,examples/shared_vpc/main.tf,0,# todo,# TODO: Switch to released version once,"# TODO: Switch to released version once 
 # https://github.com/terraform-google-modules/terraform-google-network/pull/47 
 # is merged and released.  This is here to fix the `Error: Unsupported block 
 # type` on the `triggers` block in network's main.tf file. 
 # 
 # source  = ""terraform-google-modules/network/google"" 
 # version = ""0.8.0""","module ""vpc"" {
  # TODO: Switch to released version once
  # https://github.com/terraform-google-modules/terraform-google-network/pull/47
  # is merged and released.  This is here to fix the `Error: Unsupported block
  # type` on the `triggers` block in network's main.tf file.
  #
  # source  = ""terraform-google-modules/network/google""
  # version = ""0.8.0""
  source = ""git::https://github.com/terraform-google-modules/terraform-google-network.git?ref=aaron-lane-0.12""

  project_id   = module.host-project.project_id
  network_name = var.network_name

  delete_default_internet_gateway_routes = ""true""
  shared_vpc_host                        = ""true""

  subnets = [
    {
      subnet_name   = local.subnet_01
      subnet_ip     = ""10.10.10.0/24""
      subnet_region = ""us-west1""
    },
    {
      subnet_name           = local.subnet_02
      subnet_ip             = ""10.10.20.0/24""
      subnet_region         = ""us-west1""
      subnet_private_access = ""true""
      subnet_flow_logs      = ""true""
    },
  ]

  secondary_ranges = {
    ""${local.subnet_01}"" = [
      {
        range_name    = ""${local.subnet_01}-01""
        ip_cidr_range = ""192.168.64.0/24""
      },
      {
        range_name    = ""${local.subnet_01}-02""
        ip_cidr_range = ""192.168.65.0/24""
      },
    ]

    ""${local.subnet_02}"" = [
      {
        range_name    = ""${local.subnet_02}-01""
        ip_cidr_range = ""192.168.66.0/24""
      },
    ]
  }
}
",module,"module ""vpc"" {
  # source  = ""terraform-google-modules/network/google""
  # version = ""~> 1.4.0""

  project_id   = module.host-project.project_id
  network_name = var.network_name

  delete_default_internet_gateway_routes = true
  shared_vpc_host                        = true

  subnets = [
    {
      subnet_name   = local.subnet_01
      subnet_ip     = ""10.10.10.0/24""
      subnet_region = ""us-west1""
    },
    {
      subnet_name           = local.subnet_02
      subnet_ip             = ""10.10.20.0/24""
      subnet_region         = ""us-west1""
      subnet_private_access = true
      subnet_flow_logs      = true
    },
  ]

  secondary_ranges = {
    ""${local.subnet_01}"" = [
      {
        range_name    = ""${local.subnet_01}-01""
        ip_cidr_range = ""192.168.64.0/24""
      },
      {
        range_name    = ""${local.subnet_01}-02""
        ip_cidr_range = ""192.168.65.0/24""
      },
    ]

    ""${local.subnet_02}"" = [
      {
        range_name    = ""${local.subnet_02}-01""
        ip_cidr_range = ""192.168.66.0/24""
      },
    ]
  }
}
",module,52,,c40a3d3c37d2cf4d9376d08a31b8ac05af21360e,6557d7a7d4dacc0ddcfcb9c89e61627d4dfe1a90,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/c40a3d3c37d2cf4d9376d08a31b8ac05af21360e/examples/shared_vpc/main.tf#L52,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/6557d7a7d4dacc0ddcfcb9c89e61627d4dfe1a90/examples/shared_vpc/main.tf,2019-07-12 13:49:19-04:00,2019-10-19 11:56:49-07:00,4,1
https://github.com/ManagedKube/kubernetes-ops,10,terraform-modules/aws/msk_1.0.9/module/security-group-variables.tf,terraform-modules/aws/msk_1.0.9/module/security-group-variables.tf,0,implementation,"# This file does NOT provide implementation of the inputs, as that","# security-group-variables Version: 3 
 # 
 # Copy this file from https://github.com/cloudposse/terraform-aws-security-group/blob/master/exports/security-group-variables.tf 
 # and EDIT IT TO SUIT YOUR PROJECT. Update the version number above if you update this file from a later version. 
 # Unlike null-label context.tf, this file cannot be automatically updated 
 # because of the tight integration with the module using it. 
 ## 
 # Delete this top comment block, except for the first line (version number), 
 # REMOVE COMMENTS below that are intended for the initial implementor and not maintainers or end users. 
 # 
 # This file provides the standard inputs that all Cloud Posse Open Source 
 # Terraform module that create AWS Security Groups should implement. 
 # This file does NOT provide implementation of the inputs, as that 
 # of course varies with each module. 
 # 
 # This file declares some standard outputs modules should create, 
 # but the declarations should be moved to `outputs.tf` and of course 
 # may need to be modified based on the module's use of security-group. 
 #  ","variable ""create_security_group"" {
  type        = bool
  description = ""Set `true` to create and configure a new security group. If false, `associated_security_group_ids` must be provided.""
  default     = true
}
",variable,"variable ""create_security_group"" {
  type        = bool
  description = ""Set `true` to create and configure a new security group. If false, `associated_security_group_ids` must be provided.""
  default     = true
}
",variable,13,13.0,c8193c7d74e2f7c624f0867337294cb66a2b9469,c8193c7d74e2f7c624f0867337294cb66a2b9469,https://github.com/ManagedKube/kubernetes-ops/blob/c8193c7d74e2f7c624f0867337294cb66a2b9469/terraform-modules/aws/msk_1.0.9/module/security-group-variables.tf#L13,https://github.com/ManagedKube/kubernetes-ops/blob/c8193c7d74e2f7c624f0867337294cb66a2b9469/terraform-modules/aws/msk_1.0.9/module/security-group-variables.tf#L13,2023-12-14 10:29:30-08:00,2023-12-14 10:29:30-08:00,1,0
https://github.com/terraform-aws-modules/terraform-aws-eks,150,node_groups.tf,node_groups.tf,0,hack,# Hack to ensure ordering of resource creation.,"# Hack to ensure ordering of resource creation. 
 # This is a homemade `depends_on` https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2 
 # Do not create node_groups before other resources are ready and removes race conditions 
 # Ensure these resources are created before ""unlocking"" the data source. 
 # Will be removed in Terraform 0.13","module ""node_groups"" {
  source                 = ""./modules/node_groups""
  create_eks             = var.create_eks
  cluster_name           = coalescelist(aws_eks_cluster.this[*].name, [""""])[0]
  default_iam_role_arn   = coalescelist(aws_iam_role.workers[*].arn, [""""])[0]
  workers_group_defaults = local.workers_group_defaults
  tags                   = var.tags
  node_groups_defaults   = var.node_groups_defaults
  node_groups            = var.node_groups

  # Hack to ensure ordering of resource creation.
  # This is a homemade `depends_on` https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2
  # Do not create node_groups before other resources are ready and removes race conditions
  # Ensure these resources are created before ""unlocking"" the data source.
  # Will be removed in Terraform 0.13
  ng_depends_on = [
    aws_eks_cluster.this,
    kubernetes_config_map.aws_auth,
    aws_iam_role_policy_attachment.workers_AmazonEKSWorkerNodePolicy,
    aws_iam_role_policy_attachment.workers_AmazonEKS_CNI_Policy,
    aws_iam_role_policy_attachment.workers_AmazonEC2ContainerRegistryReadOnly
  ]
}
",module,"module ""node_groups"" {
  source = ""./modules/node_groups""

  create_eks = var.create_eks

  cluster_name        = local.cluster_name
  cluster_endpoint    = local.cluster_endpoint
  cluster_auth_base64 = local.cluster_auth_base64

  default_iam_role_arn                 = coalescelist(aws_iam_role.workers[*].arn, [""""])[0]
  ebs_optimized_not_supported          = local.ebs_optimized_not_supported
  workers_group_defaults               = local.workers_group_defaults
  worker_security_group_id             = local.worker_security_group_id
  worker_additional_security_group_ids = var.worker_additional_security_group_ids

  node_groups_defaults = var.node_groups_defaults
  node_groups          = var.node_groups

  tags = var.tags

  depends_on = [
    aws_eks_cluster.this,
    aws_iam_role_policy_attachment.workers_AmazonEKSWorkerNodePolicy,
    aws_iam_role_policy_attachment.workers_AmazonEKS_CNI_Policy,
    aws_iam_role_policy_attachment.workers_AmazonEC2ContainerRegistryReadOnly
  ]
}
",module,11,,616d30ec674ff1d125710755f5073b1665bbd1af,56e93d77de58f311f1d1d7051f40bf77e7b03524,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/616d30ec674ff1d125710755f5073b1665bbd1af/node_groups.tf#L11,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/56e93d77de58f311f1d1d7051f40bf77e7b03524/node_groups.tf,2020-06-28 02:31:23+02:00,2021-11-06 20:19:03+01:00,7,1
https://github.com/alphagov/govuk-aws,454,terraform/projects/infra-security-groups/deploy.tf,terraform/projects/infra-security-groups/deploy.tf,0,todo,# TODO test whether egress rules are needed on ELBs,# TODO test whether egress rules are needed on ELBs,"resource ""aws_security_group_rule"" ""allow_deploy_internal_elb_egress"" {
  type              = ""egress""
  from_port         = 0
  to_port           = 0
  protocol          = ""-1""
  cidr_blocks       = [""0.0.0.0/0""]
  security_group_id = ""${aws_security_group.deploy_internal_elb.id}""
}
",resource,the block associated got renamed or deleted,,91,,22e8325b92cd6b89bac6fd311576b41e9e4c393b,7fd13330f8d6238e108f76ce76a76a28a99caaaf,https://github.com/alphagov/govuk-aws/blob/22e8325b92cd6b89bac6fd311576b41e9e4c393b/terraform/projects/infra-security-groups/deploy.tf#L91,https://github.com/alphagov/govuk-aws/blob/7fd13330f8d6238e108f76ce76a76a28a99caaaf/terraform/projects/infra-security-groups/deploy.tf,2017-11-27 10:27:49+00:00,2018-01-02 17:41:32+00:00,2,1
https://github.com/compiler-explorer/infra,270,terraform/security.tf,terraform/security.tf,0,todo,// TODO remove these,// TODO remove these,"resource ""aws_security_group_rule"" ""CE_GPUHttpFromAlb"" {
  security_group_id        = aws_security_group.CompilerExplorer.id
  type                     = ""ingress""
  from_port                = 1081
  to_port                  = 1081
  source_security_group_id = aws_security_group.CompilerExplorerAlb.id
  protocol                 = ""tcp""
  description              = ""Allow HTTP access from the ALB""
}
",resource,the block associated got renamed or deleted,,55,,85295c876b56c7417ea7917c51c0a20ddb9b0a07,7e0840cb5b4fdc5b0c79ab82b68ed66fcd522e01,https://github.com/compiler-explorer/infra/blob/85295c876b56c7417ea7917c51c0a20ddb9b0a07/terraform/security.tf#L55,https://github.com/compiler-explorer/infra/blob/7e0840cb5b4fdc5b0c79ab82b68ed66fcd522e01/terraform/security.tf,2022-11-14 21:02:28-06:00,2022-11-15 07:39:11-06:00,2,1
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,376,modules/organization/logging.tf,modules/organization/logging.tf,0,# todo,# TODO(jccb): use a condition to limit writer-identity only to this bucket,# TODO(jccb): use a condition to limit writer-identity only to this bucket,"resource ""google_project_iam_member"" ""bucket-sinks-binding"" {
  for_each = local.sink_bindings[""logging""]
  project  = split(""/"", each.value.destination)[1]
  role     = ""roles/logging.bucketWriter""
  member   = google_logging_organization_sink.sink[each.key].writer_identity
  # TODO(jccb): use a condition to limit writer-identity only to this bucket
}
",resource,"resource ""google_project_iam_member"" ""bucket-sinks-binding"" {
  for_each = local.sink_bindings[""logging""]
  project  = split(""/"", each.value.destination.target)[1]
  role     = ""roles/logging.bucketWriter""
  member   = google_logging_organization_sink.sink[each.key].writer_identity

  condition {
    title       = ""${each.key} bucket writer""
    description = ""Grants bucketWriter to ${google_logging_organization_sink.sink[each.key].writer_identity} used by log sink ${each.key} on ${var.organization_id}""
    expression  = ""resource.name.endsWith('${each.value.destination.target}')""
  }
}
",resource,86,,174de3a087ed8d2fed06c31db40e10cc535269b6,486d398c7d68ce1b0784a540feb7be5fb2c680c1,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/174de3a087ed8d2fed06c31db40e10cc535269b6/modules/organization/logging.tf#L86,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/486d398c7d68ce1b0784a540feb7be5fb2c680c1/modules/organization/logging.tf,2021-12-13 08:41:02+01:00,2022-11-11 19:22:05+01:00,6,1
https://github.com/kubernetes-sigs/kubespray,10,contrib/terraform/openstack/modules/compute/main.tf,contrib/terraform/openstack/modules/compute/main.tf,0,hack,# The join() hack is described here: https://github.com/hashicorp/terraform/issues/11566,"# The join() hack is described here: https://github.com/hashicorp/terraform/issues/11566 
 # As a workaround for creating ""dynamic"" lists (when, for example, no bastion host is created) ","resource ""openstack_compute_instance_v2"" ""k8s_master"" {
  name              = ""${var.cluster_name}-k8s-master-${count.index+1}""
  count             = ""${var.number_of_k8s_masters}""
  availability_zone = ""${element(var.az_list, count.index)}""
  image_name        = ""${var.image}""
  flavor_id         = ""${var.flavor_k8s_master}""
  key_pair          = ""${openstack_compute_keypair_v2.k8s.name}""

  network {
    name = ""${var.network_name}""
  }

  # The join() hack is described here: https://github.com/hashicorp/terraform/issues/11566
  # As a workaround for creating ""dynamic"" lists (when, for example, no bastion host is created)

  security_groups = [""${compact(list(
    openstack_networking_secgroup_v2.k8s_master.name,
    join("" "", openstack_networking_secgroup_v2.bastion.*.id),
    openstack_networking_secgroup_v2.k8s.name,
    ""default"",
   ))}""]
  metadata = {
    ssh_user         = ""${var.ssh_user}""
    kubespray_groups = ""etcd,kube-master,${var.supplementary_master_groups},k8s-cluster,vault""
    depends_on       = ""${var.network_id}""
  }
  provisioner ""local-exec"" {
    command = ""sed s/USER/${var.ssh_user}/ contrib/terraform/openstack/ansible_bastion_template.txt | sed s/BASTION_ADDRESS/${element( concat(var.bastion_fips, var.k8s_master_fips), 0)}/ > contrib/terraform/group_vars/no-floating.yml""
  }
}
",resource,"resource ""openstack_compute_instance_v2"" ""k8s_master"" {
  name              = ""${var.cluster_name}-k8s-master-${count.index+1}""
  count             = ""${var.number_of_k8s_masters}""
  availability_zone = ""${element(var.az_list, count.index)}""
  image_name        = ""${var.image}""
  flavor_id         = ""${var.flavor_k8s_master}""
  key_pair          = ""${openstack_compute_keypair_v2.k8s.name}""

  network {
    name = ""${var.network_name}""
  }

  security_groups = [""${openstack_networking_secgroup_v2.k8s_master.name}"",
    ""${openstack_networking_secgroup_v2.k8s.name}"",
    ""default"",
  ]

  metadata = {
    ssh_user         = ""${var.ssh_user}""
    kubespray_groups = ""etcd,kube-master,${var.supplementary_master_groups},k8s-cluster,vault""
    depends_on       = ""${var.network_id}""
  }

  provisioner ""local-exec"" {
    command = ""sed s/USER/${var.ssh_user}/ contrib/terraform/openstack/ansible_bastion_template.txt | sed s/BASTION_ADDRESS/${element( concat(var.bastion_fips, var.k8s_master_fips), 0)}/ > contrib/terraform/group_vars/no-floating.yml""
  }
}
",resource,105,,20ebb49568547d9621bfdd13945c725a991d5916,7f1d9ff543247a4a1868eab44e79a7fa4438ab70,https://github.com/kubernetes-sigs/kubespray/blob/20ebb49568547d9621bfdd13945c725a991d5916/contrib/terraform/openstack/modules/compute/main.tf#L105,https://github.com/kubernetes-sigs/kubespray/blob/7f1d9ff543247a4a1868eab44e79a7fa4438ab70/contrib/terraform/openstack/modules/compute/main.tf,2019-04-09 04:01:09-07:00,2019-04-15 07:22:08-07:00,2,1
https://github.com/Worklytics/psoxy,2528,infra/modules/gcp-secrets/main.tf,infra/modules/gcp-secrets/main.tf,0,# todo,# TODO: remove this in v0.5,# TODO: remove this in v0.5,"resource ""google_secret_manager_secret_version"" ""version"" {
  for_each = local.secrets_w_terraform_managed_values

  secret      = google_secret_manager_secret.secret[each.key].id
  secret_data = coalesce(each.value.value, ""placeholder value - fill me"")
  # NOTE: can't set `enabled = false` here in placeholder case, bc we bind secret to env var so
  # CloudFunction update will fail as can't bind to ':latest'

  lifecycle {
    create_before_destroy = true

    # TODO: remove this in v0.5
    ignore_changes = [
      enabled, # if secret version disabled after creation, let it be (placeholder case)
    ]
  }
}
",resource,"resource ""google_secret_manager_secret_version"" ""version"" {
  for_each = local.secrets_w_terraform_managed_values

  secret      = google_secret_manager_secret.secret[each.key].id
  secret_data = coalesce(each.value.value, ""placeholder value - fill me"")
  # NOTE: can't set `enabled = false` here in placeholder case, bc we bind secret to env var so
  # CloudFunction update will fail as can't bind to ':latest'

  lifecycle {
    create_before_destroy = true

    # TODO: remove this in v0.5
    ignore_changes = [
      enabled, # if secret version disabled after creation, let it be (placeholder case)
    ]
  }
}
",resource,52,60.0,140ba148a513ad7b6a450120b0a8d39d58c1d908,5e25bc21633cc46c7ba0066959fc9268dedfe92f,https://github.com/Worklytics/psoxy/blob/140ba148a513ad7b6a450120b0a8d39d58c1d908/infra/modules/gcp-secrets/main.tf#L52,https://github.com/Worklytics/psoxy/blob/5e25bc21633cc46c7ba0066959fc9268dedfe92f/infra/modules/gcp-secrets/main.tf#L60,2023-07-25 08:16:19-07:00,2023-08-21 11:08:12-07:00,3,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1911,fast/stages/2-security/vpc-sc.tf,fast/stages/2-security/vpc-sc.tf,0,# todo,# TODO(ludomagno): implement vpc accessible services via variable or factory file,"# TODO(ludomagno): allow passing in restricted services via variable and factory file 
 # TODO(ludomagno): implement vpc accessible services via variable or factory file ","module ""vpc-sc"" {
  source = ""../../../modules/vpc-sc""
  # only enable if the default perimeter is defined
  count         = var.vpc_sc.perimeter_default == null ? 0 : 1
  access_policy = null
  access_policy_create = {
    parent = ""organizations/${var.organization.id}""
    title  = ""default""
  }
  access_levels   = var.vpc_sc.access_levels
  egress_policies = var.vpc_sc.egress_policies
  ingress_policies = merge(
    var.vpc_sc.ingress_policies,
    local.vpc_sc_ingress_policies
  )
  factories_config = var.factories_config.vpc_sc
  service_perimeters_regular = {
    default = {
      spec = (
        var.vpc_sc.perimeter_default.dry_run ? local.vpc_sc_perimeter : null
      )
      status = (
        !var.vpc_sc.perimeter_default.dry_run ? local.vpc_sc_perimeter : null
      )
      use_explicit_dry_run_spec = var.vpc_sc.perimeter_default.dry_run
    }
  }
}
",module,"module ""vpc-sc"" {
  source = ""../../../modules/vpc-sc""
  # only enable if the default perimeter is defined
  count         = var.vpc_sc.perimeter_default == null ? 0 : 1
  access_policy = var.access_policy
  access_policy_create = var.access_policy != null ? null : {
    parent = ""organizations/${var.organization.id}""
    title  = ""default""
  }
  access_levels   = var.vpc_sc.access_levels
  egress_policies = var.vpc_sc.egress_policies
  ingress_policies = merge(
    var.vpc_sc.ingress_policies,
    local.vpc_sc_ingress_policies
  )
  factories_config = var.factories_config.vpc_sc
  service_perimeters_regular = {
    default = {
      spec = (
        var.vpc_sc.perimeter_default.dry_run ? local.vpc_sc_perimeter : null
      )
      status = (
        !var.vpc_sc.perimeter_default.dry_run ? local.vpc_sc_perimeter : null
      )
      use_explicit_dry_run_spec = var.vpc_sc.perimeter_default.dry_run
    }
  }
}
",module,62,62.0,8511170412355efd6c5995431f2e5617d28d604e,7a5dd4e6db197daa52da8a8d877ce86b5c93182e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/8511170412355efd6c5995431f2e5617d28d604e/fast/stages/2-security/vpc-sc.tf#L62,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/7a5dd4e6db197daa52da8a8d877ce86b5c93182e/fast/stages/2-security/vpc-sc.tf#L62,2024-04-07 20:14:39-07:00,2024-05-15 09:17:13+00:00,3,0
https://github.com/terraform-aws-modules/terraform-aws-eks,168,examples/launch_templates_with_managed_node_groups/launchtemplate.tf,examples/launch_templates_with_managed_node_groups/launchtemplate.tf,0,xxx,# This is based on the LT that EKS would create if no custom one is specified (aws ec2 describe-launch-template-versions --launch-template-id xxx),"# This is based on the LT that EKS would create if no custom one is specified (aws ec2 describe-launch-template-versions --launch-template-id xxx) 
 # there are several more options one could set but you probably dont need to modify them 
 # you can take the default and add your custom AMI and/or custom tags 
 # 
 # Trivia: AWS transparently creates a copy of your LaunchTemplate and actually uses that copy then for the node group. If you DONT use a custom AMI, 
 # then the default user-data for bootstrapping a cluster is merged in the copy.","resource ""aws_launch_template"" ""default"" {
  name_prefix            = ""eks-example-""
  description            = ""Default Launch-Template""
  update_default_version = true

  block_device_mappings {
    device_name = ""/dev/xvda""

    ebs {
      volume_size           = 100
      volume_type           = ""gp2""
      delete_on_termination = true
      # encrypted             = true

      # Enable this if you want to encrypt your node root volumes with a KMS/CMK. encryption of PVCs is handled via k8s StorageClass tho
      # you also need to attach data.aws_iam_policy_document.ebs_decryption.json from the disk_encryption_policy.tf to the KMS/CMK key then !!
      # kms_key_id            = var.kms_key_arn
    }
  }

  instance_type = var.instance_type

  monitoring {
    enabled = true
  }

  network_interfaces {
    associate_public_ip_address = false
    delete_on_termination       = true
    security_groups             = [module.eks.worker_security_group_id]
  }

  # if you want to use a custom AMI
  # image_id      = var.ami_id

  # If you use a custom AMI, you need to supply via user-data, the bootstrap script as EKS DOESNT merge its managed user-data then
  # you can add more than the minimum code you see in the template, e.g. install SSM agent, see https://github.com/aws/containers-roadmap/issues/593#issuecomment-577181345
  #
  # (optionally you can use https://registry.terraform.io/providers/hashicorp/cloudinit/latest/docs/data-sources/cloudinit_config to render the script, example: https://github.com/terraform-aws-modules/terraform-aws-eks/pull/997#issuecomment-705286151)

  # user_data = base64encode(
  #   data.template_file.launch_template_userdata.rendered,
  # )


  # Supplying custom tags to EKS instances is another use-case for LaunchTemplates
  tag_specifications {
    resource_type = ""instance""

    tags = {
      CustomTag = ""EKS example""
    }
  }

  # Supplying custom tags to EKS instances root volumes is another use-case for LaunchTemplates. (doesnt add tags to dynamically provisioned volumes via PVC tho)
  tag_specifications {
    resource_type = ""volume""

    tags = {
      CustomTag = ""EKS example""
    }
  }

  # Tag the LT itself
  tags = {
    CustomTag = ""EKS example""
  }

  lifecycle {
    create_before_destroy = true
  }
}
",resource,,,14,0.0,571e4e7f4bca0c30a6d714a4cbebb9aaaf69c88a,ee9f0c646a45ca9baa6174a036d1e09bcccb87b1,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/571e4e7f4bca0c30a6d714a4cbebb9aaaf69c88a/examples/launch_templates_with_managed_node_groups/launchtemplate.tf#L14,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/ee9f0c646a45ca9baa6174a036d1e09bcccb87b1/examples/launch_templates_with_managed_node_groups/launchtemplate.tf#L0,2020-11-02 08:35:12+01:00,2022-01-05 13:01:31+01:00,6,2
https://github.com/pingcap/tidb-operator,10,deploy/alicloud/main.tf,deploy/aliyun/main.tf,1,fix,# TODO: use helm and kubernetes provider when upstream get this fixed,"# Workaround: Terraform cannot specify provider dependency, so we take over kubernetes and helm stuffs, 
 # But we cannot ouput kubernetes and helm resources in this way. 
 # TODO: use helm and kubernetes provider when upstream get this fixed","resource ""null_resource"" ""deploy-tidb-cluster"" {
  depends_on = [""null_resource.setup-env"", ""local_file.tidb-cluster-values""]

  triggers {
    values = ""${data.template_file.tidb-cluster-values.rendered}""
  }

  provisioner ""local-exec"" {
    command = <<EOS
helm upgrade --install tidb-cluster ${path.module}/charts/tidb-cluster --namespace=tidb -f ${local.tidb_cluster_values_path}
echo ""TiDB cluster setup complete!""
EOS

    environment = {
      KUBECONFIG = ""${local.kubeconfig}""
    }
  }
}
",resource,the block associated got renamed or deleted,,121,,eebd686956c0b2adf67a10e1a376659c95c571a3,042b1a97fbbdf342297002990564828e6644a3f0,https://github.com/pingcap/tidb-operator/blob/eebd686956c0b2adf67a10e1a376659c95c571a3/deploy/alicloud/main.tf#L121,https://github.com/pingcap/tidb-operator/blob/042b1a97fbbdf342297002990564828e6644a3f0/deploy/aliyun/main.tf,2019-05-06 19:59:43+08:00,2019-07-23 19:44:58+08:00,4,1
https://github.com/terraform-aws-modules/terraform-aws-eks,21,local.tf,local.tf,0,workaround,# to workaround terraform not supporting short circut evaluation,"# Followed recommendation http://67bricks.com/blog/?p=85 
 # to workaround terraform not supporting short circut evaluation",,,the block associated got renamed or deleted,,5,,7e4e93eeec4a2d0e84440258297688d36b569501,c9986f5e01c785875cb1e9cfa21ba195ef1bbab7,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/7e4e93eeec4a2d0e84440258297688d36b569501/local.tf#L5,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/c9986f5e01c785875cb1e9cfa21ba195ef1bbab7/local.tf,2018-07-09 10:40:51+02:00,2019-08-06 18:05:54+02:00,43,1
https://github.com/magma/magma,56,orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf,orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf,0,crap,# Set timeout for scrape,"# Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator 
 # labels: 
 # Set timeout for scrape","resource ""helm_release"" ""yace_exporter"" {
  count = var.cloudwatch_exporter_enabled ? 1 : 0

  name       = ""yace-exporter""
  namespace  = kubernetes_namespace.monitoring.metadata[0].name
  repository = ""https://mogaal.github.io/helm-charts/""
  chart      = ""prometheus-yace-exporter""
  timeout    = 600
  values = [<<EOT
# Default values for prometheus-yace-exporter.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: quay.io/invisionag/yet-another-cloudwatch-exporter
  tag: v0.25.0-alpha
  pullPolicy: IfNotPresent

nameOverride: """"
fullnameOverride: """"

service:
  type: ClusterIP
  port: 80
  annotations:
    prometheus.io/scrape: ""true""
    prometheus.io/port: ""5000""
  labels: {}

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: ""true""
  hosts:
    - host: chart-example.local
      paths: []

  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}

podAnnoatations: {}

podLabels: {}

aws:
  role:

  # The name of a pre-created secret in which AWS credentials are stored. When
  # set, aws_access_key_id is assumed to be in a field called access_key,
  # aws_secret_access_key is assumed to be in a field called secret_key, and the
  # session token, if it exists, is assumed to be in a field called
  # security_token
  secret:
    name:
    includesSessionToken: false

  # Note: Do not specify the aws_access_key_id and aws_secret_access_key if you specified role or secret.name before
  aws_access_key_id:
  aws_secret_access_key:

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  annotations: {}
  labels: {}
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

rbac:
  # Specifies whether RBAC resources should be created
  create: true

serviceMonitor:
  # When set true then use a ServiceMonitor to configure scraping
  enabled: true
  # Set the namespace the ServiceMonitor should be deployed
  namespace: ${var.orc8r_kubernetes_namespace}
  # Set how frequently Prometheus should scrape
  interval: 30s
  # Set targetPort for serviceMonitor
  port: http
  # Set path to cloudwatch-exporter telemtery-path
  telemetryPath: /metrics
  # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator
  # labels:
  # Set timeout for scrape
  timeout: 10s


config: |-
  discovery:
    exportedTagsOnMetrics:
      ec2:
        - Name
      ebs:
        - VolumeId
    jobs:
    - regions:
        - ${var.region}
      type: ""es""
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: FreeStorageSpace
          statistics:
          - 'Sum'
          period: 600
          length: 60
        - name: ClusterStatus.green
          statistics:
          - 'Minimum'
          period: 600
          length: 60
        - name: ClusterStatus.yellow
          statistics:
          - 'Maximum'
          period: 600
          length: 60
        - name: ClusterStatus.red
          statistics:
          - 'Maximum'
          period: 600
          length: 60
    - type: ""elb""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: HealthyHostCount
          statistics:
          - 'Minimum'
          period: 600
          length: 600
        - name: HTTPCode_Backend_4XX
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
    - type: ""ec2""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: NetworkIn
          statistics:
          - 'Sum'
          period: 600
          length: 600
        - name: NetworkOut
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
  EOT
  ]
  set_sensitive {
    name  = ""aws.aws_access_key_id""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].id : """"
  }
  set_sensitive {
    name  = ""aws.aws_secret_access_key""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].secret : """"
  }
}",resource,"resource ""helm_release"" ""yace_exporter"" {
  count = var.cloudwatch_exporter_enabled ? 1 : 0

  name       = ""yace-exporter""
  namespace  = kubernetes_namespace.monitoring[0].metadata[0].name
  repository = ""https://mogaal.github.io/helm-charts/""
  chart      = ""prometheus-yace-exporter""
  timeout    = 600
  values = [<<EOT
# Default values for prometheus-yace-exporter.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: quay.io/invisionag/yet-another-cloudwatch-exporter
  tag: v0.25.0-alpha
  pullPolicy: IfNotPresent

nameOverride: """"
fullnameOverride: """"

service:
  type: ClusterIP
  port: 80
  annotations:
    prometheus.io/scrape: ""true""
    prometheus.io/port: ""5000""
  labels: {}

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: ""true""
  hosts:
    - host: chart-example.local
      paths: []

  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}

podAnnoatations: {}

podLabels: {}

aws:
  role:

  # The name of a pre-created secret in which AWS credentials are stored. When
  # set, aws_access_key_id is assumed to be in a field called access_key,
  # aws_secret_access_key is assumed to be in a field called secret_key, and the
  # session token, if it exists, is assumed to be in a field called
  # security_token
  secret:
    name:
    includesSessionToken: false

  # Note: Do not specify the aws_access_key_id and aws_secret_access_key if you specified role or secret.name before
  aws_access_key_id:
  aws_secret_access_key:

serviceAccount:
  # Specifies whether a ServiceAccount should be created
  create: true
  annotations: {}
  labels: {}
  # The name of the ServiceAccount to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

rbac:
  # Specifies whether RBAC resources should be created
  create: true

serviceMonitor:
  # When set true then use a ServiceMonitor to configure scraping
  enabled: true
  # Set the namespace the ServiceMonitor should be deployed
  namespace: ${var.orc8r_kubernetes_namespace}
  # Set how frequently Prometheus should scrape
  interval: 30s
  # Set targetPort for serviceMonitor
  port: http
  # Set path to cloudwatch-exporter telemtery-path
  telemetryPath: /metrics
  # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator
  # labels:
  # Set timeout for scrape
  timeout: 10s


config: |-
  discovery:
    exportedTagsOnMetrics:
      ec2:
        - Name
      ebs:
        - VolumeId
    jobs:
    - regions:
        - ${var.region}
      type: ""es""
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: FreeStorageSpace
          statistics:
          - 'Sum'
          period: 600
          length: 60
        - name: ClusterStatus.green
          statistics:
          - 'Minimum'
          period: 600
          length: 60
        - name: ClusterStatus.yellow
          statistics:
          - 'Maximum'
          period: 600
          length: 60
        - name: ClusterStatus.red
          statistics:
          - 'Maximum'
          period: 600
          length: 60
    - type: ""elb""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: HealthyHostCount
          statistics:
          - 'Minimum'
          period: 600
          length: 600
        - name: HTTPCode_Backend_4XX
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
    - type: ""ec2""
      regions:
        - ${var.region}
      searchTags:
        - Key: magma-uuid
          Value: ${var.magma_uuid}
      metrics:
        - name: NetworkIn
          statistics:
          - 'Sum'
          period: 600
          length: 600
        - name: NetworkOut
          statistics:
          - 'Sum'
          period: 60
          length: 900
          delay: 300
          nilToZero: true
  EOT
  ]
  set_sensitive {
    name  = ""aws.aws_access_key_id""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].id : """"
  }
  set_sensitive {
    name  = ""aws.aws_secret_access_key""
    value = var.cloudwatch_exporter_enabled ? aws_iam_access_key.yace_access_key[0].secret : """"
  }
}",resource,162,162.0,cbee95c6b1cdad7ba277e02b920f715e21c97df6,b72801aa1cccc468273bbb99e5ec4fafa3c052c9,https://github.com/magma/magma/blob/cbee95c6b1cdad7ba277e02b920f715e21c97df6/orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf#L162,https://github.com/magma/magma/blob/b72801aa1cccc468273bbb99e5ec4fafa3c052c9/orc8r/cloud/deploy/terraform/orc8r-helm-aws/yace.tf#L162,2021-06-04 09:12:15+03:00,2022-03-10 13:24:52+00:00,3,0
https://github.com/chanzuckerberg/cztack,127,snowflake-table-grant-all/main.tf,snowflake-table-grant-all/main.tf,0,hack,//           It is a bit of a hack but probably can't do much better with the current structure of the provider (and its limitations).,"// HACK(el): The way the provider works, we can only have one grant per (db, share, table, on_future, with_grant_option) grant 
 //           because of this, if we simulate an ALL grant for a tuple X for role foo through this module 
 //           we couldn't then issue a SELECT grant for the same tuple X for a different role bar. 
 //           We therefore expand this module so that you can issue specific privilege grants to other roles and shares 
 //           It is a bit of a hack but probably can't do much better with the current structure of the provider (and its limitations).","resource snowflake_table_grant all {
  for_each = toset(local.privileges)

  database_name = var.database_name
  schema_name   = var.schema_name
  table_name    = var.table_name

  // HACK(el): The way the provider works, we can only have one grant per (db, share, table, on_future, with_grant_option) grant
  //           because of this, if we simulate an ALL grant for a tuple X for role foo through this module
  //           we couldn't then issue a SELECT grant for the same tuple X for a different role bar.
  //           We therefore expand this module so that you can issue specific privilege grants to other roles and shares
  //           It is a bit of a hack but probably can't do much better with the current structure of the provider (and its limitations).
  roles = setunion(
    var.roles,
    lookup(var.per_privilege_grants, each.value, { roles = [], shares = [] }).roles,
  )
  shares = setunion(
    var.shares,
    lookup(var.per_privilege_grants, each.value, { roles = [], shares = [] }).shares,
  )

  on_future         = var.on_future
  with_grant_option = var.with_grant_option

  privilege = each.value
}
",resource,,,24,0.0,fafd05ae12d4d162cb2ba2715fc61c36b14a53a7,7cebb2e1dd3a50f9ef31130f550d44a6ac273813,https://github.com/chanzuckerberg/cztack/blob/fafd05ae12d4d162cb2ba2715fc61c36b14a53a7/snowflake-table-grant-all/main.tf#L24,https://github.com/chanzuckerberg/cztack/blob/7cebb2e1dd3a50f9ef31130f550d44a6ac273813/snowflake-table-grant-all/main.tf#L0,2020-12-04 12:50:00-08:00,2020-12-10 10:30:42-08:00,2,2
https://github.com/Worklytics/psoxy,484,infra/modules/aws-psoxy-lambda/variables.tf,infra/modules/aws-psoxy-lambda/variables.tf,0,# todo,# TODO: remove after v0.4.x,# TODO: remove after v0.4.x,"variable ""function_parameters"" {
  type = list(object({
    name     = string
    writable = bool
  }))
  description = ""IGNORED; Parameter names and expected grant to create for function""
  default     = []
}
",variable,"variable ""function_parameters"" {
  type = list(object({
    name     = string
    writable = bool
  }))
  description = ""IGNORED; Parameter names and expected grant to create for function""
  default     = []
}
",variable,87,145.0,d1d2ef9b4a358d71a7d8e228a8291a4569a6f3cf,b50ff85175c9c27d056239d60e41575bdd715b02,https://github.com/Worklytics/psoxy/blob/d1d2ef9b4a358d71a7d8e228a8291a4569a6f3cf/infra/modules/aws-psoxy-lambda/variables.tf#L87,https://github.com/Worklytics/psoxy/blob/b50ff85175c9c27d056239d60e41575bdd715b02/infra/modules/aws-psoxy-lambda/variables.tf#L145,2022-10-27 14:12:45-07:00,2024-03-06 21:11:28+00:00,8,0
https://github.com/nebari-dev/nebari,8,qhub/template/{{ cookiecutter.repo_directory }}/infrastructure/modules/kubernetes/services/monitoring/main.tf,qhub/template/{{ cookiecutter.repo_directory }}/infrastructure/modules/kubernetes/services/monitoring/main.tf,0,crap,# and the annotation app.kubernetes.io/scrape=true,"# This job will scrape from any service with the label app.kubernetes.io/component=traefik-internal-service 
 # and the annotation app.kubernetes.io/scrape=true","resource ""helm_release"" ""kube-prometheus-stack-helm-deployment"" {
  name       = ""kube-prometheus-stack""
  namespace  = var.namespace
  repository = ""https://prometheus-community.github.io/helm-charts""
  chart      = ""kube-prometheus-stack""
  version    = ""16.12.0""

  values = [<<EOT
prometheus:    
  prometheusSpec:    
    additionalScrapeConfigs:    
    
    # This job will scrape from any service with the label app.kubernetes.io/component=traefik-internal-service
    # and the annotation app.kubernetes.io/scrape=true 
    - job_name: 'traefik'    
    
      kubernetes_sd_configs:    
        - role: service    
          
      relabel_configs:    
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]    
        action: keep    
        regex: true    
      - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]    
        action: replace    
        target_label: __metrics_path__    
        regex: (.+)    
      - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
        action: replace    
        regex: ([^:]+)(?::\d+)?;(\d+)    
        replacement: $1:$2    
        target_label: __address__
EOT
  ]

  set {
    name  = ""grafana.grafana\\.ini.server.domain""
    value = var.external-url
  }

  set {
    name  = ""grafana.grafana\\.ini.server.root_url""
    value = ""%(protocol)s://%(domain)s/monitoring""
  }

  set {
    name  = ""grafana.grafana\\.ini.server.server_from_sub_path""
    value = ""true""
  }
}
",resource,,,14,0.0,8d1ae18da01ec173c05970f466058a048c6f83ac,e65621ed9fc3e374626cc3929742df6ba94fc8d7,https://github.com/nebari-dev/nebari/blob/8d1ae18da01ec173c05970f466058a048c6f83ac/qhub/template/{{ cookiecutter.repo_directory }}/infrastructure/modules/kubernetes/services/monitoring/main.tf#L14,https://github.com/nebari-dev/nebari/blob/e65621ed9fc3e374626cc3929742df6ba94fc8d7/qhub/template/{{ cookiecutter.repo_directory }}/infrastructure/modules/kubernetes/services/monitoring/main.tf#L0,2021-09-13 17:39:56-05:00,2022-02-03 11:12:40-05:00,2,2
https://github.com/compiler-explorer/infra,274,terraform/tg.tf,terraform/tg.tf,0,todo,// TODO remove me,// TODO remove me,"resource ""aws_alb_target_group"" ""gpu"" {
  lifecycle {
    create_before_destroy = true
  }
  name                 = ""GPUGroup""
  port                 = 1081
  protocol             = ""HTTP""
  vpc_id               = module.ce_network.vpc.id
  deregistration_delay = 15
  health_check {
    path                = ""/healthcheck""
    timeout             = 3
    unhealthy_threshold = 3
    healthy_threshold   = 2
    interval            = 5
    protocol            = ""HTTP""
  }
}
",resource,the block associated got renamed or deleted,,60,,85295c876b56c7417ea7917c51c0a20ddb9b0a07,7e0840cb5b4fdc5b0c79ab82b68ed66fcd522e01,https://github.com/compiler-explorer/infra/blob/85295c876b56c7417ea7917c51c0a20ddb9b0a07/terraform/tg.tf#L60,https://github.com/compiler-explorer/infra/blob/7e0840cb5b4fdc5b0c79ab82b68ed66fcd522e01/terraform/tg.tf,2022-11-14 21:02:28-06:00,2022-11-15 07:39:11-06:00,2,1
https://github.com/nasa/cumulus,37,example/cumulus-tf/main.tf,example/cumulus-tf/main.tf,0,todo,# TODO Add this aws_sns_topic_subscription,"# TODO Add this aws_sns_topic_subscription 
 # Subscribes to module.archive.aws_sns_topic.sftracker 
 # - Endpoint: 
 #     Fn::GetAtt: 
 #       - SnsS3TestLambdaFunction 
 #       - Arn 
 #   Protocol: lambda  
 # TODO Add this permission to example 
 # Related to module.archive.aws_sns_topic.sftracker 
 # sftracker2ndlambdaSubscriptionPermission: 
 #   Type: AWS::Lambda::Permission 
 #   Properties: 
 #     FunctionName: 
 #       Fn::GetAtt: 
 #         - SnsS3TestLambdaFunction 
 #         - Arn 
 #     Action: lambda:InvokeFunction 
 #     Principal: sns.amazonaws.com 
 #     SourceArn: 
 #       Ref: sftrackerSns ",,,the block associated got renamed or deleted,,69,,59283cd10c3891258816b76160848a494ffc35b7,965387f0c8733a7fc3ba8746173ff4f25bfd328c,https://github.com/nasa/cumulus/blob/59283cd10c3891258816b76160848a494ffc35b7/example/cumulus-tf/main.tf#L69,https://github.com/nasa/cumulus/blob/965387f0c8733a7fc3ba8746173ff4f25bfd328c/example/cumulus-tf/main.tf,2019-08-16 13:47:55-04:00,2019-09-06 10:48:26-04:00,4,1
https://github.com/camptocamp/devops-stack,98,examples/kind/main.tf,examples/kind/main.tf,0,fix,# For now random value is passed to base_domain. Redirections will not work before fix.,"# TODO fix: the base domain is defined later. Proposal: remove redirection from traefik module and add it in dependent modules. 
 # For now random value is passed to base_domain. Redirections will not work before fix.","module ""traefik"" {
  source = ""git::https://github.com/camptocamp/devops-stack-module-traefik.git//kind?ref=v1.0.0""

  cluster_name = local.cluster_name

  # TODO fix: the base domain is defined later. Proposal: remove redirection from traefik module and add it in dependent modules.
  # For now random value is passed to base_domain. Redirections will not work before fix.
  base_domain = ""placeholder.com""

  argocd_namespace = module.argocd_bootstrap.argocd_namespace

  dependency_ids = {
    argocd = module.argocd_bootstrap.id
  }
}
",module,"module ""traefik"" {
  source = ""git::https://github.com/camptocamp/devops-stack-module-traefik.git//kind?ref=v6.2.0""

  argocd_project = local.cluster_name

  app_autosync           = local.app_autosync
  enable_service_monitor = local.enable_service_monitor

  dependency_ids = {
    argocd = module.argocd_bootstrap.id
  }
}
",module,73,,e3e1a35b6a90a2990878d6c06775a6dba94637af,10d0d93a6e34b9b8e29beca8c6e01ec6ca3a8244,https://github.com/camptocamp/devops-stack/blob/e3e1a35b6a90a2990878d6c06775a6dba94637af/examples/kind/main.tf#L73,https://github.com/camptocamp/devops-stack/blob/10d0d93a6e34b9b8e29beca8c6e01ec6ca3a8244/examples/kind/main.tf,2023-05-16 13:05:31+02:00,2024-03-15 09:27:05+01:00,29,1
https://github.com/nebari-dev/nebari,13,qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf,qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf,0,workaround,#        is a workaround to meet the requirement of having a schema.,"# FIXME: Make this an actual schema instead of this dummy schema that 
 #        is a workaround to meet the requirement of having a schema.","resource ""kubernetes_manifest"" ""main"" {
  manifest = {
    apiVersion = ""apiextensions.k8s.io/v1""
    kind       = ""CustomResourceDefinition""
    metadata = {
      name = ""daskclusters.gateway.dask.org""
    }
    spec = {
      group   = ""gateway.dask.org""
      names = {
        kind     = ""DaskCluster""
        listKind = ""DaskClusterList""
        plural   = ""daskclusters""
        singular = ""daskcluster""
      }
      scope = ""Namespaced""
      versions = [{
        name = ""v1alpha1""
        served = true
        storage = true
        subresources = {
          status = {}
        }

        # NOTE: While we define a schema, it is a dummy schema that doesn't
        #       validate anything. We just have it to comply with the schema of
        #       a CustomResourceDefinition that requires it.
        #
        #       A decision has been made to not implement an actual schema at
        #       this point in time due to the additional maintenance work it
        #       would require.
        #
        #       Reference: https://github.com/dask/dask-gateway/issues/434
        #
        schema = {
          openAPIV3Schema = {
            type = ""object""
            # FIXME: Make this an actual schema instead of this dummy schema that
            #        is a workaround to meet the requirement of having a schema.
            x-kubernetes-preserve-unknown-fields = true
          }
        }
      }]
    }
  }
}
",resource,"resource ""kubernetes_manifest"" ""main"" {
  manifest = {
    apiVersion = ""apiextensions.k8s.io/v1""
    kind       = ""CustomResourceDefinition""
    metadata = {
      name = ""daskclusters.gateway.dask.org""
    }
    spec = {
      group = ""gateway.dask.org""
      names = {
        kind     = ""DaskCluster""
        listKind = ""DaskClusterList""
        plural   = ""daskclusters""
        singular = ""daskcluster""
      }
      scope = ""Namespaced""
      versions = [{
        name    = ""v1alpha1""
        served  = true
        storage = true
        subresources = {
          status = {}
        }

        # NOTE: While we define a schema, it is a dummy schema that doesn't
        #       validate anything. We just have it to comply with the schema of
        #       a CustomResourceDefinition that requires it.
        #
        #       A decision has been made to not implement an actual schema at
        #       this point in time due to the additional maintenance work it
        #       would require.
        #
        #       Reference: https://github.com/dask/dask-gateway/issues/434
        #
        schema = {
          openAPIV3Schema = {
            type = ""object""
            # FIXME: Make this an actual schema instead of this dummy schema that
            #        is a workaround to meet the requirement of having a schema.
            x-kubernetes-preserve-unknown-fields = true
          }
        }
      }]
    }
  }
}
",resource,39,39.0,e65621ed9fc3e374626cc3929742df6ba94fc8d7,d0cc26638fbc9e69aa736105ffd61cfb50d561d6,https://github.com/nebari-dev/nebari/blob/e65621ed9fc3e374626cc3929742df6ba94fc8d7/qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf#L39,https://github.com/nebari-dev/nebari/blob/d0cc26638fbc9e69aa736105ffd61cfb50d561d6/qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf#L39,2022-02-03 11:12:40-05:00,2022-05-26 14:22:33-07:00,2,0
https://github.com/SUSE/ha-sap-terraform-deployments,2,libvirt/terraform/modules/host/variables.tf,libvirt/modules/host/variables.tf,1,hack,"# HACK: """" cannot be used as a default because of https://github.com/hashicorp/hil/issues/50","# HACK: """" cannot be used as a default because of https://github.com/hashicorp/hil/issues/50","variable ""ssh_key_path"" {
  description = ""path of additional pub ssh key you want to use to access VMs, see README_ADVANCED.md""
  default = ""/dev/null""
  # HACK: """" cannot be used as a default because of https://github.com/hashicorp/hil/issues/50
}
",variable,,,34,0.0,ca70a8dbd37c0acdc707942a2a17b1c9980103ee,931fa7a92fa223adb7017f2208562f0ac5e0a81c,https://github.com/SUSE/ha-sap-terraform-deployments/blob/ca70a8dbd37c0acdc707942a2a17b1c9980103ee/libvirt/terraform/modules/host/variables.tf#L34,https://github.com/SUSE/ha-sap-terraform-deployments/blob/931fa7a92fa223adb7017f2208562f0ac5e0a81c/libvirt/modules/host/variables.tf#L0,2019-01-08 15:23:54+00:00,2019-08-27 16:50:17+02:00,14,2
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,851,fast/stages/03-gke-multitenant/prod/gke-clusters.tf,blueprints/gke/multitenant-fleet/gke-clusters.tf,1,# todo,# TODO(jccb): change fabric module,"# resource_usage_export_config = { 
 #   enabled = true 
 #   dataset = module.gke-dataset-resource-usage.id 
 # } 
 # TODO: the attributes below are ""primed"" from project-level defaults 
 #       in locals, merge defaults with cluster-level stuff 
 # TODO(jccb): change fabric module","module ""gke-cluster"" {
  source                   = ""../../../../modules/gke-cluster""
  for_each                 = local.clusters
  project_id               = module.gke-project-0.project_id
  name                     = each.key
  description              = each.value.description
  location                 = each.value.location
  network                  = each.value.net.vpc
  subnetwork               = each.value.net.subnet
  secondary_range_pods     = each.value.net.pods
  secondary_range_services = each.value.net.services
  labels                   = each.value.labels
  addons = {
    cloudrun_config                       = each.value.overrides.cloudrun_config
    dns_cache_config                      = true
    http_load_balancing                   = true
    gce_persistent_disk_csi_driver_config = true
    horizontal_pod_autoscaling            = true
    config_connector_config               = true
    kalm_config                           = false
    # enable only if enable_dataplane_v2 is changed to false below
    network_policy_config = false
    istio_config = {
      enabled = false
      tls     = false
    }
  }
  # change these here for all clusters if absolutely needed
  # authenticator_security_group = var.authenticator_security_group
  enable_dataplane_v2         = true
  enable_l4_ilb_subsetting    = false
  enable_intranode_visibility = true
  enable_shielded_nodes       = true
  workload_identity           = true
  private_cluster_config = {
    enable_private_nodes    = true
    enable_private_endpoint = true
    master_ipv4_cidr_block  = each.value.net.master_range
    master_global_access    = true
  }
  dns_config = each.value.dns_domain == null ? null : {
    cluster_dns        = ""CLOUD_DNS""
    cluster_dns_scope  = ""VPC_SCOPE""
    cluster_dns_domain = ""${each.key}.${var.dns_domain}""
  }
  logging_config    = [""SYSTEM_COMPONENTS"", ""WORKLOADS""]
  monitoring_config = [""SYSTEM_COMPONENTS"", ""WORKLOADS""]

  # if you don't have compute.networks.updatePeering in the host
  # project, comment out the next line and ask your network admin to
  # create the peering for you
  peering_config = {
    export_routes = true
    import_routes = false
    project_id    = var.vpc_host_project
  }
  # resource_usage_export_config = {
  #   enabled = true
  #   dataset = module.gke-dataset-resource-usage.id
  # }
  # TODO: the attributes below are ""primed"" from project-level defaults
  #       in locals, merge defaults with cluster-level stuff
  # TODO(jccb): change fabric module
  database_encryption = (
    each.value.overrides.database_encryption_key == null ? {
      enabled  = false
      state    = null
      key_name = null
      } : {
      enabled  = true
      state    = ""ENCRYPTED""
      key_name = each.value.overrides.database_encryption_key
    }
  )
  default_max_pods_per_node   = each.value.overrides.max_pods_per_node
  enable_binary_authorization = each.value.overrides.enable_binary_authorization
  master_authorized_ranges    = each.value.overrides.master_authorized_ranges
  pod_security_policy         = each.value.overrides.pod_security_policy
  release_channel             = each.value.overrides.release_channel
  vertical_pod_autoscaling    = each.value.overrides.vertical_pod_autoscaling
  # dynamic ""cluster_autoscaling"" {
  #   for_each = each.value.cluster_autoscaling == null ? {} : { 1 = 1 }
  #   content {
  #     enabled    = true
  #     cpu_min    = each.value.cluster_autoscaling.cpu_min
  #     cpu_max    = each.value.cluster_autoscaling.cpu_max
  #     memory_min = each.value.cluster_autoscaling.memory_min
  #     memory_max = each.value.cluster_autoscaling.memory_max
  #   }
  # }

  depends_on = [
    google_project_iam_member.host_project_bindings
  ]
}
",module,"module ""gke-cluster"" {
  source      = ""../../../modules/gke-cluster""
  for_each    = local.clusters
  name        = each.key
  project_id  = module.gke-project-0.project_id
  description = each.value.description
  location    = each.value.location
  vpc_config = {
    network    = var.vpc_config.vpc_self_link
    subnetwork = each.value.net.subnet
    secondary_range_names = {
      pods     = each.value.net.pods
      services = each.value.net.services
    }
    master_authorized_ranges = each.value.overrides.master_authorized_ranges
  }
  labels = each.value.labels
  enable_addons = {
    cloudrun                       = each.value.overrides.cloudrun_config
    config_connector               = true
    dns_cache                      = true
    gce_persistent_disk_csi_driver = true
    gcp_filestore_csi_driver       = each.value.overrides.gcp_filestore_csi_driver_config
    gke_backup_agent               = false
    horizontal_pod_autoscaling     = true
    http_load_balancing            = true
  }
  enable_features = {
    cloud_dns = var.dns_domain == null ? null : {
      cluster_dns        = ""CLOUD_DNS""
      cluster_dns_scope  = ""VPC_SCOPE""
      cluster_dns_domain = ""${each.key}.${var.dns_domain}""
    }
    database_encryption = (
      each.value.overrides.database_encryption_key == null
      ? null
      : {
        state    = ""ENCRYPTED""
        key_name = each.value.overrides.database_encryption_key
      }
    )
    dataplane_v2         = true
    groups_for_rbac      = var.authenticator_security_group
    intranode_visibility = true
    pod_security_policy  = each.value.overrides.pod_security_policy
    resource_usage_export = {
      dataset = module.gke-dataset-resource-usage.dataset_id
    }
    shielded_nodes           = true
    vertical_pod_autoscaling = each.value.overrides.vertical_pod_autoscaling
    workload_identity        = true
  }
  private_cluster_config = {
    enable_private_endpoint = true
    master_ipv4_cidr_block  = each.value.net.master_range
    master_global_access    = true
    peering_config = var.peering_config == null ? null : {
      export_routes = var.peering_config.export_routes
      import_routes = var.peering_config.import_routes
      project_id    = var.vpc_config.host_project_id
    }
  }
  logging_config    = [""SYSTEM_COMPONENTS"", ""WORKLOADS""]
  monitoring_config = [""SYSTEM_COMPONENTS"", ""WORKLOADS""]
  max_pods_per_node = each.value.overrides.max_pods_per_node
  release_channel   = each.value.overrides.release_channel
}
",module,88,,f3f9a4a88cedd64f6fc91b64666046aa6726a2f3,16822e94ab70d75099214b9db786affcb231fbf6,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/f3f9a4a88cedd64f6fc91b64666046aa6726a2f3/fast/stages/03-gke-multitenant/prod/gke-clusters.tf#L88,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/16822e94ab70d75099214b9db786affcb231fbf6/blueprints/gke/multitenant-fleet/gke-clusters.tf,2022-06-08 11:41:50+02:00,2022-10-10 09:38:21+02:00,15,1
https://github.com/terraform-aws-modules/terraform-aws-iam,5,modules/iam-role-for-service-accounts-eks/policies.tf,modules/iam-role-for-service-accounts-eks/policies.tf,0,todo,# TODO - remove this at next breaking change,# TODO - remove this at next breaking change,"locals {
  # TODO - remove this at next breaking change
  karpenter_controller_cluster_name = var.karpenter_controller_cluster_name != ""*"" ? var.karpenter_controller_cluster_name : var.karpenter_controller_cluster_id
}
",locals,"locals {
  # TODO - remove this at next breaking change
  karpenter_controller_cluster_name = var.karpenter_controller_cluster_name != ""*"" ? var.karpenter_controller_cluster_name : var.karpenter_controller_cluster_id
}
",locals,590,659.0,fdee003477c5f86c4236be08ef6a69dffbcc39fd,f9d5e28996ca282af4c09cb97f6291cf77ac03ea,https://github.com/terraform-aws-modules/terraform-aws-iam/blob/fdee003477c5f86c4236be08ef6a69dffbcc39fd/modules/iam-role-for-service-accounts-eks/policies.tf#L590,https://github.com/terraform-aws-modules/terraform-aws-iam/blob/f9d5e28996ca282af4c09cb97f6291cf77ac03ea/modules/iam-role-for-service-accounts-eks/policies.tf#L659,2023-05-22 15:55:09-04:00,2024-04-08 18:43:26-04:00,13,0
https://github.com/aws-observability/terraform-aws-observability-accelerator,51,workloads/infra/rules.tf,modules/workloads/infra/rules.tf,1,crap,#           summary: Node Exporter text file collector failed to scrape.,,,,the block associated got renamed or deleted,,629,,de50a0dfb5dd8a18a6b206ead81a49679b85a857,60ab8b13bf236ceb04960b343b36383241d813db,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/de50a0dfb5dd8a18a6b206ead81a49679b85a857/workloads/infra/rules.tf#L629,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/60ab8b13bf236ceb04960b343b36383241d813db/modules/workloads/infra/rules.tf,2022-08-26 17:30:03+02:00,2022-08-26 17:30:03+02:00,2,1
https://github.com/oracle-terraform-modules/terraform-oci-oke,360,modules/network/subnets.tf,modules/network/subnets.tf,0,todo,"# TODO enumerate worker pools for public/private overrides, conditional subnets for both","# Map of subnets for standard components with additional configuration derived 
 # TODO enumerate worker pools for public/private overrides, conditional subnets for both","locals {
  # VCN subnet configuration
  # See https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengnetworkconfig.htm#vcnconfig
  # May be undefined when VCN is neither created nor required, e.g. when creating only workers for
  # an existing cluster. Fallback value is unused.
  vcn_cidr = length(var.vcn_cidrs) > 0 ? element(var.vcn_cidrs, 0) : ""0.0.0.0/16""

  # Filter configured subnets eligible for resource creation
  subnet_cidrs_new = {
    for k, v in var.subnets : k => merge(v, {
      ""type"" = (lookup(v, ""netnum"", null) == null && lookup(v, ""newbits"", null) != null ? ""newbits""
        : (lookup(v, ""netnum"", null) != null && lookup(v, ""newbits"", null) != null ? ""netnum""
          : (lookup(v, ""cidr"", null) != null ? ""cidr""
            : (lookup(v, ""id"", null) != null ? ""id""
      : ""invalid""))))
    })
  }

  # Handle subnets configured with provided CIDRs
  subnet_cidrs_cidr_input = {
    for k, v in local.subnet_cidrs_new : k => lookup(v, ""cidr"") if v.type == ""cidr""
  }

  # Handle subnets configured with only newbits for sizing
  subnet_cidrs_newbits_input = {
    for k, v in local.subnet_cidrs_new : k => lookup(v, ""newbits"") if v.type == ""newbits""
  }

  # Generate CIDR ranges for subnets to be created
  subnet_cidrs_newbits_ranges = cidrsubnets(local.vcn_cidr, values(local.subnet_cidrs_newbits_input)...)
  subnet_cidrs_newbits_resolved = length(local.vcn_cidr) > 0 ? {
    for k, v in local.subnet_cidrs_newbits_input : k => element(local.subnet_cidrs_newbits_ranges, index(keys(local.subnet_cidrs_newbits_input), k))
  } : {}

  # Handle subnets configured with netnum + newbits for sizing
  subnet_cidrs_netnum_newbits_ranges = {
    for k, v in local.subnet_cidrs_new : k => cidrsubnet(local.vcn_cidr, lookup(v, ""newbits""), lookup(v, ""netnum""))
    if v.type == ""netnum""
  }

  // Combine provided and calculated subnet CIDRs
  subnet_cidrs_all = merge(local.subnet_cidrs_cidr_input, local.subnet_cidrs_newbits_resolved, local.subnet_cidrs_netnum_newbits_ranges)

  # Map of subnets for standard components with additional configuration derived
  # TODO enumerate worker pools for public/private overrides, conditional subnets for both
  subnet_info = {
    bastion  = { create = var.create_bastion, is_public = var.bastion_is_public }
    cp       = { create = var.create_cluster, is_public = var.control_plane_is_public }
    workers  = { create = var.create_cluster, is_public = var.worker_is_public }
    pods     = { create = var.create_cluster && var.cni_type == ""npn"" }
    operator = { create = var.create_operator }
    fss      = { create = var.create_fss }
    int_lb = {
      create         = var.create_cluster && contains([""both"", ""internal""], var.load_balancers),
      create_seclist = true, dns_label = ""ilb"",
    }
    pub_lb = {
      create         = var.create_cluster && contains([""both"", ""public""], var.load_balancers),
      create_seclist = true, is_public = true, dns_label = ""plb"",
    }
  }

  # Create subnets if when all are true:
  # - Associated component is enabled OR configured with create == 'always'
  # - Subnet is configured with newbits and/or netnum/cidr
  # - Not configured with create == 'never'
  # - Not configured with an existing 'id'
  subnets_to_create = length(var.vcn_cidrs) > 0 ? merge(
    { for k, v in local.subnet_info : k =>
      # Override `create = true` if configured with ""always""
      merge(v, lookup(lookup(var.subnets, k, {}), ""create"", ""auto"") == ""always"" ? { ""create"" = true } : {})
      if alltrue([                                                       # Filter disabled subnets from output
        contains(keys(local.subnet_cidrs_all), k),                       # has a calculated CIDR range (not id input)
        lookup(lookup(var.subnets, k, {}), ""create"", ""auto"") != ""never"", # not disabled
        anytrue([
          tobool(lookup(v, ""create"", true)),                               # automatically enabled
          lookup(lookup(var.subnets, k, {}), ""create"", ""auto"") == ""always"" # force enabled
        ]),
      ])
    }
  ) : {}

  subnet_output = { for k, v in var.subnets :
    k => lookup(v, ""id"", null) != null ? v.id : lookup(lookup(oci_core_subnet.oke, k, {}), ""id"", null)
  }
}
",locals,"locals {
  # VCN subnet configuration
  # See https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengnetworkconfig.htm#vcnconfig
  # May be undefined when VCN is neither created nor required, e.g. when creating only workers for
  # an existing cluster. Fallback value is unused.
  vcn_cidr = length(var.vcn_cidrs) > 0 ? element(var.vcn_cidrs, 0) : ""0.0.0.0/16""

  # Filter configured subnets eligible for resource creation
  subnet_cidrs_new = {
    for k, v in var.subnets : k => merge(v, {
      ""type"" = (lookup(v, ""netnum"", null) == null && lookup(v, ""newbits"", null) != null ? ""newbits""
        : (lookup(v, ""netnum"", null) != null && lookup(v, ""newbits"", null) != null ? ""netnum""
          : (lookup(v, ""cidr"", null) != null ? ""cidr""
            : (lookup(v, ""id"", null) != null ? ""id""
      : ""invalid""))))
    }) if try(v.create, ""auto"") != ""never""
  }

  # Handle subnets configured with provided CIDRs
  subnet_cidrs_cidr_input = {
    for k, v in local.subnet_cidrs_new : k => lookup(v, ""cidr"") if v.type == ""cidr""
  }

  # Handle subnets configured with only newbits for sizing
  subnet_cidrs_newbits_input = {
    for k, v in local.subnet_cidrs_new : k => lookup(v, ""newbits"") if v.type == ""newbits""
  }

  # Generate CIDR ranges for subnets to be created
  subnet_cidrs_newbits_ranges = cidrsubnets(local.vcn_cidr, values(local.subnet_cidrs_newbits_input)...)
  subnet_cidrs_newbits_resolved = length(local.vcn_cidr) > 0 ? {
    for k, v in local.subnet_cidrs_newbits_input : k => element(local.subnet_cidrs_newbits_ranges, index(keys(local.subnet_cidrs_newbits_input), k))
  } : {}

  # Handle subnets configured with netnum + newbits for sizing
  subnet_cidrs_netnum_newbits_ranges = {
    for k, v in local.subnet_cidrs_new : k => cidrsubnet(local.vcn_cidr, lookup(v, ""newbits""), lookup(v, ""netnum""))
    if v.type == ""netnum""
  }

  // Combine provided and calculated subnet CIDRs
  subnet_cidrs_all = merge(
    local.subnet_cidrs_cidr_input,
    local.subnet_cidrs_newbits_resolved,
    local.subnet_cidrs_netnum_newbits_ranges,
  )

  # Map of subnets for standard components with additional configuration derived
  # TODO enumerate worker pools for public/private overrides, conditional subnets for both
  subnet_info = {
    bastion  = { create = var.create_bastion, is_public = var.bastion_is_public }
    cp       = { create = var.create_cluster, is_public = var.control_plane_is_public }
    workers  = { create = var.create_cluster, is_public = var.worker_is_public }
    pods     = { create = var.create_cluster && var.cni_type == ""npn"" }
    operator = { create = var.create_operator }
    fss      = { create = contains(keys(var.subnets), ""fss"") }
    int_lb = {
      create         = var.create_cluster && contains([""both"", ""internal""], var.load_balancers),
      create_seclist = true, dns_label = ""ilb"",
    }
    pub_lb = {
      create         = var.create_cluster && contains([""both"", ""public""], var.load_balancers),
      create_seclist = true, is_public = true, dns_label = ""plb"",
    }
  }

  # Map of configured subnets to specified/generated dns_label when enabled
  # If `assign_dns = true`, use dns_label for subnet if specified or first 2 characters of subnet key
  subnet_dns_labels = { for k, v in var.subnets :
    k => coalesce(lookup(v, ""dns_label"", null), substr(k, 0, 2))
    if var.assign_dns
  }

  # Create subnets if when all are true:
  # - Associated component is enabled OR configured with create == 'always'
  # - Subnet is configured with newbits and/or netnum/cidr
  # - Not configured with create == 'never'
  # - Not configured with an existing 'id'
  subnets_to_create = try(merge(
    { for k, v in local.subnet_info : k =>
      # Override `create = true` if configured with ""always""
      merge(v, lookup(try(lookup(var.subnets, k), { create = ""never"" }), ""create"", ""auto"") == ""always"" ? { ""create"" = true } : {})
      if alltrue([                                                                              # Filter disabled subnets from output
        contains(keys(local.subnet_cidrs_all), k),                                              # has a calculated CIDR range (not id input)
        lookup(try(lookup(var.subnets, k), { create = ""never"" }), ""create"", ""auto"") != ""never"", # not disabled
        anytrue([
          tobool(lookup(v, ""create"", true)),                                                      # automatically enabled
          lookup(try(lookup(var.subnets, k), { create = ""never"" }), ""create"", ""auto"") == ""always"" # force enabled
        ]),
      ])
    }
  ), {})

  subnet_output = { for k, v in var.subnets :
    k => lookup(v, ""id"", null) != null ? v.id : lookup(lookup(oci_core_subnet.oke, k, {}), ""id"", null)
  }
}
",locals,48,52.0,f49f1da39d79cf260d80dcb10ee8e399828e6e1c,9f96c38f72409cd56437e73fccf0bc5fb82efd48,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/f49f1da39d79cf260d80dcb10ee8e399828e6e1c/modules/network/subnets.tf#L48,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/9f96c38f72409cd56437e73fccf0bc5fb82efd48/modules/network/subnets.tf#L52,2023-10-25 16:40:02+11:00,2024-03-28 20:16:45+11:00,10,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,295,kube.example.tf,kube.example.tf,0,fix,# Or use the value below to automatically select an optimal mirror (by default we have it fixed to one german mirror that we know works for most locations),"### The following values are entirely optional (and can be removed from this if unused)  
 # You can refine a base domain name to be use in this form of nodename.base_domain for setting the reserve dns inside Hetzner 
 # base_domain = ""mycluster.example.com""  
 # Cluster Autoscaler 
 # Providing at least one map for the array enables the cluster autoscaler feature, default is disabled 
 # Please note that the autoscaler should not be used with initial_k3s_channel < ""v1.25"". So ideally lock it to ""v1.25"". 
 # * Example below: 
 # autoscaler_nodepools = [ 
 #   { 
 #     name        = ""autoscaler"" 
 #     server_type = ""cpx21"" # must be same or better than the control_plane server type (regarding disk size)! 
 #     location    = ""fsn1"" 
 #     min_nodes   = 0 
 #     max_nodes   = 5 
 #   } 
 # ]  
 # Enable etcd snapshot backups to S3 storage. 
 # Just provide a map with the needed settings (according to your S3 storage provider) and backups to S3 will 
 # be enabled (with the default settings for etcd snapshots). 
 # Cloudflare's R2 offers 10GB, 10 million reads and 1 million writes per month for free. 
 # For proper context, have a look at https://docs.k3s.io/backup-restore. 
 # etcd_s3_backup = { 
 #   etcd-s3-endpoint        = ""xxxx.r2.cloudflarestorage.com"" 
 #   etcd-s3-access-key      = ""<access-key>"" 
 #   etcd-s3-secret-key      = ""<secret-key>"" 
 #   etcd-s3-bucket          = ""k3s-etcd-snapshots"" 
 # }  
 # To use local storage on the nodes, you can enable Longhorn, default is ""false"". 
 # See a full recap on how to configure agent nodepools for longhorn here https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/discussions/373#discussioncomment-3983159 
 # enable_longhorn = true  
 # By default, longhorn is pulled from https://charts.longhorn.io. 
 # If you need a version of longhorn which assures compatibility with rancher you can set this variable to https://charts.rancher.io. 
 # longhorn_repository = ""https://charts.rancher.io""  
 # The namespace for longhorn deployment, default is ""longhorn-system"". 
 # longhorn_namespace = ""longhorn-system""  
 # The file system type for Longhorn, if enabled (ext4 is the default, otherwise you can choose xfs). 
 # longhorn_fstype = ""xfs""  
 # how many replica volumes should longhorn create (default is 3). 
 # longhorn_replica_count = 1  
 # When you enable Longhorn, you can go with the default settings and just modify the above two variables OR you can add a longhorn_values variable 
 # with all needed helm values, see towards the end of the file in the advanced section. 
 # If that file is present, the system will use it during the deploy, if not it will use the default values with the two variable above that can be customized. 
 # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.  
 # Also, you can choose to use a Hetzner volume with Longhorn. By default, it will use the nodes own storage space, but if you add an attribute of 
 # longhorn_volume_size ( not a variable, just a possible agent nodepool attribute) with a value between 10 and 10000 GB to your agent nodepool definition, it will create and use the volume in question. 
 # See the agent nodepool section for an example of how to do that.  
 # To disable Hetzner CSI storage, you can set the following to ""true"", default is ""false"". 
 # disable_hetzner_csi = true  
 # If you want to use a specific Hetzner CCM and CSI version, set them below; otherwise, leave them as-is for the latest versions. 
 # hetzner_ccm_version = """" 
 # hetzner_csi_version = """"  
 # If you want to specify the Kured version, set it below - otherwise it'll use the latest version available. 
 # kured_version = """"  
 # If you want to specify what time Kured starts restarting nodes, set it below. Default is 3AM. 
 # kured_start_time = """"  
 # If you want to specify what time Kured stops restarting nodes, set it below. Default is 8AM. 
 # kured_end_time = """"  
 # If you want to specify what timezone Kured uses, set it below. Default is Local. 
 # kured_time_zone = """"  
 # If you want to enable the Nginx ingress controller (https://kubernetes.github.io/ingress-nginx/) instead of Traefik, you can set this to ""nginx"". Default is ""traefik"". 
 # By the default we load optimal Traefik and Nginx ingress controller config for Hetzner, however you may need to tweak it to your needs, so to do, 
 # we allow you to add a traefik_values and nginx_values, see towards the end of this file in the advanced section. 
 # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration. 
 # If you want to disable both controllers set this to ""none"" 
 # ingress_controller = ""nginx""  
 # You can change the number of replicas for selected ingress controller here. The default 0 means autoselecting based on number of agent nodes (1 node = 1 replica, 2 nodes = 2 replicas, 3+ nodes = 3 replicas) 
 # ingress_replica_count = 1  
 # Use the klipperLB (similar to metalLB), instead of the default Hetzner one, that has an advantage of dropping the cost of the setup. 
 # Automatically ""true"" in the case of single node cluster (as it does not make sense to use the Hetzner LB in that situation). 
 # It can work with any ingress controller that you choose to deploy. 
 # Please note that because the klipperLB points to all nodes, we automatically allow scheduling on the control plane when it is active. 
 # enable_klipper_metal_lb = ""true""  
 # If you want to configure additional arguments for traefik, enter them here as a list and in the form of traefik CLI arguments; see https://doc.traefik.io/traefik/reference/static-configuration/cli/ 
 # They are the options that go into the additionalArguments section of the Traefik helm values file. 
 # Example: traefik_additional_options = [""--log.level=DEBUG"", ""--tracing=true""] 
 # traefik_additional_options = []  
 # By default traefik is configured to redirect http traffic to https, you can set this to ""false"" to disable the redirection. 
 # traefik_redirect_to_https = false  
 # If you want to disable the metric server set this to ""false"". Default is ""true"". 
 # enable_metrics_server = false  
 # If you want to allow non-control-plane workloads to run on the control-plane nodes, set this to ""true"". The default is ""false"". 
 # True by default for single node clusters, and when enable_klipper_metal_lb is true. In those cases, the value below will be ignored. 
 # allow_scheduling_on_control_plane = true  
 # If you want to disable the automatic upgrade of k3s, you can set below to ""false"". 
 # Ideally, keep it on, to always have the latest Kubernetes version, but lock the initial_k3s_channel to a kube major version, 
 # of your choice, like v1.25 or v1.26. That way you get the best of both worlds without the breaking changes risk. 
 # For production use, always use an HA setup with at least 3 control-plane nodes and 2 agents, and keep this on for maximum security.  
 # The default is ""true"" (in HA setup i.e. at least 3 control plane nodes & 2 agents, just keep it enabled since it works flawlessly). 
 # automatically_upgrade_k3s = false  
 # The default is ""true"" (in HA setup it works wonderfully well, with automatic roll-back to the previous snapshot in case of an issue). 
 # IMPORTANT! For non-HA clusters i.e. when the number of control-plane nodes is < 3, you have to turn it off. 
 # automatically_upgrade_os = false  
 # If you need more control over kured and the reboot behaviour, you can pass additional options to kured. 
 # For example limiting reboots to certain timeframes. For all options see: https://kured.dev/docs/configuration/ 
 # The default options are: `--reboot-command=/usr/bin/systemctl reboot --pre-reboot-node-labels=kured=rebooting --post-reboot-node-labels=kured=done --period=5m` 
 # Defaults can be overridden by using the same key. 
 # kured_options = { 
 #   ""reboot-days"": ""su"" 
 #   ""start-time"": ""3am"" 
 #   ""end-time"": ""8am"" 
 # }  
 # Allows you to specify either stable, latest, testing or supported minor versions. 
 # see https://rancher.com/docs/k3s/latest/en/upgrades/basic/ and https://update.k3s.io/v1-release/channels 
 #  If you are going to use Rancher addons for instance, it's always a good idea to fix the kube version to latest - 0.01, 
 # at the time of writing the latest is v1.26, so setting the value below to ""v1.25"" will insure maximum compatibility with Rancher, Longhorn and so on! 
 # The default is ""v1.25"". 
 # initial_k3s_channel = ""stable""  
 # The cluster name, by default ""k3s"" 
 # cluster_name = """"  
 # Whether to use the cluster name in the node name, in the form of {cluster_name}-{nodepool_name}, the default is ""true"". 
 # use_cluster_name_in_node_name = false  
 # Extra k3s registries. This is useful if you have private registries and you 
 # want to pull images without additional secrets. 
 # registries.yaml file docs: https://docs.k3s.io/installation/private-registry 
 /* k3s_registries = <<-EOT 
 mirrors: 
 hub.my_registry.com: 
 endpoint: 
 - ""hub.my_registry.com"" 
 configs: 
 hub.my_registry.com: 
 auth: 
 username: username 
 password: password 
 EOT */  
 # If you want to allow all outbound traffic you can set this to ""false"". Default is ""true"". 
 # restrict_outbound_traffic = false  
 # Adding extra firewall rules, like opening a port 
 # More info on the format here https://registry.terraform.io/providers/hetznercloud/hcloud/latest/docs/resources/firewall 
 # extra_firewall_rules = [ 
 #   # For Postgres 
 #   { 
 #     direction       = ""in"" 
 #     protocol        = ""tcp"" 
 #     port            = ""5432"" 
 #     source_ips      = [""0.0.0.0/0"", ""::/0""] 
 #     destination_ips = [] # Won't be used for this rule 
 #   }, 
 #   # To Allow ArgoCD access to resources via SSH 
 #   { 
 #     direction       = ""out"" 
 #     protocol        = ""tcp"" 
 #     port            = ""22"" 
 #     source_ips      = [] # Won't be used for this rule 
 #     destination_ips = [""0.0.0.0/0"", ""::/0""] 
 #   } 
 # ]  
 # If you want to configure a different CNI for k3s, use this flag 
 # possible values: flannel (Default), calico, and cilium 
 # As for Cilium, we allow infinite configurations via helm values, please check the CNI section of the readme over at https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/#cni. 
 # Also, see the cilium_values at towards the end of this file, in the advanced section. 
 # cni_plugin = ""cilium""  
 # If you want to disable the k3s default network policy controller, use this flag! 
 # Both Calico and Ciliun cni_plugin values override this value to true automatically, the default is ""false"". 
 # disable_network_policy = true  
 # If you want to disable the automatic use of placement group ""spread"". See https://docs.hetzner.com/cloud/placement-groups/overview/ 
 # That may be useful if you need to deploy more than 500 nodes! The default is ""false"". 
 # placement_group_disable = true  
 # By default, we allow ICMP ping in to the nodes, to check for liveness for instance. If you do not want to allow that, you can. Just set this flag to true (false by default). 
 # block_icmp_ping_in = true  
 # You can enable cert-manager (installed by Helm behind the scenes) with the following flag, the default is ""false"". 
 # enable_cert_manager = true  
 # By default we use a known good mirror to download the needed OpenSUSE, but for instance, it may not be available in US-East location, in which case, 
 # you can find a working mirror for you at https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2.mirrorlist, 
 # Or use the value below to automatically select an optimal mirror (by default we have it fixed to one german mirror that we know works for most locations) 
 # opensuse_microos_mirror_link = ""https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2""  
 # IP Addresses to use for the DNS Servers, set to an empty list to use the ones provided by Hetzner, defaults to [""1.1.1.1"", "" 1.0.0.1"", ""8.8.8.8""]. 
 # For rancher installs, best to leave it as default. 
 # dns_servers = []  
 # When this is enabled, rather than the first node, all external traffic will be routed via a control-plane loadbalancer, allowing for high availability. 
 # The default is false. 
 # use_control_plane_lb = true  
 # Let's say you are not using the control plane LB solution above, and still want to have one hostname point to all your control-plane nodes. 
 # You could create multiple A records of to let's say cp.cluster.my.org pointing to all of your control-plane nodes ips. 
 # In which case, you need to define that hostname in the k3s TLS-SANs config to allow connection through it. It can be hostnames or IP addresses. 
 # additional_tls_sans = [""cp.cluster.my.org""]  
 # Oftentimes, you need to communicate to the cluster from inside the cluster itself, in which case it is important to set this value, as it will configure the hostname 
 # at the load balancer level, and will save you from many slows downs when initiating communications from inside. Later on, you can point your DNS to the IP given 
 # to the LB. And if you have other services pointing to it, you are also free to create CNAMES to point to it, or whatever you see fit. 
 # If set, it will apply to either ingress controllers, Traefik or Ingress-Nginx. 
 # lb_hostname = """"  
 # You can enable Rancher (installed by Helm behind the scenes) with the following flag, the default is ""false"". 
 # When Rancher is enabled, it automatically installs cert-manager too, and it uses rancher's own self-signed certificates. 
 # See for options https://rancher.com/docs/rancher/v2.0-v2.4/en/installation/resources/advanced/helm2/helm-rancher/#choose-your-ssl-configuration 
 # The easiest thing is to leave everything as is (using the default rancher self-signed certificate) and put Cloudflare in front of it. 
 # As for the number of replicas, by default it is set to the numbe of control plane nodes. 
 # You can customized all of the above by adding a rancher_values variable see at the end of this file in the advanced section. 
 # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration. 
 # IMPORTANT: Rancher's install is quite memory intensive, you will require at least 4GB if RAM, meaning cx21 server type (for your control plane). 
 # ALSO, in order for Rancher to successfully deploy, you have to set the ""rancher_hostname"". 
 # enable_rancher = true  
 # If using Rancher you can set the Rancher hostname, it must be unique hostname even if you do not use it. 
 # If not pointing the DNS, you can just port-forward locally via kubectl to get access to the dashboard. 
 # If you already set the lb_hostname above and are using a Hetzner LB, you do not need to set this one, as it will be used by default. 
 # But if you set this one explicitly, it will have preference over the lb_hostname in rancher settings. 
 # rancher_hostname = ""rancher.xyz.dev""  
 # When Rancher is deployed, by default is uses the ""latest"" channel. But this can be customized. 
 # The allowed values are ""stable"" or ""latest"". 
 # rancher_install_channel = ""stable""  
 # Finally, you can specify a bootstrap-password for your rancher instance. Minimum 48 characters long! 
 # If you leave empty, one will be generated for you. 
 # (Can be used by another rancher2 provider to continue setup of rancher outside this module.) 
 # rancher_bootstrap_password = """"  
 # Separate from the above Rancher config (only use one or the other). You can import this cluster directly on an 
 # an already active Rancher install. By clicking ""import cluster"" choosing ""generic"", giving it a name and pasting 
 # the cluster registration url below. However, you can also ignore that and apply the url via kubectl as instructed 
 # by Rancher in the wizard, and that would register your cluster too. 
 # More information about the registration can be found here https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/registered-clusters/ 
 # rancher_registration_manifest_url = ""https://rancher.xyz.dev/v3/import/xxxxxxxxxxxxxxxxxxYYYYYYYYYYYYYYYYYYYzzzzzzzzzzzzzzzzzzzzz.yaml""  
 # Extra values that will be passed to the `extra-manifests/kustomization.yaml.tpl` if its present. 
 # extra_kustomize_parameters={}  
 # It is best practice to turn this off, but for backwards compatibility it is set to ""true"" by default. 
 # See https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/issues/349 
 # When ""false"". The kubeconfig file can instead be created by executing: ""terraform output --raw kubeconfig > cluster_kubeconfig.yaml"" 
 # Always be careful to not commit this file! 
 # create_kubeconfig = false  
 # Don't create the kustomize backup. This can be helpful for automation. 
 # create_kustomization = false  
 ### ADVANCED - Custom helm values for packages above (search _values if you want to located where those are mentioned upper in this file) 
 #  Inside the _values variable below are examples, up to you to find out the best helm values possible, we do not provide support for customized helm values. 
 # Please understand that the indentation is very important, inside the EOTs, as those are proper yaml helm values. 
 # We advise you to use the default values, and only change them if you know what you are doing!  
 # Cilium, all Cilium helm values can be found at https://github.com/cilium/cilium/blob/master/install/kubernetes/cilium/values.yaml 
 # The following is an example, please note that the current indentation inside the EOT is important. 
 /*   cilium_values = <<EOT 
 ipam: 
 mode: kubernetes 
 devices: ""eth1"" 
 k8s: 
 requireIPv4PodCIDR: true 
 kubeProxyReplacement: strict 
 EOT */  
 # Cert manager, all cert-manager helm values can be found at https://github.com/cert-manager/cert-manager/blob/master/deploy/charts/cert-manager/values.yaml 
 # The following is an example, please note that the current indentation inside the EOT is important. 
 /*   cert_manager_values = <<EOT 
 installCRDs: true 
 replicaCount: 3 
 webhook: 
 replicaCount: 3 
 cainjector: 
 replicaCount: 3 
 EOT */  
 # Longhorn, all Longhorn helm values can be found at https://github.com/longhorn/longhorn/blob/master/chart/values.yaml 
 # The following is an example, please note that the current indentation inside the EOT is important. 
 /*   longhorn_values = <<EOT 
 defaultSettings: 
 defaultDataPath: /var/longhorn 
 persistence: 
 defaultFsType: ext4 
 defaultClassReplicaCount: 3 
 defaultClass: true 
 EOT */  
 # Nginx, all Nginx helm values can be found at https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml 
 # You can also have a look at https://kubernetes.github.io/ingress-nginx/, to understand how it works, and all the options at your disposal. 
 # The following is an example, please note that the current indentation inside the EOT is important. 
 /*   nginx_ingress_values = <<EOT 
 controller: 
 watchIngressWithoutClass: ""true"" 
 kind: ""DaemonSet"" 
 config: 
 ""use-forwarded-headers"": ""true"" 
 ""compute-full-forwarded-for"": ""true"" 
 ""use-proxy-protocol"": ""true"" 
 service: 
 annotations: 
 ""load-balancer.hetzner.cloud/name"": ""k3s"" 
 ""load-balancer.hetzner.cloud/use-private-ip"": ""true"" 
 ""load-balancer.hetzner.cloud/disable-private-ingress"": ""true"" 
 ""load-balancer.hetzner.cloud/location"": ""nbg1"" 
 ""load-balancer.hetzner.cloud/type"": ""lb11"" 
 ""load-balancer.hetzner.cloud/uses-proxyprotocol"": ""true"" 
 EOT */  
 # Rancher, all Rancher helm values can be found at https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/chart-options/ 
 # The following is an example, please note that the current indentation inside the EOT is important. 
 /*   rancher_values = <<EOT 
 ingress: 
 tls: 
 source: ""rancher"" 
 hostname: ""rancher.example.com"" 
 replicas: 1 
 bootstrapPassword: ""supermario"" 
 EOT */ ","module ""kube-hetzner"" {
  providers = {
    hcloud = hcloud
  }
  hcloud_token = local.hcloud_token

  # Then fill or edit the below values. Only the first values starting with a * are obligatory; the rest can remain with their default values, or you
  # could adapt them to your needs.

  # * For local dev, path to the git repo
  # source = ""../../kube-hetzner/""
  # If you want to use the latest master branch
  # source = ""github.com/kube-hetzner/terraform-hcloud-kube-hetzner""
  # For normal use, this is the path to the terraform registry
  source = ""kube-hetzner/kube-hetzner/hcloud""

  # You can optionally specify a version number
  # version = ""1.2.0""

  # Note that some values, notably ""location"" and ""public_key"" have no effect after initializing the cluster.
  # This is to keep Terraform from re-provisioning all nodes at once, which would lose data. If you want to update
  # those, you should instead change the value here and manually re-provision each node. Grep for ""lifecycle"".

  # Customize the SSH port (by default 22)
  # ssh_port = 2222

  # * Your ssh public key
  ssh_public_key = file(""~/.ssh/id_rsa.pub"")
  # * Your private key must be ""ssh_private_key = null"" when you want to use ssh-agent for a Yubikey-like device authentification or an SSH key-pair with a passphrase.
  # For more details on SSH see https://github.com/kube-hetzner/kube-hetzner/blob/master/docs/ssh.md
  ssh_private_key = file(""~/.ssh/id_rsa"")
  # You can add additional SSH public Keys to grant other team members root access to your cluster nodes.
  # ssh_additional_public_keys = []

  # You can also add additional SSH public Keys which are saved in the hetzner cloud by a label.
  # See https://docs.hetzner.cloud/#label-selector
  # ssh_hcloud_key_label = ""role=admin""

  # If you want to use an ssh key that is already registered within hetzner cloud, you can pass its id.
  # If no id is passed, a new ssh key will be registered within hetzner cloud.
  # It is important that exactly this key is passed via `ssh_public_key` & `ssh_private_key` vars.
  # hcloud_ssh_key_id = """"

  # These can be customized, or left with the default values
  # * For Hetzner locations see https://docs.hetzner.com/general/others/data-centers-and-connection/
  network_region = ""eu-central"" # change to `us-east` if location is ash

  # For the control planes, at least three nodes are the minimum for HA. Otherwise, you need to turn off the automatic upgrades (see README).
  # **It must always be an ODD number, never even!** Search the internet for ""splitbrain problem with etcd"" or see https://rancher.com/docs/k3s/latest/en/installation/ha-embedded/
  # For instance, one is ok (non-HA), two is not ok, and three is ok (becomes HA). It does not matter if they are in the same nodepool or not! So they can be in different locations and of various types.

  # Of course, you can choose any number of nodepools you want, with the location you want. The only constraint on the location is that you need to stay in the same network region, Europe, or the US.
  # For the server type, the minimum instance supported is cpx11 (just a few cents more than cx11); see https://www.hetzner.com/cloud.

  # IMPORTANT: Before you create your cluster, you can do anything you want with the nodepools, but you need at least one of each, control plane and agent.
  # Once the cluster is up and running, you can change nodepool count and even set it to 0 (in the case of the first control-plane nodepool, the minimum is 1).
  # You can also rename it (if the count is 0), but do not remove a nodepool from the list.

  # The only nodepools that are safe to remove from the list are at the end. That is due to how subnets and IPs get allocated (FILO).
  # You can, however, freely add other nodepools at the end of each list if you want. The maximum number of nodepools you can create combined for both lists is 255.
  # Also, before decreasing the count of any nodepools to 0, it's essential to drain and cordon the nodes in question. Otherwise, it will leave your cluster in a bad state.

  # Before initializing the cluster, you can change all parameters and add or remove any nodepools. You need at least one nodepool of each kind, control plane, and agent.
  # The nodepool names are entirely arbitrary, you can choose whatever you want, but no special characters or underscore, and they must be unique; only alphanumeric characters and dashes are allowed.

  # If you want to have a single node cluster, have one control plane nodepools with a count of 1, and one agent nodepool with a count of 0.

  # Please note that changing labels and taints after the first run will have no effect. If needed, you can do that through Kubernetes directly.

  # * Example below:

  control_plane_nodepools = [
    {
      name        = ""control-plane-fsn1"",
      server_type = ""cpx11"",
      location    = ""fsn1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""control-plane-nbg1"",
      server_type = ""cpx11"",
      location    = ""nbg1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""control-plane-hel1"",
      server_type = ""cpx11"",
      location    = ""hel1"",
      labels      = [],
      taints      = [],
      count       = 1
    }
  ]

  agent_nodepools = [
    {
      name        = ""agent-small"",
      server_type = ""cpx11"",
      location    = ""fsn1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""agent-large"",
      server_type = ""cpx21"",
      location    = ""nbg1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""storage"",
      server_type = ""cpx21"",
      location    = ""fsn1"",
      # Fully optional, just a demo.
      labels      = [
        ""node.kubernetes.io/server-usage=storage""
      ],
      taints      = [],
      count       = 1
      # In the case of using Longhorn, you can use Hetzner volumes instead of using the node's own storage by specifying a value from 10 to 10000 (in GB)
      # It will create one volume per node in the nodepool, and configure Longhorn to use them.
      # Something worth noting is that Volume storage is slower than node storage, which is achieved by not mentioning longhorn_volume_size or setting it to 0.
      # So for something like DBs, you definitely want node storage, for other things like backups, volume storage is fine, and cheaper.
      # longhorn_volume_size = 20
    }
  ]
  # Add custom control plane configuration options here.
  # E.g to enable monitoring for etcd, proxy etc:
  # control_planes_custom_config = {
  #  etcd-expose-metrics = true,
  #  kube-controller-manager-arg = ""bind-address=0.0.0.0"",
  #  kube-proxy-arg =""metrics-bind-address=0.0.0.0"",
  #  kube-scheduler-arg = ""bind-address=0.0.0.0"",
  # }

  # * LB location and type, the latter will depend on how much load you want it to handle, see https://www.hetzner.com/cloud/load-balancer
  load_balancer_type     = ""lb11""
  load_balancer_location = ""fsn1""

  ### The following values are entirely optional (and can be removed from this if unused)

  # You can refine a base domain name to be use in this form of nodename.base_domain for setting the reserve dns inside Hetzner
  # base_domain = ""mycluster.example.com""

  # Cluster Autoscaler
  # Providing at least one map for the array enables the cluster autoscaler feature, default is disabled
  # Please note that the autoscaler should not be used with initial_k3s_channel < ""v1.25"". So ideally lock it to ""v1.25"".
  # * Example below:
  # autoscaler_nodepools = [
  #   {
  #     name        = ""autoscaler""
  #     server_type = ""cpx21"" # must be same or better than the control_plane server type (regarding disk size)!
  #     location    = ""fsn1""
  #     min_nodes   = 0
  #     max_nodes   = 5
  #   }
  # ]

  # Enable etcd snapshot backups to S3 storage.
  # Just provide a map with the needed settings (according to your S3 storage provider) and backups to S3 will
  # be enabled (with the default settings for etcd snapshots).
  # Cloudflare's R2 offers 10GB, 10 million reads and 1 million writes per month for free.
  # For proper context, have a look at https://docs.k3s.io/backup-restore.
  # etcd_s3_backup = {
  #   etcd-s3-endpoint        = ""xxxx.r2.cloudflarestorage.com""
  #   etcd-s3-access-key      = ""<access-key>""
  #   etcd-s3-secret-key      = ""<secret-key>""
  #   etcd-s3-bucket          = ""k3s-etcd-snapshots""
  # }

  # To use local storage on the nodes, you can enable Longhorn, default is ""false"".
  # See a full recap on how to configure agent nodepools for longhorn here https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/discussions/373#discussioncomment-3983159
  # enable_longhorn = true

  # By default, longhorn is pulled from https://charts.longhorn.io.
  # If you need a version of longhorn which assures compatibility with rancher you can set this variable to https://charts.rancher.io. 
  # longhorn_repository = ""https://charts.rancher.io""

  # The namespace for longhorn deployment, default is ""longhorn-system"".
  # longhorn_namespace = ""longhorn-system""

  # The file system type for Longhorn, if enabled (ext4 is the default, otherwise you can choose xfs).
  # longhorn_fstype = ""xfs""

  # how many replica volumes should longhorn create (default is 3).
  # longhorn_replica_count = 1

  # When you enable Longhorn, you can go with the default settings and just modify the above two variables OR you can add a longhorn_values variable
  # with all needed helm values, see towards the end of the file in the advanced section.
  # If that file is present, the system will use it during the deploy, if not it will use the default values with the two variable above that can be customized.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.

  # Also, you can choose to use a Hetzner volume with Longhorn. By default, it will use the nodes own storage space, but if you add an attribute of
  # longhorn_volume_size ( not a variable, just a possible agent nodepool attribute) with a value between 10 and 10000 GB to your agent nodepool definition, it will create and use the volume in question.
  # See the agent nodepool section for an example of how to do that.

  # To disable Hetzner CSI storage, you can set the following to ""true"", default is ""false"".
  # disable_hetzner_csi = true

  # If you want to use a specific Hetzner CCM and CSI version, set them below; otherwise, leave them as-is for the latest versions.
  # hetzner_ccm_version = """"
  # hetzner_csi_version = """"

  # If you want to specify the Kured version, set it below - otherwise it'll use the latest version available.
  # kured_version = """"

  # If you want to specify what time Kured starts restarting nodes, set it below. Default is 3AM.
  # kured_start_time = """"

  # If you want to specify what time Kured stops restarting nodes, set it below. Default is 8AM.
  # kured_end_time = """"

  # If you want to specify what timezone Kured uses, set it below. Default is Local.
  # kured_time_zone = """"

  # If you want to enable the Nginx ingress controller (https://kubernetes.github.io/ingress-nginx/) instead of Traefik, you can set this to ""nginx"". Default is ""traefik"".
  # By the default we load optimal Traefik and Nginx ingress controller config for Hetzner, however you may need to tweak it to your needs, so to do,
  # we allow you to add a traefik_values and nginx_values, see towards the end of this file in the advanced section.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.
  # If you want to disable both controllers set this to ""none""
  # ingress_controller = ""nginx""

  # You can change the number of replicas for selected ingress controller here. The default 0 means autoselecting based on number of agent nodes (1 node = 1 replica, 2 nodes = 2 replicas, 3+ nodes = 3 replicas)
  # ingress_replica_count = 1

  # Use the klipperLB (similar to metalLB), instead of the default Hetzner one, that has an advantage of dropping the cost of the setup.
  # Automatically ""true"" in the case of single node cluster (as it does not make sense to use the Hetzner LB in that situation).
  # It can work with any ingress controller that you choose to deploy.
  # Please note that because the klipperLB points to all nodes, we automatically allow scheduling on the control plane when it is active.
  # enable_klipper_metal_lb = ""true""

  # If you want to configure additional arguments for traefik, enter them here as a list and in the form of traefik CLI arguments; see https://doc.traefik.io/traefik/reference/static-configuration/cli/
  # They are the options that go into the additionalArguments section of the Traefik helm values file.
  # Example: traefik_additional_options = [""--log.level=DEBUG"", ""--tracing=true""]
  # traefik_additional_options = []

  # By default traefik is configured to redirect http traffic to https, you can set this to ""false"" to disable the redirection.
  # traefik_redirect_to_https = false

  # If you want to disable the metric server set this to ""false"". Default is ""true"".
  # enable_metrics_server = false

  # If you want to allow non-control-plane workloads to run on the control-plane nodes, set this to ""true"". The default is ""false"".
  # True by default for single node clusters, and when enable_klipper_metal_lb is true. In those cases, the value below will be ignored.
  # allow_scheduling_on_control_plane = true

  # If you want to disable the automatic upgrade of k3s, you can set below to ""false"".
  # Ideally, keep it on, to always have the latest Kubernetes version, but lock the initial_k3s_channel to a kube major version,
  # of your choice, like v1.25 or v1.26. That way you get the best of both worlds without the breaking changes risk.
  # For production use, always use an HA setup with at least 3 control-plane nodes and 2 agents, and keep this on for maximum security.

  # The default is ""true"" (in HA setup i.e. at least 3 control plane nodes & 2 agents, just keep it enabled since it works flawlessly).
  # automatically_upgrade_k3s = false

  # The default is ""true"" (in HA setup it works wonderfully well, with automatic roll-back to the previous snapshot in case of an issue).
  # IMPORTANT! For non-HA clusters i.e. when the number of control-plane nodes is < 3, you have to turn it off.
  # automatically_upgrade_os = false

  # If you need more control over kured and the reboot behaviour, you can pass additional options to kured.
  # For example limiting reboots to certain timeframes. For all options see: https://kured.dev/docs/configuration/
  # The default options are: `--reboot-command=/usr/bin/systemctl reboot --pre-reboot-node-labels=kured=rebooting --post-reboot-node-labels=kured=done --period=5m`
  # Defaults can be overridden by using the same key.
  # kured_options = {
  #   ""reboot-days"": ""su""
  #   ""start-time"": ""3am""
  #   ""end-time"": ""8am""
  # }

  # Allows you to specify either stable, latest, testing or supported minor versions.
  # see https://rancher.com/docs/k3s/latest/en/upgrades/basic/ and https://update.k3s.io/v1-release/channels
  #  If you are going to use Rancher addons for instance, it's always a good idea to fix the kube version to latest - 0.01,
  # at the time of writing the latest is v1.26, so setting the value below to ""v1.25"" will insure maximum compatibility with Rancher, Longhorn and so on!
  # The default is ""v1.25"".
  # initial_k3s_channel = ""stable""

  # The cluster name, by default ""k3s""
  # cluster_name = """"

  # Whether to use the cluster name in the node name, in the form of {cluster_name}-{nodepool_name}, the default is ""true"".
  # use_cluster_name_in_node_name = false

  # Extra k3s registries. This is useful if you have private registries and you
  # want to pull images without additional secrets.
  # registries.yaml file docs: https://docs.k3s.io/installation/private-registry
  /* k3s_registries = <<-EOT
    mirrors:
      hub.my_registry.com:
        endpoint:
          - ""hub.my_registry.com""
    configs:
      hub.my_registry.com:
        auth:
          username: username
          password: password
  EOT */

  # If you want to allow all outbound traffic you can set this to ""false"". Default is ""true"".
  # restrict_outbound_traffic = false

  # Adding extra firewall rules, like opening a port
  # More info on the format here https://registry.terraform.io/providers/hetznercloud/hcloud/latest/docs/resources/firewall
  # extra_firewall_rules = [
  #   # For Postgres
  #   {
  #     direction       = ""in""
  #     protocol        = ""tcp""
  #     port            = ""5432""
  #     source_ips      = [""0.0.0.0/0"", ""::/0""]
  #     destination_ips = [] # Won't be used for this rule
  #   },
  #   # To Allow ArgoCD access to resources via SSH
  #   {
  #     direction       = ""out""
  #     protocol        = ""tcp""
  #     port            = ""22""
  #     source_ips      = [] # Won't be used for this rule
  #     destination_ips = [""0.0.0.0/0"", ""::/0""]
  #   }
  # ]

  # If you want to configure a different CNI for k3s, use this flag
  # possible values: flannel (Default), calico, and cilium
  # As for Cilium, we allow infinite configurations via helm values, please check the CNI section of the readme over at https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/#cni.
  # Also, see the cilium_values at towards the end of this file, in the advanced section.
  # cni_plugin = ""cilium""

  # If you want to disable the k3s default network policy controller, use this flag!
  # Both Calico and Ciliun cni_plugin values override this value to true automatically, the default is ""false"".
  # disable_network_policy = true

  # If you want to disable the automatic use of placement group ""spread"". See https://docs.hetzner.com/cloud/placement-groups/overview/
  # That may be useful if you need to deploy more than 500 nodes! The default is ""false"".
  # placement_group_disable = true

  # By default, we allow ICMP ping in to the nodes, to check for liveness for instance. If you do not want to allow that, you can. Just set this flag to true (false by default).
  # block_icmp_ping_in = true

  # You can enable cert-manager (installed by Helm behind the scenes) with the following flag, the default is ""false"".
  # enable_cert_manager = true

  # By default we use a known good mirror to download the needed OpenSUSE, but for instance, it may not be available in US-East location, in which case,
  # you can find a working mirror for you at https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2.mirrorlist,
  # Or use the value below to automatically select an optimal mirror (by default we have it fixed to one german mirror that we know works for most locations)
  # opensuse_microos_mirror_link = ""https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2""

  # IP Addresses to use for the DNS Servers, set to an empty list to use the ones provided by Hetzner, defaults to [""1.1.1.1"", "" 1.0.0.1"", ""8.8.8.8""].
  # For rancher installs, best to leave it as default.
  # dns_servers = []

  # When this is enabled, rather than the first node, all external traffic will be routed via a control-plane loadbalancer, allowing for high availability.
  # The default is false.
  # use_control_plane_lb = true

  # Let's say you are not using the control plane LB solution above, and still want to have one hostname point to all your control-plane nodes.
  # You could create multiple A records of to let's say cp.cluster.my.org pointing to all of your control-plane nodes ips.
  # In which case, you need to define that hostname in the k3s TLS-SANs config to allow connection through it. It can be hostnames or IP addresses.
  # additional_tls_sans = [""cp.cluster.my.org""]

  # Oftentimes, you need to communicate to the cluster from inside the cluster itself, in which case it is important to set this value, as it will configure the hostname
  # at the load balancer level, and will save you from many slows downs when initiating communications from inside. Later on, you can point your DNS to the IP given
  # to the LB. And if you have other services pointing to it, you are also free to create CNAMES to point to it, or whatever you see fit.
  # If set, it will apply to either ingress controllers, Traefik or Ingress-Nginx.
  # lb_hostname = """"

  # You can enable Rancher (installed by Helm behind the scenes) with the following flag, the default is ""false"".
  # When Rancher is enabled, it automatically installs cert-manager too, and it uses rancher's own self-signed certificates.
  # See for options https://rancher.com/docs/rancher/v2.0-v2.4/en/installation/resources/advanced/helm2/helm-rancher/#choose-your-ssl-configuration
  # The easiest thing is to leave everything as is (using the default rancher self-signed certificate) and put Cloudflare in front of it.
  # As for the number of replicas, by default it is set to the numbe of control plane nodes.
  # You can customized all of the above by adding a rancher_values variable see at the end of this file in the advanced section.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.
  # IMPORTANT: Rancher's install is quite memory intensive, you will require at least 4GB if RAM, meaning cx21 server type (for your control plane).
  # ALSO, in order for Rancher to successfully deploy, you have to set the ""rancher_hostname"".
  # enable_rancher = true

  # If using Rancher you can set the Rancher hostname, it must be unique hostname even if you do not use it.
  # If not pointing the DNS, you can just port-forward locally via kubectl to get access to the dashboard.
  # If you already set the lb_hostname above and are using a Hetzner LB, you do not need to set this one, as it will be used by default.
  # But if you set this one explicitly, it will have preference over the lb_hostname in rancher settings.
  # rancher_hostname = ""rancher.xyz.dev""

  # When Rancher is deployed, by default is uses the ""latest"" channel. But this can be customized.
  # The allowed values are ""stable"" or ""latest"".
  # rancher_install_channel = ""stable""

  # Finally, you can specify a bootstrap-password for your rancher instance. Minimum 48 characters long!
  # If you leave empty, one will be generated for you.
  # (Can be used by another rancher2 provider to continue setup of rancher outside this module.)
  # rancher_bootstrap_password = """"

  # Separate from the above Rancher config (only use one or the other). You can import this cluster directly on an
  # an already active Rancher install. By clicking ""import cluster"" choosing ""generic"", giving it a name and pasting
  # the cluster registration url below. However, you can also ignore that and apply the url via kubectl as instructed
  # by Rancher in the wizard, and that would register your cluster too.
  # More information about the registration can be found here https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/registered-clusters/
  # rancher_registration_manifest_url = ""https://rancher.xyz.dev/v3/import/xxxxxxxxxxxxxxxxxxYYYYYYYYYYYYYYYYYYYzzzzzzzzzzzzzzzzzzzzz.yaml""

  # Extra values that will be passed to the `extra-manifests/kustomization.yaml.tpl` if its present.
  # extra_kustomize_parameters={}

  # It is best practice to turn this off, but for backwards compatibility it is set to ""true"" by default.
  # See https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/issues/349
  # When ""false"". The kubeconfig file can instead be created by executing: ""terraform output --raw kubeconfig > cluster_kubeconfig.yaml""
  # Always be careful to not commit this file!
  # create_kubeconfig = false

  # Don't create the kustomize backup. This can be helpful for automation.
  # create_kustomization = false

  ### ADVANCED - Custom helm values for packages above (search _values if you want to located where those are mentioned upper in this file)
  #  Inside the _values variable below are examples, up to you to find out the best helm values possible, we do not provide support for customized helm values.
  # Please understand that the indentation is very important, inside the EOTs, as those are proper yaml helm values.
  # We advise you to use the default values, and only change them if you know what you are doing!

  # Cilium, all Cilium helm values can be found at https://github.com/cilium/cilium/blob/master/install/kubernetes/cilium/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   cilium_values = <<EOT
ipam:
  mode: kubernetes
devices: ""eth1""
k8s:
  requireIPv4PodCIDR: true
kubeProxyReplacement: strict
  EOT */

  # Cert manager, all cert-manager helm values can be found at https://github.com/cert-manager/cert-manager/blob/master/deploy/charts/cert-manager/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   cert_manager_values = <<EOT
installCRDs: true
replicaCount: 3
webhook:
  replicaCount: 3
cainjector:
  replicaCount: 3
  EOT */

  # Longhorn, all Longhorn helm values can be found at https://github.com/longhorn/longhorn/blob/master/chart/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   longhorn_values = <<EOT
defaultSettings:
  defaultDataPath: /var/longhorn
persistence:
  defaultFsType: ext4
  defaultClassReplicaCount: 3
  defaultClass: true
  EOT */

  # Nginx, all Nginx helm values can be found at https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml
  # You can also have a look at https://kubernetes.github.io/ingress-nginx/, to understand how it works, and all the options at your disposal.
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   nginx_ingress_values = <<EOT
controller:
  watchIngressWithoutClass: ""true""
  kind: ""DaemonSet""
  config:
    ""use-forwarded-headers"": ""true""
    ""compute-full-forwarded-for"": ""true""
    ""use-proxy-protocol"": ""true""
  service:
    annotations:
      ""load-balancer.hetzner.cloud/name"": ""k3s""
      ""load-balancer.hetzner.cloud/use-private-ip"": ""true""
      ""load-balancer.hetzner.cloud/disable-private-ingress"": ""true""
      ""load-balancer.hetzner.cloud/location"": ""nbg1""
      ""load-balancer.hetzner.cloud/type"": ""lb11""
      ""load-balancer.hetzner.cloud/uses-proxyprotocol"": ""true""
  EOT */

  # Rancher, all Rancher helm values can be found at https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/chart-options/
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   rancher_values = <<EOT
ingress:
  tls:
    source: ""rancher""
hostname: ""rancher.example.com""
replicas: 1
bootstrapPassword: ""supermario""
  EOT */

}
",module,"module ""kube-hetzner"" {
  providers = {
    hcloud = hcloud
  }
  hcloud_token = local.hcloud_token

  # Then fill or edit the below values. Only the first values starting with a * are obligatory; the rest can remain with their default values, or you
  # could adapt them to your needs.

  # * For local dev, path to the git repo
  # source = ""../../kube-hetzner/""
  # If you want to use the latest master branch
  # source = ""github.com/kube-hetzner/terraform-hcloud-kube-hetzner""
  # For normal use, this is the path to the terraform registry
  source = ""kube-hetzner/kube-hetzner/hcloud""

  # You can optionally specify a version number
  # version = ""1.2.0""

  # Note that some values, notably ""location"" and ""public_key"" have no effect after initializing the cluster.
  # This is to keep Terraform from re-provisioning all nodes at once, which would lose data. If you want to update
  # those, you should instead change the value here and manually re-provision each node. Grep for ""lifecycle"".

  # Customize the SSH port (by default 22)
  # ssh_port = 2222

  # * Your ssh public key
  ssh_public_key = file(""~/.ssh/id_rsa.pub"")
  # * Your private key must be ""ssh_private_key = null"" when you want to use ssh-agent for a Yubikey-like device authentification or an SSH key-pair with a passphrase.
  # For more details on SSH see https://github.com/kube-hetzner/kube-hetzner/blob/master/docs/ssh.md
  ssh_private_key = file(""~/.ssh/id_rsa"")
  # You can add additional SSH public Keys to grant other team members root access to your cluster nodes.
  # ssh_additional_public_keys = []

  # You can also add additional SSH public Keys which are saved in the hetzner cloud by a label.
  # See https://docs.hetzner.cloud/#label-selector
  # ssh_hcloud_key_label = ""role=admin""

  # If you want to use an ssh key that is already registered within hetzner cloud, you can pass its id.
  # If no id is passed, a new ssh key will be registered within hetzner cloud.
  # It is important that exactly this key is passed via `ssh_public_key` & `ssh_private_key` vars.
  # hcloud_ssh_key_id = """"

  # These can be customized, or left with the default values
  # * For Hetzner locations see https://docs.hetzner.com/general/others/data-centers-and-connection/
  network_region = ""eu-central"" # change to `us-east` if location is ash

  # For the control planes, at least three nodes are the minimum for HA. Otherwise, you need to turn off the automatic upgrades (see README).
  # **It must always be an ODD number, never even!** Search the internet for ""splitbrain problem with etcd"" or see https://rancher.com/docs/k3s/latest/en/installation/ha-embedded/
  # For instance, one is ok (non-HA), two is not ok, and three is ok (becomes HA). It does not matter if they are in the same nodepool or not! So they can be in different locations and of various types.

  # Of course, you can choose any number of nodepools you want, with the location you want. The only constraint on the location is that you need to stay in the same network region, Europe, or the US.
  # For the server type, the minimum instance supported is cpx11 (just a few cents more than cx11); see https://www.hetzner.com/cloud.

  # IMPORTANT: Before you create your cluster, you can do anything you want with the nodepools, but you need at least one of each, control plane and agent.
  # Once the cluster is up and running, you can change nodepool count and even set it to 0 (in the case of the first control-plane nodepool, the minimum is 1).
  # You can also rename it (if the count is 0), but do not remove a nodepool from the list.

  # The only nodepools that are safe to remove from the list are at the end. That is due to how subnets and IPs get allocated (FILO).
  # You can, however, freely add other nodepools at the end of each list if you want. The maximum number of nodepools you can create combined for both lists is 255.
  # Also, before decreasing the count of any nodepools to 0, it's essential to drain and cordon the nodes in question. Otherwise, it will leave your cluster in a bad state.

  # Before initializing the cluster, you can change all parameters and add or remove any nodepools. You need at least one nodepool of each kind, control plane, and agent.
  # The nodepool names are entirely arbitrary, you can choose whatever you want, but no special characters or underscore, and they must be unique; only alphanumeric characters and dashes are allowed.

  # If you want to have a single node cluster, have one control plane nodepools with a count of 1, and one agent nodepool with a count of 0.

  # Please note that changing labels and taints after the first run will have no effect. If needed, you can do that through Kubernetes directly.

  # * Example below:

  control_plane_nodepools = [
    {
      name        = ""control-plane-fsn1"",
      server_type = ""cpx11"",
      location    = ""fsn1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""control-plane-nbg1"",
      server_type = ""cpx11"",
      location    = ""nbg1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""control-plane-hel1"",
      server_type = ""cpx11"",
      location    = ""hel1"",
      labels      = [],
      taints      = [],
      count       = 1
    }
  ]

  agent_nodepools = [
    {
      name        = ""agent-small"",
      server_type = ""cpx11"",
      location    = ""fsn1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""agent-large"",
      server_type = ""cpx21"",
      location    = ""nbg1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""storage"",
      server_type = ""cpx21"",
      location    = ""fsn1"",
      # Fully optional, just a demo.
      labels      = [
        ""node.kubernetes.io/server-usage=storage""
      ],
      taints      = [],
      count       = 1
      # In the case of using Longhorn, you can use Hetzner volumes instead of using the node's own storage by specifying a value from 10 to 10000 (in GB)
      # It will create one volume per node in the nodepool, and configure Longhorn to use them.
      # Something worth noting is that Volume storage is slower than node storage, which is achieved by not mentioning longhorn_volume_size or setting it to 0.
      # So for something like DBs, you definitely want node storage, for other things like backups, volume storage is fine, and cheaper.
      # longhorn_volume_size = 20
    }
  ]
  # Add custom control plane configuration options here.
  # E.g to enable monitoring for etcd, proxy etc:
  # control_planes_custom_config = {
  #  etcd-expose-metrics = true,
  #  kube-controller-manager-arg = ""bind-address=0.0.0.0"",
  #  kube-proxy-arg =""metrics-bind-address=0.0.0.0"",
  #  kube-scheduler-arg = ""bind-address=0.0.0.0"",
  # }

  # * LB location and type, the latter will depend on how much load you want it to handle, see https://www.hetzner.com/cloud/load-balancer
  load_balancer_type     = ""lb11""
  load_balancer_location = ""fsn1""

  ### The following values are entirely optional (and can be removed from this if unused)

  # You can refine a base domain name to be use in this form of nodename.base_domain for setting the reserve dns inside Hetzner
  # base_domain = ""mycluster.example.com""

  # Cluster Autoscaler
  # Providing at least one map for the array enables the cluster autoscaler feature, default is disabled
  # Please note that the autoscaler should not be used with initial_k3s_channel < ""v1.25"". So ideally lock it to ""v1.25"".
  # * Example below:
  # autoscaler_nodepools = [
  #   {
  #     name        = ""autoscaler""
  #     server_type = ""cpx21"" # must be same or better than the control_plane server type (regarding disk size)!
  #     location    = ""fsn1""
  #     min_nodes   = 0
  #     max_nodes   = 5
  #   }
  # ]

  # Enable etcd snapshot backups to S3 storage.
  # Just provide a map with the needed settings (according to your S3 storage provider) and backups to S3 will
  # be enabled (with the default settings for etcd snapshots).
  # Cloudflare's R2 offers 10GB, 10 million reads and 1 million writes per month for free.
  # For proper context, have a look at https://docs.k3s.io/backup-restore.
  # etcd_s3_backup = {
  #   etcd-s3-endpoint        = ""xxxx.r2.cloudflarestorage.com""
  #   etcd-s3-access-key      = ""<access-key>""
  #   etcd-s3-secret-key      = ""<secret-key>""
  #   etcd-s3-bucket          = ""k3s-etcd-snapshots""
  # }

  # To use local storage on the nodes, you can enable Longhorn, default is ""false"".
  # See a full recap on how to configure agent nodepools for longhorn here https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/discussions/373#discussioncomment-3983159
  # enable_longhorn = true

  # By default, longhorn is pulled from https://charts.longhorn.io.
  # If you need a version of longhorn which assures compatibility with rancher you can set this variable to https://charts.rancher.io. 
  # longhorn_repository = ""https://charts.rancher.io""

  # The namespace for longhorn deployment, default is ""longhorn-system"".
  # longhorn_namespace = ""longhorn-system""

  # The file system type for Longhorn, if enabled (ext4 is the default, otherwise you can choose xfs).
  # longhorn_fstype = ""xfs""

  # how many replica volumes should longhorn create (default is 3).
  # longhorn_replica_count = 1

  # When you enable Longhorn, you can go with the default settings and just modify the above two variables OR you can add a longhorn_values variable
  # with all needed helm values, see towards the end of the file in the advanced section.
  # If that file is present, the system will use it during the deploy, if not it will use the default values with the two variable above that can be customized.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.

  # Also, you can choose to use a Hetzner volume with Longhorn. By default, it will use the nodes own storage space, but if you add an attribute of
  # longhorn_volume_size ( not a variable, just a possible agent nodepool attribute) with a value between 10 and 10000 GB to your agent nodepool definition, it will create and use the volume in question.
  # See the agent nodepool section for an example of how to do that.

  # To disable Hetzner CSI storage, you can set the following to ""true"", default is ""false"".
  # disable_hetzner_csi = true

  # If you want to use a specific Hetzner CCM and CSI version, set them below; otherwise, leave them as-is for the latest versions.
  # hetzner_ccm_version = """"
  # hetzner_csi_version = """"

  # If you want to specify the Kured version, set it below - otherwise it'll use the latest version available.
  # kured_version = """"

  # If you want to specify what time Kured starts restarting nodes, set it below. Default is 3AM.
  # kured_start_time = """"

  # If you want to specify what time Kured stops restarting nodes, set it below. Default is 8AM.
  # kured_end_time = """"

  # If you want to specify what timezone Kured uses, set it below. Default is Local.
  # kured_time_zone = """"

  # If you want to enable the Nginx ingress controller (https://kubernetes.github.io/ingress-nginx/) instead of Traefik, you can set this to ""nginx"". Default is ""traefik"".
  # By the default we load optimal Traefik and Nginx ingress controller config for Hetzner, however you may need to tweak it to your needs, so to do,
  # we allow you to add a traefik_values and nginx_values, see towards the end of this file in the advanced section.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.
  # If you want to disable both controllers set this to ""none""
  # ingress_controller = ""nginx""

  # You can change the number of replicas for selected ingress controller here. The default 0 means autoselecting based on number of agent nodes (1 node = 1 replica, 2 nodes = 2 replicas, 3+ nodes = 3 replicas)
  # ingress_replica_count = 1

  # Use the klipperLB (similar to metalLB), instead of the default Hetzner one, that has an advantage of dropping the cost of the setup.
  # Automatically ""true"" in the case of single node cluster (as it does not make sense to use the Hetzner LB in that situation).
  # It can work with any ingress controller that you choose to deploy.
  # Please note that because the klipperLB points to all nodes, we automatically allow scheduling on the control plane when it is active.
  # enable_klipper_metal_lb = ""true""

  # If you want to configure additional arguments for traefik, enter them here as a list and in the form of traefik CLI arguments; see https://doc.traefik.io/traefik/reference/static-configuration/cli/
  # They are the options that go into the additionalArguments section of the Traefik helm values file.
  # Example: traefik_additional_options = [""--log.level=DEBUG"", ""--tracing=true""]
  # traefik_additional_options = []

  # By default traefik is configured to redirect http traffic to https, you can set this to ""false"" to disable the redirection.
  # traefik_redirect_to_https = false

  # If you want to disable the metric server set this to ""false"". Default is ""true"".
  # enable_metrics_server = false

  # If you want to allow non-control-plane workloads to run on the control-plane nodes, set this to ""true"". The default is ""false"".
  # True by default for single node clusters, and when enable_klipper_metal_lb is true. In those cases, the value below will be ignored.
  # allow_scheduling_on_control_plane = true

  # If you want to disable the automatic upgrade of k3s, you can set below to ""false"".
  # Ideally, keep it on, to always have the latest Kubernetes version, but lock the initial_k3s_channel to a kube major version,
  # of your choice, like v1.25 or v1.26. That way you get the best of both worlds without the breaking changes risk.
  # For production use, always use an HA setup with at least 3 control-plane nodes and 2 agents, and keep this on for maximum security.

  # The default is ""true"" (in HA setup i.e. at least 3 control plane nodes & 2 agents, just keep it enabled since it works flawlessly).
  # automatically_upgrade_k3s = false

  # The default is ""true"" (in HA setup it works wonderfully well, with automatic roll-back to the previous snapshot in case of an issue).
  # IMPORTANT! For non-HA clusters i.e. when the number of control-plane nodes is < 3, you have to turn it off.
  # automatically_upgrade_os = false

  # If you need more control over kured and the reboot behaviour, you can pass additional options to kured.
  # For example limiting reboots to certain timeframes. For all options see: https://kured.dev/docs/configuration/
  # The default options are: `--reboot-command=/usr/bin/systemctl reboot --pre-reboot-node-labels=kured=rebooting --post-reboot-node-labels=kured=done --period=5m`
  # Defaults can be overridden by using the same key.
  # kured_options = {
  #   ""reboot-days"": ""su""
  #   ""start-time"": ""3am""
  #   ""end-time"": ""8am""
  # }

  # Allows you to specify either stable, latest, testing or supported minor versions.
  # see https://rancher.com/docs/k3s/latest/en/upgrades/basic/ and https://update.k3s.io/v1-release/channels
  #  If you are going to use Rancher addons for instance, it's always a good idea to fix the kube version to latest - 0.01,
  # at the time of writing the latest is v1.26, so setting the value below to ""v1.25"" will insure maximum compatibility with Rancher, Longhorn and so on!
  # The default is ""v1.25"".
  # initial_k3s_channel = ""stable""

  # The cluster name, by default ""k3s""
  # cluster_name = """"

  # Whether to use the cluster name in the node name, in the form of {cluster_name}-{nodepool_name}, the default is ""true"".
  # use_cluster_name_in_node_name = false

  # Extra k3s registries. This is useful if you have private registries and you
  # want to pull images without additional secrets.
  # registries.yaml file docs: https://docs.k3s.io/installation/private-registry
  /* k3s_registries = <<-EOT
    mirrors:
      hub.my_registry.com:
        endpoint:
          - ""hub.my_registry.com""
    configs:
      hub.my_registry.com:
        auth:
          username: username
          password: password
  EOT */

  # If you want to allow all outbound traffic you can set this to ""false"". Default is ""true"".
  # restrict_outbound_traffic = false

  # Adding extra firewall rules, like opening a port
  # More info on the format here https://registry.terraform.io/providers/hetznercloud/hcloud/latest/docs/resources/firewall
  # extra_firewall_rules = [
  #   # For Postgres
  #   {
  #     direction       = ""in""
  #     protocol        = ""tcp""
  #     port            = ""5432""
  #     source_ips      = [""0.0.0.0/0"", ""::/0""]
  #     destination_ips = [] # Won't be used for this rule
  #   },
  #   # To Allow ArgoCD access to resources via SSH
  #   {
  #     direction       = ""out""
  #     protocol        = ""tcp""
  #     port            = ""22""
  #     source_ips      = [] # Won't be used for this rule
  #     destination_ips = [""0.0.0.0/0"", ""::/0""]
  #   }
  # ]

  # If you want to configure a different CNI for k3s, use this flag
  # possible values: flannel (Default), calico, and cilium
  # As for Cilium, we allow infinite configurations via helm values, please check the CNI section of the readme over at https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/#cni.
  # Also, see the cilium_values at towards the end of this file, in the advanced section.
  # cni_plugin = ""cilium""

  # If you want to disable the k3s default network policy controller, use this flag!
  # Both Calico and Ciliun cni_plugin values override this value to true automatically, the default is ""false"".
  # disable_network_policy = true

  # If you want to disable the automatic use of placement group ""spread"". See https://docs.hetzner.com/cloud/placement-groups/overview/
  # That may be useful if you need to deploy more than 500 nodes! The default is ""false"".
  # placement_group_disable = true

  # By default, we allow ICMP ping in to the nodes, to check for liveness for instance. If you do not want to allow that, you can. Just set this flag to true (false by default).
  # block_icmp_ping_in = true

  # You can enable cert-manager (installed by Helm behind the scenes) with the following flag, the default is ""false"".
  # enable_cert_manager = true

  # By default we use a known good mirror to download the needed OpenSUSE, but for instance, it may not be available in US-East location, in which case,
  # you can find a working mirror for you at https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2.mirrorlist,
  # Or use the value below to automatically select an optimal mirror (by default we have it fixed to one german mirror that we know works for most locations)
  # opensuse_microos_mirror_link = ""https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2""

  # IP Addresses to use for the DNS Servers, set to an empty list to use the ones provided by Hetzner, defaults to [""1.1.1.1"", "" 1.0.0.1"", ""8.8.8.8""].
  # For rancher installs, best to leave it as default.
  # dns_servers = []

  # When this is enabled, rather than the first node, all external traffic will be routed via a control-plane loadbalancer, allowing for high availability.
  # The default is false.
  # use_control_plane_lb = true

  # Let's say you are not using the control plane LB solution above, and still want to have one hostname point to all your control-plane nodes.
  # You could create multiple A records of to let's say cp.cluster.my.org pointing to all of your control-plane nodes ips.
  # In which case, you need to define that hostname in the k3s TLS-SANs config to allow connection through it. It can be hostnames or IP addresses.
  # additional_tls_sans = [""cp.cluster.my.org""]

  # Oftentimes, you need to communicate to the cluster from inside the cluster itself, in which case it is important to set this value, as it will configure the hostname
  # at the load balancer level, and will save you from many slows downs when initiating communications from inside. Later on, you can point your DNS to the IP given
  # to the LB. And if you have other services pointing to it, you are also free to create CNAMES to point to it, or whatever you see fit.
  # If set, it will apply to either ingress controllers, Traefik or Ingress-Nginx.
  # lb_hostname = """"

  # You can enable Rancher (installed by Helm behind the scenes) with the following flag, the default is ""false"".
  # When Rancher is enabled, it automatically installs cert-manager too, and it uses rancher's own self-signed certificates.
  # See for options https://rancher.com/docs/rancher/v2.0-v2.4/en/installation/resources/advanced/helm2/helm-rancher/#choose-your-ssl-configuration
  # The easiest thing is to leave everything as is (using the default rancher self-signed certificate) and put Cloudflare in front of it.
  # As for the number of replicas, by default it is set to the numbe of control plane nodes.
  # You can customized all of the above by adding a rancher_values variable see at the end of this file in the advanced section.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.
  # IMPORTANT: Rancher's install is quite memory intensive, you will require at least 4GB if RAM, meaning cx21 server type (for your control plane).
  # ALSO, in order for Rancher to successfully deploy, you have to set the ""rancher_hostname"".
  # enable_rancher = true

  # If using Rancher you can set the Rancher hostname, it must be unique hostname even if you do not use it.
  # If not pointing the DNS, you can just port-forward locally via kubectl to get access to the dashboard.
  # If you already set the lb_hostname above and are using a Hetzner LB, you do not need to set this one, as it will be used by default.
  # But if you set this one explicitly, it will have preference over the lb_hostname in rancher settings.
  # rancher_hostname = ""rancher.xyz.dev""

  # When Rancher is deployed, by default is uses the ""latest"" channel. But this can be customized.
  # The allowed values are ""stable"" or ""latest"".
  # rancher_install_channel = ""stable""

  # Finally, you can specify a bootstrap-password for your rancher instance. Minimum 48 characters long!
  # If you leave empty, one will be generated for you.
  # (Can be used by another rancher2 provider to continue setup of rancher outside this module.)
  # rancher_bootstrap_password = """"

  # Separate from the above Rancher config (only use one or the other). You can import this cluster directly on an
  # an already active Rancher install. By clicking ""import cluster"" choosing ""generic"", giving it a name and pasting
  # the cluster registration url below. However, you can also ignore that and apply the url via kubectl as instructed
  # by Rancher in the wizard, and that would register your cluster too.
  # More information about the registration can be found here https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/registered-clusters/
  # rancher_registration_manifest_url = ""https://rancher.xyz.dev/v3/import/xxxxxxxxxxxxxxxxxxYYYYYYYYYYYYYYYYYYYzzzzzzzzzzzzzzzzzzzzz.yaml""

  # Extra values that will be passed to the `extra-manifests/kustomization.yaml.tpl` if its present.
  # extra_kustomize_parameters={}

  # It is best practice to turn this off, but for backwards compatibility it is set to ""true"" by default.
  # See https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/issues/349
  # When ""false"". The kubeconfig file can instead be created by executing: ""terraform output --raw kubeconfig > cluster_kubeconfig.yaml""
  # Always be careful to not commit this file!
  # create_kubeconfig = false

  # Don't create the kustomize backup. This can be helpful for automation.
  # create_kustomization = false

  ### ADVANCED - Custom helm values for packages above (search _values if you want to located where those are mentioned upper in this file)
  #  Inside the _values variable below are examples, up to you to find out the best helm values possible, we do not provide support for customized helm values.
  # Please understand that the indentation is very important, inside the EOTs, as those are proper yaml helm values.
  # We advise you to use the default values, and only change them if you know what you are doing!

  # Cilium, all Cilium helm values can be found at https://github.com/cilium/cilium/blob/master/install/kubernetes/cilium/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   cilium_values = <<EOT
ipam:
  mode: kubernetes
devices: ""eth1""
k8s:
  requireIPv4PodCIDR: true
kubeProxyReplacement: strict
  EOT */

  # Cert manager, all cert-manager helm values can be found at https://github.com/cert-manager/cert-manager/blob/master/deploy/charts/cert-manager/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   cert_manager_values = <<EOT
installCRDs: true
replicaCount: 3
webhook:
  replicaCount: 3
cainjector:
  replicaCount: 3
  EOT */

  # Longhorn, all Longhorn helm values can be found at https://github.com/longhorn/longhorn/blob/master/chart/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   longhorn_values = <<EOT
defaultSettings:
  defaultDataPath: /var/longhorn
persistence:
  defaultFsType: ext4
  defaultClassReplicaCount: 3
  defaultClass: true
  EOT */

  # Nginx, all Nginx helm values can be found at https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml
  # You can also have a look at https://kubernetes.github.io/ingress-nginx/, to understand how it works, and all the options at your disposal.
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   nginx_ingress_values = <<EOT
controller:
  watchIngressWithoutClass: ""true""
  kind: ""DaemonSet""
  config:
    ""use-forwarded-headers"": ""true""
    ""compute-full-forwarded-for"": ""true""
    ""use-proxy-protocol"": ""true""
  service:
    annotations:
      ""load-balancer.hetzner.cloud/name"": ""k3s""
      ""load-balancer.hetzner.cloud/use-private-ip"": ""true""
      ""load-balancer.hetzner.cloud/disable-private-ingress"": ""true""
      ""load-balancer.hetzner.cloud/location"": ""nbg1""
      ""load-balancer.hetzner.cloud/type"": ""lb11""
      ""load-balancer.hetzner.cloud/uses-proxyprotocol"": ""true""
  EOT */

  # Rancher, all Rancher helm values can be found at https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/chart-options/
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   rancher_values = <<EOT
ingress:
  tls:
    source: ""rancher""
hostname: ""rancher.example.com""
replicas: 1
bootstrapPassword: ""supermario""
  EOT */

}
",module,354,354.0,1e07f152504e14250afb8fe82fed2545baec642f,1e07f152504e14250afb8fe82fed2545baec642f,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/1e07f152504e14250afb8fe82fed2545baec642f/kube.example.tf#L354,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/1e07f152504e14250afb8fe82fed2545baec642f/kube.example.tf#L354,2023-01-21 12:53:45+01:00,2023-01-21 12:53:45+01:00,1,0
https://github.com/ManagedKube/kubernetes-ops,8,terraform-modules/aws/msk_1.0.9/module/security-group-variables.tf,terraform-modules/aws/msk_1.0.9/module/security-group-variables.tf,0,implement,# REMOVE COMMENTS below that are intended for the initial implementor and not maintainers or end users.,"# security-group-variables Version: 3 
 # 
 # Copy this file from https://github.com/cloudposse/terraform-aws-security-group/blob/master/exports/security-group-variables.tf 
 # and EDIT IT TO SUIT YOUR PROJECT. Update the version number above if you update this file from a later version. 
 # Unlike null-label context.tf, this file cannot be automatically updated 
 # because of the tight integration with the module using it. 
 ## 
 # Delete this top comment block, except for the first line (version number), 
 # REMOVE COMMENTS below that are intended for the initial implementor and not maintainers or end users. 
 # 
 # This file provides the standard inputs that all Cloud Posse Open Source 
 # Terraform module that create AWS Security Groups should implement. 
 # This file does NOT provide implementation of the inputs, as that 
 # of course varies with each module. 
 # 
 # This file declares some standard outputs modules should create, 
 # but the declarations should be moved to `outputs.tf` and of course 
 # may need to be modified based on the module's use of security-group. 
 #  ","variable ""create_security_group"" {
  type        = bool
  description = ""Set `true` to create and configure a new security group. If false, `associated_security_group_ids` must be provided.""
  default     = true
}
",variable,"variable ""create_security_group"" {
  type        = bool
  description = ""Set `true` to create and configure a new security group. If false, `associated_security_group_ids` must be provided.""
  default     = true
}
",variable,9,9.0,c8193c7d74e2f7c624f0867337294cb66a2b9469,c8193c7d74e2f7c624f0867337294cb66a2b9469,https://github.com/ManagedKube/kubernetes-ops/blob/c8193c7d74e2f7c624f0867337294cb66a2b9469/terraform-modules/aws/msk_1.0.9/module/security-group-variables.tf#L9,https://github.com/ManagedKube/kubernetes-ops/blob/c8193c7d74e2f7c624f0867337294cb66a2b9469/terraform-modules/aws/msk_1.0.9/module/security-group-variables.tf#L9,2023-12-14 10:29:30-08:00,2023-12-14 10:29:30-08:00,1,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,2,eks.tf,eks.tf,0,todo,# TODO Create a new Self-Managed Node group and remove worker_create_cluster_primary_security_group_rules and worker_groups_launch_template,"#############################END OF EKS CLUSTER MODULE #############################################################  
 # TODO handle this in aws-ia TF EKS module 
 //  map_roles    = local.common_roles 
 //  map_users    = var.map_users 
 //  map_accounts = var.map_accounts  
 # TODO Create a new Self-Managed Node group and remove worker_create_cluster_primary_security_group_rules and worker_groups_launch_template 
 #---------------------------------------------------------------------------------- 
 #   Self-managed node group (worker group) 
 #---------------------------------------------------------------------------------- 
 # Conditionally allow Worker nodes <-> primary cluster SG traffic 
 # See https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/faq.md#im-using-both-aws-managed-node-groups-and-self-managed-worker-groups-and-pods-scheduled-on-a-aws-managed-node-groups-are-unable-resolve-dns-even-communication-between-pods","module ""eks"" {
  create_eks      = var.create_eks
  manage_aws_auth = false
  source          = ""terraform-aws-modules/eks/aws""
  version         = ""17.1.0""
  cluster_name    = module.eks-label.id
  cluster_version = var.kubernetes_version

  vpc_id = var.create_vpc == false ? var.vpc_id : module.vpc.vpc_id

  subnets                         = var.create_vpc == false ? var.private_subnet_ids : module.vpc.private_subnets
  cluster_endpoint_private_access = var.endpoint_private_access
  cluster_endpoint_public_access  = var.endpoint_public_access
  enable_irsa                     = var.enable_irsa
  kubeconfig_output_path          = ""./kubeconfig/""

  tags = module.eks-label.tags

  cluster_enabled_log_types = var.enabled_cluster_log_types

  cluster_encryption_config = [
    {
      provider_key_arn = aws_kms_key.eks.arn
      resources = [
      ""secrets""]
    }
  ]

  #############################END OF EKS CLUSTER MODULE #############################################################

  # TODO handle this in aws-ia TF EKS module
  //  map_roles    = local.common_roles
  //  map_users    = var.map_users
  //  map_accounts = var.map_accounts

  # TODO Create a new Self-Managed Node group and remove worker_create_cluster_primary_security_group_rules and worker_groups_launch_template
  #----------------------------------------------------------------------------------
  #   Self-managed node group (worker group)
  #----------------------------------------------------------------------------------
  # Conditionally allow Worker nodes <-> primary cluster SG traffic
  # See https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/docs/faq.md#im-using-both-aws-managed-node-groups-and-self-managed-worker-groups-and-pods-scheduled-on-a-aws-managed-node-groups-are-unable-resolve-dns-even-communication-between-pods
  worker_create_cluster_primary_security_group_rules = var.enable_self_managed_nodegroups

  # Conditionally create a self-managed node group (worker group) - either Windows or Linux
  worker_groups_launch_template = var.enable_self_managed_nodegroups ? [{
    name     = var.self_managed_nodegroup_name
    platform = local.self_managed_node_platform

    # Use custom AMI, user data script template, and its parameters, if provided in input. 
    # Otherwise, use default EKS-optimized AMI, user data script for Windows / Linux.
    ami_id                       = var.self_managed_node_ami_id != """" ? var.self_managed_node_ami_id : var.enable_windows_support ? data.aws_ami.windows2019core.id : data.aws_ami.amazonlinux2eks.id
    userdata_template_file       = var.self_managed_node_userdata_template_file != """" ? var.self_managed_node_userdata_template_file : var.enable_windows_support ? ""./templates/userdata-windows.tpl"" : ""./templates/userdata-amazonlinux2eks.tpl""
    userdata_template_extra_args = var.self_managed_node_userdata_template_extra_params

    override_instance_types = var.self_managed_node_instance_types
    root_encrypted          = true
    root_volume_size        = var.self_managed_node_volume_size

    iam_instance_profile_name = var.enable_windows_support ? module.windows_support_iam[0].windows_instance_profile.name : null
    asg_desired_capacity      = var.self_managed_node_desired_size
    asg_min_size              = var.self_managed_node_min_size
    asg_max_size              = var.self_managed_node_max_size

    kubelet_extra_args = ""--node-labels=Environment=${var.environment},Zone=${var.zone},WorkerType=SELF_MANAGED_${upper(local.self_managed_node_platform)}""

    # Extra tags, needed for cluster autoscaler autodiscovery
    tags = var.cluster_autoscaler_enable ? [{
      key                 = ""k8s.io/cluster-autoscaler/enabled"",
      value               = true,
      propagate_at_launch = true
      }, {
      key                 = ""k8s.io/cluster-autoscaler/${module.eks-label.id}"",
      value               = ""owned"",
      propagate_at_launch = true
    }] : []
  }] : []

}
",module,"module ""eks"" {
  create_eks      = var.create_eks
  manage_aws_auth = false # Replaced by the auth.tf file

  #TODO Refer to internal AWS-IA module
  source  = ""terraform-aws-modules/eks/aws""
  version = ""17.1.0""

  cluster_name    = module.eks-label.id
  cluster_version = var.kubernetes_version

  # NETWORK CONFIG
  vpc_id  = var.create_vpc == false ? var.vpc_id : module.vpc.vpc_id
  subnets = var.create_vpc == false ? var.private_subnet_ids : module.vpc.private_subnets

  cluster_endpoint_private_access = var.endpoint_private_access
  cluster_endpoint_public_access  = var.endpoint_public_access

  # IRSA
  enable_irsa            = var.enable_irsa
  kubeconfig_output_path = ""./kubeconfig/""

  # TAGS
  tags = module.eks-label.tags

  # CLUSTER LOGGING
  cluster_enabled_log_types = var.enabled_cluster_log_types

  # CLUSTER ENCRYPTION
  cluster_encryption_config = [
    {
      provider_key_arn = aws_kms_key.eks.arn
      resources = [
      ""secrets""]
    }
  ]
}
",module,86,,50f6e2c2dcd3479177d5c5001732c013be2fe6de,125390ed86df57dc9d8064df92b36e14cc8eb3e2,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/50f6e2c2dcd3479177d5c5001732c013be2fe6de/eks.tf#L86,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/125390ed86df57dc9d8064df92b36e14cc8eb3e2/eks.tf,2021-08-27 00:35:58+01:00,2021-09-13 14:12:34+01:00,4,1
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,370,autoscaler-agents.tf,autoscaler-agents.tf,0,implement,"# for now we use x86 snapshot, this needs to implement logic to autodetect between x86 and aarch64 based on server type","# for now we use x86 snapshot, this needs to implement logic to autodetect between x86 and aarch64 based on server type","locals {
  cluster_prefix = var.use_cluster_name_in_node_name ? ""${var.cluster_name}-"" : """"
  autoscaler_yaml = length(var.autoscaler_nodepools) == 0 ? """" : templatefile(
    ""${path.module}/templates/autoscaler.yaml.tpl"",
    {
      cloudinit_config = base64encode(data.cloudinit_config.autoscaler-config[0].rendered)
      ca_image         = var.cluster_autoscaler_image
      ca_version       = var.cluster_autoscaler_version
      ssh_key          = local.hcloud_ssh_key_id
      # for now we use the k3s network, as we cannot reference subnet-ids in autoscaler
      ipv4_subnet_id = hcloud_network.k3s.id
      # for now we use x86 snapshot, this needs to implement logic to autodetect between x86 and aarch64 based on server type
      snapshot_id    = data.hcloud_image.microos_x86_snapshot.id
      firewall_id    = hcloud_firewall.k3s.id
      cluster_name   = local.cluster_prefix
      node_pools     = var.autoscaler_nodepools
  })
  # A concatenated list of all autoscaled nodes
  autoscaled_nodes = length(var.autoscaler_nodepools) == 0 ? {} : {
    for v in concat([
      for k, v in data.
      hcloud_servers.autoscaled_nodes : [for v in v.servers : v]
    ]...) : v.name => v
  }
}
",locals,"locals {
  cluster_prefix = var.use_cluster_name_in_node_name ? ""${var.cluster_name}-"" : """"
  first_nodepool_snapshot_id = length(var.autoscaler_nodepools) == 0 ? """" : (
    substr(var.autoscaler_nodepools[0].server_type, 0, 3) == ""cax"" ? data.hcloud_image.microos_arm_snapshot.id : data.hcloud_image.microos_x86_snapshot.id
  )
  autoscaler_yaml = length(var.autoscaler_nodepools) == 0 ? """" : templatefile(
    ""${path.module}/templates/autoscaler.yaml.tpl"",
    {
      cloudinit_config = base64encode(data.cloudinit_config.autoscaler-config[0].rendered)
      ca_image         = var.cluster_autoscaler_image
      ca_version       = var.cluster_autoscaler_version
      ssh_key          = local.hcloud_ssh_key_id
      ipv4_subnet_id   = hcloud_network.k3s.id
      snapshot_id      = local.first_nodepool_snapshot_id
      firewall_id      = hcloud_firewall.k3s.id
      cluster_name     = local.cluster_prefix
      node_pools       = var.autoscaler_nodepools
  })
  # A concatenated list of all autoscaled nodes
  autoscaled_nodes = length(var.autoscaler_nodepools) == 0 ? {} : {
    for v in concat([
      for k, v in data.
      hcloud_servers.autoscaled_nodes : [for v in v.servers : v]
    ]...) : v.name => v
  }
}
",locals,12,,297f4a16d496bfaa684bbbcbeb9139501638d3f2,1cbf8e25d38e525a13959ef56e21ea9bebd2e9f8,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/297f4a16d496bfaa684bbbcbeb9139501638d3f2/autoscaler-agents.tf#L12,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/1cbf8e25d38e525a13959ef56e21ea9bebd2e9f8/autoscaler-agents.tf,2023-04-14 16:01:31+02:00,2023-04-16 08:28:52+02:00,3,1
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1760,modules/net-lb-app-ext-regional/backend-service.tf,modules/net-lb-app-ext-regional/backend-service.tf,0,# todo,# TODO(jccb): add connection_tracking_policy block,"# TODO(jccb): add security_policy block 
 # TODO(jccb): add connection_tracking_policy block ","resource ""google_compute_region_backend_service"" ""default"" {
  provider = google-beta
  for_each = var.backend_service_configs
  project = (
    each.value.project_id == null
    ? var.project_id
    : each.value.project_id
  )
  name                            = ""${var.name}-${each.key}""
  region                          = var.region
  description                     = var.description
  affinity_cookie_ttl_sec         = each.value.affinity_cookie_ttl_sec
  connection_draining_timeout_sec = each.value.connection_draining_timeout_sec
  enable_cdn                      = each.value.enable_cdn
  health_checks = length(each.value.health_checks) == 0 ? null : [
    for k in each.value.health_checks : lookup(local.hc_ids, k, k)
  ]
  # external regional load balancer is always EXTERNAL_MANAGER.
  # TODO(jccb): double check if this is true
  load_balancing_scheme = ""EXTERNAL_MANAGED""
  #TODO(jccb): add locality_lb_policy with MAGLEV and WEIGHTED_MAGLEV when scheme EXTERNAL
  port_name = (
    each.value.port_name == null
    ? lower(each.value.protocol == null ? var.protocol : each.value.protocol)
    : each.value.port_name
  )
  protocol = (
    each.value.protocol == null ? var.protocol : each.value.protocol
  )
  session_affinity = each.value.session_affinity
  timeout_sec      = each.value.timeout_sec

  dynamic ""backend"" {
    for_each = { for b in coalesce(each.value.backends, []) : b.backend => b }
    content {
      group           = lookup(local.group_ids, backend.key, backend.key)
      balancing_mode  = backend.value.balancing_mode # UTILIZATION, RATE
      capacity_scaler = backend.value.capacity_scaler
      description     = backend.value.description
      max_connections = try(
        backend.value.max_connections.per_group, null
      )
      max_connections_per_endpoint = try(
        backend.value.max_connections.per_endpoint, null
      )
      max_connections_per_instance = try(
        backend.value.max_connections.per_instance, null
      )
      max_rate = try(
        backend.value.max_rate.per_group, null
      )
      max_rate_per_endpoint = try(
        backend.value.max_rate.per_endpoint, null
      )
      max_rate_per_instance = try(
        backend.value.max_rate.per_instance, null
      )
      max_utilization = backend.value.max_utilization
    }
  }

  dynamic ""cdn_policy"" {
    for_each = (
      each.value.cdn_policy == null ? [] : [each.value.cdn_policy]
    )
    iterator = cdn
    content {
      cache_mode                   = cdn.value.cache_mode
      client_ttl                   = cdn.value.client_ttl
      default_ttl                  = cdn.value.default_ttl
      max_ttl                      = cdn.value.max_ttl
      negative_caching             = cdn.value.negative_caching
      serve_while_stale            = cdn.value.serve_while_stale
      signed_url_cache_max_age_sec = cdn.value.signed_url_cache_max_age_sec
      dynamic ""cache_key_policy"" {
        for_each = (
          cdn.value.cache_key_policy == null
          ? []
          : [cdn.value.cache_key_policy]
        )
        iterator = ck
        content {
          include_host           = ck.value.include_host
          include_named_cookies  = ck.value.include_named_cookies
          include_protocol       = ck.value.include_protocol
          include_query_string   = ck.value.include_query_string
          query_string_blacklist = ck.value.query_string_blacklist
          query_string_whitelist = ck.value.query_string_whitelist
        }
      }
      dynamic ""negative_caching_policy"" {
        for_each = (
          cdn.value.negative_caching_policy == null
          ? []
          : [cdn.value.negative_caching_policy]
        )
        iterator = nc
        content {
          code = nc.value.code
          ttl  = nc.value.ttl
        }
      }
    }
  }

  dynamic ""circuit_breakers"" {
    for_each = (
      each.value.circuit_breakers == null ? [] : [each.value.circuit_breakers]
    )
    iterator = cb
    content {
      max_connections             = cb.value.max_connections
      max_pending_requests        = cb.value.max_pending_requests
      max_requests                = cb.value.max_requests
      max_requests_per_connection = cb.value.max_requests_per_connection
      max_retries                 = cb.value.max_retries
      dynamic ""connect_timeout"" {
        for_each = (
          cb.value.connect_timeout == null ? [] : [cb.value.connect_timeout]
        )
        content {
          seconds = connect_timeout.value.seconds
          nanos   = connect_timeout.value.nanos
        }
      }
    }
  }

  dynamic ""consistent_hash"" {
    for_each = (
      each.value.consistent_hash == null ? [] : [each.value.consistent_hash]
    )
    iterator = ch
    content {
      http_header_name  = ch.value.http_header_name
      minimum_ring_size = ch.value.minimum_ring_size
      dynamic ""http_cookie"" {
        for_each = ch.value.http_cookie == null ? [] : [ch.value.http_cookie]
        content {
          name = http_cookie.value.name
          path = http_cookie.value.path
          dynamic ""ttl"" {
            for_each = (
              http_cookie.value.ttl == null ? [] : [http_cookie.value.ttl]
            )
            content {
              seconds = ttl.value.seconds
              nanos   = ttl.value.nanos
            }
          }
        }
      }
    }
  }

  dynamic ""iap"" {
    for_each = each.value.iap_config == null ? [] : [each.value.iap_config]
    content {
      oauth2_client_id            = iap.value.oauth2_client_id
      oauth2_client_secret        = iap.value.oauth2_client_secret
      oauth2_client_secret_sha256 = iap.value.oauth2_client_secret_sha256
    }
  }

  dynamic ""log_config"" {
    for_each = each.value.log_sample_rate == null ? [] : [""""]
    content {
      enable      = true
      sample_rate = each.value.log_sample_rate
    }
  }

  dynamic ""outlier_detection"" {
    for_each = (
      each.value.outlier_detection == null ? [] : [each.value.outlier_detection]
    )
    iterator = od
    content {
      consecutive_errors                    = od.value.consecutive_errors
      consecutive_gateway_failure           = od.value.consecutive_gateway_failure
      enforcing_consecutive_errors          = od.value.enforcing_consecutive_errors
      enforcing_consecutive_gateway_failure = od.value.enforcing_consecutive_gateway_failure
      enforcing_success_rate                = od.value.enforcing_success_rate
      max_ejection_percent                  = od.value.max_ejection_percent
      success_rate_minimum_hosts            = od.value.success_rate_minimum_hosts
      success_rate_request_volume           = od.value.success_rate_request_volume
      success_rate_stdev_factor             = od.value.success_rate_stdev_factor
      dynamic ""base_ejection_time"" {
        for_each = (
          od.value.base_ejection_time == null ? [] : [od.value.base_ejection_time]
        )
        content {
          seconds = base_ejection_time.value.seconds
          nanos   = base_ejection_time.value.nanos
        }
      }
      dynamic ""interval"" {
        for_each = (
          od.value.interval == null ? [] : [od.value.interval]
        )
        content {
          seconds = interval.value.seconds
          nanos   = interval.value.nanos
        }
      }
    }
  }
}
",resource,"resource ""google_compute_region_backend_service"" ""default"" {
  provider = google-beta
  for_each = var.backend_service_configs
  project = (
    each.value.project_id == null
    ? var.project_id
    : each.value.project_id
  )
  name                            = ""${var.name}-${each.key}""
  region                          = var.region
  description                     = var.description
  affinity_cookie_ttl_sec         = each.value.affinity_cookie_ttl_sec
  connection_draining_timeout_sec = each.value.connection_draining_timeout_sec
  enable_cdn                      = each.value.enable_cdn
  health_checks = length(each.value.health_checks) == 0 ? null : [
    for k in each.value.health_checks : lookup(local.hc_ids, k, k)
  ]
  # external regional load balancer is always EXTERNAL_MANAGER.
  # TODO(jccb): double check if this is true
  load_balancing_scheme = ""EXTERNAL_MANAGED""
  #TODO(jccb): add locality_lb_policy with MAGLEV and WEIGHTED_MAGLEV when scheme EXTERNAL
  port_name = (
    each.value.port_name == null
    ? lower(each.value.protocol == null ? var.protocol : each.value.protocol)
    : each.value.port_name
  )
  protocol = (
    each.value.protocol == null ? var.protocol : each.value.protocol
  )
  session_affinity = each.value.session_affinity
  timeout_sec      = each.value.timeout_sec

  dynamic ""backend"" {
    for_each = { for b in coalesce(each.value.backends, []) : b.backend => b }
    content {
      group           = lookup(local.group_ids, backend.key, backend.key)
      balancing_mode  = backend.value.balancing_mode # UTILIZATION, RATE
      capacity_scaler = backend.value.capacity_scaler
      description     = backend.value.description
      max_connections = try(
        backend.value.max_connections.per_group, null
      )
      max_connections_per_endpoint = try(
        backend.value.max_connections.per_endpoint, null
      )
      max_connections_per_instance = try(
        backend.value.max_connections.per_instance, null
      )
      max_rate = try(
        backend.value.max_rate.per_group, null
      )
      max_rate_per_endpoint = try(
        backend.value.max_rate.per_endpoint, null
      )
      max_rate_per_instance = try(
        backend.value.max_rate.per_instance, null
      )
      max_utilization = backend.value.max_utilization
    }
  }

  dynamic ""cdn_policy"" {
    for_each = (
      each.value.cdn_policy == null ? [] : [each.value.cdn_policy]
    )
    iterator = cdn
    content {
      cache_mode                   = cdn.value.cache_mode
      client_ttl                   = cdn.value.client_ttl
      default_ttl                  = cdn.value.default_ttl
      max_ttl                      = cdn.value.max_ttl
      negative_caching             = cdn.value.negative_caching
      serve_while_stale            = cdn.value.serve_while_stale
      signed_url_cache_max_age_sec = cdn.value.signed_url_cache_max_age_sec
      dynamic ""cache_key_policy"" {
        for_each = (
          cdn.value.cache_key_policy == null
          ? []
          : [cdn.value.cache_key_policy]
        )
        iterator = ck
        content {
          include_host           = ck.value.include_host
          include_named_cookies  = ck.value.include_named_cookies
          include_protocol       = ck.value.include_protocol
          include_query_string   = ck.value.include_query_string
          query_string_blacklist = ck.value.query_string_blacklist
          query_string_whitelist = ck.value.query_string_whitelist
        }
      }
      dynamic ""negative_caching_policy"" {
        for_each = (
          cdn.value.negative_caching_policy == null
          ? []
          : [cdn.value.negative_caching_policy]
        )
        iterator = nc
        content {
          code = nc.value.code
          ttl  = nc.value.ttl
        }
      }
    }
  }

  dynamic ""circuit_breakers"" {
    for_each = (
      each.value.circuit_breakers == null ? [] : [each.value.circuit_breakers]
    )
    iterator = cb
    content {
      max_connections             = cb.value.max_connections
      max_pending_requests        = cb.value.max_pending_requests
      max_requests                = cb.value.max_requests
      max_requests_per_connection = cb.value.max_requests_per_connection
      max_retries                 = cb.value.max_retries
      dynamic ""connect_timeout"" {
        for_each = (
          cb.value.connect_timeout == null ? [] : [cb.value.connect_timeout]
        )
        content {
          seconds = connect_timeout.value.seconds
          nanos   = connect_timeout.value.nanos
        }
      }
    }
  }

  dynamic ""consistent_hash"" {
    for_each = (
      each.value.consistent_hash == null ? [] : [each.value.consistent_hash]
    )
    iterator = ch
    content {
      http_header_name  = ch.value.http_header_name
      minimum_ring_size = ch.value.minimum_ring_size
      dynamic ""http_cookie"" {
        for_each = ch.value.http_cookie == null ? [] : [ch.value.http_cookie]
        content {
          name = http_cookie.value.name
          path = http_cookie.value.path
          dynamic ""ttl"" {
            for_each = (
              http_cookie.value.ttl == null ? [] : [http_cookie.value.ttl]
            )
            content {
              seconds = ttl.value.seconds
              nanos   = ttl.value.nanos
            }
          }
        }
      }
    }
  }

  dynamic ""iap"" {
    for_each = each.value.iap_config == null ? [] : [each.value.iap_config]
    content {
      oauth2_client_id            = iap.value.oauth2_client_id
      oauth2_client_secret        = iap.value.oauth2_client_secret
      oauth2_client_secret_sha256 = iap.value.oauth2_client_secret_sha256
    }
  }

  dynamic ""log_config"" {
    for_each = each.value.log_sample_rate == null ? [] : [""""]
    content {
      enable      = true
      sample_rate = each.value.log_sample_rate
    }
  }

  dynamic ""outlier_detection"" {
    for_each = (
      each.value.outlier_detection == null ? [] : [each.value.outlier_detection]
    )
    iterator = od
    content {
      consecutive_errors                    = od.value.consecutive_errors
      consecutive_gateway_failure           = od.value.consecutive_gateway_failure
      enforcing_consecutive_errors          = od.value.enforcing_consecutive_errors
      enforcing_consecutive_gateway_failure = od.value.enforcing_consecutive_gateway_failure
      enforcing_success_rate                = od.value.enforcing_success_rate
      max_ejection_percent                  = od.value.max_ejection_percent
      success_rate_minimum_hosts            = od.value.success_rate_minimum_hosts
      success_rate_request_volume           = od.value.success_rate_request_volume
      success_rate_stdev_factor             = od.value.success_rate_stdev_factor
      dynamic ""base_ejection_time"" {
        for_each = (
          od.value.base_ejection_time == null ? [] : [od.value.base_ejection_time]
        )
        content {
          seconds = base_ejection_time.value.seconds
          nanos   = base_ejection_time.value.nanos
        }
      }
      dynamic ""interval"" {
        for_each = (
          od.value.interval == null ? [] : [od.value.interval]
        )
        content {
          seconds = interval.value.seconds
          nanos   = interval.value.nanos
        }
      }
    }
  }
}
",resource,40,40.0,8beb621e070226b7f11a82807a706170ae7040ea,8beb621e070226b7f11a82807a706170ae7040ea,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/8beb621e070226b7f11a82807a706170ae7040ea/modules/net-lb-app-ext-regional/backend-service.tf#L40,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/8beb621e070226b7f11a82807a706170ae7040ea/modules/net-lb-app-ext-regional/backend-service.tf#L40,2024-01-05 16:59:27+01:00,2024-01-05 16:59:27+01:00,1,0
https://github.com/terraform-aws-modules/terraform-aws-eks,7,main.tf,main.tf,0,fix,"/** # terraform-aws-eks  * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html).  * | Branch | Build status                                                                                                                                                      | * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) |  * ## Assumptions  ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources.  * ## Usage example  * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry:  * ```hcl * module ""eks"" { *   source                = ""terraform-aws-modules/eks/aws"" *   version               = ""0.1.0"" *   cluster_name          = ""test-eks-cluster"" *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] *   tags                  = ""${map(""Environment"", ""test"")}"" *   vpc_id                = ""vpc-abcde012"" * } * ```  * ## Dependencies  * The `configure_kubectl_session` variable requires that both [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) (>=1.10) and [`heptio-authenticator-aws`](https://github.com/heptio/authenticator#4-set-up-kubectl-to-use-heptio-authenticator-for-aws-tokens) are installed and on your shell's PATH.  * ## Testing  * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them:  * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). * 2. Install bundler and the gems from our Gemfile: * *     ```bash *     gem install bundler && bundle install *     ``` * * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. * 4. Test using `bundle exec kitchen test` from the root of the repo.  * For now, connectivity to the kubernetes cluster is not tested but will be in the * future. If `configure_kubectl_session` is set `true`, once the test fixture has * converged, you can query the test cluster from that terminal session with * `kubectl get nodes --watch --kubeconfig kubeconfig`.  * ## Doc generation  * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). * Generate them like so:  * ```bash * go get github.com/segmentio/terraform-docs * terraform-docs md ./ | cat -s | ghead -n -1 > README.md * ```  * ## Contributing  * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section.  * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md).  * ## IAM Permissions  * Testing and using this repo requires a minimum set of IAM permissions. Test permissions * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md).  * ## Change log  * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes.  * ## Authors  * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)!  * ## License  * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details.*/","/** 
 # terraform-aws-eks 
  
 * A terraform module to create a managed Kubernetes cluster on AWS EKS. Available 
 * through the [Terraform registry](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws). 
 * Inspired by and adapted from [this doc](https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html) 
 * and its [source code](https://github.com/terraform-providers/terraform-provider-aws/tree/master/examples/eks-getting-started). 
 * Read the [AWS docs on EKS to get connected to the k8s dashboard](https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html). 
  
 * | Branch | Build status                                                                                                                                                      | 
 * | ------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------- | 
 * | master | [![build Status](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks.svg?branch=master)](https://travis-ci.org/terraform-aws-modules/terraform-aws-eks) | 
  
 * ## Assumptions 
  
 ** You want to create an EKS cluster and an autoscaling group of workers for the cluster. 
 ** You want these resources to exist within security groups that allow communication and coordination. These can be user provided or created within the module. 
 ** You've created a Virtual Private Cloud (VPC) and subnets where you intend to put the EKS resources. 
  
 * ## Usage example 
  
 * A full example leveraging other community modules is contained in the [examples/eks_test_fixture directory](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture). Here's the gist of using it via the Terraform registry: 
  
 * ```hcl 
 * module ""eks"" { 
 *   source                = ""terraform-aws-modules/eks/aws"" 
 *   version               = ""0.1.0"" 
 *   cluster_name          = ""test-eks-cluster"" 
 *   subnets               = [""subnet-abcde012"", ""subnet-bcde012a""] 
 *   tags                  = ""${map(""Environment"", ""test"")}"" 
 *   vpc_id                = ""vpc-abcde012"" 
 * } 
 * ``` 
  
 * ## Dependencies 
  
 * The `configure_kubectl_session` variable requires that both [`kubectl`](https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl) 
 (>=1.10) and [`heptio-authenticator-aws`](https://github.com/heptio/authenticator#4-set-up-kubectl-to-use-heptio-authenticator-for-aws-tokens) 
 are installed and on your shell's PATH. 
  
 * ## Testing 
  
 * This module has been packaged with [awspec](https://github.com/k1LoW/awspec) tests through [kitchen](https://kitchen.ci/) and [kitchen-terraform](https://newcontext-oss.github.io/kitchen-terraform/). To run them: 
  
 * 1. Install [rvm](https://rvm.io/rvm/install) and the ruby version specified in the [Gemfile](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/Gemfile). 
 * 2. Install bundler and the gems from our Gemfile: 
 * 
 *     ```bash 
 *     gem install bundler && bundle install 
 *     ``` 
 * 
 * 3. Ensure your AWS environment is configured (i.e. credentials and region) for test. 
 * 4. Test using `bundle exec kitchen test` from the root of the repo. 
  
 * For now, connectivity to the kubernetes cluster is not tested but will be in the 
 * future. If `configure_kubectl_session` is set `true`, once the test fixture has 
 * converged, you can query the test cluster from that terminal session with 
 * `kubectl get nodes --watch --kubeconfig kubeconfig`. 
  
 * ## Doc generation 
  
 * Documentation should be modified within `main.tf` and generated using [terraform-docs](https://github.com/segmentio/terraform-docs). 
 * Generate them like so: 
  
 * ```bash 
 * go get github.com/segmentio/terraform-docs 
 * terraform-docs md ./ | cat -s | ghead -n -1 > README.md 
 * ``` 
  
 * ## Contributing 
  
 * Report issues/questions/feature requests on in the [issues](https://github.com/terraform-aws-modules/terraform-aws-eks/issues/new) section. 
  
 * Full contributing [guidelines are covered here](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/master/CONTRIBUTING.md). 
  
 * ## IAM Permissions 
  
 * Testing and using this repo requires a minimum set of IAM permissions. Test permissions 
 * are listed in the [eks_test_fixture README](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/examples/eks_test_fixture/README.md). 
  
 * ## Change log 
  
 * The [changelog](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/CHANGELOG.md) captures all important release notes. 
  
 * ## Authors 
  
 * Created and maintained by [Brandon O'Connor](https://github.com/brandoconnor) - brandon@atscale.run. 
 * Many thanks to [the contributors listed here](https://github.com/terraform-aws-modules/terraform-aws-eks/graphs/contributors)! 
  
 * ## License 
  
 * MIT Licensed. See [LICENSE](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master/LICENSE) for full details. 
 */ ","provider ""null"" {}
",provider,"provider ""null"" {}
",provider,1,,9d3b5caff4681f9d33a08ebf73ff55d42e3236cb,abe72915f3cf5e0bc8090f1d14bd01627a19fcca,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/9d3b5caff4681f9d33a08ebf73ff55d42e3236cb/main.tf#L1,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/abe72915f3cf5e0bc8090f1d14bd01627a19fcca/main.tf,2018-06-11 16:15:11-07:00,2018-06-25 01:24:58-07:00,2,1
https://github.com/terraform-aws-modules/terraform-aws-eks,474,variables.tf,variables.tf,0,todo,"# TODO - at next breaking change, make fd00:ec2::123/128 the default","# TODO - at next breaking change, make fd00:ec2::123/128 the default","variable ""node_security_group_ntp_ipv6_cidr_block"" {
  description = ""IPv4 CIDR block to allow NTP egress. Default is public IP space, but [Amazon Time Sync Service](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/set-time.html) can be used as well with `[\""fd00:ec2::123/128\""]`""
  type        = list(string)
  default     = [""::/0""]
}
",variable,the block associated got renamed or deleted,,332,,4543ab454bea80b64381b88a631d955a7cfae247,b2e97ca3dcbcd76063f1c932aa5199b4f49a2aa1,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/4543ab454bea80b64381b88a631d955a7cfae247/variables.tf#L332,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/b2e97ca3dcbcd76063f1c932aa5199b4f49a2aa1/variables.tf,2022-06-28 12:16:20-04:00,2022-12-05 16:26:23-05:00,5,1
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,973,examples/data-solutions/data-playground/main.tf,blueprints/data-solutions/data-playground/main.tf,1,# todo,# TODO: Add shared VPC support,"############################################################################### 
 #                         Vertex AI Notebook                                   # 
 ############################################################################### 
 # TODO: Add encryption_key to Vertex AI notebooks as well 
 # TODO: Add shared VPC support","resource ""google_notebooks_instance"" ""playground"" {
  name         = ""data-play-notebook""
  location     = format(""%s-%s"", var.region, ""b"")
  machine_type = ""e2-medium""
  project      = module.project.project_id

  container_image {
    repository = ""gcr.io/deeplearning-platform-release/base-cpu""
    tag        = ""latest""
  }

  install_gpu_driver = true
  boot_disk_type     = ""PD_SSD""
  boot_disk_size_gb  = 110

  no_public_ip    = false
  no_proxy_access = false

  network = module.vpc.network.id
  subnet  = module.vpc.subnets[format(""%s/%s"", var.region, var.vpc_config.subnet_name)].id
}
",resource,"resource ""google_notebooks_instance"" ""playground"" {
  name         = ""${var.prefix}-notebook""
  location     = format(""%s-%s"", var.region, ""b"")
  machine_type = ""e2-medium""
  project      = module.project.project_id

  container_image {
    repository = ""gcr.io/deeplearning-platform-release/base-cpu""
    tag        = ""latest""
  }

  install_gpu_driver = true
  boot_disk_type     = ""PD_SSD""
  boot_disk_size_gb  = 110
  disk_encryption    = try(local.service_encryption_keys.compute != null, false) ? ""CMEK"" : null
  kms_key            = try(local.service_encryption_keys.compute, null)

  no_public_ip    = true
  no_proxy_access = false

  network = local.vpc
  subnet  = local.subnet

  service_account = module.service-account-notebook.email

  #TODO Uncomment once terraform-provider-google/issues/9273 is fixed
  # tags = [""ssh""]
  depends_on = [
    google_project_iam_member.shared_vpc,
  ]
}
",resource,92,,54d805dac04ba4d43b189d22b0751e45ca69f366,07a7be29e3bd898dfdfb8ce39dadf7a663fa2fcf,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/54d805dac04ba4d43b189d22b0751e45ca69f366/examples/data-solutions/data-playground/main.tf#L92,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/07a7be29e3bd898dfdfb8ce39dadf7a663fa2fcf/blueprints/data-solutions/data-playground/main.tf,2022-07-10 09:27:18+02:00,2023-01-19 00:33:31+01:00,10,1
https://github.com/oracle-terraform-modules/terraform-oci-oke,205,modules/network/subnets.tf,modules/network/subnets.tf,0,todo,"# TODO reflect default security_list_id instead of ignore
",# TODO reflect default security_list_id instead of ignore,"resource ""oci_core_subnet"" ""oke"" {
  for_each = { for k, v in local.subnet_info : k => v
    if(lookup(v, ""create"", true) == true || lookup(lookup(var.subnets, k, {}), ""create"", ""auto"") == ""always"")
    && contains(keys(local.subnet_cidrs), k)
    && lookup(var.subnets, ""create"", ""auto"") != ""never""
    && lookup(var.subnets, ""id"", """") == """"
  }

  compartment_id             = var.compartment_id
  vcn_id                     = var.vcn_id
  cidr_block                 = lookup(local.subnet_cidrs, each.key)
  display_name               = ""${each.key}-${var.state_id}""
  dns_label                  = var.assign_dns ? lookup(var.subnets, ""id"", substr(each.key, 0, 2)) : null
  prohibit_public_ip_on_vnic = lookup(each.value, ""public"", false) == false
  route_table_id             = lookup(each.value, ""public"", false) == false ? var.nat_route_table_id : var.ig_route_table_id
  security_list_ids          = compact([lookup(lookup(oci_core_security_list.oke, each.key, {}), ""id"", null)])
  defined_tags               = local.defined_tags
  freeform_tags              = local.freeform_tags

  lifecycle {
    # TODO reflect default security_list_id instead of ignore
    ignore_changes = [security_list_ids, freeform_tags, defined_tags, dns_label]
  }
}
",resource,"resource ""oci_core_subnet"" ""oke"" {
  for_each = local.subnets_to_create

  compartment_id             = var.compartment_id
  vcn_id                     = var.vcn_id
  cidr_block                 = lookup(local.subnet_cidrs_all, each.key)
  display_name               = ""${each.key}-${var.state_id}""
  dns_label                  = var.assign_dns ? lookup(var.subnets, ""id"", substr(each.key, 0, 2)) : null
  prohibit_public_ip_on_vnic = !tobool(lookup(each.value, ""is_public"", false))
  route_table_id             = !tobool(lookup(each.value, ""is_public"", false)) ? var.nat_route_table_id : var.ig_route_table_id
  security_list_ids          = compact([lookup(lookup(oci_core_security_list.oke, each.key, {}), ""id"", null)])
  defined_tags               = local.defined_tags
  freeform_tags              = local.freeform_tags

  lifecycle {
    # TODO reflect default security_list_id instead of ignore
    ignore_changes = [security_list_ids, freeform_tags, defined_tags, dns_label, display_name, cidr_block]
  }
}
",resource,57,,6c867cd8e9cbf559742f56658989bcded0d1fd89,f49f1da39d79cf260d80dcb10ee8e399828e6e1c,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/modules/network/subnets.tf#L57,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/f49f1da39d79cf260d80dcb10ee8e399828e6e1c/modules/network/subnets.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,12,1
https://github.com/aws-observability/terraform-aws-observability-accelerator,48,workloads/infra/rules.tf,modules/workloads/infra/rules.tf,1,crap,#       - alert: NodeTextFileCollectorScrapeError,,,,the block associated got renamed or deleted,,623,,de50a0dfb5dd8a18a6b206ead81a49679b85a857,60ab8b13bf236ceb04960b343b36383241d813db,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/de50a0dfb5dd8a18a6b206ead81a49679b85a857/workloads/infra/rules.tf#L623,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/60ab8b13bf236ceb04960b343b36383241d813db/modules/workloads/infra/rules.tf,2022-08-26 17:30:03+02:00,2022-08-26 17:30:03+02:00,2,1
https://github.com/Worklytics/psoxy,390,infra/modules/hashicorp-vault-secrets/main.tf,infra/modules/hashicorp-vault-secrets/main.tf,0,implementation,# q: good idea? breaks notion of AWS SSM parameters secrets being an implementation of a generic,"# for use in explicit IAM policy grants? 
 # q: good idea? breaks notion of AWS SSM parameters secrets being an implementation of a generic 
 # secrets-store interface 
 # q: is to ALSO pass in some notion of access? except very different per implementation","output ""secret_ids"" {
  value = { for k, v in var.secrets : k => aws_ssm_parameter.secret[k].path }
}
",output,,,16,0.0,e8f084f7a3cb3950efeaf655a06bcbc8dcbaa137,31311227708efa985f4963b3b9c47bf2547ef506,https://github.com/Worklytics/psoxy/blob/e8f084f7a3cb3950efeaf655a06bcbc8dcbaa137/infra/modules/hashicorp-vault-secrets/main.tf#L16,https://github.com/Worklytics/psoxy/blob/31311227708efa985f4963b3b9c47bf2547ef506/infra/modules/hashicorp-vault-secrets/main.tf#L0,2022-10-04 10:51:26-07:00,2022-10-17 12:58:48-07:00,2,2
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,982,fast/stages/03-gke-multitenant/module/gke-hub.tf,fast/stages/03-gke-multitenant/module/gke-hub.tf,0,# todo,# TODO: add roles/multiclusterservicediscovery.serviceAgent and,"/** 
 * Copyright 2022 Google LLC 
 * 
 * Licensed under the Apache License, Version 2.0 (the ""License""); 
 * you may not use this file except in compliance with the License. 
 * You may obtain a copy of the License at 
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, software 
 * distributed under the License is distributed on an ""AS IS"" BASIS, 
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 * See the License for the specific language governing permissions and 
 * limitations under the License. 
 */  
 # TODO: service account 
 # https://cloud.google.com/kubernetes-engine/docs/how-to/msc-setup-with-shared-vpc-networks#shared-service-project-iam 
 # TODO: add roles/multiclusterservicediscovery.serviceAgent and 
 #       roles/compute.networkViewer to IAM condition for GKE stage SA ","locals {
  fleet_enabled = (
    var.fleet_features != null || var.fleet_workload_identity
  )
  # TODO: add condition
  fleet_mcs_enabled = false
}
",locals,"locals {
  fleet_enabled = (
    var.fleet_features != null || var.fleet_workload_identity
  )
  fleet_mcs_enabled = local.fleet_enabled && lookup(
    coalesce(var.fleet_features, {}), ""multiclusterservicediscovery"", false
  ) == true
}
",locals,19,,133fd078232ef202140450d921bb8018b60e700f,c24e66138339bb5c599e52cd88236267acac4611,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/133fd078232ef202140450d921bb8018b60e700f/fast/stages/03-gke-multitenant/module/gke-hub.tf#L19,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c24e66138339bb5c599e52cd88236267acac4611/fast/stages/03-gke-multitenant/module/gke-hub.tf,2022-07-29 11:31:34+02:00,2022-07-29 14:01:35+02:00,2,1
https://github.com/Worklytics/psoxy,483,infra/modules/aws-psoxy-lambda/variables.tf,infra/modules/aws-psoxy-lambda/variables.tf,0,# todo,# TODO: remove after 0.4.x,# TODO: remove after 0.4.x,"variable ""aws_assume_role_arn"" {
  type        = string
  description = ""IGNORED; unused role arn""
  default     = null
}
",variable,"variable ""aws_assume_role_arn"" {
  type        = string
  description = ""IGNORED; unused role arn""
  default     = null
}
",variable,20,33.0,d1d2ef9b4a358d71a7d8e228a8291a4569a6f3cf,b50ff85175c9c27d056239d60e41575bdd715b02,https://github.com/Worklytics/psoxy/blob/d1d2ef9b4a358d71a7d8e228a8291a4569a6f3cf/infra/modules/aws-psoxy-lambda/variables.tf#L20,https://github.com/Worklytics/psoxy/blob/b50ff85175c9c27d056239d60e41575bdd715b02/infra/modules/aws-psoxy-lambda/variables.tf#L33,2022-10-27 14:12:45-07:00,2024-03-06 21:11:28+00:00,8,0
https://github.com/aws-observability/terraform-aws-observability-accelerator,50,workloads/infra/rules.tf,modules/workloads/infra/rules.tf,1,crap,#           description: Node Exporter text file collector failed to scrape.,,,,the block associated got renamed or deleted,,628,,de50a0dfb5dd8a18a6b206ead81a49679b85a857,60ab8b13bf236ceb04960b343b36383241d813db,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/de50a0dfb5dd8a18a6b206ead81a49679b85a857/workloads/infra/rules.tf#L628,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/60ab8b13bf236ceb04960b343b36383241d813db/modules/workloads/infra/rules.tf,2022-08-26 17:30:03+02:00,2022-08-26 17:30:03+02:00,2,1
https://github.com/awslabs/data-on-eks,33,ai-ml/trainium-inferentia/addons.tf,ai-ml/trainium-inferentia/addons.tf,0,implement,# Commented for visibility to implement this feature in the future,"  # Commented for visibility to implement this feature in the future
  #  placement {
  #   tenancy = ""default""
  #   availability_zone = ""${local.region}d""
  #   group_name        = local.karpenter_trn1_32xl_lt_name
  # }","resource ""aws_launch_template"" ""trn1_lt"" {
  name        = local.karpenter_trn1_32xl_lt_name
  description = ""Karpenter Trn1.32xlarge Launch Template""

  user_data = data.cloudinit_config.trn1_lt.rendered

  ebs_optimized = true

  image_id = data.aws_ami.eks_gpu.id

  iam_instance_profile {
    name = module.eks_blueprints_addons.karpenter.node_instance_profile_name
  }

  # Commented for visibility to implement this feature in the future
  #  placement {
  #   tenancy = ""default""
  #   availability_zone = ""${local.region}d""
  #   group_name        = local.karpenter_trn1_32xl_lt_name
  # }

  metadata_options {
    http_endpoint               = ""enabled""
    http_tokens                 = ""required""
    http_put_response_hop_limit = 2
  }

  block_device_mappings {
    device_name = ""/dev/xvda""
    ebs {
      volume_size           = 100
      delete_on_termination = true
      volume_type           = ""gp3""
    }
  }

  monitoring {
    enabled = true
  }

  tag_specifications {
    resource_type = ""instance""

    tags = merge(local.tags, {
      ""karpenter.sh/discovery"" = local.name
    })
  }

  # First network interface with device_index=0 and network_card_index=0
  network_interfaces {
    device_index                = 0
    network_card_index          = 0
    associate_public_ip_address = false
    interface_type              = ""efa""
    delete_on_termination       = true
    security_groups             = [module.eks.node_security_group_id]
    description                 = ""Karpenter EFA config for Trainium""
  }

  # Additional network interfaces with device_index=1 and network_card_index ranging from 1 to 7
  dynamic ""network_interfaces"" {
    for_each = range(1, 8) # Create 7 additional network interfaces
    content {
      device_index                = 1
      network_card_index          = network_interfaces.value
      associate_public_ip_address = false
      interface_type              = ""efa""
      delete_on_termination       = true
      security_groups             = [module.eks.node_security_group_id]
      description                 = ""Karpenter EFA config for Trainium""
    }
  }
}
",resource,the block associated got renamed or deleted,,443,,352456833a6bee906dd49d28ad050c8f048c767b,e0abaadee27a73dc6ce0e4b5200fb1210e211809,https://github.com/awslabs/data-on-eks/blob/352456833a6bee906dd49d28ad050c8f048c767b/ai-ml/trainium-inferentia/addons.tf#L443,https://github.com/awslabs/data-on-eks/blob/e0abaadee27a73dc6ce0e4b5200fb1210e211809/ai-ml/trainium-inferentia/addons.tf,2023-12-08 15:47:41-05:00,2024-01-31 16:45:26-08:00,2,1
https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd,100,modules/secure-cd/main.tf,modules/secure-cd/main.tf,0,# todo,"# TODO: we can't be sure that they will be using the default GCE SA, so how do we automate this permissioning?","# IAM bindings for GKE projects to access container images from GAR
# TODO: we can't be sure that they will be using the default GCE SA, so how do we automate this permissioning?","data ""google_project"" ""gke_projects"" {
  for_each   = var.deploy_branch_clusters
  project_id = each.value.project_id
}
",data,"data ""google_project"" ""gke_projects"" {
  for_each   = var.deploy_branch_clusters
  project_id = each.value.project_id
}
",data,82,,52077b54c7e19811bab2673ec1deb908487243dc,ad09ca8f5508e563da78c68d75cb81849edf38af,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/52077b54c7e19811bab2673ec1deb908487243dc/modules/secure-cd/main.tf#L82,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/ad09ca8f5508e563da78c68d75cb81849edf38af/modules/secure-cd/main.tf,2021-11-19 14:29:33-06:00,2021-11-20 00:59:03-06:00,4,1
https://github.com/Worklytics/psoxy,1061,infra/modular-examples/aws/main.tf,infra/modular-examples/aws/main.tf,0,# todo,# TODO: remove in v0.5,# TODO: remove in v0.5,"moved {
  from   = module.psoxy-msft-connector
  to     = module.psoxy_msft_connector
}
",moved,"moved {
  from = module.psoxy-msft-connector
  to   = module.psoxy_msft_connector
}
",moved,342,366.0,007f5c6e60a66814bdfc727541c52b729776b5d1,8f6786b90a7f0fcb6120cc8f822fd07cab49697f,https://github.com/Worklytics/psoxy/blob/007f5c6e60a66814bdfc727541c52b729776b5d1/infra/modular-examples/aws/main.tf#L342,https://github.com/Worklytics/psoxy/blob/8f6786b90a7f0fcb6120cc8f822fd07cab49697f/infra/modular-examples/aws/main.tf#L366,2023-04-28 09:36:10-07:00,2024-04-23 19:32:11-07:00,74,0
https://github.com/Worklytics/psoxy,231,infra/modules/azuread-grant-all-users/main.tf,infra/modules/azuread-grant-all-users/main.tf,0,inconsistency,"# however, even using it JUST for delegated permissions seems to create an inconsistency when user","# TODO: if grant can be made fully through API, do it here; until then, TODO file is best option  
 # NOTE: using `azuread_service_principal_delegated_permission_grant` seems to NOT work for this, 
 # presumably it ONLY supports oauth scopes (delegated permissions) not application roles 
 # (application permissions) 
 # however, even using it JUST for delegated permissions seems to create an inconsistency when user 
 # manually grants the application permissions (resource seems to really create a 'delegated_permission_grant' 
 # entity in Azure, which is overwritten by the user and then missing on subsequent terraform runs) ","resource ""local_file"" ""todo"" {
  filename = ""TODO ${var.application_name}.md""

  content = <<EOT
# Authorize ${var.application_name}

Visit the following page in the Azure AD console and grant the required application permissions:

https://portal.azure.com/#blade/Microsoft_AAD_RegisteredApps/ApplicationMenuBlade/CallAnAPI/appId/${var.application_id}/isMSAApp/

If you are not a sufficiently privileged Azure AD Administrator (likely Application or Global
Administrator), you made need assistance from an appropriately privileged member of your IT team.

The required grants are:
```
${join(""\n"", concat(var.app_roles, var.oauth2_permission_scopes))}
```

EOT
}
",resource,"locals {
  instance_id  = coalesce(var.psoxy_instance_id, var.application_name)
  todo_content = <<EOT
# Authorize ${var.application_name}

Visit the following page in the Azure AD console and grant the required application permissions:

https://portal.azure.com/#blade/Microsoft_AAD_RegisteredApps/ApplicationMenuBlade/CallAnAPI/appId/${var.application_id}/isMSAApp/

If you are not a sufficiently privileged Azure AD Administrator (likely Application or Global
Administrator), you may need assistance from an appropriately privileged member of your IT team.

The required grants are:
```
${join(""\n"", concat(var.app_roles, var.oauth2_permission_scopes))}
```

EOT
}
",locals,18,10.0,e60f54ba57c150249298c3fcf1af52e6a2ea06ee,ce24a85e38bd513c005f315db934b61c950962a6,https://github.com/Worklytics/psoxy/blob/e60f54ba57c150249298c3fcf1af52e6a2ea06ee/infra/modules/azuread-grant-all-users/main.tf#L18,https://github.com/Worklytics/psoxy/blob/ce24a85e38bd513c005f315db934b61c950962a6/infra/modules/azuread-grant-all-users/main.tf#L10,2022-01-27 21:03:35-08:00,2024-04-17 08:37:10-07:00,10,0
https://github.com/Worklytics/psoxy,87,infra/modules/aws-psoxy-instance/main.tf,infra/modules/aws-psoxy-instance/main.tf,0,#todo,#TODO: match on subpath equivalent to var.function_name,#TODO: match on subpath equivalent to var.function_name,"resource ""aws_apigatewayv2_integration"" ""map"" {
  api_id                    = var.api_gateway.id
  integration_type          = ""HTTP_PROXY""
  connection_type           = ""INTERNET""

  integration_method        = ""GET""
  #TODO: match on subpath equivalent to var.function_name
  integration_uri           = aws_lambda_function.psoxy-instance.invoke_arn
}
",resource,"resource ""aws_apigatewayv2_integration"" ""map"" {
  api_id                    = var.api_gateway.id
  integration_type          = ""AWS_PROXY""
  connection_type           = ""INTERNET""

  #TODO: match on subpath equivalent to var.function_name ?

  integration_method        = ""POST""
  integration_uri           = aws_lambda_function.psoxy-instance.invoke_arn
  request_parameters        = {}
  request_templates         = {}
}
",resource,41,,2f81e3da70338154c1ff8ee7d1220a5b5602fcab,7444d0de8dc052089b9c9dc91394cf5172660cdd,https://github.com/Worklytics/psoxy/blob/2f81e3da70338154c1ff8ee7d1220a5b5602fcab/infra/modules/aws-psoxy-instance/main.tf#L41,https://github.com/Worklytics/psoxy/blob/7444d0de8dc052089b9c9dc91394cf5172660cdd/infra/modules/aws-psoxy-instance/main.tf,2022-01-05 16:14:33-08:00,2022-01-06 11:41:26-08:00,4,1
https://github.com/pingcap/tidb-operator,7,deploy/alicloud/main.tf,deploy/aliyun/main.tf,1,workaround,"// Workaround: ACK does not support customize node RAM role, access key is the only way get local volume provisioner working","// Workaround: ACK does not support customize node RAM role, access key is the only way get local volume provisioner working 
 // TODO: use STS when upstream get this fixed","resource ""local_file"" ""local-volume-provisioner"" {
  depends_on = [""data.template_file.local-volume-provisioner""]
  filename   = ""${local.local_volume_provisioner_path}""
  content    = ""${data.template_file.local-volume-provisioner.rendered}""
}
",resource,the block associated got renamed or deleted,,85,,eebd686956c0b2adf67a10e1a376659c95c571a3,042b1a97fbbdf342297002990564828e6644a3f0,https://github.com/pingcap/tidb-operator/blob/eebd686956c0b2adf67a10e1a376659c95c571a3/deploy/alicloud/main.tf#L85,https://github.com/pingcap/tidb-operator/blob/042b1a97fbbdf342297002990564828e6644a3f0/deploy/aliyun/main.tf,2019-05-06 19:59:43+08:00,2019-07-23 19:44:58+08:00,4,1
https://github.com/camptocamp/devops-stack,94,examples/eks-aws/main.tf,examples/eks/main.tf,1,todo,# TODO create an output for this password,"/* Available only in provider hashicorp/aws >= v4.0.0 
 resource ""random_string"" ""admin_password"" { 
 length  = 25 
 special = false 
 } # TODO create an output for this password 
  
 resource ""aws_cognito_user"" ""admin"" { 
 user_pool_id = aws_cognito_user_pool.admin.id 
 username = admin 
 password = random_string.admin_password.result 
  
 message_action = SUPRESS # Do not send welcome message since password is hardcoded and email is non-existant 
  
 attributes = { 
 email = ""admin@example.org"" 
 email_verified = true 
 terraform = true 
 } 
 } 
  
 resource ""aws_cognito_user_in_group"" ""add_admin_argocd_admin"" { 
 user_pool_id = aws_cognito_user_pool.admin.id 
 group_name   = aws_cognito_user_group.argocd_admin_group.name 
 username     = aws_cognito_user.admin.username 
 } 
 */ ","module ""eks"" {
  source = ""git::https://github.com/camptocamp/devops-stack.git//modules/eks/aws?ref=v1""

  cluster_name = ""gh-v1-cluster""
  base_domain  = ""is-sandbox.camptocamp.com""
  # cluster_version = ""1.22""

  vpc_id         = module.vpc.vpc_id
  vpc_cidr_block = module.vpc.vpc_cidr_block

  private_subnet_ids = module.vpc.private_subnets
  public_subnet_ids  = module.vpc.public_subnets

  cluster_endpoint_public_access_cidrs = [""0.0.0.0/0""]

  node_groups = {
    ""${module.eks.cluster_name}-main"" = {
      instance_type     = ""m5a.large""
      min_size          = 2
      max_size          = 3
      desired_size      = 2
      target_group_arns = module.eks.nlb_target_groups
    },
  }

  create_public_nlb = true
}
",module,"module ""eks"" {
  source = ""git::https://github.com/camptocamp/devops-stack-module-cluster-eks?ref=v2.0.2""

  cluster_name       = local.cluster_name
  kubernetes_version = local.kubernetes_version
  base_domain        = local.base_domain

  vpc_id             = module.vpc.vpc_id
  private_subnet_ids = module.vpc.private_subnets
  public_subnet_ids  = module.vpc.public_subnets

  cluster_endpoint_public_access_cidrs = [""0.0.0.0/0""]

  node_groups = {
    ""${module.eks.cluster_name}-main"" = {
      instance_type     = ""m5a.large""
      min_size          = 2
      max_size          = 3
      desired_size      = 2
      target_group_arns = module.eks.nlb_target_groups
    },
  }

  create_public_nlb = true
}
",module,45,,23a76321726eca45b1852f9cbb9a5a46dd17c13e,9f09347de36de8f813051eae5c981dc8cae5c393,https://github.com/camptocamp/devops-stack/blob/23a76321726eca45b1852f9cbb9a5a46dd17c13e/examples/eks-aws/main.tf#L45,https://github.com/camptocamp/devops-stack/blob/9f09347de36de8f813051eae5c981dc8cae5c393/examples/eks/main.tf,2023-04-03 16:40:29+02:00,2023-08-18 14:48:32+02:00,4,1
https://github.com/Azure/sap-automation,8,deploy/terraform/terraform-units/modules/sap_deployer/infrastructure.tf,deploy/terraform/terraform-units/modules/sap_deployer/infrastructure.tf,0,// todo,// TODO: Add management lock when this issue is addressed https://github.com/terraform-providers/terraform-provider-azurerm/issues/5473,"// TODO: Add management lock when this issue is addressed https://github.com/terraform-providers/terraform-provider-azurerm/issues/5473 
 //        Management lock should be implemented id a seperate Terraform workspace   
 // Create/Import management vnet","resource ""azurerm_virtual_network"" ""vnet_mgmt"" {
  count               = (local.enable_deployers && !local.vnet_mgmt_exists) ? 1 : 0
  name                = local.vnet_mgmt_name
  resource_group_name = local.rg_exists ? data.azurerm_resource_group.deployer[0].name : azurerm_resource_group.deployer[0].name
  location            = local.rg_exists ? data.azurerm_resource_group.deployer[0].location : azurerm_resource_group.deployer[0].location
  address_space       = [local.vnet_mgmt_addr]
}
",resource,"resource ""azurerm_virtual_network"" ""vnet_mgmt"" {
  count                                = (!local.vnet_mgmt_exists) ? 1 : 0
  name                                 = local.vnet_mgmt_name
  resource_group_name                  = local.resource_group_exists ? data.azurerm_resource_group.deployer[0].name : azurerm_resource_group.deployer[0].name
  location                             = local.resource_group_exists ? data.azurerm_resource_group.deployer[0].location : azurerm_resource_group.deployer[0].location
  address_space                        = [local.vnet_mgmt_addr]
}
",resource,93,34.0,6ff0b891114c36d3aeccb850d830b698cd1fe52a,db20ac2a47d9d00329385330cb4af6b3c726c400,https://github.com/Azure/sap-automation/blob/6ff0b891114c36d3aeccb850d830b698cd1fe52a/deploy/terraform/terraform-units/modules/sap_deployer/infrastructure.tf#L93,https://github.com/Azure/sap-automation/blob/db20ac2a47d9d00329385330cb4af6b3c726c400/deploy/terraform/terraform-units/modules/sap_deployer/infrastructure.tf#L34,2021-11-17 19:29:07+02:00,2024-03-11 23:15:11+05:30,24,0
https://github.com/aws-observability/terraform-aws-observability-accelerator,52,workloads/infra/rules.tf,modules/workloads/infra/rules.tf,1,fix,#           description: RAID array '{{ $labels.device }}' on {{ $labels.instance }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.,,,,the block associated got renamed or deleted,,658,,de50a0dfb5dd8a18a6b206ead81a49679b85a857,60ab8b13bf236ceb04960b343b36383241d813db,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/de50a0dfb5dd8a18a6b206ead81a49679b85a857/workloads/infra/rules.tf#L658,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/60ab8b13bf236ceb04960b343b36383241d813db/modules/workloads/infra/rules.tf,2022-08-26 17:30:03+02:00,2022-08-26 17:30:03+02:00,2,1
https://github.com/SUSE/ha-sap-terraform-deployments,393,azure/network.tf,azure/network.tf,0,todo,// Todo this security group need to be redifined by cluster roles instead of 1 generic,"// Todo this security group need to be redifined by cluster roles instead of 1 generic 
 // TODO make the monitoring rule more stricter for some instance later on","resource ""azurerm_network_security_group"" ""mysecgroup"" {
  name                = ""mysecgroup""
  location            = var.az_region
  resource_group_name = azurerm_resource_group.myrg.name
// Todo this security group need to be redifined by cluster roles instead of 1 generic
// TODO make the monitoring rule more stricter for some instance later on
  security_rule {
    name                       = ""OUTALL""
    priority                   = 100
    direction                  = ""Outbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""*""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""LOCAL""
    priority                   = 101
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""*""
    source_address_prefix      = ""10.74.0.0/16""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""SSH""
    priority                   = 1001
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""22""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HTTP""
    priority                   = 1002
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""80""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HTTPS""
    priority                   = 1003
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""443""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HAWK""
    priority                   = 1004
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""7630""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  // monitoring rules
    security_rule {
    name                       = ""nodeExporter""
    priority                   = 1005
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9100""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  security_rule {
    name                       = ""hanadbExporter""
    priority                   = 1006
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""8001""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  security_rule {
    name                       = ""hawkExporter""
    priority                   = 1007
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9001""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""prometheus""
    priority                   = 1008
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9090""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }


  tags = {
    workspace = terraform.workspace
  }
}
",resource,"resource ""azurerm_network_security_group"" ""mysecgroup"" {
  name                = ""mysecgroup""
  location            = var.az_region
  resource_group_name = azurerm_resource_group.myrg.name
  security_rule {
    name                       = ""OUTALL""
    priority                   = 100
    direction                  = ""Outbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""*""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""LOCAL""
    priority                   = 101
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""*""
    source_address_prefix      = ""10.74.0.0/16""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""SSH""
    priority                   = 1001
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""22""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HTTP""
    priority                   = 1002
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""80""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HTTPS""
    priority                   = 1003
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""443""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HAWK""
    priority                   = 1004
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""7630""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  // monitoring rules
  security_rule {
    name                       = ""nodeExporter""
    priority                   = 1005
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9100""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  security_rule {
    name                       = ""hanadbExporter""
    priority                   = 1006
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""8001""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  security_rule {
    name                       = ""hawkExporter""
    priority                   = 1007
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9001""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""prometheus""
    priority                   = 1008
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9090""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }


  tags = {
    workspace = terraform.workspace
  }
}
",resource,304,,517f39e8d665afe6d504a325cda67274995c8e59,9adcf152486838f9f5b600ead5e4ef373918a786,https://github.com/SUSE/ha-sap-terraform-deployments/blob/517f39e8d665afe6d504a325cda67274995c8e59/azure/network.tf#L304,https://github.com/SUSE/ha-sap-terraform-deployments/blob/9adcf152486838f9f5b600ead5e4ef373918a786/azure/network.tf,2019-09-05 00:02:24+02:00,2019-09-05 18:08:13+02:00,3,1
https://github.com/alphagov/govuk-aws,1090,terraform/projects/infra-database-backups-bucket/main.tf,terraform/projects/infra-database-backups-bucket/main.tf,0,# todo,"# TODO: create these only in environments where they're needed, instead of","# Integration-specific lifecycle rules. These rules are created in all 
 # environments but are only enabled in Integration. 
 # 
 # TODO: create these only in environments where they're needed, instead of 
 # creating them everywhere and leaving them disabled. 
 # 
 # TODO: these are all set to the same var.expiration_time so just replace 
 # them with one rule. Similarly for the prod ones above. ","resource ""aws_s3_bucket"" ""database_backups"" {
  bucket = ""govuk-${var.aws_environment}-database-backups""
  region = ""${var.aws_region}""

  tags {
    Name            = ""govuk-${var.aws_environment}-database-backups""
    aws_environment = ""${var.aws_environment}""
  }

  logging {
    target_bucket = ""${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}""
    target_prefix = ""s3/govuk-${var.aws_environment}-database-backups/""
  }

  # Production/Staging lifecycle rules.
  #
  # TODO: make staging use the same rules as integration. We don't need to
  # retain backups of staging for very long.

  lifecycle_rule {
    id      = ""mysql_lifecycle_rule""
    prefix  = ""mysql/""
    enabled = ""${var.aws_environment != ""integration""}""

    transition {
      storage_class = ""STANDARD_IA""
      days          = ""${var.standard_s3_storage_time}""
    }

    transition {
      storage_class = ""GLACIER""
      days          = ""${var.glacier_storage_time}""
    }

    expiration {
      days = 120
    }
  }
  lifecycle_rule {
    id      = ""postgres_lifecycle_rule""
    prefix  = ""postgres/""
    enabled = ""${var.aws_environment != ""integration""}""

    transition {
      storage_class = ""STANDARD_IA""
      days          = ""${var.standard_s3_storage_time}""
    }

    transition {
      storage_class = ""GLACIER""
      days          = ""${var.glacier_storage_time}""
    }

    expiration {
      days = 120
    }
  }
  lifecycle_rule {
    id      = ""mongo_daily_lifecycle_rule""
    prefix  = ""mongodb/daily""
    enabled = ""${var.aws_environment != ""integration""}""

    transition {
      storage_class = ""STANDARD_IA""
      days          = ""${var.standard_s3_storage_time}""
    }

    transition {
      storage_class = ""GLACIER""
      days          = ""${var.glacier_storage_time}""
    }

    expiration {
      days = 120
    }
  }
  lifecycle_rule {
    id      = ""mongo_regular_lifecycle_rule""
    prefix  = ""mongodb/regular""
    enabled = true

    expiration {
      days = ""${var.expiration_time_whisper_mongo}""
    }
  }
  lifecycle_rule {
    id      = ""whisper_lifecycle_rule""
    prefix  = ""whisper/""
    enabled = true

    expiration {
      days = ""${var.expiration_time_whisper_mongo}""
    }
  }

  # Integration-specific lifecycle rules. These rules are created in all
  # environments but are only enabled in Integration.
  #
  # TODO: create these only in environments where they're needed, instead of
  # creating them everywhere and leaving them disabled.
  #
  # TODO: these are all set to the same var.expiration_time so just replace
  # them with one rule. Similarly for the prod ones above.

  lifecycle_rule {
    id      = ""mysql_lifecycle_rule_integration""
    prefix  = ""mysql/""
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }
  }
  lifecycle_rule {
    id      = ""postgres_lifecycle_rule_integration""
    prefix  = ""postgres/""
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }
  }
  lifecycle_rule {
    id      = ""mongo_daily_lifecycle_rule_integration""
    prefix  = ""mongodb/daily""
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }
  }
  lifecycle_rule {
    id      = ""whole_bucket_lifecycle_rule_integration""
    prefix  = """"
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }

    noncurrent_version_expiration {
      days = ""1""
    }
  }

  # End of Integration-specific lifecycle rules.


  # Lifecycle rule for coronavirus find support backup

  lifecycle_rule {
    id      = ""coronavirus_find_support_lifecycle_rule""
    prefix  = ""coronavirus-find-support/production.sql.gzip""
    enabled = true

    expiration {
      days = 365
    }
  }

  # Lifecycle rule for coronavirus business volunteer form backup

  lifecycle_rule {
    id      = ""coronavirus_business_volunteer_form_lifecycle_rule""
    prefix  = ""coronavirus-business-volunteer-form/production.sql.gzip""
    enabled = true

    expiration {
      days = 365
    }
  }
  versioning {
    enabled = true
  }
  replication_configuration {
    role = ""${aws_iam_role.backup_replication_role.arn}""

    rules {
      id     = ""main_replication_rule""
      prefix = """"
      status = ""${var.replication_setting}""

      destination {
        bucket        = ""${aws_s3_bucket.database_backups_replica.arn}""
        storage_class = ""STANDARD""
      }
    }
  }
}
",resource,"resource ""aws_s3_bucket"" ""database_backups"" {
  bucket = ""govuk-${var.aws_environment}-database-backups""
  region = ""${var.aws_region}""

  tags {
    Name            = ""govuk-${var.aws_environment}-database-backups""
    aws_environment = ""${var.aws_environment}""
  }

  logging {
    target_bucket = ""${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}""
    target_prefix = ""s3/govuk-${var.aws_environment}-database-backups/""
  }

  versioning {
    # It's not entirely clear if versioning is useful on this bucket  but it was previously configured this way,
    # so we've decided not to change it. Whilst it helps protect against accidental deletion, it doesn't protect
    # against malicious actors, so shouldn't be considered a security feature.
    enabled = true
  }

  lifecycle_rule {
    # Use a long retention period in production
    id      = ""long_retention_period""
    enabled = ""${var.aws_environment == ""production""}""

    # Ideally everything would go in the Standard (Infrequent Access) storage class when created.
    # But newly created objects always go into Standard, and can only move into IA after at least 30 days.
    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html
    transition {
      storage_class = ""STANDARD_IA""
      days          = 30
    }

    # Likewise, we have to wait at least another 30 days before we can move objects into Glacier storage.
    transition {
      storage_class = ""GLACIER""
      days          = 60
    }

    # Versioning is enabled on this bucket, so this rule will 'soft delete' objects.
    # In AWS lingo, this means a 'delete marker' will be set on the current version of the object.
    # More info on how expiration rules apply to versioned buckets here:
    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#intro-lifecycle-rules-actions
    expiration {
      days = 120
    }

    # This rule will 'hard delete' objects 1 day after they were 'soft deleted'.
    # In other words: old database backups will be permanently deleted 1 day after they've expired.
    noncurrent_version_expiration {
      days = ""1""
    }
  }

  lifecycle_rule {
    # Use a short retention period in integration and staging
    id      = ""short_retention_period""
    enabled = ""${var.aws_environment != ""production""}""

    expiration {
      days = ""3""
    }

    noncurrent_version_expiration {
      days = ""1""
    }
  }

  replication_configuration {
    role = ""${aws_iam_role.backup_replication_role.arn}""

    rules {
      id     = ""main_replication_rule""
      status = ""Enabled""

      destination {
        bucket        = ""${aws_s3_bucket.database_backups_replica.arn}""
        storage_class = ""STANDARD_IA""
      }
    }
  }
}
",resource,198,,d1fcb45657475a7de489503eae548845ec8e4296,78cc3f5ce73fb5a8f6d5126ff694ac87e5a0eb79,https://github.com/alphagov/govuk-aws/blob/d1fcb45657475a7de489503eae548845ec8e4296/terraform/projects/infra-database-backups-bucket/main.tf#L198,https://github.com/alphagov/govuk-aws/blob/78cc3f5ce73fb5a8f6d5126ff694ac87e5a0eb79/terraform/projects/infra-database-backups-bucket/main.tf,2020-11-24 17:53:04+00:00,2022-01-31 17:30:52+00:00,4,1
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,209,locals.tf,locals.tf,0,# todo,# ToDo: The cilium.yaml file after it has been copied to the server currently has wrong indentation causing the helm template to fail.,"# ToDo: The cilium.yaml file after it has been copied to the server currently has wrong indentation causing the helm template to fail. 
 # After manually correcting the file on the server the installation works fine and the taint is removed as planned, so fixing the indentation is up next.","locals {
  # ssh_agent_identity is not set if the private key is passed directly, but if ssh agent is used, the public key tells ssh agent which private key to use.
  # For terraforms provisioner.connection.agent_identity, we need the public key as a string.
  ssh_agent_identity = var.ssh_private_key == null ? var.ssh_public_key : null

  # If passed, a key already registered within hetzner is used. 
  # Otherwise, a new one will be created by the module.
  hcloud_ssh_key_id = var.hcloud_ssh_key_id == null ? hcloud_ssh_key.k3s[0].id : var.hcloud_ssh_key_id

  ccm_version   = var.hetzner_ccm_version != null ? var.hetzner_ccm_version : data.github_release.hetzner_ccm.release_tag
  csi_version   = var.hetzner_csi_version != null ? var.hetzner_csi_version : data.github_release.hetzner_csi.release_tag
  kured_version = var.kured_version != null ? var.kured_version : data.github_release.kured.release_tag

  common_commands_install_k3s = [
    ""set -ex"",
    # prepare the k3s config directory
    ""mkdir -p /etc/rancher/k3s"",
    # move the config file into place
    ""mv /tmp/config.yaml /etc/rancher/k3s/config.yaml"",
    # if the server has already been initialized just stop here
    ""[ -e /etc/rancher/k3s/k3s.yaml ] && exit 0"",
  ]

  apply_k3s_selinux = [""/sbin/semodule -v -i /usr/share/selinux/packages/k3s.pp""]

  install_k3s_server = concat(local.common_commands_install_k3s, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC=server sh -""
  ], local.apply_k3s_selinux)
  install_k3s_agent = concat(local.common_commands_install_k3s, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC=agent sh -""
  ], local.apply_k3s_selinux)

  control_plane_nodes = merge([
    for pool_index, nodepool_obj in var.control_plane_nodepools : {
      for node_index in range(nodepool_obj.count) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        location : nodepool_obj.location,
        labels : concat(local.default_control_plane_labels, nodepool_obj.labels),
        taints : concat(local.default_control_plane_taints, nodepool_obj.taints),
        index : node_index
      }
    }
  ]...)

  agent_nodes = merge([
    for pool_index, nodepool_obj in var.agent_nodepools : {
      for node_index in range(nodepool_obj.count) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        location : nodepool_obj.location,
        labels : concat(local.default_agent_labels, nodepool_obj.labels),
        taints : concat(local.default_agent_taints, nodepool_obj.taints),
        index : node_index
      }
    }
  ]...)

  # The main network cidr that all subnets will be created upon
  network_ipv4_cidr = ""10.0.0.0/8""

  # The first two subnets are respectively the default subnet 10.0.0.0/16 use for potientially anything and 10.1.0.0/16 used for control plane nodes.
  # the rest of the subnets are for agent nodes in each nodepools.
  network_ipv4_subnets = [for index in range(256) : cidrsubnet(local.network_ipv4_cidr, 8, index)]

  # if we are in a single cluster config, we use the default klipper lb instead of Hetzner LB
  control_plane_count    = sum([for v in var.control_plane_nodepools : v.count])
  agent_count            = sum([for v in var.agent_nodepools : v.count])
  is_single_node_cluster = (local.control_plane_count + local.agent_count) == 1

  using_klipper_lb = var.use_klipper_lb || local.is_single_node_cluster

  # disable k3s extras
  disable_extras = concat([""local-storage""], local.using_klipper_lb ? [] : [""servicelb""], var.traefik_enabled ? [] : [
    ""traefik""
  ], var.metrics_server_enabled ? [] : [""metrics-server""])

  # Default k3s node labels
  default_agent_labels         = concat([], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])
  default_control_plane_labels = concat([], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])

  allow_scheduling_on_control_plane = local.is_single_node_cluster ? true : var.allow_scheduling_on_control_plane

  # Default k3s node taints
  default_control_plane_taints = concat([], local.allow_scheduling_on_control_plane ? [] : [""node-role.kubernetes.io/control-plane:NoSchedule""], var.cni_plugin == ""cilium"" ? [""node.cilium.io/agent-not-ready:NoExecute""] : [])
  default_agent_taints         = concat([], var.cni_plugin == ""cilium"" ? [""node.cilium.io/agent-not-ready:NoExecute""] : [])


  packages_to_install = concat(var.enable_longhorn ? [""open-iscsi"", ""nfs-client""] : [], var.extra_packages_to_install)

  # The following IPs are important to be whitelisted because they communicate with Hetzner services and enable the CCM and CSI to work properly.
  # Source https://github.com/hetznercloud/csi-driver/issues/204#issuecomment-848625566
  hetzner_metadata_service_ipv4 = ""169.254.169.254/32""
  hetzner_cloud_api_ipv4        = ""213.239.246.1/32""

  # internal Pod CIDR, used for the controller and currently for calico
  cluster_cidr_ipv4 = ""10.42.0.0/16""

  whitelisted_ips = [
    local.network_ipv4_cidr,
    local.hetzner_metadata_service_ipv4,
    local.hetzner_cloud_api_ipv4,
    ""127.0.0.1/32"",
  ]

  base_firewall_rules = concat([
    # Allowing internal cluster traffic and Hetzner metadata service and cloud API IPs
    {
      direction  = ""in""
      protocol   = ""tcp""
      port       = ""any""
      source_ips = local.whitelisted_ips
    },
    {
      direction  = ""in""
      protocol   = ""udp""
      port       = ""any""
      source_ips = local.whitelisted_ips
    },
    {
      direction  = ""in""
      protocol   = ""icmp""
      source_ips = local.whitelisted_ips
    },

    # Allow all traffic to the kube api server
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""6443""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },

    # Allow all traffic to the ssh port
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""22""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },

    # Allow basic out traffic
    # ICMP to ping outside services
    {
      direction = ""out""
      protocol  = ""icmp""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    # DNS
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""53""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""out""
      protocol  = ""udp""
      port      = ""53""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    # HTTP(s)
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""80""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""443""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    #NTP
    {
      direction = ""out""
      protocol  = ""udp""
      port      = ""123""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    }
    ], !local.using_klipper_lb ? [] : [
    # Allow incoming web traffic for single node clusters, because we are using k3s servicelb there,
    # not an external load-balancer.
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""80""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""443""
      source_ips = [
        ""0.0.0.0/0""
      ]
    }
    ], var.block_icmp_ping_in ? [] : [
    {
      direction = ""in""
      protocol  = ""icmp""
      source_ips = [
        ""0.0.0.0/0""
      ]
    }
  ])

  labels = {
    ""provisioner"" = ""terraform"",
    ""engine""      = ""k3s""
    ""cluster""     = var.cluster_name
  }

  labels_control_plane_node = {
    role = ""control_plane_node""
  }
  labels_control_plane_lb = {
    role = ""control_plane_lb""
  }

  labels_agent_node = {
    role = ""agent_node""
  }

  cni_install_resources = {
    ""calico"" = [""https://projectcalico.docs.tigera.io/manifests/calico.yaml""]
    ""cilium"" = [""cilium.yaml""]
  }

  cni_install_resource_patches = {
    ""calico"" = [""calico.yaml""]
  }

  cni_k3s_settings = {
    ""flannel"" = {
      disable-network-policy = var.disable_network_policy
    }
    ""calico"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
    ""cilium"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
  }

  # ToDo: The cilium.yaml file after it has been copied to the server currently has wrong indentation causing the helm template to fail.
  # After manually correcting the file on the server the installation works fine and the taint is removed as planned, so fixing the indentation is up next.
  default_cilium_values = <<EOT
ipam:
 operator:
  clusterPoolIPv4PodCIDRList:
   - ${local.cluster_cidr_ipv4}
devices: ""eth1""
EOT
}
",locals,"locals {
  # ssh_agent_identity is not set if the private key is passed directly, but if ssh agent is used, the public key tells ssh agent which private key to use.
  # For terraforms provisioner.connection.agent_identity, we need the public key as a string.
  ssh_agent_identity = var.ssh_private_key == null ? var.ssh_public_key : null

  # If passed, a key already registered within hetzner is used. 
  # Otherwise, a new one will be created by the module.
  hcloud_ssh_key_id = var.hcloud_ssh_key_id == null ? hcloud_ssh_key.k3s[0].id : var.hcloud_ssh_key_id

  ccm_version   = var.hetzner_ccm_version != null ? var.hetzner_ccm_version : data.github_release.hetzner_ccm.release_tag
  csi_version   = var.hetzner_csi_version != null ? var.hetzner_csi_version : data.github_release.hetzner_csi.release_tag
  kured_version = var.kured_version != null ? var.kured_version : data.github_release.kured.release_tag

  common_commands_install_k3s = [
    ""set -ex"",
    # prepare the k3s config directory
    ""mkdir -p /etc/rancher/k3s"",
    # move the config file into place
    ""mv /tmp/config.yaml /etc/rancher/k3s/config.yaml"",
    # if the server has already been initialized just stop here
    ""[ -e /etc/rancher/k3s/k3s.yaml ] && exit 0"",
  ]

  apply_k3s_selinux = [""/sbin/semodule -v -i /usr/share/selinux/packages/k3s.pp""]

  install_k3s_server = concat(local.common_commands_install_k3s, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC=server sh -""
  ], local.apply_k3s_selinux)
  install_k3s_agent = concat(local.common_commands_install_k3s, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC=agent sh -""
  ], local.apply_k3s_selinux)

  control_plane_nodes = merge([
    for pool_index, nodepool_obj in var.control_plane_nodepools : {
      for node_index in range(nodepool_obj.count) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        location : nodepool_obj.location,
        labels : concat(local.default_control_plane_labels, nodepool_obj.labels),
        taints : concat(local.default_control_plane_taints, nodepool_obj.taints),
        index : node_index
      }
    }
  ]...)

  agent_nodes = merge([
    for pool_index, nodepool_obj in var.agent_nodepools : {
      for node_index in range(nodepool_obj.count) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        location : nodepool_obj.location,
        labels : concat(local.default_agent_labels, nodepool_obj.labels),
        taints : concat(local.default_agent_taints, nodepool_obj.taints),
        index : node_index
      }
    }
  ]...)

  # The main network cidr that all subnets will be created upon
  network_ipv4_cidr = ""10.0.0.0/8""

  # The first two subnets are respectively the default subnet 10.0.0.0/16 use for potientially anything and 10.1.0.0/16 used for control plane nodes.
  # the rest of the subnets are for agent nodes in each nodepools.
  network_ipv4_subnets = [for index in range(256) : cidrsubnet(local.network_ipv4_cidr, 8, index)]

  # if we are in a single cluster config, we use the default klipper lb instead of Hetzner LB
  control_plane_count    = sum([for v in var.control_plane_nodepools : v.count])
  agent_count            = sum([for v in var.agent_nodepools : v.count])
  is_single_node_cluster = (local.control_plane_count + local.agent_count) == 1

  using_klipper_lb = var.use_klipper_lb || local.is_single_node_cluster

  # disable k3s extras
  disable_extras = concat([""local-storage""], local.using_klipper_lb ? [] : [""servicelb""], var.traefik_enabled ? [] : [
    ""traefik""
  ], var.metrics_server_enabled ? [] : [""metrics-server""])

  # Default k3s node labels
  default_agent_labels         = concat([], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])
  default_control_plane_labels = concat([], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])

  allow_scheduling_on_control_plane = local.is_single_node_cluster ? true : var.allow_scheduling_on_control_plane

  # Default k3s node taints
  default_control_plane_taints = concat([], local.allow_scheduling_on_control_plane ? [] : [""node-role.kubernetes.io/control-plane:NoSchedule""], var.cni_plugin == ""cilium"" ? [""node.cilium.io/agent-not-ready:NoExecute""] : [])
  default_agent_taints         = concat([], var.cni_plugin == ""cilium"" ? [""node.cilium.io/agent-not-ready:NoExecute""] : [])


  packages_to_install = concat(var.enable_longhorn ? [""open-iscsi"", ""nfs-client""] : [], var.extra_packages_to_install)

  # The following IPs are important to be whitelisted because they communicate with Hetzner services and enable the CCM and CSI to work properly.
  # Source https://github.com/hetznercloud/csi-driver/issues/204#issuecomment-848625566
  hetzner_metadata_service_ipv4 = ""169.254.169.254/32""
  hetzner_cloud_api_ipv4        = ""213.239.246.1/32""

  # internal Pod CIDR, used for the controller and currently for calico
  cluster_cidr_ipv4 = ""10.42.0.0/16""

  whitelisted_ips = [
    local.network_ipv4_cidr,
    local.hetzner_metadata_service_ipv4,
    local.hetzner_cloud_api_ipv4,
    ""127.0.0.1/32"",
  ]

  base_firewall_rules = concat([
    # Allowing internal cluster traffic and Hetzner metadata service and cloud API IPs
    {
      direction  = ""in""
      protocol   = ""tcp""
      port       = ""any""
      source_ips = local.whitelisted_ips
    },
    {
      direction  = ""in""
      protocol   = ""udp""
      port       = ""any""
      source_ips = local.whitelisted_ips
    },
    {
      direction  = ""in""
      protocol   = ""icmp""
      source_ips = local.whitelisted_ips
    },

    # Allow all traffic to the kube api server
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""6443""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },

    # Allow all traffic to the ssh port
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""22""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },

    # Allow basic out traffic
    # ICMP to ping outside services
    {
      direction = ""out""
      protocol  = ""icmp""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    # DNS
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""53""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""out""
      protocol  = ""udp""
      port      = ""53""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    # HTTP(s)
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""80""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""443""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    #NTP
    {
      direction = ""out""
      protocol  = ""udp""
      port      = ""123""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    }
    ], !local.using_klipper_lb ? [] : [
    # Allow incoming web traffic for single node clusters, because we are using k3s servicelb there,
    # not an external load-balancer.
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""80""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""443""
      source_ips = [
        ""0.0.0.0/0""
      ]
    }
    ], var.block_icmp_ping_in ? [] : [
    {
      direction = ""in""
      protocol  = ""icmp""
      source_ips = [
        ""0.0.0.0/0""
      ]
    }
  ])

  labels = {
    ""provisioner"" = ""terraform"",
    ""engine""      = ""k3s""
    ""cluster""     = var.cluster_name
  }

  labels_control_plane_node = {
    role = ""control_plane_node""
  }
  labels_control_plane_lb = {
    role = ""control_plane_lb""
  }

  labels_agent_node = {
    role = ""agent_node""
  }

  cni_install_resources = {
    ""calico"" = [""https://projectcalico.docs.tigera.io/manifests/calico.yaml""]
  }

  cni_install_resource_patches = {
    ""calico"" = [""calico.yaml""]
  }

  cni_k3s_settings = {
    ""flannel"" = {
      disable-network-policy = var.disable_network_policy
    }
    ""calico"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
    ""cilium"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
  }

  default_cilium_values = <<EOT
ipam:
 operator:
  clusterPoolIPv4PodCIDRList:
   - ${local.cluster_cidr_ipv4}
devices: ""eth1""
EOT
}
",locals,272,,ea97125426c35d9572981496464344a0bbd830a0,dd90ec37ba3ede05e2ec5c10f0e3024423ed1ab8,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/ea97125426c35d9572981496464344a0bbd830a0/locals.tf#L272,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/dd90ec37ba3ede05e2ec5c10f0e3024423ed1ab8/locals.tf,2022-08-14 05:16:56+02:00,2022-08-14 11:19:58+02:00,2,1
