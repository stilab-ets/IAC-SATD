Repo URL,Satd Comment Id,File Path Of First Occurence,File Path Of Last Occurence,renamed,Keyword,SATD Comment,context,bloc of first occurrence,bloc type of first occurrence,bloc of last occurrence,bloc type of last occurrence,SATD Comment Line Of First Occurence,SATD Comment Line Of Last Occurence,first Commit Hash,last Commit Hash,Link To The File Of First Occurence,Link To The File Of Last Occurence/When Adressed,Introduction Time,Last Occurence (even solved or not),number of commits,adressed ?,Computing Management Debt,IaC Code Debt,Dependency Management,Security Debt,Networking Debt,Environment-Based Configuration Debt,Monitoring and Logging Debt,Test Debt
https://github.com/kubernetes/k8s.io,204,infra/gcp/terraform/k8s-infra-prow-build-trusted/main.tf,infra/gcp/terraform/k8s-infra-prow-build-trusted/main.tf,0,// todo,// TODO: consider moving the project role binding resources into the,"// TODO: consider moving the project role binding resources into the 
 //       workload-identity-service-account module 
 // 
 // Some of the roles are assigned in bash or other terraform modules, so as 
 // to keep the permissions necessary to run this terraform module scoped to 
 // ""roles/owner"" for local.project_id ","module ""prow_build_cluster_sa"" {
  source            = ""../modules/workload-identity-service-account""
  project_id        = local.project_id
  name              = local.cluster_sa_name
  description       = ""default service account for pods in ${local.cluster_name}""
  cluster_namespace = local.pod_namespace
}
",module,the block associated got renamed or deleted,,58,,e7225f5825a089b4cc3e27beb4f430306d09103d,ad8d03744a189aff28baee8dff6437940e7d6464,https://github.com/kubernetes/k8s.io/blob/e7225f5825a089b4cc3e27beb4f430306d09103d/infra/gcp/terraform/k8s-infra-prow-build-trusted/main.tf#L58,https://github.com/kubernetes/k8s.io/blob/ad8d03744a189aff28baee8dff6437940e7d6464/infra/gcp/terraform/k8s-infra-prow-build-trusted/main.tf,2021-08-30 11:00:11-04:00,2021-09-29 06:28:42-07:00,7,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,90,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,0,# todo,# LOGIN # TODO,"##################### 
 # CONTROLLER: CLOUD # See variables_controller_instance.tf for the controller instance variables. 
 #####################  
 ######### 
 # LOGIN # TODO 
 ######### ","variable ""enable_login"" {
  description = <<EOD
Enables the creation of login nodes and instance templates.
EOD
  type        = bool
  default     = true
}
",variable,"variable ""enable_login"" {
  description = <<EOD
Enables the creation of login nodes and instance templates.
EOD
  type        = bool
  default     = true
}
",variable,91,,33bf402eaa82607a027754c6048fb0dce6d7668c,c6cada9ba77bff2158a2c1c7ef8e8c76f7978f20,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/33bf402eaa82607a027754c6048fb0dce6d7668c/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf#L91,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/c6cada9ba77bff2158a2c1c7ef8e8c76f7978f20/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,2023-10-26 09:59:26-07:00,2023-10-27 11:08:29-07:00,2,1,0,0,0,1,0,0,0,0
https://github.com/uyuni-project/sumaform,1408,modules/virthost/variables.tf,modules/virthost/variables.tf,0,fix,# WORKAROUND: temporary fix Error: HTTP 404: Not Found reading,"# default     = ""https://download.opensuse.org/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2"" 
 # WORKAROUND: temporary fix Error: HTTP 404: Not Found reading","variable ""hvm_disk_image"" {
  description = ""URL to the disk image to use for KVM guests""
  # default     = ""https://download.opensuse.org/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2""
  # WORKAROUND: temporary fix Error: HTTP 404: Not Found reading
  default = ""https://www.mirrorservice.org/sites/download.opensuse.org/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2""
}
",variable,"variable ""hvm_disk_image"" {
  description = ""URL to the disk image to use for KVM guests""
  default     = ""http://download.opensuse.org/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2""
}
",variable,75,,57e0ee45d4ed104becc937fc5eecbc9e4c2b8361,5d343d967b407b5f1645e34090646066c7fe2fed,https://github.com/uyuni-project/sumaform/blob/57e0ee45d4ed104becc937fc5eecbc9e4c2b8361/modules/virthost/variables.tf#L75,https://github.com/uyuni-project/sumaform/blob/5d343d967b407b5f1645e34090646066c7fe2fed/modules/virthost/variables.tf,2021-07-14 17:32:06+02:00,2021-08-10 12:11:54+02:00,4,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,404,modules/vpc-sc/access_levels.tf,modules/vpc-sc/access-levels.tf,1,# todo,# TODO(ludomagno): add a second variable and resource for custom access levels,"/** 
 * Copyright 2021 Google LLC 
 * 
 * Licensed under the Apache License, Version 2.0 (the ""License""); 
 * you may not use this file except in compliance with the License. 
 * You may obtain a copy of the License at 
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, software 
 * distributed under the License is distributed on an ""AS IS"" BASIS, 
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 * See the License for the specific language governing permissions and 
 * limitations under the License. 
 */  
 # TODO(ludomagno): add a second variable and resource for custom access levels  
 # this code implements ""additive"" access levels, if ""authoritative"" 
 # access levels are needed, switch to the 
 # google_access_context_manager_access_levels resource ","resource ""google_access_context_manager_access_level"" ""basic"" {
  for_each = var.access_levels
  parent   = ""accessPolicies/${local.access_policy}""
  name     = ""accessPolicies/${local.access_policy}/accessLevels/${each.key}""
  title    = each.key
  basic {
    combining_function = each.value.combining_function
    dynamic ""conditions"" {
      for_each = toset(
        each.value.conditions == null ? [] : each.value.conditions
      )
      iterator = condition
      content {
        dynamic ""device_policy"" {
          for_each = toset(
            condition.key.device_policy == null ? [] : [condition.key.device_policy]
          )
          iterator = device_policy
          content {
            dynamic ""os_constraints"" {
              for_each = toset(
                device_policy.key.os_constraints == null ? [] : device_policy.key.os_constraints
              )
              iterator = os_constraint
              content {
                minimum_version            = os_constraint.key.minimum_version
                os_type                    = os_constraint.key.os_type
                require_verified_chrome_os = os_constraint.key.require_verified_chrome_os
              }
            }
            allowed_encryption_statuses      = device_policy.key.allowed_encryption_statuses
            allowed_device_management_levels = device_policy.key.allowed_device_management_levels
            require_admin_approval           = device_policy.key.require_admin_approval
            require_corp_owned               = device_policy.key.require_corp_owned
            require_screen_lock              = device_policy.key.require_screen_lock
          }
        }
        ip_subnetworks = (
          condition.key.ip_subnetworks == null ? [] : condition.key.ip_subnetworks
        )
        members = (
          condition.key.members == null ? [] : condition.key.members
        )
        negate = condition.key.negate
        regions = (
          condition.key.regions == null ? [] : condition.key.regions
        )
        required_access_levels = (
          condition.key.required_access_levels == null
          ? []
          : condition.key.required_access_levels
        )
      }
    }
  }
}
",resource,"resource ""google_access_context_manager_access_level"" ""basic"" {
  for_each    = var.access_levels
  parent      = ""accessPolicies/${local.access_policy}""
  name        = ""accessPolicies/${local.access_policy}/accessLevels/${each.key}""
  title       = each.key
  description = each.value.description

  basic {
    combining_function = each.value.combining_function

    dynamic ""conditions"" {
      for_each = toset(each.value.conditions)
      iterator = c
      content {
        ip_subnetworks         = c.value.ip_subnetworks
        members                = c.value.members
        negate                 = c.value.negate
        regions                = c.value.regions
        required_access_levels = coalesce(c.value.required_access_levels, [])

        dynamic ""device_policy"" {
          for_each = c.value.device_policy == null ? [] : [c.value.device_policy]
          iterator = dp
          content {

            allowed_device_management_levels = (
              dp.value.allowed_device_management_levels
            )
            allowed_encryption_statuses = (
              dp.value.allowed_encryption_statuses
            )
            require_admin_approval = dp.value.key.require_admin_approval
            require_corp_owned     = dp.value.require_corp_owned
            require_screen_lock    = dp.value.require_screen_lock

            dynamic ""os_constraints"" {
              for_each = toset(
                dp.value.os_constraints == null
                ? []
                : dp.value.os_constraints
              )
              iterator = oc
              content {
                minimum_version            = oc.value.minimum_version
                os_type                    = oc.value.os_type
                require_verified_chrome_os = oc.value.require_verified_chrome_os
              }
            }

          }
        }

      }
    }

  }
}
",resource,17,,2c7dab3bb2b572c4d60bc91ba48eb9e4ffbb8913,a9c47681d8c6a8e4ed7ff4f3ccfbdcf09fc575a4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2c7dab3bb2b572c4d60bc91ba48eb9e4ffbb8913/modules/vpc-sc/access_levels.tf#L17,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/a9c47681d8c6a8e4ed7ff4f3ccfbdcf09fc575a4/modules/vpc-sc/access-levels.tf,2021-12-31 13:29:22+01:00,2022-11-10 19:34:45+01:00,5,1,0,1,0,1,0,0,0,0
https://github.com/ministryofjustice/modernisation-platform,61,terraform/modernisation-platform-account/secrets.tf,terraform/modernisation-platform-account/secrets.tf,0,nuke,# Account IDs to be excluded from auto-nuke,"# Account IDs to be excluded from auto-nuke 
 # Tfsec ignore 
 # - AWS095: No requirement currently to encrypt this secret with customer-managed KMS key 
 #tfsec:ignore:AWS095","resource ""aws_secretsmanager_secret"" ""nuke_account_blocklist"" {
  # checkov:skip=CKV_AWS_149:No requirement currently to encrypt this secret with customer-managed KMS key
  name        = ""nuke_account_blocklist""
  description = ""Account IDs to be excluded from auto-nuke. AWS-Nuke (https://github.com/rebuy-de/aws-nuke) requires at least one Account ID to be present in this blocklist, while it is recommended to add every production account to this blocklist.""
  tags        = local.tags
}
",resource,"resource ""aws_secretsmanager_secret"" ""nuke_account_blocklist"" {
  # checkov:skip=CKV2_AWS_57:Auto rotation not possible
  name        = ""nuke_account_blocklist""
  description = ""Account IDs to be excluded from auto-nuke. AWS-Nuke (https://github.com/rebuy-de/aws-nuke) requires at least one Account ID to be present in this blocklist, while it is recommended to add every production account to this blocklist.""
  kms_key_id  = aws_kms_key.secrets_key.id
  tags        = local.tags
  replica {
    region = local.replica_region
  }
}
",resource,62,115.0,193e4b1f222be19253dcc26fced826c50f64bd4c,f5df80ab6ca1c06741f9457065b2bed2f0e04125,https://github.com/ministryofjustice/modernisation-platform/blob/193e4b1f222be19253dcc26fced826c50f64bd4c/terraform/modernisation-platform-account/secrets.tf#L62,https://github.com/ministryofjustice/modernisation-platform/blob/f5df80ab6ca1c06741f9457065b2bed2f0e04125/terraform/modernisation-platform-account/secrets.tf#L115,2022-05-03 14:13:24+01:00,2024-03-22 04:50:35+00:00,22,0,0,0,0,1,0,0,0,1
https://github.com/terraform-aws-modules/terraform-aws-eks,649,modules/karpenter/variables.tf,modules/karpenter/variables.tf,0,todo,# TODO - Change default to `true` at next breaking change,"################################################################################ 
 # Pod Identity Association 
 ################################################################################ 
 # TODO - Change default to `true` at next breaking change","variable ""create_pod_identity_association"" {
  description = ""Determines whether to create pod identity association""
  type        = bool
  default     = false
}
",variable,"variable ""create_pod_identity_association"" {
  description = ""Determines whether to create pod identity association""
  type        = bool
  default     = false
}
",variable,144,144.0,cfcaf27ac78278916ebf3d51dc64a20fe0d7bf01,cfcaf27ac78278916ebf3d51dc64a20fe0d7bf01,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/cfcaf27ac78278916ebf3d51dc64a20fe0d7bf01/modules/karpenter/variables.tf#L144,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/cfcaf27ac78278916ebf3d51dc64a20fe0d7bf01/modules/karpenter/variables.tf#L144,2024-05-09 07:57:57-04:00,2024-05-09 07:57:57-04:00,1,0,1,1,0,0,0,0,0,0
https://github.com/alphagov/govuk-aws,1488,terraform/projects/app-publishing-amazonmq/main.tf,terraform/projects/app-publishing-amazonmq/main.tf,0,# todo,# TODO: remove redundant FQDN.,# TODO: remove redundant FQDN.,"resource ""aws_route53_record"" ""publishing_amazonmq_internal_root_domain_name"" {
  zone_id = data.terraform_remote_state.infra_root_dns_zones.outputs.internal_root_zone_id
  # TODO: remove redundant FQDN.
  name = ""${lower(aws_mq_broker.publishing_amazonmq.broker_name)}.${data.terraform_remote_state.infra_root_dns_zones.outputs.internal_root_domain_name}""
  type = ""A""

  alias {
    name                   = aws_lb.publishingmq_lb_internal.dns_name
    zone_id                = aws_lb.publishingmq_lb_internal.zone_id
    evaluate_target_health = true
  }
}
",resource,"resource ""aws_route53_record"" ""publishing_amazonmq_internal_root_domain_name"" {
  zone_id = data.terraform_remote_state.infra_root_dns_zones.outputs.internal_root_zone_id
  # TODO: remove redundant FQDN.
  name = ""${lower(aws_mq_broker.publishing_amazonmq.broker_name)}.${data.terraform_remote_state.infra_root_dns_zones.outputs.internal_root_domain_name}""
  type = ""A""

  alias {
    name                   = aws_lb.publishingmq_lb_internal.dns_name
    zone_id                = aws_lb.publishingmq_lb_internal.zone_id
    evaluate_target_health = true
  }
}
",resource,279,246.0,60917714286eda71ec88d3bc3966268d8b2e29d9,03b331be65675f8da737783528a48c59db3b9c7f,https://github.com/alphagov/govuk-aws/blob/60917714286eda71ec88d3bc3966268d8b2e29d9/terraform/projects/app-publishing-amazonmq/main.tf#L279,https://github.com/alphagov/govuk-aws/blob/03b331be65675f8da737783528a48c59db3b9c7f/terraform/projects/app-publishing-amazonmq/main.tf#L246,2024-04-19 15:38:13+01:00,2024-05-02 14:08:35+01:00,10,0,0,1,0,0,1,0,0,0
https://github.com/CDCgov/prime-simplereport,43,ops/stg/persistent/main.tf,ops/stg/persistent/main.tf,0,// todo,// TODO: remove this when removing old DB config,// TODO: remove this when removing old DB config,"module ""db"" {
  source      = ""../../services/postgres_db""
  env         = local.env
  rg_location = local.rg_location
  rg_name     = local.rg_name

  global_vault_id = data.azurerm_key_vault.global.id
  db_vault_id     = data.azurerm_key_vault.db_keys.id
  // TODO: delete old_subnet_id when removing the old DB configuration
  old_subnet_id = module.vnet.subnet_vm_id
  subnet_id     = module.vnet.subnet_db_id
  // TODO: remove this when removing old DB config
  dns_zone_id = module.vnet.private_dns_zone_id
  // TODO: remove this when removing old DB config
  administrator_login = ""simplereport""
  log_workspace_id    = module.monitoring.log_analytics_workspace_id
  // TODO: remove this when removing old DB config
  nophi_user_password = random_password.random_nophi_password.result

  tags = local.management_tags
}
",module,"module ""db"" {
  source      = ""../../services/postgres_db""
  env         = local.env
  rg_location = local.rg_location
  rg_name     = local.rg_name

  global_vault_id  = data.azurerm_key_vault.global.id
  db_vault_id      = data.azurerm_key_vault.db_keys.id
  subnet_id        = module.vnet.subnet_db_id
  log_workspace_id = module.monitoring.log_analytics_workspace_id

  nophi_user_password = random_password.random_nophi_password.result

  tags = local.management_tags
}
",module,58,,3e4185fa549937cff9213c325bc0fee780e41eb0,fb9a18eb71f31044ca7a58958a25dea489263ca7,https://github.com/CDCgov/prime-simplereport/blob/3e4185fa549937cff9213c325bc0fee780e41eb0/ops/stg/persistent/main.tf#L58,https://github.com/CDCgov/prime-simplereport/blob/fb9a18eb71f31044ca7a58958a25dea489263ca7/ops/stg/persistent/main.tf,2022-02-16 12:59:39-05:00,2022-04-28 23:27:57-04:00,2,1,1,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1402,fast/stages-multitenant/1-resman-tenant/branch-teams.tf,fast/stages-multitenant/1-resman-tenant/branch-teams.tf,0,# todo,# TODO(ludo): add support for CI/CD,"/** 
 * Copyright 2023 Google LLC 
 * 
 * Licensed under the Apache License, Version 2.0 (the ""License""); 
 * you may not use this file except in compliance with the License. 
 * You may obtain a copy of the License at 
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, software 
 * distributed under the License is distributed on an ""AS IS"" BASIS, 
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 * See the License for the specific language governing permissions and 
 * limitations under the License. 
 */  
 # tfdoc:file:description Team stage resources.  
 # TODO(ludo): add support for CI/CD  
 ############### top-level Teams branch and automation resources ############### ","module ""branch-teams-folder"" {
  source = ""../../../modules/folder""
  count  = var.fast_features.teams ? 1 : 0
  parent = module.root-folder.id
  name   = ""Teams""
  iam = {
    ""roles/logging.admin""                  = [local.automation_sas_iam.teams]
    ""roles/owner""                          = [local.automation_sas_iam.teams]
    ""roles/resourcemanager.folderAdmin""    = [local.automation_sas_iam.teams]
    ""roles/resourcemanager.projectCreator"" = [local.automation_sas_iam.teams]
    ""roles/compute.xpnAdmin""               = [local.automation_sas_iam.teams]
  }
  tag_bindings = {
    context = var.tags.values[""${var.tags.names.context}/teams""]
  }
}
",module,,,19,0.0,5453c585e0086227b9eda70f62cc11cf989bf14f,7a5dd4e6db197daa52da8a8d877ce86b5c93182e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/5453c585e0086227b9eda70f62cc11cf989bf14f/fast/stages-multitenant/1-resman-tenant/branch-teams.tf#L19,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/7a5dd4e6db197daa52da8a8d877ce86b5c93182e/fast/stages-multitenant/1-resman-tenant/branch-teams.tf#L0,2023-02-04 15:00:45+01:00,2024-05-15 09:17:13+00:00,4,2,0,1,0,0,0,1,0,0
https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules,26,examples/standalone-vm-series/main.tf,examples/transit_vnet_common/main.tf,1,fix,# FIXME untested,# FIXME untested,"module ""common_vmseries"" {
  source   = ""../../modules/vmseries""
  for_each = var.instances

  resource_group_name       = local.resource_group_name
  location                  = var.location
  name                      = ""${var.name_prefix}-${each.key}""
  avset_id                  = azurerm_availability_set.this[0].id
  username                  = var.username
  password                  = coalesce(var.password, random_password.password.result)
  vm_series_version         = ""9.1.3""
  vm_series_sku             = ""byol""
  bootstrap_storage_account = module.bootstrap.storage_account
  bootstrap_share_name      = module.bootstrap.storage_share_name
  data_nics = [
    {
      name   = ""${each.key}-mgmt""
      subnet = module.networks.subnet_mgmt
      # FIXME untested
      public_ip_address_id = azurerm_public_ip.mgmt[each.key].id
      enable_backend_pool  = false
    },
    {
      name                 = ""${each.key}-public""
      subnet               = module.networks.subnet_public
      public_ip_address_id = azurerm_public_ip.public[each.key].id
      lb_backend_pool_id   = module.inbound-lb.backend-pool-id
      enable_backend_pool  = true
    },
    {
      name                = ""${each.key}-private""
      subnet              = module.networks.subnet_private
      enable_backend_pool = false
    },
  ]

  depends_on = [module.bootstrap]
}
",module,"module ""common_vmseries"" {
  source   = ""../../modules/vmseries""
  for_each = var.vmseries

  resource_group_name       = local.resource_group_name
  location                  = var.location
  name                      = ""${var.name_prefix}${each.key}""
  avset_id                  = try(azurerm_availability_set.this[0].id, null)
  avzone                    = try(each.value.avzone, null)
  username                  = var.username
  password                  = coalesce(var.password, random_password.password.result)
  img_version               = var.common_vmseries_version
  img_sku                   = var.common_vmseries_sku
  vm_size                   = var.common_vmseries_vm_size
  bootstrap_storage_account = module.bootstrap.storage_account
  bootstrap_share_name      = module.bootstrap.storage_share_name
  interfaces = [
    {
      name                 = ""${each.key}-mgmt""
      subnet               = module.networks.subnet_mgmt
      public_ip_address_id = azurerm_public_ip.mgmt[each.key].id
      enable_backend_pool  = false
    },
    {
      name                 = ""${each.key}-public""
      subnet               = module.networks.subnet_public
      public_ip_address_id = azurerm_public_ip.public[each.key].id
      lb_backend_pool_id   = module.inbound-lb.backend-pool-id
      enable_backend_pool  = true
    },
    {
      name                = ""${each.key}-private""
      subnet              = module.networks.subnet_private
      enable_backend_pool = false
    },
  ]

  depends_on = [module.bootstrap]
}
",module,128,,3eaaae43a57b27b4aafe29795d03f1c7136c5b70,a4d6cf8665b63f9733d9d536d18e71f0a50c98a5,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/3eaaae43a57b27b4aafe29795d03f1c7136c5b70/examples/standalone-vm-series/main.tf#L128,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/a4d6cf8665b63f9733d9d536d18e71f0a50c98a5/examples/transit_vnet_common/main.tf,2021-03-22 12:11:14+01:00,2021-03-22 12:11:14+01:00,6,1,1,0,0,0,1,0,0,1
https://github.com/alphagov/govuk-aws,514,terraform/projects/infra-public-services/main.tf,terraform/projects/infra-public-services/main.tf,0,todo,# Apt: TODO EXTERNAL,"# 
 # Apt: TODO EXTERNAL 
 # ","resource ""aws_route53_record"" ""apt_internal_service_names"" {
  count   = ""${length(var.apt_internal_service_names)}""
  zone_id = ""${data.terraform_remote_state.infra_root_dns_zones.internal_root_zone_id}""
  name    = ""${element(var.apt_internal_service_names, count.index)}.${data.terraform_remote_state.infra_root_dns_zones.internal_root_domain_name}""
  type    = ""CNAME""
  records = [""${element(var.apt_internal_service_names, count.index)}.blue.${data.terraform_remote_state.infra_root_dns_zones.internal_root_domain_name}""]
  ttl     = ""300""
}
",resource,,,303,0.0,051e76ef4b68a6c03269aedaa2f32422853a8c22,54e05ce9e2e6d77fb915c0334170cc72826463bf,https://github.com/alphagov/govuk-aws/blob/051e76ef4b68a6c03269aedaa2f32422853a8c22/terraform/projects/infra-public-services/main.tf#L303,https://github.com/alphagov/govuk-aws/blob/54e05ce9e2e6d77fb915c0334170cc72826463bf/terraform/projects/infra-public-services/main.tf#L0,2017-12-14 16:17:17+00:00,2024-03-11 15:00:01+00:00,143,2,0,0,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,207,modules/organization/variables.tf,modules/organization/variables.tf,0,todo,# TODO exclusions also support description and disabled,# TODO exclusions also support description and disabled,"variable ""logging_sinks"" {
  description = ""Logging sinks to create for this organization.""
  type = map(object({
    destination      = string
    type             = string
    filter           = string
    iam              = bool
    include_children = bool
    # TODO exclusions also support description and disabled
    exclusions = map(string)
  }))
  default = {}
}
",variable,"variable ""logging_sinks"" {
  description = ""Logging sinks to create for the organization.""
  type = map(object({
    bigquery_use_partitioned_table = optional(bool)
    description                    = optional(string)
    destination = object({
      type   = string
      target = string
    })
    disabled         = optional(bool, false)
    exclusions       = optional(map(string), {})
    filter           = string
    include_children = optional(bool, true)
  }))
  default  = {}
  nullable = false
  validation {
    condition = alltrue([
      for k, v in var.logging_sinks :
      contains([""bigquery"", ""logging"", ""pubsub"", ""storage""], v.destination.type)
    ])
    error_message = ""Destination type must be one of 'bigquery', 'logging', 'pubsub', 'storage'.""
  }
  validation {
    condition = alltrue([
      for k, v in var.logging_sinks :
      v.bigquery_use_partitioned_table != true || v.destination.type == ""bigquery""
    ])
    error_message = ""Can only set bigquery_use_partitioned_table when destination type is `bigquery`.""
  }
}
",variable,127,,ad68fc4dfa576624a7e2caa1b96499161d8b0937,486d398c7d68ce1b0784a540feb7be5fb2c680c1,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/ad68fc4dfa576624a7e2caa1b96499161d8b0937/modules/organization/variables.tf#L127,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/486d398c7d68ce1b0784a540feb7be5fb2c680c1/modules/organization/variables.tf,2021-03-03 14:23:59+01:00,2022-11-11 19:22:05+01:00,17,1,0,1,0,0,0,0,1,0
https://github.com/Worklytics/psoxy,395,infra/modules/aws-psoxy-lambda/main.tf,infra/modules/aws-psoxy-lambda/main.tf,0,todo,# TODO : revisit this is exploiting convention,# TODO : revisit this is exploiting convention,"locals {
  # TODO : revisit; this is exploiting convention
  prefix = ""${upper(replace(var.function_name, ""-"", ""_""))}_""

  # Read grant to any param that belongs to the function (identified by param prefix)
  # Can't use same approach as for write, because some params are not defined in connector specs,
  # but later in certain modules, f.e. SERVICE_ACCOUNT_KEY or HRIS_RULES
  filtered_function_read_arns = [
    for arn in data.aws_ssm_parameters_by_path.psoxy_parameters.arns : arn if length(regexall(local.prefix, arn)) > 0
  ]

  # Write grant to any writeable param specified in the connector definition
  filtered_function_write_arns = distinct(flatten([
    for p in var.function_parameters : [
      for arn in data.aws_ssm_parameters_by_path.psoxy_parameters.arns :
      arn if endswith(arn, join("""", [local.prefix, p.name])) && p.writable
    ]
  ]))

  function_write_arns = local.filtered_function_write_arns
  function_read_arns  = concat(local.filtered_function_read_arns, var.global_parameter_arns)
}
",locals,"locals {
  prefix = ""PSOXY_${upper(replace(var.source_kind, ""-"", ""_""))}_""

  param_arn_prefix = ""arn:aws:ssm:${data.aws_arn.lambda.region}:${data.aws_arn.lambda.account}:parameter/${local.prefix}""


  # Write grant to any writeable param specified in the connector definition
  filtered_function_write_arns = distinct(flatten([
    for p in var.function_parameters : [
      for arn in data.aws_ssm_parameters_by_path.psoxy_parameters.arns :
      arn if endswith(arn, join("""", [local.prefix, p.name])) && p.writable
    ]
  ]))

  function_write_arns = local.filtered_function_write_arns
  function_read_arns  = concat(
    [""${local.param_arn_prefix}*""], # wildcard to match all params corresponding to this function
    var.global_parameter_arns
  )

  write_statements =  length(local.function_write_arns) < 1 ? [] : [{
    Action   = [
      ""ssm:PutParameter""
    ]
    Effect   = ""Allow""
    Resource = local.function_write_arns
  }]

  read_statements = [{
    Action = [
      ""ssm:GetParameter*""
    ]
    Effect   = ""Allow""
    Resource =  local.function_read_arns
  }]

  policy_statements = concat(
    local.read_statements,
    local.write_statements
  )
}
",locals,80,,2507bc18439124435a38c08eacf36f6e6002b20e,5d8f4928978c96878368db88bc908047217bcda8,https://github.com/Worklytics/psoxy/blob/2507bc18439124435a38c08eacf36f6e6002b20e/infra/modules/aws-psoxy-lambda/main.tf#L80,https://github.com/Worklytics/psoxy/blob/5d8f4928978c96878368db88bc908047217bcda8/infra/modules/aws-psoxy-lambda/main.tf,2022-10-04 13:38:52-07:00,2022-10-06 13:05:00-07:00,2,1,0,1,0,0,0,0,0,0
https://github.com/uyuni-project/sumaform,1362,backend_modules/aws/host/main.tf,backend_modules/aws/host/main.tf,0,hack,# HACK,"# HACK 
 # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner""). 
 # After the first `apply`, terraform removes those tags. The following block avoids this behavior. 
 # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider 
 # https://github.com/terraform-providers/terraform-provider-aws/issues/10689","resource ""aws_ebs_volume"" ""data_disk"" {
  count = var.additional_disk_size == null ? 0 : var.additional_disk_size > 0 ? var.quantity : 0

  availability_zone = local.availability_zone
  size              = var.additional_disk_size == null ? 0 : var.additional_disk_size
  type              = lookup(var.volume_provider_settings, ""type"", ""sc1"")
  snapshot_id       = lookup(var.volume_provider_settings, ""volume_snapshot_id"", null)
  tags = {
    Name = ""${local.resource_name_prefix}-data-volume${var.quantity > 1 ? ""-${count.index + 1}"" : """"}""
  }
  # HACK
  # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner"").
  # After the first `apply`, terraform removes those tags. The following block avoids this behavior.
  # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider
  # https://github.com/terraform-providers/terraform-provider-aws/issues/10689
  lifecycle {
    ignore_changes = [tags]
  }
}
",resource,"resource ""aws_ebs_volume"" ""data_disk"" {
  count = var.additional_disk_size == null ? 0 : var.additional_disk_size > 0 ? var.quantity : 0

  availability_zone = local.availability_zone
  size              = var.additional_disk_size == null ? 0 : var.additional_disk_size
  type              = lookup(var.volume_provider_settings, ""type"", ""sc1"")
  snapshot_id       = lookup(var.volume_provider_settings, ""volume_snapshot_id"", null)
  tags = {
    Name = ""${local.resource_name_prefix}-data-volume${var.quantity > 1 ? ""-${count.index + 1}"" : """"}""
  }
  # WORKAROUND
  # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner"").
  # After the first `apply`, terraform removes those tags. The following block avoids this behavior.
  # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider
  # See github:terraform-providers/terraform-provider-aws#10689
  lifecycle {
    ignore_changes = [tags]
  }
}
",resource,104,,15bd77941377320f15ab95c290aff17bfe80d0e7,12fc857978857ebd94f9b0906480004ca9b88c22,https://github.com/uyuni-project/sumaform/blob/15bd77941377320f15ab95c290aff17bfe80d0e7/backend_modules/aws/host/main.tf#L104,https://github.com/uyuni-project/sumaform/blob/12fc857978857ebd94f9b0906480004ca9b88c22/backend_modules/aws/host/main.tf,2020-05-04 19:34:32+01:00,2021-01-26 15:58:29+01:00,5,1,1,1,1,1,0,0,0,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,1,master.tf,init.tf,1,hack,"# This ugly hack is here, because terraform serializes the","# This ugly hack is here, because terraform serializes the 
 # embedded yaml files with ""- |2"", when there is more than 
 # one yamldocument in the embedded file. Kustomize does not understand 
 # that syntax and tries to parse the blocks content as a file, resulting 
 # in weird errors. so gnu sed with funny escaping is used to 
 # replace lines like ""- |3"" by ""- |"" (yaml block syntax). 
 # due to indendation this should not changes the embedded 
 # manifests themselves","resource ""hcloud_server"" ""first_control_plane"" {
  name = ""k3s-control-plane-0""

  image              = data.hcloud_image.linux.name
  rescue             = ""linux64""
  server_type        = var.control_plane_server_type
  location           = var.location
  ssh_keys           = [hcloud_ssh_key.k3s.id]
  firewall_ids       = [hcloud_firewall.k3s.id]
  placement_group_id = hcloud_placement_group.k3s.id

  labels = {
    ""provisioner"" = ""terraform"",
    ""engine""      = ""k3s""
  }

  connection {
    user           = ""root""
    private_key    = local.ssh_private_key
    agent_identity = local.ssh_identity
    host           = self.ipv4_address
  }

  provisioner ""file"" {
    content = templatefile(""${path.module}/templates/config.ign.tpl"", {
      name           = self.name
      ssh_public_key = local.ssh_public_key
    })
    destination = ""/root/config.ign""
  }

  # Install MicroOS
  provisioner ""remote-exec"" {
    inline = local.MicroOS_install_commands
  }

  # Issue a reboot command
  provisioner ""local-exec"" {
    command = ""ssh ${local.ssh_args} root@${self.ipv4_address} '(sleep 2; reboot)&'; sleep 3""
  }

  # Wait for MicroOS to reboot and be ready
  provisioner ""local-exec"" {
    command = <<-EOT
      until ssh ${local.ssh_args} -o ConnectTimeout=2 root@${self.ipv4_address} true 2> /dev/null
      do
        echo ""Waiting for MicroOS to reboot and become available...""
        sleep 2
      done
    EOT
  }

  # Generating k3s master config file
  provisioner ""file"" {
    content = yamlencode({
      node-name                = self.name
      cluster-init             = true
      disable-cloud-controller = true
      disable                  = [""servicelb"", ""local-storage""]
      flannel-iface            = ""eth1""
      kubelet-arg              = ""cloud-provider=external""
      node-ip                  = local.first_control_plane_network_ip
      advertise-address        = local.first_control_plane_network_ip
      token                    = random_password.k3s_token.result
      node-taint               = var.allow_scheduling_on_control_plane ? [] : [""node-role.kubernetes.io/master:NoSchedule""]
    })
    destination = ""/etc/rancher/k3s/config.yaml""
  }

  # Run the first control plane
  provisioner ""remote-exec"" {
    inline = [
      # set the hostname in a persistent fashion
      ""hostnamectl set-hostname ${self.name}"",
      # first we disable automatic reboot (after transactional updates), and configure the reboot method as kured
      ""rebootmgrctl set-strategy off && echo 'REBOOT_METHOD=kured' > /etc/transactional-update.conf"",
      # prepare a directory for our post-installation kustomizations
      ""mkdir -p /tmp/post_install"",
      # then we initiate the cluster
      ""systemctl enable k3s-server"",
      # start k3s
      ""systemctl start k3s-server"",
      # wait for k3s to get ready
      <<-EOT
      timeout 120 bash <<EOF
        until systemctl status k3s-server > /dev/null; do
          systemctl start k3s-server
          echo ""Initiating the cluster...""
          sleep 1
        done
        until [ -e /etc/rancher/k3s/k3s.yaml ]; do
          echo ""Waiting for kubectl config...""
          sleep 1
        done
        until [[ ""\$(kubectl get --raw='/readyz')"" == ""ok"" ]]; do
          echo ""Waiting for cluster to become ready...""
          sleep 1
        done
      EOF
      EOT
    ]
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content = yamlencode({
      apiVersion = ""kustomize.config.k8s.io/v1beta1""
      kind       = ""Kustomization""
      resources = [
        ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",
        ""https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml"",
        ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",
        ""./traefik.yaml""
      ]
      patchesStrategicMerge = [
        file(""${path.module}/patches/kured.yaml""),
        local.ccm_latest ? file(""${path.module}/patches/ccm_latest.yaml"") : file(""${path.module}/patches/ccm.yaml""),
        local.csi_latest ? file(""${path.module}/patches/csi_latest.yaml"") : null,
      ]
    })
    destination = ""/tmp/post_install/kustomization.yaml""
  }

  # Upload traefik config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/traefik_config.yaml.tpl"",
      {
        lb_disable_ipv6    = var.lb_disable_ipv6
        lb_server_type     = var.lb_server_type
        location           = var.location
        traefik_acme_tls   = var.traefik_acme_tls
        traefik_acme_email = var.traefik_acme_email
    })
    destination = ""/tmp/post_install/traefik.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name}"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token}"",
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = [
      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /tmp/post_install/kustomization.yaml"",
      ""kubectl apply -k /tmp/post_install"",
    ]
  }

  network {
    network_id = hcloud_network.k3s.id
    ip         = local.first_control_plane_network_ip
  }

  depends_on = [
    hcloud_network_subnet.k3s,
    hcloud_firewall.k3s
  ]
}
",resource,"resource ""null_resource"" ""kustomization"" {
  triggers = {
    # Redeploy helm charts when the underlying values change
    helm_values_yaml = join(""---\n"", [
      local.traefik_values,
      local.nginx_values,
      local.calico_values,
      local.cilium_values,
      local.longhorn_values,
      local.csi_driver_smb_values,
      local.cert_manager_values,
      local.rancher_values
    ])
    # Redeploy when versions of addons need to be updated
    versions = join(""\n"", [
      coalesce(var.initial_k3s_channel, ""N/A""),
      coalesce(var.cluster_autoscaler_version, ""N/A""),
      coalesce(var.hetzner_ccm_version, ""N/A""),
      coalesce(var.hetzner_csi_version, ""N/A""),
      coalesce(var.kured_version, ""N/A""),
      coalesce(var.calico_version, ""N/A""),
      coalesce(var.cilium_version, ""N/A""),
      coalesce(var.traefik_version, ""N/A""),
      coalesce(var.nginx_version, ""N/A""),
    ])
    options = join(""\n"", [
      for option, value in local.kured_options : ""${option}=${value}""
    ])
  }

  connection {
    user           = ""root""
    private_key    = var.ssh_private_key
    agent_identity = local.ssh_agent_identity
    host           = module.control_planes[keys(module.control_planes)[0]].ipv4_address
    port           = var.ssh_port
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content     = local.kustomization_backup_yaml
    destination = ""/var/post_install/kustomization.yaml""
  }

  # Upload traefik ingress controller config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/traefik_ingress.yaml.tpl"",
      {
        version          = var.traefik_version
        values           = indent(4, trimspace(local.traefik_values))
        target_namespace = local.ingress_controller_namespace
    })
    destination = ""/var/post_install/traefik_ingress.yaml""
  }

  # Upload nginx ingress controller config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/nginx_ingress.yaml.tpl"",
      {
        version          = var.nginx_version
        values           = indent(4, trimspace(local.nginx_values))
        target_namespace = local.ingress_controller_namespace
    })
    destination = ""/var/post_install/nginx_ingress.yaml""
  }

  # Upload the CCM patch config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/ccm.yaml.tpl"",
      {
        cluster_cidr_ipv4   = var.cluster_ipv4_cidr
        default_lb_location = var.load_balancer_location
        using_klipper_lb    = local.using_klipper_lb
    })
    destination = ""/var/post_install/ccm.yaml""
  }

  # Upload the calico patch config, for the kustomization of the calico manifest
  # This method is a stub which could be replaced by a more practical helm implementation
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/calico.yaml.tpl"",
      {
        values = trimspace(local.calico_values)
    })
    destination = ""/var/post_install/calico.yaml""
  }

  # Upload the cilium install file
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cilium.yaml.tpl"",
      {
        values  = indent(4, trimspace(local.cilium_values))
        version = var.cilium_version
    })
    destination = ""/var/post_install/cilium.yaml""
  }

  # Upload the system upgrade controller plans config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/plans.yaml.tpl"",
      {
        channel          = var.initial_k3s_channel
        disable_eviction = !var.system_upgrade_enable_eviction
    })
    destination = ""/var/post_install/plans.yaml""
  }

  # Upload the Longhorn config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/longhorn.yaml.tpl"",
      {
        longhorn_namespace  = var.longhorn_namespace
        longhorn_repository = var.longhorn_repository
        values              = indent(4, trimspace(local.longhorn_values))
    })
    destination = ""/var/post_install/longhorn.yaml""
  }

  # Upload the csi-driver-smb config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/csi-driver-smb.yaml.tpl"",
      {
        values = indent(4, trimspace(local.csi_driver_smb_values))
    })
    destination = ""/var/post_install/csi-driver-smb.yaml""
  }

  # Upload the cert-manager config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cert_manager.yaml.tpl"",
      {
        values = indent(4, trimspace(local.cert_manager_values))
    })
    destination = ""/var/post_install/cert_manager.yaml""
  }

  # Upload the Rancher config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/rancher.yaml.tpl"",
      {
        rancher_install_channel = var.rancher_install_channel
        values                  = indent(4, trimspace(local.rancher_values))
    })
    destination = ""/var/post_install/rancher.yaml""
  }

  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/kured.yaml.tpl"",
      {
        options = local.kured_options
      }
    )
    destination = ""/var/post_install/kured.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""set -ex"",
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${data.hcloud_network.k3s.name} --dry-run=client -o yaml | kubectl apply -f -"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token} --dry-run=client -o yaml | kubectl apply -f -"",
      local.csi_version != null ? ""curl https://raw.githubusercontent.com/hetznercloud/csi-driver/${coalesce(local.csi_version, ""v2.4.0"")}/deploy/kubernetes/hcloud-csi.yml -o /var/post_install/hcloud-csi.yml"" : ""echo 'Skipping hetzner csi.'""
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = concat([
      ""set -ex"",

      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /var/post_install/kustomization.yaml"",

      # Wait for k3s to become ready (we check one more time) because in some edge cases,
      # the cluster had become unvailable for a few seconds, at this very instant.
      <<-EOT
      timeout 360 bash <<EOF
        until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do
          echo ""Waiting for the cluster to become ready...""
          sleep 2
        done
      EOF
      EOT
      ]
      ,

      [
        # Ready, set, go for the kustomization
        ""kubectl apply -k /var/post_install"",
        ""echo 'Waiting for the system-upgrade-controller deployment to become available...'"",
        ""kubectl -n system-upgrade wait --for=condition=available --timeout=360s deployment/system-upgrade-controller"",
        ""sleep 7"", # important as the system upgrade controller CRDs sometimes don't get ready right away, especially with Cilium.
        ""kubectl -n system-upgrade apply -f /var/post_install/plans.yaml""
      ],
      local.has_external_load_balancer ? [] : [
        <<-EOT
      timeout 360 bash <<EOF
      until [ -n ""\$(kubectl get -n ${local.ingress_controller_namespace} service/${lookup(local.ingress_controller_service_names, var.ingress_controller)} --output=jsonpath='{.status.loadBalancer.ingress[0].${var.lb_hostname != """" ? ""hostname"" : ""ip""}}' 2> /dev/null)"" ]; do
          echo ""Waiting for load-balancer to get an IP...""
          sleep 2
      done
      EOF
      EOT
    ])
  }

  depends_on = [
    hcloud_load_balancer.cluster,
    null_resource.control_planes,
    random_password.rancher_bootstrap,
    hcloud_volume.longhorn_volume
  ]
}
",resource,149,295.0,8ba33a12c807193bc1d50076ec7bd3b5d681255a,7b82c726a6d9bc3da643eefbee1b587e0126d889,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/8ba33a12c807193bc1d50076ec7bd3b5d681255a/master.tf#L149,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/7b82c726a6d9bc3da643eefbee1b587e0126d889/init.tf#L295,2022-02-12 00:52:13+01:00,2024-05-06 13:56:24+02:00,187,0,1,0,0,0,0,1,0,0
https://github.com/terraform-google-modules/terraform-google-kubernetes-engine,8,modules/safer-cluster/main.tf,modules/safer-cluster/main.tf,0,// todo,"// TODO(mmontan): define a registry_project parameter in the private_beta_cluster,","// TODO(mmontan): define a registry_project parameter in the private_beta_cluster, 
 // so that we can give GCS permissions to the service account on a project 
 // that hosts only container-images and not data.","module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // Disable the dashboard. It creates risk by running as a very sensitive user.
  kubernetes_dashboard = false

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy          = true
  network_policy_provider = ""CALICO""

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  disable_legacy_metadata_endpoints = true

  node_pools        = var.node_pools
  node_pools_labels = var.node_pools_labels

  // TODO(mmontan): check whether we need to restrict these
  // settings.
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  // TODO(mmontan): we generally considered applying
  // just the cloud-platofrm scope and use Cloud IAM
  // If we have Workload Identity, are there advantages
  // in restricting scopes even more?
  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  // We should use IP Alias.
  configure_ip_masq = false

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = length(var.compute_engine_service_account) > 0 ? false : true
  service_account        = var.compute_engine_service_account

  // TODO(mmontan): define a registry_project parameter in the private_beta_cluster,
  // so that we can give GCS permissions to the service account on a project
  // that hosts only container-images and not data.
  grant_registry_access = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // TODO(mmontan): link to a couple of policies.
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id
  node_metadata                    = ""SECURE""

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // TODO(mmontan): investigate whether this should be a recommended setting
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group
}
",module,"module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy = true

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  node_pools        = var.node_pools
  node_pools_labels = var.node_pools_labels

  // TODO(mmontan): check whether we need to restrict these
  // settings.
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = var.compute_engine_service_account == """" ? true : false
  service_account        = var.compute_engine_service_account
  registry_project_id    = var.registry_project_id
  grant_registry_access  = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // Example: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#example
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // Intranode Visibility enables you to capture flow logs for traffic between pods and create FW rules that apply to traffic between pods.
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group

  enable_shielded_nodes = var.enable_shielded_nodes
}
",module,107,,e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5,5a194719faa144ad0a7ee578663d336358f5073c,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5/modules/safer-cluster/main.tf#L107,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/5a194719faa144ad0a7ee578663d336358f5073c/modules/safer-cluster/main.tf,2019-10-04 15:21:33-07:00,2019-11-13 15:10:12-06:00,6,1,1,1,0,1,0,0,0,0
https://github.com/alphagov/govuk-aws,348,terraform/projects/infra-security-groups/draft-frontend.tf,terraform/projects/infra-security-groups/draft-frontend.tf,0,# todo,# TODO: most application instances need to talk to draft-frontend - we could,"# TODO: most application instances need to talk to draft-frontend - we could 
 # split out some security for application and service instances?","resource ""aws_security_group_rule"" ""allow_management_to_draft-frontend_elb_https"" {
  type      = ""ingress""
  from_port = 443
  to_port   = 443
  protocol  = ""tcp""

  security_group_id        = ""${aws_security_group.draft-frontend_elb.id}""
  source_security_group_id = ""${aws_security_group.management.id}""
}
",resource,the block associated got renamed or deleted,,47,,3067a0d38467044b34ab13c78f254c3be734c10f,7fd13330f8d6238e108f76ce76a76a28a99caaaf,https://github.com/alphagov/govuk-aws/blob/3067a0d38467044b34ab13c78f254c3be734c10f/terraform/projects/infra-security-groups/draft-frontend.tf#L47,https://github.com/alphagov/govuk-aws/blob/7fd13330f8d6238e108f76ce76a76a28a99caaaf/terraform/projects/infra-security-groups/draft-frontend.tf,2017-09-22 10:05:04+01:00,2018-01-02 17:41:32+00:00,2,1,0,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-iam,14,test/fixtures/update/main.tf,test/fixtures/update/main.tf,0,# todo,# TODO: Temporary. Have to change it manually after each destroy,"# TODO: Temporary. Have to change it manually after each destroy 
 #       since project names stay reserved even after deletion. 
 #       Used to test static IAM. We can get rid of it if 
 #       we make a 2 step converge process with the var.random_hexes 
 #       generation being the first step.","locals {
  static_n  = 2
  dynamic_n = 1 # Dynamic mode supports only 1 entity atm.
  mode      = ""authoritative""
  prefix    = ""test-iam""

  # TODO: Temporary. Have to change it manually after each destroy
  #       since project names stay reserved even after deletion.
  #       Used to test static IAM. We can get rid of it if
  #       we make a 2 step converge process with the var.random_hexes
  #       generation being the first step.
  uniqSuffix = ""prj6""

  member_group_0 = [
    ""serviceAccount:${var.member1}"",
  ]

  member_group_1 = [
    ""serviceAccount:${var.member1}"",
    ""serviceAccount:${var.member2}""
  ]

  project_bindings = {
    ""roles/iam.roleViewer"" = local.member_group_0
    ""roles/logging.viewer"" = local.member_group_1

    # Uncomment the following role and re`converge` to test
    # whether some resources were recreated.
    # Expectation is that no resource are gonna be recreated, but only
    # new ones added.
    # ""roles/iam.securityReviewer"" = local.member_group_0
  }
}
",locals,"locals {
  static_n  = 2
  dynamic_n = 1 # Dynamic mode supports only 1 entity atm.
  mode      = ""authoritative""
  prefix    = ""test-iam""

  member_group_0 = [
    ""serviceAccount:${var.member1}"",
  ]

  member_group_1 = [
    ""serviceAccount:${var.member1}"",
    ""serviceAccount:${var.member2}""
  ]

  project_bindings = zipmap(
    slice([
      ""roles/iam.roleViewer"",
      ""roles/logging.viewer"",
      ""roles/iam.securityReviewer""
    ], 0, var.roles),
    slice([
      local.member_group_0,
      local.member_group_1,
      local.member_group_0
    ], 0, var.roles)
  )
}
",locals,23,,665a160dd97a99c80a325250d7fea70fa9f4fac5,0de33e0943c7eabd31dc618990477d11614de007,https://github.com/terraform-google-modules/terraform-google-iam/blob/665a160dd97a99c80a325250d7fea70fa9f4fac5/test/fixtures/update/main.tf#L23,https://github.com/terraform-google-modules/terraform-google-iam/blob/0de33e0943c7eabd31dc618990477d11614de007/test/fixtures/update/main.tf,2019-10-14 19:29:14+03:00,2019-10-17 13:08:12+03:00,2,1,0,1,0,0,0,0,0,0
https://github.com/aws-observability/terraform-aws-observability-accelerator,30,examples/existing-cluster-with-base-and-infra/main.tf,examples/existing-cluster-with-base-and-infra/main.tf,0,todo,# create a new Grafana workspace - TODO review design,# create a new Grafana workspace - TODO review design,"module ""eks_observability_accelerator"" {
  # source = ""aws-ia/terrarom-aws-observability-accelerator?ref=dev""
  source = ""../../""

  aws_region     = var.aws_region
  eks_cluster_id = var.eks_cluster_id

  # deploys AWS Distro for OpenTelemetry operator into the cluster
  enable_amazon_eks_adot = true

  # reusing existing certificate manager? defaults to true
  enable_cert_manager = true

  # # -- or enable opentelemetry operator
  enable_opentelemetry_operator = false
  #open_telemetry_operator_config = map() // custom config

  # creates a new AMP workspace, defaults to true
  enable_managed_prometheus = false

  # reusing existing AMP -- needs data source for alerting rules
  managed_prometheus_id     = var.managed_prometheus_workspace_id
  managed_prometheus_region = null # defaults to the current region, useful for cross region scenarios (same account)

  # sets up the AMP alert manager at the workspace level
  enable_alertmanager = true

  # create a new Grafana workspace - TODO review design
  enable_managed_grafana       = false
  managed_grafana_workspace_id = var.managed_grafana_workspace_id
  grafana_api_key              = var.grafana_api_key

  tags = local.tags
}
",module,"module ""eks_observability_accelerator"" {
  # source = ""aws-ia/terrarom-aws-observability-accelerator""
  source = ""../../""

  aws_region     = var.aws_region
  eks_cluster_id = var.eks_cluster_id

  # deploys AWS Distro for OpenTelemetry operator into the cluster
  enable_amazon_eks_adot = true

  # reusing existing certificate manager? defaults to true
  enable_cert_manager = true

  # creates a new AMP workspace, defaults to true
  enable_managed_prometheus = false

  # reusing existing AMP -- needs data source for alerting rules
  managed_prometheus_workspace_id     = var.managed_prometheus_workspace_id
  managed_prometheus_workspace_region = null # defaults to the current region, useful for cross region scenarios (same account)

  # sets up the AMP alert manager at the workspace level
  enable_alertmanager = true

  # reusing existing Amazon Managed Grafana workspace
  enable_managed_grafana       = false
  managed_grafana_workspace_id = var.managed_grafana_workspace_id
  grafana_api_key              = var.grafana_api_key

  tags = local.tags
}
",module,77,,5b5d5e7dd17a83d36e2a857da5feda77a8b95776,6dca51b135de9dae7aa62a61e7eebc9895c27412,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/5b5d5e7dd17a83d36e2a857da5feda77a8b95776/examples/existing-cluster-with-base-and-infra/main.tf#L77,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/6dca51b135de9dae7aa62a61e7eebc9895c27412/examples/existing-cluster-with-base-and-infra/main.tf,2022-08-26 17:30:03+02:00,2022-08-26 17:30:03+02:00,2,1,1,1,0,0,0,0,1,0
https://github.com/SUSE/ha-sap-terraform-deployments,155,libvirt/terraform/modules/monitoring/variables.tf,libvirt/terraform/modules/monitoring/variables.tf,0,// todo,// TODO: verify if it is needed,"// TODO: verify if it is needed 
 // variable ""server_configuration"" { 
 //   description = ""use ${module.<SERVER_NAME>.configuration}, see the main.tf example file"" 
 //  type = ""map"" 
 //} ","variable ""public_key_location"" {
  description = ""path of additional pub ssh key you want to use to access VMs""
  default     = ""/dev/null""

  # HACK: """" cannot be used as a default because of https://github.com/hashicorp/hil/issues/50
}
",variable,"variable ""public_key_location"" {
  description = ""path of additional pub ssh key you want to use to access VMs""
  default     = ""/dev/null""

  # HACK: """" cannot be used as a default because of https://github.com/hashicorp/hil/issues/50
}
",variable,52,,ded573f03083c78a15ca8c85606954edfc0c5ad5,d32b0c93370b96b65c4de6378081d2af5a1b56bf,https://github.com/SUSE/ha-sap-terraform-deployments/blob/ded573f03083c78a15ca8c85606954edfc0c5ad5/libvirt/terraform/modules/monitoring/variables.tf#L52,https://github.com/SUSE/ha-sap-terraform-deployments/blob/d32b0c93370b96b65c4de6378081d2af5a1b56bf/libvirt/terraform/modules/monitoring/variables.tf,2019-07-22 11:53:34+02:00,2019-07-23 12:37:23+02:00,3,1,1,1,0,0,0,0,0,0
https://github.com/nasa/cumulus,268,lambdas/data-migration/main.tf,lambdas/data-migration1/main.tf,1,// todo,// TODO: should we be creating our own security group here?,// TODO: should we be creating our own security group here?,"resource ""aws_lambda_function"" ""data_migration"" {
  function_name    = ""${var.prefix}-data-migration""
  filename         = local.lambda_path
  source_code_hash = filebase64sha256(local.lambda_path)
  handler          = ""index.handler""
  role             = aws_iam_role.data_migration.arn
  runtime          = ""nodejs12.x""
  timeout          = 60
  memory_size      = 128

  environment {
    variables = {
      PG_HOST          = var.pg_host
      PG_USER          = var.pg_user
      PG_PASSWORD      = var.pg_password
      PG_DATABASE      = var.pg_database
      CollectionsTable = var.dynamo_tables.collections.name
    }
  }

  dynamic ""vpc_config"" {
    for_each = length(var.subnet_ids) == 0 ? [] : [1]
    content {
      subnet_ids = var.subnet_ids
      // TODO: should we be creating our own security group here?
      security_group_ids = [
        aws_security_group.data_migration[0].id
      ]
    }
  }

  tags = var.tags
}
",resource,the block associated got renamed or deleted,,93,,8480e4969011afe1186f85d493806bcf48ea9698,6b78c090ce1b3288d967bd8f225319b8b5d37dc2,https://github.com/nasa/cumulus/blob/8480e4969011afe1186f85d493806bcf48ea9698/lambdas/data-migration/main.tf#L93,https://github.com/nasa/cumulus/blob/6b78c090ce1b3288d967bd8f225319b8b5d37dc2/lambdas/data-migration1/main.tf,2020-08-21 17:47:34-04:00,2020-08-28 17:34:02-04:00,4,1,1,1,0,1,1,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,204,outputs.tf,outputs.tf,0,todo,"//"", module.aws_eks.cluster_oidc_issuer_url)[1], ""EKS Cluster not enabled"") # TODO - remove `split()` since `oidc_provider` coverss https:// removal","value       = try(split(""//"", module.aws_eks.cluster_oidc_issuer_url)[1], ""EKS Cluster not enabled"") # TODO - remove `split()` since `oidc_provider` coverss https:// removal","output ""eks_oidc_issuer_url"" {
  description = ""The URL on the EKS cluster OIDC Issuer""
  value       = try(split(""//"", module.aws_eks.cluster_oidc_issuer_url)[1], ""EKS Cluster not enabled"") # TODO - remove `split()` since `oidc_provider` coverss https:// removal
}
",output,,,21,0.0,deec7d5caea47c06dea48fa616ad2c56e52c3cce,19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/deec7d5caea47c06dea48fa616ad2c56e52c3cce/outputs.tf#L21,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1/outputs.tf#L0,2022-04-29 14:37:13-07:00,2023-06-05 10:07:47-04:00,11,2,1,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,2895,infra/modules/gcp-secrets/main.tf,infra/modules/gcp-secrets/main.tf,0,# todo,# TODO: put each.value.description somewhere shouldn't be a 'label' annotations not yet supprted,"# TODO: put each.value.description somewhere; shouldn't be a 'label'; annotations not yet supprted 
 # by google terraform provider ","resource ""google_secret_manager_secret"" ""secret"" {
  for_each = var.secrets

  project   = var.secret_project
  secret_id = ""${var.path_prefix}${each.key}""
  labels    = merge(
    var.default_labels,
    {
      terraform_managed_value = each.value.value_managed_by_tf
    }
  )

  # TODO: put each.value.description somewhere; shouldn't be a 'label'; annotations not yet supprted
  # by google terraform provider

  replication {
    user_managed {
      dynamic ""replicas"" {
        for_each = local.replica_locations
        content {
          location = replicas.value
        }
      }
    }
  }

  lifecycle {
    ignore_changes = [
      replication, # for backwards compatibility; replication can't be changed after secrets created
      labels
    ]
  }
}
",resource,"resource ""google_secret_manager_secret"" ""secret"" {
  for_each = var.secrets

  project   = var.secret_project
  secret_id = ""${var.path_prefix}${each.key}""
  labels = merge(
    var.default_labels,
    {
      terraform_managed_value = each.value.value_managed_by_tf
    }
  )

  # TODO: put each.value.description somewhere; shouldn't be a 'label'; annotations not yet supprted
  # by google terraform provider

  replication {
    user_managed {
      dynamic ""replicas"" {
        for_each = local.replica_locations
        content {
          location = replicas.value
        }
      }
    }
  }

  lifecycle {
    ignore_changes = [
      replication, # for backwards compatibility; replication can't be changed after secrets created
      labels
    ]
  }
}
",resource,25,25.0,96eefe71e3fe43be1be749d78545c2c25a17ac83,5e25bc21633cc46c7ba0066959fc9268dedfe92f,https://github.com/Worklytics/psoxy/blob/96eefe71e3fe43be1be749d78545c2c25a17ac83/infra/modules/gcp-secrets/main.tf#L25,https://github.com/Worklytics/psoxy/blob/5e25bc21633cc46c7ba0066959fc9268dedfe92f/infra/modules/gcp-secrets/main.tf#L25,2023-08-11 06:21:43-07:00,2023-08-21 11:08:12-07:00,2,0,0,1,1,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-kubernetes-engine,1,examples/simple_regional/main.tf,examples/simple_regional/main.tf,0,todo,# TODO remove,"region = ""us-east4"" # TODO remove","provider ""google"" {
  credentials = ""${file(local.credentials_file_path)}""
  region = ""us-east4"" # TODO remove
}
",provider,"provider ""google"" {
  credentials = ""${file(local.credentials_file_path)}""
}
",provider,23,,28c28dd1c900c08d9cbfb0aaa22d7dfed990678c,d0ee13342a737a1bd2f7bfddb46c1f21a8c1562f,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/28c28dd1c900c08d9cbfb0aaa22d7dfed990678c/examples/simple_regional/main.tf#L23,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/d0ee13342a737a1bd2f7bfddb46c1f21a8c1562f/examples/simple_regional/main.tf,2018-10-01 18:09:56-04:00,2018-10-05 17:09:52-04:00,2,1,0,1,1,0,0,0,0,0
https://github.com/Worklytics/psoxy,1521,infra/modules/gcp-secrets/main.tf,infra/modules/gcp-secrets/main.tf,0,# todo,# TODO: avoid creating version here at all if value == null,"# TODO: avoid creating version here at all if value == null 
 # (problem is that Terraform complains if trying to use any derivative of var.secrets in a for_each, 
 #  bc it's sensitive - not sure why it doesn't complain about the for_each over var.secrets directly)","resource ""google_secret_manager_secret_version"" ""version"" {
  for_each = var.secrets

  secret      = google_secret_manager_secret.secret[each.key].id
  secret_data = coalesce(each.value.value, ""placeholder value - fill me"")
  # NOTE: can't set `enabled = false` here, bc we bind secret to env var so CloudFunction update will fail

  lifecycle {
    create_before_destroy = true
  }
}
",resource,"resource ""google_secret_manager_secret_version"" ""version"" {
  for_each = local.secrets_w_terraform_managed_values

  secret      = google_secret_manager_secret.secret[each.key].id
  secret_data = coalesce(each.value.value, ""placeholder value - fill me"")
  # NOTE: can't set `enabled = false` here in placeholder case, bc we bind secret to env var so
  # CloudFunction update will fail as can't bind to ':latest'

  lifecycle {
    create_before_destroy = true

    # TODO: remove this in v0.5
    ignore_changes = [
      enabled, # if secret version disabled after creation, let it be (placeholder case)
    ]
  }
}
",resource,31,,f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e,140ba148a513ad7b6a450120b0a8d39d58c1d908,https://github.com/Worklytics/psoxy/blob/f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e/infra/modules/gcp-secrets/main.tf#L31,https://github.com/Worklytics/psoxy/blob/140ba148a513ad7b6a450120b0a8d39d58c1d908/infra/modules/gcp-secrets/main.tf,2023-06-16 14:08:45-07:00,2023-07-25 08:16:19-07:00,6,1,0,1,1,1,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,30,modules/aws-eks-self-managed-node-groups/secgroups.tf,modules/aws-eks-self-managed-node-groups/secgroups.tf,0,#todo,#TODO This may not be required since cluster_security_group_id outbound is open to 0.0.0.0/0,"//------------------IMPORTANT  
 #TODO This may not be required since cluster_security_group_id outbound is open to 0.0.0.0/0","resource ""aws_security_group_rule"" ""control_plane_egress_to_worker_https"" {
  count = local.self_managed_node_group[""create_worker_security_group""] == true ? 1 : 0

  description              = ""Allow cluster security group to send communication to worker security groups""
  type                     = ""egress""
  from_port                = 443
  to_port                  = 443
  protocol                 = ""tcp""
  security_group_id        = var.cluster_security_group_id
  source_security_group_id = aws_security_group.self_managed_ng[0].id

}
",resource,,,102,0.0,fd23f8a3a15c3ce9bb1b0692a21bd43f6b0ec439,6f7908ab23255067c61f3614124ec7818b60ba8f,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/fd23f8a3a15c3ce9bb1b0692a21bd43f6b0ec439/modules/aws-eks-self-managed-node-groups/secgroups.tf#L102,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/6f7908ab23255067c61f3614124ec7818b60ba8f/modules/aws-eks-self-managed-node-groups/secgroups.tf#L0,2021-10-03 20:47:56+01:00,2022-03-17 17:03:58+00:00,7,2,0,1,0,1,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1642,modules/gke-cluster-standard/variables.tf,modules/gke-cluster-standard/variables.tf,0,todo,# TODO add kube state metrics and validation,"# TODO add kube state metrics and validation  
 # Google Cloud Managed Service for Prometheus","variable ""monitoring_config"" {
  description = ""Monitoring configuration. Google Cloud Managed Service for Prometheus is enabled by default.""
  type = object({
    enable_system_metrics = optional(bool, true)

    # Control plane metrics
    enable_api_server_metrics         = optional(bool, false)
    enable_controller_manager_metrics = optional(bool, false)
    enable_scheduler_metrics          = optional(bool, false)

    # TODO add kube state metrics and validation

    # Google Cloud Managed Service for Prometheus
    enable_managed_prometheus = optional(bool, true)
  })
  default  = {}
  nullable = false
  validation {
    condition = anytrue([
      var.monitoring_config.enable_api_server_metrics,
      var.monitoring_config.enable_controller_manager_metrics,
      var.monitoring_config.enable_scheduler_metrics,
    ]) ? var.monitoring_config.enable_system_metrics : true
    error_message = ""System metrics are the minimum required component for enabling metrics collection.""
  }
}
",variable,"variable ""monitoring_config"" {
  description = ""Monitoring configuration. Google Cloud Managed Service for Prometheus is enabled by default.""
  type = object({
    enable_system_metrics = optional(bool, true)

    # Control plane metrics
    enable_api_server_metrics         = optional(bool, false)
    enable_controller_manager_metrics = optional(bool, false)
    enable_scheduler_metrics          = optional(bool, false)

    # Kube state metrics
    enable_daemonset_metrics   = optional(bool, false)
    enable_deployment_metrics  = optional(bool, false)
    enable_hpa_metrics         = optional(bool, false)
    enable_pod_metrics         = optional(bool, false)
    enable_statefulset_metrics = optional(bool, false)
    enable_storage_metrics     = optional(bool, false)

    # Google Cloud Managed Service for Prometheus
    enable_managed_prometheus = optional(bool, true)
  })
  default  = {}
  nullable = false
  validation {
    condition = anytrue([
      var.monitoring_config.enable_api_server_metrics,
      var.monitoring_config.enable_controller_manager_metrics,
      var.monitoring_config.enable_scheduler_metrics,
      var.monitoring_config.enable_daemonset_metrics,
      var.monitoring_config.enable_deployment_metrics,
      var.monitoring_config.enable_hpa_metrics,
      var.monitoring_config.enable_pod_metrics,
      var.monitoring_config.enable_statefulset_metrics,
      var.monitoring_config.enable_storage_metrics,
    ]) ? var.monitoring_config.enable_system_metrics : true
    error_message = ""System metrics are the minimum required component for enabling metrics collection.""
  }
  validation {
    condition = anytrue([
      var.monitoring_config.enable_daemonset_metrics,
      var.monitoring_config.enable_deployment_metrics,
      var.monitoring_config.enable_hpa_metrics,
      var.monitoring_config.enable_pod_metrics,
      var.monitoring_config.enable_statefulset_metrics,
      var.monitoring_config.enable_storage_metrics,
    ]) ? var.monitoring_config.enable_managed_prometheus : true
    error_message = ""Kube state metrics collection requires Google Cloud Managed Service for Prometheus to be enabled.""
  }
}
",variable,209,,b3dc91b5cd6375d9e589f149d98985a895fffbc2,6eb862a7754e9796cc26386227ea763c579edcdc,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b3dc91b5cd6375d9e589f149d98985a895fffbc2/modules/gke-cluster-standard/variables.tf#L209,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/6eb862a7754e9796cc26386227ea763c579edcdc/modules/gke-cluster-standard/variables.tf,2023-09-14 23:25:57+01:00,2023-09-15 12:18:45+01:00,2,1,1,1,0,0,0,0,1,0
https://github.com/camptocamp/devops-stack,95,examples/eks-aws/main.tf,examples/eks/main.tf,1,todo,# TODO Add variable when we configure the Traefik Dashboard,# TODO Add variable when we configure the Traefik Dashboard,"module ""helloworld_apps"" {
  source = ""git::https://github.com/camptocamp/devops-stack-module-applicationset.git""

  depends_on = [module.argocd]

  name                   = ""helloworld-apps""
  argocd_namespace       = local.argocd_namespace
  project_dest_namespace = ""*""
  project_source_repos = [
    ""https://github.com/camptocamp/devops-stack-helloworld-templates.git"",
  ]

  generators = [
    {
      git = {
        repoURL  = ""https://github.com/camptocamp/devops-stack-helloworld-templates.git""
        revision = ""main""

        directories = [
          {
            path = ""apps/*""
          }
        ]
      }
    }
  ]
  template = {
    metadata = {
      name = ""{{path.basename}}""
    }

    spec = {
      project = ""helloworld-apps""

      source = {
        repoURL        = ""https://github.com/camptocamp/devops-stack-helloworld-templates.git""
        targetRevision = ""main""
        path           = ""{{path}}""

        helm = {
          valueFiles = []
          # The following value defines this global variables that will be available to all apps in apps/*
          # These are needed to generate the ingresses containing the name and base domain of the cluster.
          values = <<-EOT
            cluster:
              name: ""${module.eks.cluster_name}""
              domain: ""${module.eks.base_domain}""
            apps:
              traefik_dashboard: false # TODO Add variable when we configure the Traefik Dashboard
              grafana: ${module.grafana.grafana_enabled || module.prometheus-stack.grafana_enabled}
              prometheus: ${module.prometheus-stack.prometheus_enabled}
              thanos: ${module.thanos.thanos_enabled}
              alertmanager: ${module.prometheus-stack.alertmanager_enabled}
          EOT
        }
      }

      destination = {
        name      = ""in-cluster""
        namespace = ""{{path.basename}}""
      }

      syncPolicy = {
        automated = {
          allowEmpty = false
          selfHeal   = true
          prune      = true
        }
        syncOptions = [
          ""CreateNamespace=true""
        ]
      }
    }
  }
}
",module,the block associated got renamed or deleted,,336,,23a76321726eca45b1852f9cbb9a5a46dd17c13e,9f09347de36de8f813051eae5c981dc8cae5c393,https://github.com/camptocamp/devops-stack/blob/23a76321726eca45b1852f9cbb9a5a46dd17c13e/examples/eks-aws/main.tf#L336,https://github.com/camptocamp/devops-stack/blob/9f09347de36de8f813051eae5c981dc8cae5c393/examples/eks/main.tf,2023-04-03 16:40:29+02:00,2023-08-18 14:48:32+02:00,4,1,1,1,0,0,1,0,1,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1763,modules/net-lb-app-ext-regional/main.tf,modules/net-lb-app-ext-regional/main.tf,0,# todo,# TODO(jccb): double check if this is true,"# external regional load balancer is always EXTERNAL_MANAGER. 
 # TODO(jccb): double check if this is true","resource ""google_compute_forwarding_rule"" ""default"" {
  provider    = google-beta
  project     = var.project_id
  name        = var.name
  region      = var.region
  description = var.description
  ip_address  = var.address
  ip_protocol = ""TCP""
  # external regional load balancer is always EXTERNAL_MANAGER.
  # TODO(jccb): double check if this is true
  load_balancing_scheme = ""EXTERNAL_MANAGED""
  port_range            = join("","", local.fwd_rule_ports)
  labels                = var.labels
  target                = local.fwd_rule_target
  network               = var.vpc
  # external regional app lb only supports standard tier
  network_tier = ""STANDARD""
}
",resource,"resource ""google_compute_forwarding_rule"" ""default"" {
  provider    = google-beta
  project     = var.project_id
  name        = var.name
  region      = var.region
  description = var.description
  ip_address  = var.address
  ip_protocol = ""TCP""
  # external regional load balancer is always EXTERNAL_MANAGER.
  # TODO(jccb): double check if this is true
  load_balancing_scheme = ""EXTERNAL_MANAGED""
  port_range            = join("","", local.fwd_rule_ports)
  labels                = var.labels
  target                = local.fwd_rule_target
  network               = var.vpc
  # external regional app lb only supports standard tier
  network_tier = ""STANDARD""
}
",resource,41,41.0,8beb621e070226b7f11a82807a706170ae7040ea,3af7e257d21f889ffaf7b32a3bab974fdbfda6e4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/8beb621e070226b7f11a82807a706170ae7040ea/modules/net-lb-app-ext-regional/main.tf#L41,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3af7e257d21f889ffaf7b32a3bab974fdbfda6e4/modules/net-lb-app-ext-regional/main.tf#L41,2024-01-05 16:59:27+01:00,2024-04-17 10:23:48+02:00,2,0,0,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,20,modules/dns/variables.tf,modules/dns/variables.tf,0,# todo,# TODO(ludoo): add support for forwarding path attribute,# TODO(ludoo): add support for forwarding path attribute,"variable ""forwarders"" {
  description = ""List of target name servers, only valid for 'forwarding' zone types.""
  type        = list(string)
  default     = []
}
",variable,"variable ""forwarders"" {
  description = ""Map of {IPV4_ADDRESS => FORWARDING_PATH} for 'forwarding' zone types. Path can be 'default', 'private', or null for provider default.""
  type        = map(string)
  default     = {}
}
",variable,59,,c486bfc66f9814e33b410602cb557a5e4d532912,27aa0aa64c7d3c3f9d35b2a2d54f8e95860cb913,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c486bfc66f9814e33b410602cb557a5e4d532912/modules/dns/variables.tf#L59,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/27aa0aa64c7d3c3f9d35b2a2d54f8e95860cb913/modules/dns/variables.tf,2020-04-03 14:06:48+02:00,2020-11-20 08:35:58+01:00,6,1,0,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,21,main.tf,main.tf,0,#todo,"#TODO: terraform 0.12 will enable ""expiration_mode ? a_table_expiration : null"" (https://github.com/hashicorp/terraform/issues/17968)","#TODO: terraform 0.12 will enable ""expiration_mode ? a_table_expiration : null"" (https://github.com/hashicorp/terraform/issues/17968)","resource ""google_bigquery_dataset"" ""main"" {
  dataset_id    = ""${var.dataset_id}""
  friendly_name = ""${var.dataset_name}""
  description   = ""${var.description}""
  location      = ""${local.location}""

  #TODO: terraform 0.12 will enable ""expiration_mode ? a_table_expiration : null"" (https://github.com/hashicorp/terraform/issues/17968)
  default_table_expiration_ms = ""${var.expiration}""
  project                     = ""${var.project_id}""
  labels                      = ""${var.dataset_labels}""

  # TODO: revisit terraform 0.12 if arrays are available
  # access {
  #   role   = ""READER""
  #   domain = ""adigangi.com""
  # }
}
",resource,"resource ""google_bigquery_dataset"" ""main"" {
  dataset_id    = ""${var.dataset_id}""
  friendly_name = ""${var.dataset_name}""
  description   = ""${var.description}""
  location      = ""${local.location}""

  default_table_expiration_ms = ""${var.expiration}""
  project                     = ""${var.project_id}""
  labels                      = ""${var.dataset_labels}""
}
",resource,30,,7f922f7e9df197df38c9b09dfa6e3614d71f19f5,a7966ae4afc7bb73842fb9fd6f0f708b359cf36e,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/7f922f7e9df197df38c9b09dfa6e3614d71f19f5/main.tf#L30,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/a7966ae4afc7bb73842fb9fd6f0f708b359cf36e/main.tf,2019-01-16 18:10:54-05:00,2019-01-28 12:41:47-05:00,2,1,1,0,1,0,0,0,0,0
https://github.com/Worklytics/psoxy,4989,infra/modules/gcp/main.tf,infra/modules/gcp/main.tf,0,# todo,"# TODO: This is will supported since 0.5 psoxy version, as google provider needs to be updated","# TODO: This is will supported since 0.5 psoxy version, as google provider needs to be updated 
 /*resource ""google_artifact_registry_repository"" ""psoxy-functions-repo"" { 
 location      = var.bucket_location 
 project       = var.project_id 
 repository_id = ""psoxy-functions"" 
 description   = ""Docker repository used on the cloud functions"" 
 format        = ""DOCKER"" 
  
 ## Not supported in current google providers, needs 5.14 as there it is GA 
 # See https://github.com/hashicorp/terraform-provider-google/blob/main/CHANGELOG.md#5140-jan-29-2024 
 # but even is present in the Documentation (https://registry.terraform.io/providers/hashicorp/google/4.80.0/docs/resources/artifact_registry_repository#argument-reference) 
 # when applied it throws an error with the message: ""An argument named ""cleanup_policy_dry_run"" is not expected here"" 
 # and ""no block for cleanup_policies"" is expected 
 *//*cleanup_policy_dry_run = false  
 # https://cloud.google.com/artifact-registry/docs/repositories/cleanup-policy#json_2 
 # https://registry.terraform.io/providers/hashicorp/google/4.80.0/docs/resources/artifact_registry_repository#argument-reference","resource ""google_secret_manager_secret"" ""pseudonym_salt"" {
  project   = var.project_id
  secret_id = ""${var.config_parameter_prefix}PSOXY_SALT""
  labels = merge(
    var.default_labels,
    {
      terraform_managed_value = true
    }
  )

  replication {
    user_managed {
      dynamic ""replicas"" {
        for_each = var.secret_replica_locations
        content {
          location = replicas.value
        }
      }
    }
  }

  lifecycle {
    ignore_changes = [
      replication, # can't change replication after creation
      labels
    ]
  }

  depends_on = [
    google_project_service.gcp_infra_api
  ]
}
",resource,"resource ""google_secret_manager_secret"" ""pseudonym_salt"" {
  project   = var.project_id
  secret_id = ""${var.config_parameter_prefix}PSOXY_SALT""
  labels = merge(
    var.default_labels,
    {
      terraform_managed_value = true
    }
  )

  replication {
    user_managed {
      dynamic ""replicas"" {
        for_each = var.secret_replica_locations
        content {
          location = replicas.value
        }
      }
    }
  }

  lifecycle {
    ignore_changes = [
      replication, # can't change replication after creation
      labels
    ]
  }

  depends_on = [
    google_project_service.gcp_infra_api
  ]
}
",resource,33,33.0,0db6077bf0549cf79dc2e0cb57563d5ac2453feb,0db6077bf0549cf79dc2e0cb57563d5ac2453feb,https://github.com/Worklytics/psoxy/blob/0db6077bf0549cf79dc2e0cb57563d5ac2453feb/infra/modules/gcp/main.tf#L33,https://github.com/Worklytics/psoxy/blob/0db6077bf0549cf79dc2e0cb57563d5ac2453feb/infra/modules/gcp/main.tf#L33,2024-04-03 10:54:59-07:00,2024-04-03 10:54:59-07:00,1,0,1,1,1,0,0,0,0,0
https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules,49,modules/vmss/outputs.tf,modules/vmss/outputs.tf,0,todo,"# tflint-ignore: terraform_naming_convention # TODO rename to scale_set_name, but bundle with next breaking change","# tflint-ignore: terraform_naming_convention # TODO rename to scale_set_name, but bundle with next breaking change","output ""inbound-scale-set-name"" {
  description = ""Name of inbound scale set.""
  value       = azurerm_virtual_machine_scale_set.this.name
}
",output,the block associated got renamed or deleted,,1,,486a451e0009a3a296ff1d2c9683aede8176c3e9,d43a5b9c4a5127efc4f2d8c82c814e0e6a2bf796,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/486a451e0009a3a296ff1d2c9683aede8176c3e9/modules/vmss/outputs.tf#L1,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/d43a5b9c4a5127efc4f2d8c82c814e0e6a2bf796/modules/vmss/outputs.tf,2021-05-21 15:58:34+02:00,2021-09-17 16:38:00+02:00,2,1,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,125,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/controller.tf,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/controller.tf,0,# todo,# TODO: add support for proper access_config,# TODO: add support for proper access_config,"locals {
  # TODO: add support for proper access_config
  access_config = {
    nat_ip       = null
    network_tier = ""STANDARD""
  }
}
",locals,"locals {
  # TODO: add support for proper access_config
  access_config = {
    nat_ip       = null
    network_tier = null
  }
}
",locals,88,90.0,c47d346676eb04eb46385f018a745fb865c081a0,367dbbc03d00a523c457d066229e6c9376bcd5c9,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/c47d346676eb04eb46385f018a745fb865c081a0/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/controller.tf#L88,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/367dbbc03d00a523c457d066229e6c9376bcd5c9/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/controller.tf#L90,2023-11-15 01:15:19+00:00,2024-05-01 19:59:12+00:00,23,0,0,1,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,429,infra/aws/terraform/kops-infra-ci/terraform.tf,infra/aws/terraform/kops-infra-ci/terraform.tf,0,// todo,// TODO(ameukam): stop used hardcoded account id. Preferably use SSO user,// TODO(ameukam): stop used hardcoded account id. Preferably use SSO user,"terraform {
  backend ""s3"" {
    bucket = ""k8s-infra-kops-ci-tf-state""
    region = ""us-east-2""
    key    = ""kops-infra-ci/terraform.tfstate""
    // TODO(ameukam): stop used hardcoded account id. Preferably use SSO user
    role_arn     = ""arn:aws:iam::808842816990:role/OrganizationAccountAccessRole""
    session_name = ""kops-infra-ci""
  }

  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.11.0""
    }
  }
}
",terraform,"terraform {
  backend ""s3"" {
    bucket = ""k8s-infra-kops-ci-tf-state""
    region = ""us-east-2""
    key    = ""kops-infra-ci/terraform.tfstate""
    // TODO(ameukam): stop used hardcoded account id. Preferably use SSO user
    role_arn     = ""arn:aws:iam::808842816990:role/OrganizationAccountAccessRole""
    session_name = ""kops-infra-ci""
  }

  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.40.0""
    }
  }
}
",terraform,22,22.0,fdb6862000701a42dfee5eacf413cf4bdc633ccb,d67626296482f3df01968377c828ffac093efee8,https://github.com/kubernetes/k8s.io/blob/fdb6862000701a42dfee5eacf413cf4bdc633ccb/infra/aws/terraform/kops-infra-ci/terraform.tf#L22,https://github.com/kubernetes/k8s.io/blob/d67626296482f3df01968377c828ffac093efee8/infra/aws/terraform/kops-infra-ci/terraform.tf#L22,2023-08-08 15:17:59+02:00,2024-03-12 17:14:02+01:00,3,0,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1044,examples/third-party-solutions/wordpress/cloudrun/main.tf,examples/third-party-solutions/wordpress/cloudrun/main.tf,0,todo,# TODO https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/cloud_run_service,ports = [{ # TODO https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/cloud_run_service,"module ""cloud_run"" {
  source     = ""../../../../modules/cloud-run""
  project_id = module.project.project_id
  name     = ""${var.prefix}-cr-wordpress""
  region = var.region
  cloudsql_instances = module.cloudsql.connection_name
  vpc_connector = {
    create          = true
    name            = ""wp-connector""
    egress_settings = ""all-traffic""
  }
  vpc_connector_config = {
    network       = module.vpc.self_link
    ip_cidr_range = ""10.8.0.0/28"" # !!!
  }
#        ""run.googleapis.com/ingress"" = ""internal-and-cloud-load-balancing""

  containers = [{
    image = var.wordpress_image
    ports = [{ # TODO https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/cloud_run_service
      name           = ""http1""
      protocol       = ""TCP""
      container_port = 80
    }]
    options = {
      command  = null
      args     = null
      env_from = null
      env      = {
        ""DB_HOST""        : module.cloudsql.ip #google_sql_database_instance.cloud_sql.private_ip_address
        ""DB_USER""        : local.cloud_sql_conf.user
        ""DB_PASSWORD""    : local.cloud_sql_conf.pass
        ""DB_NAME""        : local.cloud_sql_conf.db
        ""WORDPRESS_DEBUG"": ""1""
      }
    }
    resources = null
    volume_mounts = null
  }]

  iam = {
    ""roles/run.invoker"": [var.cloud_run_invoker]
  }
}
",module,"module ""cloud_run"" { # create the Cloud Run service
  source     = ""../../../../modules/cloud-run""
  project_id = module.project.project_id
  name     = ""${local.prefix}cr-wordpress""
  region = var.region

  containers = [{
    image = var.wordpress_image
    ports = [{
      name           = ""http1""
      protocol       = null
      container_port = var.wordpress_port
    }]
    options = {
      command  = null
      args     = null
      env_from = null
      env      = { # set up the database connection
        ""APACHE_HTTP_PORT_NUMBER""    : var.wordpress_port
        ""WORDPRESS_DATABASE_HOST""    : module.cloudsql.ip
        ""WORDPRESS_DATABASE_NAME""    : local.cloud_sql_conf.db
        ""WORDPRESS_DATABASE_USER""    : local.cloud_sql_conf.user
        ""WORDPRESS_DATABASE_PASSWORD"": local.cloud_sql_conf.pass
        ""WORDPRESS_USERNAME""         : local.wp_user
        ""WORDPRESS_PASSWORD""         : random_password.wp_password.result
      }
    }
    resources = null
    volume_mounts = null
  }]

  iam = {
    ""roles/run.invoker"": [var.cloud_run_invoker]
  }

  revision_annotations = {
    autoscaling = {
      min_scale = 1
      max_scale = 2
    }
    # connect to CloudSQL
    cloudsql_instances = [ module.cloudsql.connection_name ]
    vpcaccess_connector = null
    vpcaccess_egress    = ""all-traffic"" # allow all traffic
  }
  ingress_settings = ""all""

  vpc_connector_create = { # create a VPC connector for the ClouSQL VPC
    ip_cidr_range = var.connector_cidr
    name          = ""${local.prefix}wp-connector""
    vpc_self_link = module.vpc.self_link
  }
}
",module,93,,bcb4a334a0fe06b39831a3c60d501a9fc2a6ca7c,d27f09e7797bfd8567f05966646c94836ee908f2,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/bcb4a334a0fe06b39831a3c60d501a9fc2a6ca7c/examples/third-party-solutions/wordpress/cloudrun/main.tf#L93,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d27f09e7797bfd8567f05966646c94836ee908f2/examples/third-party-solutions/wordpress/cloudrun/main.tf,2022-08-09 09:21:24+00:00,2022-08-31 14:19:54+00:00,3,1,1,1,0,0,1,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,50,modules/data_warehouse/main.tf,modules/data_warehouse/main.tf,0,# todo,# # TODO: File bug for this to be a pickable service account,"# # Get the Pub/Sub service account to trigger the pub/sub notification 
 # # TODO: File bug for this to be a pickable service account","resource ""google_project_iam_member"" ""pub_sub_permissions_token"" {
  project = module.project-services.project_id
  role    = ""roles/iam.serviceAccountTokenCreator""
  member  = ""serviceAccount:service-${data.google_project.project.number}@gcp-sa-pubsub.iam.gserviceaccount.com""
}
",resource,the block associated got renamed or deleted,,667,,9e795e42f78b757e0a92100d368e6bd297a97418,d2c125676f176c8fa33eb9dad12b7ed992dee6ac,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/9e795e42f78b757e0a92100d368e6bd297a97418/modules/data_warehouse/main.tf#L667,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/d2c125676f176c8fa33eb9dad12b7ed992dee6ac/modules/data_warehouse/main.tf,2023-03-24 12:04:44-05:00,2023-09-20 06:49:10-06:00,10,1,0,1,0,1,0,0,0,0
https://github.com/wireapp/wire-server-deploy,1,terraform/modules/aws_vpc_security_groups/main.tf,terraform/modules/aws-vpc-security-groups/main.tf,1,# todo,# TODO: refactor this when creating a second environment. re: names being unique for security groups.,"# TODO: refactor this when creating a second environment. re: names being unique for security groups.  
 # A security group for ssh from the outside world. should only be applied to our bastion hosts.","resource ""aws_security_group"" ""world_ssh_in"" {
  name        = ""world_ssh_in""
  description = ""ssh in from the outside world""
  vpc_id      = var.vpc_id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
    }

  tags = {
    Name = ""world_ssh_in""
  }
}
",resource,"resource ""aws_security_group"" ""world_ssh_in"" {
  name        = ""world_ssh_in""
  description = ""ssh in from the outside world""
  vpc_id      = var.vpc_id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""world_ssh_in""
  }
}
",resource,1,1.0,cb61b733457d5a1ccdb5c9dcb5fde4cab9e5d7e0,e7f081ca744de60bf9d10cb037828b995883d2aa,https://github.com/wireapp/wire-server-deploy/blob/cb61b733457d5a1ccdb5c9dcb5fde4cab9e5d7e0/terraform/modules/aws_vpc_security_groups/main.tf#L1,https://github.com/wireapp/wire-server-deploy/blob/e7f081ca744de60bf9d10cb037828b995883d2aa/terraform/modules/aws-vpc-security-groups/main.tf#L1,2020-04-23 17:54:17+01:00,2020-08-26 16:29:39+02:00,5,0,0,1,0,1,0,1,0,0
https://github.com/alphagov/govuk-aws,955,terraform/projects/infra-security/main.tf,terraform/projects/infra-security/main.tf,0,# todo,# TODO: this cloudtrail bucket is deprecated and is safe to remove,"# TODO: this cloudtrail bucket is deprecated and is safe to remove 
 # once terraform has been applied with force_destroy = true enabled","resource ""aws_s3_bucket"" ""cloudtrail"" {
  bucket        = ""${var.stackname}-${var.aws_environment}-cloudtrail""
  acl           = ""private""
  force_destroy = ""true""

  tags {
    Name        = ""${var.stackname}-${var.aws_environment}-cloudtrail""
    Environment = ""${var.aws_environment}""
  }
}
",resource,the block associated got renamed or deleted,,174,,d29ca713d4d8e26283ac6d693a28cdd0d56bfb1b,305b3fed7dc197332ada901217205b5aae30a0bf,https://github.com/alphagov/govuk-aws/blob/d29ca713d4d8e26283ac6d693a28cdd0d56bfb1b/terraform/projects/infra-security/main.tf#L174,https://github.com/alphagov/govuk-aws/blob/305b3fed7dc197332ada901217205b5aae30a0bf/terraform/projects/infra-security/main.tf,2019-08-12 12:51:32+01:00,2019-10-01 12:04:55+01:00,3,1,1,1,0,0,0,0,0,0
https://github.com/Azure/Avere,22,src/terraform/modules/hammerspace/anvil/outputs.tf,src/terraform/modules/hammerspace/anvil/outputs.tf,0,todo,// TODO - which password works?,// TODO - which password works?,"output ""web_ui_password"" {
  value = azurerm_linux_virtual_machine.anvilvm == null || length(azurerm_linux_virtual_machine.anvilvm) == 0 ? """" : local.is_high_availability ? azurerm_linux_virtual_machine.anvilvm[1].virtual_machine_id : azurerm_linux_virtual_machine.anvilvm[0].virtual_machine_id
}
",output,"output ""web_ui_password"" {
  value = azurerm_linux_virtual_machine.anvilvm == null || length(azurerm_linux_virtual_machine.anvilvm) == 0 ? [] : local.is_high_availability ? [azurerm_linux_virtual_machine.anvilvm[0].virtual_machine_id, azurerm_linux_virtual_machine.anvilvm[1].virtual_machine_id] : [azurerm_linux_virtual_machine.anvilvm[0].virtual_machine_id]
}
",output,9,,47847b35c4a7b8d24584e2c9690ea7a41d5bc3f1,a64c98d189a8f44b087f399cfa0864ecef7b3eeb,https://github.com/Azure/Avere/blob/47847b35c4a7b8d24584e2c9690ea7a41d5bc3f1/src/terraform/modules/hammerspace/anvil/outputs.tf#L9,https://github.com/Azure/Avere/blob/a64c98d189a8f44b087f399cfa0864ecef7b3eeb/src/terraform/modules/hammerspace/anvil/outputs.tf,2021-03-02 06:48:40-05:00,2021-05-02 10:11:01-04:00,6,1,0,1,0,0,0,0,0,1
https://github.com/oracle-terraform-modules/terraform-oci-oke,288,modules/iam/tagging.tf,modules/iam/tagging.tf,0,todo,// TODO Support reactivation of retired tag w/ update,"state            = ""ACTIVE"" // TODO Support reactivation of retired tag w/ update","data ""oci_identity_tags"" ""oke"" {
  count            = var.create_iam_resources ? 1 : 0
  provider         = oci.home
  tag_namespace_id = local.tag_namespace_id_found
  state            = ""ACTIVE"" // TODO Support reactivation of retired tag w/ update
}
",data,the block associated got renamed or deleted,,20,,cf7f4da8a0c56d0350bd86c2ef1011e3f6c2f3b2,e2ac866a96bd7171c980727c46078cc438643225,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/cf7f4da8a0c56d0350bd86c2ef1011e3f6c2f3b2/modules/iam/tagging.tf#L20,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/e2ac866a96bd7171c980727c46078cc438643225/modules/iam/tagging.tf,2023-10-25 16:40:02+11:00,2024-03-28 20:16:45+11:00,5,1,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1748,modules/net-vpc/subnets.tf,modules/net-vpc/subnets.tf,0,fix,# Until https://github.com/hashicorp/terraform-provider-google/issues/16804 is fixed,"# Until https://github.com/hashicorp/terraform-provider-google/issues/16804 is fixed 
 # ignore permadiff in ipv6_access_type for proxy_only subnets","resource ""google_compute_subnetwork"" ""proxy_only"" {
  for_each      = local.subnets_proxy_only
  project       = var.project_id
  network       = local.network.name
  name          = each.value.name
  region        = each.value.region
  ip_cidr_range = each.value.ip_cidr_range
  description = coalesce(
    each.value.description,
    ""Terraform-managed proxy-only subnet for Regional HTTPS, Internal HTTPS or Cross-Regional HTTPS Internal LB.""
  )
  purpose = each.value.global ? ""GLOBAL_MANAGED_PROXY"" : ""REGIONAL_MANAGED_PROXY""
  role    = each.value.active ? ""ACTIVE"" : ""BACKUP""

  lifecycle {
    # Until https://github.com/hashicorp/terraform-provider-google/issues/16804 is fixed
    # ignore permadiff in ipv6_access_type for proxy_only subnets
    ignore_changes = [ipv6_access_type]
  }
}
",resource,"resource ""google_compute_subnetwork"" ""proxy_only"" {
  for_each      = local.subnets_proxy_only
  project       = var.project_id
  network       = local.network.name
  name          = each.value.name
  region        = each.value.region
  ip_cidr_range = each.value.ip_cidr_range
  description = coalesce(
    each.value.description,
    ""Terraform-managed proxy-only subnet for Regional HTTPS, Internal HTTPS or Cross-Regional HTTPS Internal LB.""
  )
  purpose = each.value.global ? ""GLOBAL_MANAGED_PROXY"" : ""REGIONAL_MANAGED_PROXY""
  role    = each.value.active ? ""ACTIVE"" : ""BACKUP""
}
",resource,187,,0d486fb34e64c368f8a4cb8c58092486c2440b7c,93d9b60d54f8b4652cafbdb07a085a670702515b,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/0d486fb34e64c368f8a4cb8c58092486c2440b7c/modules/net-vpc/subnets.tf#L187,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/93d9b60d54f8b4652cafbdb07a085a670702515b/modules/net-vpc/subnets.tf,2023-12-19 11:01:03+01:00,2024-03-05 08:11:06+01:00,4,1,0,1,1,0,1,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,40,test/setup/iam.tf,test/setup/iam.tf,0,// todo,// TODO: Descope,"""roles/owner"" // TODO: Descope","locals {
  int_required_roles = [
    ""roles/bigquery.admin"",
    ""roles/cloudkms.cryptoKeyEncrypterDecrypter"",
    ""roles/owner"" // TODO: Descope
  ]
}
",locals,"locals {
  int_required_roles = [
    ""roles/bigquery.admin"",
    ""roles/cloudkms.cryptoKeyEncrypterDecrypter"",
    ""roles/owner"" // TODO: Descope
  ]
}
",locals,21,21.0,ad3c3472b644fe79c37ae1416b28faf5e0cbe271,d4f61d3ee2427d8d42cab767c0326074c56d2c17,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/ad3c3472b644fe79c37ae1416b28faf5e0cbe271/test/setup/iam.tf#L21,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/d4f61d3ee2427d8d42cab767c0326074c56d2c17/test/setup/iam.tf#L21,2023-02-17 11:54:10-06:00,2023-02-17 14:43:20-06:00,2,0,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,149,community/modules/compute/schedmd-slurm-gcp-v6-nodeset-tpu/main.tf,community/modules/compute/schedmd-slurm-gcp-v6-nodeset-tpu/main.tf,0,# todo,"# TODO: rename to subnetwork_self_link, requires changes to the scripts","# TODO: rename to subnetwork_self_link, requires changes to the scripts","locals {

  nodeset_tpu = {
    node_count_static      = var.node_count_static
    node_count_dynamic_max = var.node_count_dynamic_max
    nodeset_name           = var.name
    node_type              = var.node_type

    accelerator_config = var.accelerator_config
    tf_version         = var.tf_version
    preemptible        = var.preemptible
    preserve_tpu       = var.preserve_tpu

    data_disks   = var.data_disks
    docker_image = var.docker_image

    enable_public_ip = !var.disable_public_ips
    # TODO: rename to subnetwork_self_link, requires changes to the scripts
    subnetwork      = var.subnetwork_self_link
    service_account = var.service_account
    zone            = var.zone
  }
}
",locals,"locals {

  nodeset_tpu = {
    node_count_static      = var.node_count_static
    node_count_dynamic_max = var.node_count_dynamic_max
    nodeset_name           = var.name
    node_type              = var.node_type

    accelerator_config = var.accelerator_config
    tf_version         = var.tf_version
    preemptible        = var.preemptible
    preserve_tpu       = var.preserve_tpu

    data_disks   = var.data_disks
    docker_image = var.docker_image

    enable_public_ip = !var.disable_public_ips
    subnetwork       = var.subnetwork_self_link
    service_account  = var.service_account
    zone             = var.zone
  }
}
",locals,37,,3d1072da48450aa22b844bb5c288415b270616cc,a7adc269a3069a1ab27baed7c8f5e136a3f46f3e,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/3d1072da48450aa22b844bb5c288415b270616cc/community/modules/compute/schedmd-slurm-gcp-v6-nodeset-tpu/main.tf#L37,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/a7adc269a3069a1ab27baed7c8f5e136a3f46f3e/community/modules/compute/schedmd-slurm-gcp-v6-nodeset-tpu/main.tf,2024-01-02 14:51:06-08:00,2024-02-13 17:40:08-08:00,3,1,0,1,0,0,1,0,0,0
https://github.com/nasa/cumulus,21,tf-modules/cumulus/archive.tf,tf-modules/cumulus/archive.tf,0,todo,# TODO Get these dynamically,# TODO Get these dynamically,"module ""archive"" {
  source = ""../archive""

  prefix = var.prefix

  permissions_boundary_arn = var.permissions_boundary_arn

  ecs_cluster_name = aws_ecs_cluster.default.name

  elasticsearch_arn      = ""XXX""
  elasticsearch_hostname = ""XXX""

  system_bucket     = var.system_bucket
  public_buckets    = var.public_buckets
  protected_buckets = var.protected_buckets
  private_buckets   = var.private_buckets

  vpc_id            = var.vpc_id
  lambda_subnet_ids = var.lambda_subnet_ids

  cmr_client_id   = var.cmr_client_id
  cmr_environment = var.cmr_environment
  cmr_provider    = var.cmr_provider
  cmr_username    = var.cmr_username
  cmr_password    = var.cmr_password

  urs_url             = ""https://uat.urs.earthdata.nasa.gov""
  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password

  # TODO Get these dynamically
  dynamo_tables = {
    AccessTokens    = ""${var.prefix}-AccessTokensTable""
    AsyncOperations = ""${var.prefix}-AsyncOperationsTable""
    Collections     = ""${var.prefix}-CollectionsTable""
    Executions      = ""${var.prefix}-ExecutionsTable""
    Granules        = ""${var.prefix}-GranulesTable""
    Pdrs            = ""${var.prefix}-PdrsTable""
    Providers       = ""${var.prefix}-ProvidersTable""
    Rules           = ""${var.prefix}-RulesTable""
    Users           = ""${var.prefix}-UsersTable""
  }
}
",module,"module ""archive"" {
  source = ""../archive""

  prefix = var.prefix

  permissions_boundary_arn = var.permissions_boundary_arn

  ecs_cluster_name = aws_ecs_cluster.default.name

  elasticsearch_domain_arn        = var.elasticsearch_domain_arn
  elasticsearch_hostname          = var.elasticsearch_hostname
  elasticsearch_security_group_id = var.elasticsearch_security_group_id

  ems_host = ""change-ems-host""

  system_bucket     = var.system_bucket
  public_buckets    = var.public_buckets
  protected_buckets = var.protected_buckets
  private_buckets   = var.private_buckets

  vpc_id            = var.vpc_id
  lambda_subnet_ids = var.lambda_subnet_ids

  cmr_client_id   = var.cmr_client_id
  cmr_environment = var.cmr_environment
  cmr_provider    = var.cmr_provider
  cmr_username    = var.cmr_username
  cmr_password    = var.cmr_password

  urs_url             = ""https://uat.urs.earthdata.nasa.gov""
  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password

  token_secret = var.token_secret

  dynamo_tables = var.dynamo_tables

  api_port = var.archive_api_port

  schedule_sf_function_arn      = data.aws_lambda_function.schedule_sf.arn
  message_consumer_function_arn = data.aws_lambda_function.message_consumer.arn
  # TODO This should eventually come from the ingest module
  kinesis_inbound_event_logger = var.kinesis_inbound_event_logger

  # TODO We need to figure out how to make this dynamic
  background_queue_name = ""backgroundProcessing""

  distribution_api_id = module.distribution.rest_api_id
  distribution_url    = module.distribution.distribution_url

  users = var.archive_api_users
}
",module,31,,1da53282470313085da6e713a94458500df71f6c,ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7,https://github.com/nasa/cumulus/blob/1da53282470313085da6e713a94458500df71f6c/tf-modules/cumulus/archive.tf#L31,https://github.com/nasa/cumulus/blob/ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7/tf-modules/cumulus/archive.tf,2019-08-02 16:32:51-04:00,2019-08-14 14:23:38-04:00,2,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1351,blueprints/data-solutions/shielded-folder/main.tf,blueprints/data-solutions/shielded-folder/main.tf,0,#todo,#TODO VPCSC: Access levels,#TODO VPCSC: Access levels,"data ""google_projects"" ""folder-projects"" {
  filter = ""parent.id:${split(""/"", module.folder.id)[1]}""
}
",data,"data ""google_projects"" ""folder-projects"" {
  filter = ""parent.id:${split(""/"", module.folder.id)[1]}""

  depends_on = [
    module.sec-project,
    module.log-export-project
  ]
}
",data,80,,1189e38788529cd0de5483d3505e436778644b38,79373721df7c0d803aba0fbc8eb7cae3c63d747c,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/1189e38788529cd0de5483d3505e436778644b38/blueprints/data-solutions/shielded-folder/main.tf#L80,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/79373721df7c0d803aba0fbc8eb7cae3c63d747c/blueprints/data-solutions/shielded-folder/main.tf,2023-01-25 18:30:21+01:00,2023-08-09 11:23:07+00:00,10,1,0,1,0,1,1,0,0,0
https://github.com/Worklytics/psoxy,5,infra/modules/google-workspace-dwd-connection/main.tf,infra/modules/google-workspace-dwd-connection/main.tf,0,# todo,"# TODO: extract this to its own repo or something, so can consume from our main infra repo. it's","# infra for a Google Workspace API connector 
 #  (eg, OAuth Client in a GCP project)  
 # TODO: extract this to its own repo or something, so can consume from our main infra repo. it's 
 # similar to src/modules/google-workspace-dwd-connector/main.tf in the main infra repo  
 # service account to personify connector","resource ""google_service_account"" ""connector-sa"" {
  account_id   = var.connector_service_account_id
  display_name = var.display_name
  description  = var.description
  project      = var.project_id
}
",resource,"locals {
  # sa_account_ids must be 6-30 chars, and must start with a letter, use only lowercase letters,
  # numbers and - (inside)
  # see https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_service_account

  # trim trailing replaces, replace enclosed spaces with dashes, and everything as lower case
  trimmed_id = lower(replace(trim(var.connector_service_account_id, "" ""), "" "", ""-""))

  # pad if too short
  padded_id = length(local.trimmed_id) < 6 ? ""psoxy-${local.trimmed_id}"" : local.trimmed_id

  # hash if too long
  # TODO: md5 here is 32 chars of hex, so some risk of collision by truncating
  sa_account_id = length(local.padded_id) < 31 ? local.padded_id : substr(md5(local.padded_id), 0, 30)

  instance_id = coalesce(var.instance_id, var.display_name)
}
",locals,4,4.0,1259c535e4d315fea708946a07b95f255b249721,419ab7426298f38d950186bd64303ef628cc2fc5,https://github.com/Worklytics/psoxy/blob/1259c535e4d315fea708946a07b95f255b249721/infra/modules/google-workspace-dwd-connection/main.tf#L4,https://github.com/Worklytics/psoxy/blob/419ab7426298f38d950186bd64303ef628cc2fc5/infra/modules/google-workspace-dwd-connection/main.tf#L4,2021-10-06 09:58:36-07:00,2023-12-20 09:36:51-08:00,24,0,0,1,0,1,0,0,0,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,210,locals.tf,locals.tf,0,fix,"# After manually correcting the file on the server the installation works fine and the taint is removed as planned, so fixing the indentation is up next.","# ToDo: The cilium.yaml file after it has been copied to the server currently has wrong indentation causing the helm template to fail. 
 # After manually correcting the file on the server the installation works fine and the taint is removed as planned, so fixing the indentation is up next.","locals {
  # ssh_agent_identity is not set if the private key is passed directly, but if ssh agent is used, the public key tells ssh agent which private key to use.
  # For terraforms provisioner.connection.agent_identity, we need the public key as a string.
  ssh_agent_identity = var.ssh_private_key == null ? var.ssh_public_key : null

  # If passed, a key already registered within hetzner is used. 
  # Otherwise, a new one will be created by the module.
  hcloud_ssh_key_id = var.hcloud_ssh_key_id == null ? hcloud_ssh_key.k3s[0].id : var.hcloud_ssh_key_id

  ccm_version   = var.hetzner_ccm_version != null ? var.hetzner_ccm_version : data.github_release.hetzner_ccm.release_tag
  csi_version   = var.hetzner_csi_version != null ? var.hetzner_csi_version : data.github_release.hetzner_csi.release_tag
  kured_version = var.kured_version != null ? var.kured_version : data.github_release.kured.release_tag

  common_commands_install_k3s = [
    ""set -ex"",
    # prepare the k3s config directory
    ""mkdir -p /etc/rancher/k3s"",
    # move the config file into place
    ""mv /tmp/config.yaml /etc/rancher/k3s/config.yaml"",
    # if the server has already been initialized just stop here
    ""[ -e /etc/rancher/k3s/k3s.yaml ] && exit 0"",
  ]

  apply_k3s_selinux = [""/sbin/semodule -v -i /usr/share/selinux/packages/k3s.pp""]

  install_k3s_server = concat(local.common_commands_install_k3s, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC=server sh -""
  ], local.apply_k3s_selinux)
  install_k3s_agent = concat(local.common_commands_install_k3s, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC=agent sh -""
  ], local.apply_k3s_selinux)

  control_plane_nodes = merge([
    for pool_index, nodepool_obj in var.control_plane_nodepools : {
      for node_index in range(nodepool_obj.count) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        location : nodepool_obj.location,
        labels : concat(local.default_control_plane_labels, nodepool_obj.labels),
        taints : concat(local.default_control_plane_taints, nodepool_obj.taints),
        index : node_index
      }
    }
  ]...)

  agent_nodes = merge([
    for pool_index, nodepool_obj in var.agent_nodepools : {
      for node_index in range(nodepool_obj.count) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        location : nodepool_obj.location,
        labels : concat(local.default_agent_labels, nodepool_obj.labels),
        taints : concat(local.default_agent_taints, nodepool_obj.taints),
        index : node_index
      }
    }
  ]...)

  # The main network cidr that all subnets will be created upon
  network_ipv4_cidr = ""10.0.0.0/8""

  # The first two subnets are respectively the default subnet 10.0.0.0/16 use for potientially anything and 10.1.0.0/16 used for control plane nodes.
  # the rest of the subnets are for agent nodes in each nodepools.
  network_ipv4_subnets = [for index in range(256) : cidrsubnet(local.network_ipv4_cidr, 8, index)]

  # if we are in a single cluster config, we use the default klipper lb instead of Hetzner LB
  control_plane_count    = sum([for v in var.control_plane_nodepools : v.count])
  agent_count            = sum([for v in var.agent_nodepools : v.count])
  is_single_node_cluster = (local.control_plane_count + local.agent_count) == 1

  using_klipper_lb = var.use_klipper_lb || local.is_single_node_cluster

  # disable k3s extras
  disable_extras = concat([""local-storage""], local.using_klipper_lb ? [] : [""servicelb""], var.traefik_enabled ? [] : [
    ""traefik""
  ], var.metrics_server_enabled ? [] : [""metrics-server""])

  # Default k3s node labels
  default_agent_labels         = concat([], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])
  default_control_plane_labels = concat([], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])

  allow_scheduling_on_control_plane = local.is_single_node_cluster ? true : var.allow_scheduling_on_control_plane

  # Default k3s node taints
  default_control_plane_taints = concat([], local.allow_scheduling_on_control_plane ? [] : [""node-role.kubernetes.io/control-plane:NoSchedule""], var.cni_plugin == ""cilium"" ? [""node.cilium.io/agent-not-ready:NoExecute""] : [])
  default_agent_taints         = concat([], var.cni_plugin == ""cilium"" ? [""node.cilium.io/agent-not-ready:NoExecute""] : [])


  packages_to_install = concat(var.enable_longhorn ? [""open-iscsi"", ""nfs-client""] : [], var.extra_packages_to_install)

  # The following IPs are important to be whitelisted because they communicate with Hetzner services and enable the CCM and CSI to work properly.
  # Source https://github.com/hetznercloud/csi-driver/issues/204#issuecomment-848625566
  hetzner_metadata_service_ipv4 = ""169.254.169.254/32""
  hetzner_cloud_api_ipv4        = ""213.239.246.1/32""

  # internal Pod CIDR, used for the controller and currently for calico
  cluster_cidr_ipv4 = ""10.42.0.0/16""

  whitelisted_ips = [
    local.network_ipv4_cidr,
    local.hetzner_metadata_service_ipv4,
    local.hetzner_cloud_api_ipv4,
    ""127.0.0.1/32"",
  ]

  base_firewall_rules = concat([
    # Allowing internal cluster traffic and Hetzner metadata service and cloud API IPs
    {
      direction  = ""in""
      protocol   = ""tcp""
      port       = ""any""
      source_ips = local.whitelisted_ips
    },
    {
      direction  = ""in""
      protocol   = ""udp""
      port       = ""any""
      source_ips = local.whitelisted_ips
    },
    {
      direction  = ""in""
      protocol   = ""icmp""
      source_ips = local.whitelisted_ips
    },

    # Allow all traffic to the kube api server
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""6443""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },

    # Allow all traffic to the ssh port
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""22""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },

    # Allow basic out traffic
    # ICMP to ping outside services
    {
      direction = ""out""
      protocol  = ""icmp""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    # DNS
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""53""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""out""
      protocol  = ""udp""
      port      = ""53""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    # HTTP(s)
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""80""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""443""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    #NTP
    {
      direction = ""out""
      protocol  = ""udp""
      port      = ""123""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    }
    ], !local.using_klipper_lb ? [] : [
    # Allow incoming web traffic for single node clusters, because we are using k3s servicelb there,
    # not an external load-balancer.
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""80""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""443""
      source_ips = [
        ""0.0.0.0/0""
      ]
    }
    ], var.block_icmp_ping_in ? [] : [
    {
      direction = ""in""
      protocol  = ""icmp""
      source_ips = [
        ""0.0.0.0/0""
      ]
    }
  ])

  labels = {
    ""provisioner"" = ""terraform"",
    ""engine""      = ""k3s""
    ""cluster""     = var.cluster_name
  }

  labels_control_plane_node = {
    role = ""control_plane_node""
  }
  labels_control_plane_lb = {
    role = ""control_plane_lb""
  }

  labels_agent_node = {
    role = ""agent_node""
  }

  cni_install_resources = {
    ""calico"" = [""https://projectcalico.docs.tigera.io/manifests/calico.yaml""]
    ""cilium"" = [""cilium.yaml""]
  }

  cni_install_resource_patches = {
    ""calico"" = [""calico.yaml""]
  }

  cni_k3s_settings = {
    ""flannel"" = {
      disable-network-policy = var.disable_network_policy
    }
    ""calico"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
    ""cilium"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
  }

  # ToDo: The cilium.yaml file after it has been copied to the server currently has wrong indentation causing the helm template to fail.
  # After manually correcting the file on the server the installation works fine and the taint is removed as planned, so fixing the indentation is up next.
  default_cilium_values = <<EOT
ipam:
 operator:
  clusterPoolIPv4PodCIDRList:
   - ${local.cluster_cidr_ipv4}
devices: ""eth1""
EOT
}
",locals,"locals {
  # ssh_agent_identity is not set if the private key is passed directly, but if ssh agent is used, the public key tells ssh agent which private key to use.
  # For terraforms provisioner.connection.agent_identity, we need the public key as a string.
  ssh_agent_identity = var.ssh_private_key == null ? var.ssh_public_key : null

  # If passed, a key already registered within hetzner is used. 
  # Otherwise, a new one will be created by the module.
  hcloud_ssh_key_id = var.hcloud_ssh_key_id == null ? hcloud_ssh_key.k3s[0].id : var.hcloud_ssh_key_id

  ccm_version   = var.hetzner_ccm_version != null ? var.hetzner_ccm_version : data.github_release.hetzner_ccm.release_tag
  csi_version   = var.hetzner_csi_version != null ? var.hetzner_csi_version : data.github_release.hetzner_csi.release_tag
  kured_version = var.kured_version != null ? var.kured_version : data.github_release.kured.release_tag

  common_commands_install_k3s = [
    ""set -ex"",
    # prepare the k3s config directory
    ""mkdir -p /etc/rancher/k3s"",
    # move the config file into place
    ""mv /tmp/config.yaml /etc/rancher/k3s/config.yaml"",
    # if the server has already been initialized just stop here
    ""[ -e /etc/rancher/k3s/k3s.yaml ] && exit 0"",
  ]

  apply_k3s_selinux = [""/sbin/semodule -v -i /usr/share/selinux/packages/k3s.pp""]

  install_k3s_server = concat(local.common_commands_install_k3s, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC=server sh -""
  ], local.apply_k3s_selinux)
  install_k3s_agent = concat(local.common_commands_install_k3s, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC=agent sh -""
  ], local.apply_k3s_selinux)

  control_plane_nodes = merge([
    for pool_index, nodepool_obj in var.control_plane_nodepools : {
      for node_index in range(nodepool_obj.count) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        location : nodepool_obj.location,
        labels : concat(local.default_control_plane_labels, nodepool_obj.labels),
        taints : concat(local.default_control_plane_taints, nodepool_obj.taints),
        index : node_index
      }
    }
  ]...)

  agent_nodes = merge([
    for pool_index, nodepool_obj in var.agent_nodepools : {
      for node_index in range(nodepool_obj.count) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        location : nodepool_obj.location,
        labels : concat(local.default_agent_labels, nodepool_obj.labels),
        taints : concat(local.default_agent_taints, nodepool_obj.taints),
        index : node_index
      }
    }
  ]...)

  # The main network cidr that all subnets will be created upon
  network_ipv4_cidr = ""10.0.0.0/8""

  # The first two subnets are respectively the default subnet 10.0.0.0/16 use for potientially anything and 10.1.0.0/16 used for control plane nodes.
  # the rest of the subnets are for agent nodes in each nodepools.
  network_ipv4_subnets = [for index in range(256) : cidrsubnet(local.network_ipv4_cidr, 8, index)]

  # if we are in a single cluster config, we use the default klipper lb instead of Hetzner LB
  control_plane_count    = sum([for v in var.control_plane_nodepools : v.count])
  agent_count            = sum([for v in var.agent_nodepools : v.count])
  is_single_node_cluster = (local.control_plane_count + local.agent_count) == 1

  using_klipper_lb = var.use_klipper_lb || local.is_single_node_cluster

  # disable k3s extras
  disable_extras = concat([""local-storage""], local.using_klipper_lb ? [] : [""servicelb""], var.traefik_enabled ? [] : [
    ""traefik""
  ], var.metrics_server_enabled ? [] : [""metrics-server""])

  # Default k3s node labels
  default_agent_labels         = concat([], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])
  default_control_plane_labels = concat([], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])

  allow_scheduling_on_control_plane = local.is_single_node_cluster ? true : var.allow_scheduling_on_control_plane

  # Default k3s node taints
  default_control_plane_taints = concat([], local.allow_scheduling_on_control_plane ? [] : [""node-role.kubernetes.io/control-plane:NoSchedule""], var.cni_plugin == ""cilium"" ? [""node.cilium.io/agent-not-ready:NoExecute""] : [])
  default_agent_taints         = concat([], var.cni_plugin == ""cilium"" ? [""node.cilium.io/agent-not-ready:NoExecute""] : [])


  packages_to_install = concat(var.enable_longhorn ? [""open-iscsi"", ""nfs-client""] : [], var.extra_packages_to_install)

  # The following IPs are important to be whitelisted because they communicate with Hetzner services and enable the CCM and CSI to work properly.
  # Source https://github.com/hetznercloud/csi-driver/issues/204#issuecomment-848625566
  hetzner_metadata_service_ipv4 = ""169.254.169.254/32""
  hetzner_cloud_api_ipv4        = ""213.239.246.1/32""

  # internal Pod CIDR, used for the controller and currently for calico
  cluster_cidr_ipv4 = ""10.42.0.0/16""

  whitelisted_ips = [
    local.network_ipv4_cidr,
    local.hetzner_metadata_service_ipv4,
    local.hetzner_cloud_api_ipv4,
    ""127.0.0.1/32"",
  ]

  base_firewall_rules = concat([
    # Allowing internal cluster traffic and Hetzner metadata service and cloud API IPs
    {
      direction  = ""in""
      protocol   = ""tcp""
      port       = ""any""
      source_ips = local.whitelisted_ips
    },
    {
      direction  = ""in""
      protocol   = ""udp""
      port       = ""any""
      source_ips = local.whitelisted_ips
    },
    {
      direction  = ""in""
      protocol   = ""icmp""
      source_ips = local.whitelisted_ips
    },

    # Allow all traffic to the kube api server
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""6443""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },

    # Allow all traffic to the ssh port
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""22""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },

    # Allow basic out traffic
    # ICMP to ping outside services
    {
      direction = ""out""
      protocol  = ""icmp""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    # DNS
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""53""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""out""
      protocol  = ""udp""
      port      = ""53""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    # HTTP(s)
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""80""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""443""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    #NTP
    {
      direction = ""out""
      protocol  = ""udp""
      port      = ""123""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    }
    ], !local.using_klipper_lb ? [] : [
    # Allow incoming web traffic for single node clusters, because we are using k3s servicelb there,
    # not an external load-balancer.
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""80""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""443""
      source_ips = [
        ""0.0.0.0/0""
      ]
    }
    ], var.block_icmp_ping_in ? [] : [
    {
      direction = ""in""
      protocol  = ""icmp""
      source_ips = [
        ""0.0.0.0/0""
      ]
    }
  ])

  labels = {
    ""provisioner"" = ""terraform"",
    ""engine""      = ""k3s""
    ""cluster""     = var.cluster_name
  }

  labels_control_plane_node = {
    role = ""control_plane_node""
  }
  labels_control_plane_lb = {
    role = ""control_plane_lb""
  }

  labels_agent_node = {
    role = ""agent_node""
  }

  cni_install_resources = {
    ""calico"" = [""https://projectcalico.docs.tigera.io/manifests/calico.yaml""]
  }

  cni_install_resource_patches = {
    ""calico"" = [""calico.yaml""]
  }

  cni_k3s_settings = {
    ""flannel"" = {
      disable-network-policy = var.disable_network_policy
    }
    ""calico"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
    ""cilium"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
  }

  default_cilium_values = <<EOT
ipam:
 operator:
  clusterPoolIPv4PodCIDRList:
   - ${local.cluster_cidr_ipv4}
devices: ""eth1""
EOT
}
",locals,273,,ea97125426c35d9572981496464344a0bbd830a0,dd90ec37ba3ede05e2ec5c10f0e3024423ed1ab8,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/ea97125426c35d9572981496464344a0bbd830a0/locals.tf#L273,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/dd90ec37ba3ede05e2ec5c10f0e3024423ed1ab8/locals.tf,2022-08-14 05:16:56+02:00,2022-08-14 11:19:58+02:00,2,1,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1249,blueprints/cloud-operations/network-dashboard/main.tf,blueprints/cloud-operations/network-dashboard/main.tf,0,# todo,# TODO: service_account_email,# TODO: service_account_email,"module ""cloud-function"" {
  v2          = var.cf_version == ""V2""
  source      = ""../../../modules/cloud-function""
  project_id  = local.monitoring_project
  name        = ""network-dashboard-cloud-function""
  bucket_name = ""${local.monitoring_project}-network-dashboard-bucket""
  bucket_config = {
    location = var.region
  }
  region = var.region

  bundle_config = {
    source_dir  = ""cloud-function""
    output_path = ""cloud-function.zip""
  }

  function_config = {
    timeout     = 480 # Timeout in seconds, increase it if your CF timeouts and use v2 if > 9 minutes.
    entry_point = ""main""
    runtime     = ""python39""
    instances   = 1
    memory      = 256 # Memory in MB

  }

  environment_variables = {
    MONITORED_PROJECTS_LIST = local.projects
    MONITORED_FOLDERS_LIST  = local.folders
    MONITORING_PROJECT_ID   = local.monitoring_project
    ORGANIZATION_ID         = var.organization_id
    CF_VERSION              = var.cf_version
  }

  service_account = module.service-account-function.email
  # Internal only doesn't seem to work with CFv2:
  ingress_settings = var.cf_version == ""V2"" ? ""ALLOW_ALL"" : ""ALLOW_INTERNAL_ONLY""

  trigger_config = var.cf_version == ""V2"" ? {
    v2 = {
      event_type   = ""google.cloud.pubsub.topic.v1.messagePublished""
      pubsub_topic = module.pubsub.topic.id
      # TODO: service_account_email
    }
    } : {
    v1 = {
      event    = ""google.pubsub.topic.publish""
      resource = module.pubsub.topic.id
    }
  }
}
",module,"module ""cloud-function"" {
  v2          = var.cf_version == ""V2""
  source      = ""../../../modules/cloud-function""
  project_id  = local.monitoring_project
  name        = ""network-dashboard-cloud-function""
  bucket_name = ""${local.monitoring_project}-network-dashboard-bucket""
  bucket_config = {
    location = var.region
  }
  region = var.region

  bundle_config = {
    source_dir  = ""cloud-function""
    output_path = ""cloud-function.zip""
  }

  function_config = {
    timeout     = 480 # Timeout in seconds, increase it if your CF timeouts and use v2 if > 9 minutes.
    entry_point = ""main""
    runtime     = ""python39""
    instances   = 1
    memory      = 256 # Memory in MB

  }

  environment_variables = {
    MONITORED_PROJECTS_LIST = local.projects
    MONITORED_FOLDERS_LIST  = local.folders
    MONITORING_PROJECT_ID   = local.monitoring_project
    ORGANIZATION_ID         = var.organization_id
    CF_VERSION              = var.cf_version
  }

  service_account = module.service-account-function.email
  # Internal only doesn't seem to work with CFv2:
  ingress_settings = var.cf_version == ""V2"" ? ""ALLOW_ALL"" : ""ALLOW_INTERNAL_ONLY""

  trigger_config = var.cf_version == ""V2"" ? {
    v2 = {
      event_type             = ""google.cloud.pubsub.topic.v1.messagePublished""
      pubsub_topic           = module.pubsub.topic.id
      service_account_create = true
    }
    } : {
    v1 = {
      event    = ""google.pubsub.topic.publish""
      resource = module.pubsub.topic.id
    }
  }
}
",module,174,,9e37a915c8ae3a675dfd0815f654a665bd701d21,755ff7b1d2fb9aef3627efd19cbd720fa8cddf68,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9e37a915c8ae3a675dfd0815f654a665bd701d21/blueprints/cloud-operations/network-dashboard/main.tf#L174,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/755ff7b1d2fb9aef3627efd19cbd720fa8cddf68/blueprints/cloud-operations/network-dashboard/main.tf,2022-11-16 16:44:01+01:00,2022-11-16 16:44:01+01:00,2,1,1,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-kubernetes-engine,9,modules/safer-cluster/main.tf,modules/safer-cluster/main.tf,0,// todo,// TODO(mmontan): link to a couple of policies.,"// Define PodSecurityPolicies for differnet applications. 
 // TODO(mmontan): link to a couple of policies.","module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // Disable the dashboard. It creates risk by running as a very sensitive user.
  kubernetes_dashboard = false

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy          = true
  network_policy_provider = ""CALICO""

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  disable_legacy_metadata_endpoints = true

  node_pools        = var.node_pools
  node_pools_labels = var.node_pools_labels

  // TODO(mmontan): check whether we need to restrict these
  // settings.
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  // TODO(mmontan): we generally considered applying
  // just the cloud-platofrm scope and use Cloud IAM
  // If we have Workload Identity, are there advantages
  // in restricting scopes even more?
  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  // We should use IP Alias.
  configure_ip_masq = false

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = length(var.compute_engine_service_account) > 0 ? false : true
  service_account        = var.compute_engine_service_account

  // TODO(mmontan): define a registry_project parameter in the private_beta_cluster,
  // so that we can give GCS permissions to the service account on a project
  // that hosts only container-images and not data.
  grant_registry_access = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // TODO(mmontan): link to a couple of policies.
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id
  node_metadata                    = ""SECURE""

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // TODO(mmontan): investigate whether this should be a recommended setting
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group
}
",module,"module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy = true

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  node_pools        = var.node_pools
  node_pools_labels = var.node_pools_labels

  // TODO(mmontan): check whether we need to restrict these
  // settings.
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = var.compute_engine_service_account == """" ? true : false
  service_account        = var.compute_engine_service_account
  registry_project_id    = var.registry_project_id
  grant_registry_access  = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // Example: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#example
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // Intranode Visibility enables you to capture flow logs for traffic between pods and create FW rules that apply to traffic between pods.
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group

  enable_shielded_nodes = var.enable_shielded_nodes
}
",module,145,,e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5,5a194719faa144ad0a7ee578663d336358f5073c,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5/modules/safer-cluster/main.tf#L145,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/5a194719faa144ad0a7ee578663d336358f5073c/modules/safer-cluster/main.tf,2019-10-04 15:21:33-07:00,2019-11-13 15:10:12-06:00,6,1,1,0,0,1,0,0,0,0
https://github.com/CDCgov/prime-simplereport,38,ops/services/postgres_db/main.tf,ops/services/postgres_db/main.tf,0,# todo,### TODO: delete the old configuration above once all environments have been cut,"### New Postgres Flexible Server configuration 
 ### TODO: delete the old configuration above once all environments have been cut 
 ### over to the new DBs so Terraform can clean up the old infrastructure. ","resource ""azurerm_postgresql_flexible_server"" ""db"" {
  name                = ""simple-report-${var.env}-flexible-db""
  location            = var.rg_location
  resource_group_name = var.rg_name
  sku_name            = var.env == ""prod"" ? ""MO_Standard_E8ds_v4"" : ""MO_Standard_E4ds_v4""
  version             = ""13""
  delegated_subnet_id = var.subnet_id
  private_dns_zone_id = var.dns_zone_id

  // TODO: replace with commented-out line below when removing old DB config
  administrator_login = ""simple_report""
  //administrator_login    = var.administrator_login
  administrator_password = data.azurerm_key_vault_secret.db_password.value

  storage_mb                   = 524288 // 512 GB
  backup_retention_days        = 7
  geo_redundant_backup_enabled = false

  tags = var.tags

  // Time is Eastern
  maintenance_window {
    day_of_week  = 0
    start_hour   = 0
    start_minute = 0
  }

  # See note at https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/postgresql_flexible_server#high_availability
  lifecycle {
    ignore_changes = [
      zone,
      high_availability.0.standby_availability_zone
    ]
  }
}
",resource,"resource ""azurerm_postgresql_flexible_server"" ""db"" {
  name                = ""simple-report-${var.env}-flexible-db""
  location            = var.rg_location
  resource_group_name = var.rg_name
  sku_name            = var.env == ""prod"" ? ""MO_Standard_E8ds_v4"" : ""MO_Standard_E4ds_v4""
  version             = ""13""
  delegated_subnet_id = var.subnet_id


  administrator_login    = var.administrator_login
  administrator_password = data.azurerm_key_vault_secret.db_password.value

  storage_mb                   = 524288 // 512 GB
  backup_retention_days        = 7
  geo_redundant_backup_enabled = false

  tags = var.tags

  // Time is Eastern
  maintenance_window {
    day_of_week  = 0
    start_hour   = 0
    start_minute = 0
  }

  # See note at https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/postgresql_flexible_server#high_availability
  lifecycle {
    ignore_changes = [
      zone,
      high_availability.0.standby_availability_zone
    ]
  }
}
",resource,65,,3e4185fa549937cff9213c325bc0fee780e41eb0,fb9a18eb71f31044ca7a58958a25dea489263ca7,https://github.com/CDCgov/prime-simplereport/blob/3e4185fa549937cff9213c325bc0fee780e41eb0/ops/services/postgres_db/main.tf#L65,https://github.com/CDCgov/prime-simplereport/blob/fb9a18eb71f31044ca7a58958a25dea489263ca7/ops/services/postgres_db/main.tf,2022-02-16 12:59:39-05:00,2022-04-28 23:27:57-04:00,2,1,1,1,0,0,0,1,0,0
https://github.com/ministryofjustice/cloud-platform-infrastructure,221,terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf,terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf,0,fix,# Pinned the version until this fix get merged https://github.com/terraform-aws-modules/terraform-aws-eks/pull/1447,"# Issue in v17.1.0, where each plan will have a change for the templates, this cause our divergence pipeline fail"" 
 # Pinned the version until this fix get merged https://github.com/terraform-aws-modules/terraform-aws-eks/pull/1447","locals {
  node_groups_count = {
    live    = ""54""
    manager = ""4""
    default = ""3""
  }

  node_size = {
    live    = [""r5.xlarge"", ""r4.xlarge""]
    manager = [""m5.xlarge"", ""m4.xlarge""]
    default = [""m5.large"", ""m4.large""]
  }

  monitoring_node_size = {
    live    = [""r5.2xlarge"", ""r4.2xlarge""]
    manager = [""t3.medium"", ""t2.medium""]
    default = [""t3.medium"", ""t2.medium""]
  }

  default_ng = {
    desired_capacity = lookup(local.node_groups_count, terraform.workspace, local.node_groups_count[""default""])
    max_capacity     = 60
    min_capacity     = 1
    subnets          = data.aws_subnet_ids.private.ids

    create_launch_template = true
    pre_userdata = templatefile(""${path.module}/templates/user-data.tpl"", {
      dockerhub_credentials = base64encode(""${var.dockerhub_user}:${var.dockerhub_token}"")
    })

    # Issue in v17.1.0, where each plan will have a change for the templates, this cause our divergence pipeline fail""
    # Pinned the version until this fix get merged https://github.com/terraform-aws-modules/terraform-aws-eks/pull/1447
    launch_template_version = ""1""

    instance_types = lookup(local.node_size, terraform.workspace, local.node_size[""default""])
    k8s_labels = {
      Terraform = ""true""
      Cluster   = terraform.workspace
      Domain    = local.fqdn
    }
    additional_tags = {
      default_ng    = ""true""
      application   = ""moj-cloud-platform""
      business-unit = ""platforms""
    }
  }

  monitoring_ng = {
    desired_capacity = 2
    max_capacity     = 3
    min_capacity     = 1
    subnets          = [sort(data.aws_subnet_ids.private.ids)[2]]

    create_launch_template = true
    pre_userdata = templatefile(""${path.module}/templates/user-data.tpl"", {
      dockerhub_credentials = base64encode(""${var.dockerhub_user}:${var.dockerhub_token}"")
    })

    # Issue in v17.1.0, where each plan will have a change for the templates, this cause our divergence pipeline fail""
    # Pinned the version until this fix get merged https://github.com/terraform-aws-modules/terraform-aws-eks/pull/1447
    launch_template_version = ""1""

    instance_types = lookup(local.monitoring_node_size, terraform.workspace, local.monitoring_node_size[""default""])
    k8s_labels = {
      Terraform                                     = ""true""
      ""cloud-platform.justice.gov.uk/monitoring-ng"" = ""true""
      Cluster                                       = terraform.workspace
      Domain                                        = local.fqdn
    }
    additional_tags = {
      monitoring_ng = ""true""
      application   = ""moj-cloud-platform""
      business-unit = ""platforms""
    }
    taints = [
      {
        key    = ""monitoring-node""
        value  = true
        effect = ""NO_SCHEDULE""
      }
    ]
  }

}
",locals,"locals {
  # desired_capcity change is a manual step after initial cluster creation (when no cluster-autoscaler)
  # https://github.com/terraform-aws-modules/terraform-aws-eks/issues/835
  node_groups_count = {
    live    = ""54""
    manager = ""4""
    default = ""3""
  }

  node_size = {
    live    = [""r5.xlarge"", ""r4.xlarge""]
    manager = [""m5.xlarge"", ""m4.xlarge""]
    default = [""m5.large"", ""m4.large""]
  }

  monitoring_node_size = {
    live    = [""r5.2xlarge"", ""r4.2xlarge""]
    manager = [""t3.medium"", ""t2.medium""]
    default = [""t3.medium"", ""t2.medium""]
  }

  default_ng = {
    desired_capacity = lookup(local.node_groups_count, terraform.workspace, local.node_groups_count[""default""])
    max_capacity     = 60
    min_capacity     = 1
    subnets          = data.aws_subnet_ids.private.ids

    create_launch_template = true
    pre_userdata = templatefile(""${path.module}/templates/user-data.tpl"", {
      dockerhub_credentials = base64encode(""${var.dockerhub_user}:${var.dockerhub_token}"")
    })

    instance_types = lookup(local.node_size, terraform.workspace, local.node_size[""default""])
    k8s_labels = {
      Terraform = ""true""
      Cluster   = terraform.workspace
      Domain    = local.fqdn
    }
    additional_tags = {
      default_ng    = ""true""
      application   = ""moj-cloud-platform""
      business-unit = ""platforms""
    }
  }

  monitoring_ng = {
    desired_capacity = 2
    max_capacity     = 3
    min_capacity     = 1
    subnets          = data.aws_subnet_ids.private_zone_2b.ids

    create_launch_template = true
    pre_userdata = templatefile(""${path.module}/templates/user-data.tpl"", {
      dockerhub_credentials = base64encode(""${var.dockerhub_user}:${var.dockerhub_token}"")
    })

    instance_types = lookup(local.monitoring_node_size, terraform.workspace, local.monitoring_node_size[""default""])
    k8s_labels = {
      Terraform                                     = ""true""
      ""cloud-platform.justice.gov.uk/monitoring-ng"" = ""true""
      Cluster                                       = terraform.workspace
      Domain                                        = local.fqdn
    }
    additional_tags = {
      monitoring_ng = ""true""
      application   = ""moj-cloud-platform""
      business-unit = ""platforms""
    }
    taints = [
      {
        key    = ""monitoring-node""
        value  = true
        effect = ""NO_SCHEDULE""
      }
    ]
  }

}
",locals,79,,811baa9677ab082b6f6b3f88277bdb3e259ddf0f,708bae2fed32eb1c2d97e6929605a5f021f572a2,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/811baa9677ab082b6f6b3f88277bdb3e259ddf0f/terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf#L79,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/708bae2fed32eb1c2d97e6929605a5f021f572a2/terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf,2021-08-26 14:44:12+01:00,2021-11-08 20:11:02+00:00,9,1,0,1,1,0,0,0,0,0
https://github.com/CDCgov/prime-simplereport,35,ops/services/postgres_db/_output.tf,ops/services/postgres_db/_output.tf,0,todo,// TODO - change these to azurerm_postgresql_flexible_server when removing the old DB config,// TODO - change these to azurerm_postgresql_flexible_server when removing the old DB config,"output ""server_name"" {
  value = azurerm_postgresql_server.db.name
}
",output,"output ""server_name"" {
  value = azurerm_postgresql_flexible_server.db.name
}
",output,1,,3e4185fa549937cff9213c325bc0fee780e41eb0,fb9a18eb71f31044ca7a58958a25dea489263ca7,https://github.com/CDCgov/prime-simplereport/blob/3e4185fa549937cff9213c325bc0fee780e41eb0/ops/services/postgres_db/_output.tf#L1,https://github.com/CDCgov/prime-simplereport/blob/fb9a18eb71f31044ca7a58958a25dea489263ca7/ops/services/postgres_db/_output.tf,2022-02-16 12:59:39-05:00,2022-04-28 23:27:57-04:00,2,1,1,1,0,0,0,0,0,0
https://github.com/compiler-explorer/infra,17,terraform/lc.tf,terraform/lc.tf,0,// todo,"// TODO: once this is proven right, add `name` to each of these","// TODO: once this is proven right, add `name` to each of these","resource ""aws_launch_configuration"" ""CompilerExplorer-beta-c5"" {
  image_id = ""${local.image_id}""
  instance_type = ""c5.large""
  iam_instance_profile = ""XaniaBlog""
  key_name = ""mattgodbolt""
  security_groups = [
    ""${aws_security_group.CompilerExplorer.id}""]
  associate_public_ip_address = true
  user_data = ""${local.beta_user_data}""
  enable_monitoring = false
  ebs_optimized = true
  spot_price = ""0.05""

  root_block_device {
    volume_type = ""gp2""
    volume_size = 10
    delete_on_termination = true
  }
}
",resource,"resource ""aws_launch_configuration"" ""CompilerExplorer-beta-c5"" {
  lifecycle {
    create_before_destroy = true
  }

  name = ""compiler-explorer-beta-c5""
  image_id = ""${local.image_id}""
  instance_type = ""c5.large""
  iam_instance_profile = ""XaniaBlog""
  key_name = ""mattgodbolt""
  security_groups = [
    ""${aws_security_group.CompilerExplorer.id}""
  ]
  associate_public_ip_address = true
  user_data = ""${local.beta_user_data}""
  enable_monitoring = false
  ebs_optimized = true
  spot_price = ""0.05""

  root_block_device {
    volume_type = ""gp2""
    volume_size = 10
    delete_on_termination = true
  }
}
",resource,6,,52b74b9730d4cb1537f8ff48e00f9cde7727eaf5,6f10aecb7bbf12c4f77a19f10c88e9f8770fb28c,https://github.com/compiler-explorer/infra/blob/52b74b9730d4cb1537f8ff48e00f9cde7727eaf5/terraform/lc.tf#L6,https://github.com/compiler-explorer/infra/blob/6f10aecb7bbf12c4f77a19f10c88e9f8770fb28c/terraform/lc.tf,2018-11-05 17:43:45-06:00,2018-11-05 18:27:54-06:00,2,1,1,1,0,0,0,0,0,0
https://github.com/uyuni-project/sumaform,208,modules/libvirt/base/main.tf,modules/libvirt/base/main.tf,0,hack,// HACK: work around https://github.com/hashicorp/terraform/issues/9549,// HACK: work around https://github.com/hashicorp/terraform/issues/9549,"output ""configuration"" {
  // HACK: work around https://github.com/hashicorp/terraform/issues/9549
  value = ""${
    map(
      ""opensuse421"", ""${libvirt_volume.opensuse421.id}"",
      ""sles11sp3"", ""${libvirt_volume.sles11sp3.id}"",
      ""sles11sp4"", ""${libvirt_volume.sles11sp4.id}"",
      ""sles12"", ""${libvirt_volume.sles12.id}"",
      ""sles12sp1"", ""${libvirt_volume.sles12sp1.id}"",
      ""network_name"", ""${var.network_name}"",

      ""cc_username"", ""${var.cc_username}"",
      ""cc_password"", ""${var.cc_password}"",
      ""package_mirror"", ""${replace(var.package_mirror, ""/^$/"", ""null"")}"",
      ""pool"", ""${var.pool}"",
      ""bridge"", ""${var.bridge}"",
      ""use_avahi"", ""${element(list(""False"", ""True""), var.use_avahi)}"",
      ""domain"", ""${var.domain}"",
      ""name_prefix"", ""${var.name_prefix}""
    )
  }""
}
",output,"output ""configuration"" {
  value = {
    network_name = ""${var.network_name}""
    cc_username = ""${var.cc_username}""
    cc_password = ""${var.cc_password}""
    package_mirror = ""${var.package_mirror == """" ? ""null"" : var.package_mirror}""
    pool = ""${var.pool}""
    bridge = ""${var.bridge}""
    use_avahi = ""${var.use_avahi == 1 ? ""True"" : ""False""}""
    domain = ""${var.domain}""
    name_prefix = ""${var.name_prefix}""
  }
}
",output,32,,0d3a83d22f57360dec522bcf1e5dd7adb3b9f9c3,e61be3bacea9a5e1a8f88284838880959fe67bfd,https://github.com/uyuni-project/sumaform/blob/0d3a83d22f57360dec522bcf1e5dd7adb3b9f9c3/modules/libvirt/base/main.tf#L32,https://github.com/uyuni-project/sumaform/blob/e61be3bacea9a5e1a8f88284838880959fe67bfd/modules/libvirt/base/main.tf,2016-11-04 17:34:25+01:00,2017-02-23 07:20:04+01:00,10,1,0,0,1,0,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,206,examples/tls-with-aws-pca-issuer/main.tf,patterns/tls-with-aws-pca-issuer/main.tf,1,workaround,# Using kubectl to workaround kubernetes provider issue https://github.com/hashicorp/terraform-provider-kubernetes/issues/1453,"#------------------------------- 
 # This resource creates a CRD of Certificate Kind, which then represents certificate issued from ACM PCA, 
 # mounted as K8 secret 
 #-------------------------------  
 # Using kubectl to workaround kubernetes provider issue https://github.com/hashicorp/terraform-provider-kubernetes/issues/1453","resource ""kubectl_manifest"" ""example_pca_certificate"" {
  yaml_body = yamlencode({
    apiVersion = ""cert-manager.io/v1""
    kind       = ""Certificate""

    metadata = {
      name      = var.certificate_name
      namespace = ""default""
    }

    spec = {
      commonName = var.certificate_dns
      duration   = ""2160h0m0s""
      issuerRef = {
        group = ""awspca.cert-manager.io""
        kind  = ""AWSPCAClusterIssuer""
        name : module.eks_blueprints.eks_cluster_id
      }
      renewBefore = ""360h0m0s""
      secretName  = join(""-"", [var.certificate_name, ""clusterissuer""]) # This is the name with which the K8 Secret will be available
      usages = [
        ""server auth"",
        ""client auth""
      ]
      privateKey = {
        algorithm : ""RSA""
        size : 2048
      }
    }
  })

  depends_on = [
    module.eks_blueprints_kubernetes_addons,
    kubectl_manifest.cluster_pca_issuer,
  ]
}
",resource,"resource ""kubectl_manifest"" ""pca_certificate"" {
  yaml_body = yamlencode({
    apiVersion = ""cert-manager.io/v1""
    kind       = ""Certificate""

    metadata = {
      name      = var.certificate_name
      namespace = ""default""
    }

    spec = {
      commonName = var.certificate_dns
      duration   = ""2160h0m0s""
      issuerRef = {
        group = ""awspca.cert-manager.io""
        kind  = ""AWSPCAClusterIssuer""
        name : module.eks.cluster_name
      }
      renewBefore = ""360h0m0s""
      secretName  = join(""-"", [var.certificate_name, ""clusterissuer""]) # This is the name with which the K8 Secret will be available
      usages = [
        ""server auth"",
        ""client auth""
      ]
      privateKey = {
        algorithm : ""RSA""
        size : 2048
      }
    }
  })

  depends_on = [
    kubectl_manifest.cluster_pca_issuer,
  ]
}
",resource,175,194.0,5602e3dad7e60945fd99df1bd0d52345e8a495f5,26309565d04b8b1cc2fc3bed7b227768ebb25e3a,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/5602e3dad7e60945fd99df1bd0d52345e8a495f5/examples/tls-with-aws-pca-issuer/main.tf#L175,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/26309565d04b8b1cc2fc3bed7b227768ebb25e3a/patterns/tls-with-aws-pca-issuer/main.tf#L194,2022-05-03 09:05:20-07:00,2024-04-22 09:49:57-07:00,20,0,1,0,1,0,0,0,0,0
https://github.com/camptocamp/devops-stack,97,examples/kind/main.tf,examples/kind/main.tf,0,fix,# TODO fix: the base domain is defined later. Proposal: remove redirection from traefik module and add it in dependent modules.,"# TODO fix: the base domain is defined later. Proposal: remove redirection from traefik module and add it in dependent modules. 
 # For now random value is passed to base_domain. Redirections will not work before fix.","module ""traefik"" {
  source = ""git::https://github.com/camptocamp/devops-stack-module-traefik.git//kind?ref=v1.0.0""

  cluster_name = local.cluster_name

  # TODO fix: the base domain is defined later. Proposal: remove redirection from traefik module and add it in dependent modules.
  # For now random value is passed to base_domain. Redirections will not work before fix.
  base_domain = ""placeholder.com""

  argocd_namespace = module.argocd_bootstrap.argocd_namespace

  dependency_ids = {
    argocd = module.argocd_bootstrap.id
  }
}
",module,"module ""traefik"" {
  source = ""git::https://github.com/camptocamp/devops-stack-module-traefik.git//kind?ref=v6.2.0""

  argocd_project = local.cluster_name

  app_autosync           = local.app_autosync
  enable_service_monitor = local.enable_service_monitor

  dependency_ids = {
    argocd = module.argocd_bootstrap.id
  }
}
",module,72,,e3e1a35b6a90a2990878d6c06775a6dba94637af,10d0d93a6e34b9b8e29beca8c6e01ec6ca3a8244,https://github.com/camptocamp/devops-stack/blob/e3e1a35b6a90a2990878d6c06775a6dba94637af/examples/kind/main.tf#L72,https://github.com/camptocamp/devops-stack/blob/10d0d93a6e34b9b8e29beca8c6e01ec6ca3a8244/examples/kind/main.tf,2023-05-16 13:05:31+02:00,2024-03-15 09:27:05+01:00,29,1,1,1,0,0,1,0,0,0
https://github.com/CDCgov/prime-simplereport,93,ops/services/postgres_db/vault.tf,ops/services/postgres_db/vault.tf,0,//todo,"//TODO: Change this to use a TF-generated password, like db-password-no-phi. See #3673 for the additional work.","//TODO: Change this to use a TF-generated password, like db-password-no-phi. See #3673 for the additional work.","data ""azurerm_key_vault_secret"" ""db_password"" {
  name         = ""simple-report-${var.env_level}-db-password""
  key_vault_id = var.global_vault_id
}
",data,"data ""azurerm_key_vault_secret"" ""db_password"" {
  name         = ""simple-report-${var.env}-db-password""
  key_vault_id = var.global_vault_id
}
",data,8,8.0,c617c1ab4838016aa686ecff84fde0e6be45b8a5,295bb12754a8f740634cd16d7a3442d18f5d0216,https://github.com/CDCgov/prime-simplereport/blob/c617c1ab4838016aa686ecff84fde0e6be45b8a5/ops/services/postgres_db/vault.tf#L8,https://github.com/CDCgov/prime-simplereport/blob/295bb12754a8f740634cd16d7a3442d18f5d0216/ops/services/postgres_db/vault.tf#L8,2022-05-05 00:48:15-05:00,2023-10-05 19:10:51+00:00,2,0,0,1,0,1,0,0,0,0
https://github.com/nebari-dev/nebari,34,src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/monitoring/loki/main.tf,src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/monitoring/loki/main.tf,0,implement,# We configure MinIO by using the AWS config because MinIO implements the S3 API,# We configure MinIO by using the AWS config because MinIO implements the S3 API,"resource ""helm_release"" ""grafana-loki"" {
  name       = ""nebari-loki""
  namespace  = var.namespace
  repository = ""https://grafana.github.io/helm-charts""
  chart      = ""loki""
  version    = var.loki-helm-chart-version

  values = concat([
    file(""${path.module}/values_loki.yaml""),
    jsonencode({
      loki : {
        storage : {
          s3 : {
            endpoint : local.minio-url,
            accessKeyId : ""admin""
            secretAccessKey : random_password.minio_root_password.result,
            s3ForcePathStyle : true
          }
        }
      }
      storageConfig : {
        # We configure MinIO by using the AWS config because MinIO implements the S3 API
        aws : {
          s3 : local.minio-url
          s3ForcePathStyle : true
        }
      }
      write : { nodeSelector : local.node-selector }
      read : { nodeSelector : local.node-selector }
      backend : { nodeSelector : local.node-selector }
      gateway : { nodeSelector : local.node-selector }
    })
  ], var.grafana-loki-overrides)

  depends_on = [helm_release.loki-minio]
}
",resource,"resource ""helm_release"" ""grafana-loki"" {
  name       = ""nebari-loki""
  namespace  = var.namespace
  repository = ""https://grafana.github.io/helm-charts""
  chart      = ""loki""
  version    = var.loki-helm-chart-version

  values = concat([
    file(""${path.module}/values_loki.yaml""),
    jsonencode({
      loki : {
        storage : {
          s3 : {
            endpoint : local.minio-url,
            accessKeyId : ""admin""
            secretAccessKey : random_password.minio_root_password.result,
            s3ForcePathStyle : true
          }
        }
      }
      storageConfig : {
        # We configure MinIO by using the AWS config because MinIO implements the S3 API
        aws : {
          s3 : local.minio-url
          s3ForcePathStyle : true
        }
      }
      write : { nodeSelector : local.node-selector }
      read : { nodeSelector : local.node-selector }
      backend : { nodeSelector : local.node-selector }
      gateway : { nodeSelector : local.node-selector }
    })
  ], var.grafana-loki-overrides)

  depends_on = [helm_release.loki-minio]
}
",resource,72,72.0,0210a47a1acbe4940e0f6ff18fb72f630e224230,0210a47a1acbe4940e0f6ff18fb72f630e224230,https://github.com/nebari-dev/nebari/blob/0210a47a1acbe4940e0f6ff18fb72f630e224230/src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/monitoring/loki/main.tf#L72,https://github.com/nebari-dev/nebari/blob/0210a47a1acbe4940e0f6ff18fb72f630e224230/src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/monitoring/loki/main.tf#L72,2024-03-08 16:15:52-03:00,2024-03-08 16:15:52-03:00,1,0,1,0,0,1,0,0,1,0
https://github.com/wireapp/wire-server-deploy,34,terraform/modules/aws-vpc-security-groups/main.tf,terraform/modules/aws-vpc-security-groups/main.tf,0,hack,"# HACK: running out of security groups, adding this here since all k8s nodes need to talk to S3.","# HACK: running out of security groups, adding this here since all k8s nodes need to talk to S3. 
 # S3","resource ""aws_security_group"" ""k8s_private"" {
  name        = ""k8s_private""
  description = ""hosts that are allowed to the private ports of the kubernetes nodes.""
  vpc_id      = var.vpc_id

  # FIXME: tighten this up.
  egress {
    from_port   = 0
    to_port     = 65535
    protocol    = ""tcp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  # FIXME: tighten this up. need UDP for flannel.
  egress {
    from_port   = 0
    to_port     = 65535
    protocol    = ""udp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  # HACK: running out of security groups, adding this here since all k8s nodes need to talk to S3.
  # S3
  egress {
    description = """"
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = var.s3_CIDRs
  }

  tags = {
    Name = ""k8s_private""
  }
}
",resource,"resource ""aws_security_group"" ""k8s_private"" {
  name        = ""k8s_private""
  description = ""hosts that are allowed to the private ports of the kubernetes nodes.""
  vpc_id      = var.vpc_id

  # FIXME: tighten this up.
  egress {
    from_port   = 0
    to_port     = 65535
    protocol    = ""tcp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  # FIXME: tighten this up. need UDP for flannel.
  egress {
    from_port   = 0
    to_port     = 65535
    protocol    = ""udp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  tags = {
    Name = ""k8s_private""
  }
}
",resource,291,,f239eeced44a73fb235171e9af52b1776e6ed6fc,e7f081ca744de60bf9d10cb037828b995883d2aa,https://github.com/wireapp/wire-server-deploy/blob/f239eeced44a73fb235171e9af52b1776e6ed6fc/terraform/modules/aws-vpc-security-groups/main.tf#L291,https://github.com/wireapp/wire-server-deploy/blob/e7f081ca744de60bf9d10cb037828b995883d2aa/terraform/modules/aws-vpc-security-groups/main.tf,2020-07-13 19:15:38+01:00,2020-08-26 16:29:39+02:00,2,1,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,761,examples/cloud-operations/network-dashboard/variables.tf,blueprints/cloud-operations/network-dashboard/variables.tf,1,# todo,# TODO: support folder instead of a list of projects?,# TODO: support folder instead of a list of projects?,"variable ""monitored_projects_list"" {
  type        = list(string)
  description = ""ID of the projects to be monitored (where limits and quotas data will be pulled)""
}
",variable,"variable ""monitored_projects_list"" {
  type        = list(string)
  description = ""ID of the projects to be monitored (where limits and quotas data will be pulled)""
}
",variable,36,,9f3ee4dc2299a7e9034cf25988d9c4b59c0fd70b,a709febfdb4234cec10da9d4949ebe889f20a6b9,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9f3ee4dc2299a7e9034cf25988d9c4b59c0fd70b/examples/cloud-operations/network-dashboard/variables.tf#L36,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/a709febfdb4234cec10da9d4949ebe889f20a6b9/blueprints/cloud-operations/network-dashboard/variables.tf,2022-03-08 18:36:02+01:00,2022-09-30 10:51:16+02:00,8,1,0,1,0,0,0,0,0,0
https://github.com/cattle-ops/terraform-aws-gitlab-runner,103,modules/terminate-agent-hook/iam.tf,modules/terminate-agent-hook/iam.tf,0,fix,# checkov:skip=CKV_AWS_356:False positive and fixed with version 2.3.293,"  # checkov:skip=CKV_AWS_111:I didn't found any condition to limit the access.
  # checkov:skip=CKV_AWS_356:False positive and fixed with version 2.3.293","data ""aws_iam_policy_document"" ""spot_request_housekeeping"" {
  # checkov:skip=CKV_AWS_111:I didn't found any condition to limit the access.
  # checkov:skip=CKV_AWS_356:False positive and fixed with version 2.3.293
  statement {
    sid = ""SpotRequestHousekeepingList""

    effect = ""Allow""
    actions = [
      ""ec2:CancelSpotInstanceRequests"",
      ""ec2:DescribeSpotInstanceRequests""
    ]
    # I didn't found any condition to limit the access
    resources = [""*""]
  }
}
",data,"data ""aws_iam_policy_document"" ""spot_request_housekeeping"" {
  # checkov:skip=CKV_AWS_111:I didn't found any condition to limit the access.
  # checkov:skip=CKV_AWS_356:False positive and fixed with version 2.3.293
  statement {
    sid = ""SpotRequestHousekeepingList""

    effect = ""Allow""
    actions = [
      ""ec2:CancelSpotInstanceRequests"",
      ""ec2:DescribeSpotInstanceRequests""
    ]
    # I didn't found any condition to limit the access
    resources = [""*""]
  }
}
",data,121,123.0,f25a86b5ada3e78a56c33db07a0110354f5e2e5d,8b76c0fd68deca6b40bd372ddb07f38abc7e668c,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/f25a86b5ada3e78a56c33db07a0110354f5e2e5d/modules/terminate-agent-hook/iam.tf#L121,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/8b76c0fd68deca6b40bd372ddb07f38abc7e668c/modules/terminate-agent-hook/iam.tf#L123,2023-06-15 09:42:30+02:00,2023-11-09 10:29:27+01:00,2,0,0,0,0,1,0,0,0,1
https://github.com/Worklytics/psoxy,97,infra/modules/aws-psoxy-instance/main.tf,infra/modules/aws-psoxy-instance/main.tf,0,#todo,#TODO: match on subpath equivalent to var.function_name ?,#TODO: match on subpath equivalent to var.function_name ?,"resource ""aws_apigatewayv2_integration"" ""map"" {
  api_id                    = var.api_gateway.id
  integration_type          = ""AWS_PROXY""
  connection_type           = ""INTERNET""

  #TODO: match on subpath equivalent to var.function_name ?

  integration_method        = ""POST""
  integration_uri           = aws_lambda_function.psoxy-instance.invoke_arn
  request_parameters        = {}
  request_templates         = {}
}
",resource,"resource ""aws_apigatewayv2_integration"" ""map"" {
  api_id                    = var.api_gateway.id
  integration_type          = ""AWS_PROXY""
  connection_type           = ""INTERNET""

  integration_method        = ""POST""
  integration_uri           = aws_lambda_function.psoxy-instance.invoke_arn
  request_parameters        = {}
  request_templates         = {}
}
",resource,18,,7444d0de8dc052089b9c9dc91394cf5172660cdd,2ed09153e988500e594a556bbc9f6bd64c986bad,https://github.com/Worklytics/psoxy/blob/7444d0de8dc052089b9c9dc91394cf5172660cdd/infra/modules/aws-psoxy-instance/main.tf#L18,https://github.com/Worklytics/psoxy/blob/2ed09153e988500e594a556bbc9f6bd64c986bad/infra/modules/aws-psoxy-instance/main.tf,2022-01-06 11:41:26-08:00,2022-01-12 16:14:43-08:00,9,1,0,1,0,0,1,0,0,0
https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules,1,modules/vm-series/variables.tf,modules/vm-series/variables.tf,0,fix,# FIXME remove,"default = ""-"" # FIXME remove","variable ""sep"" {
  default = ""-"" # FIXME remove
}
",variable,the block associated got renamed or deleted,,70,,14e7264e068be5eb612e06b46b346a1e4e1ce2ad,74d261f5175126cb4f9d187bcc2c99bdf49f616a,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/14e7264e068be5eb612e06b46b346a1e4e1ce2ad/modules/vm-series/variables.tf#L70,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/74d261f5175126cb4f9d187bcc2c99bdf49f616a/modules/vm-series/variables.tf,2021-01-26 10:48:30+01:00,2021-01-26 10:48:30+01:00,3,1,0,1,0,0,0,0,0,0
https://github.com/pingcap/tidb-operator,2,deploy/aws/main.tf,deploy/aws/main.tf,0,hack,# HACK: depends_on for the helm and kubernetes provider,"# HACK: depends_on for the helm and kubernetes provider 
 # Passing provider configuration value via a local_file","resource ""local_file"" ""kubeconfig"" {
  # HACK: depends_on for the helm and kubernetes provider
  # Passing provider configuration value via a local_file
  depends_on = [""module.eks""]
  sensitive_content = ""${module.eks.kubeconfig}""
  filename = ""${path.module}/credentials/kubeconfig_${var.cluster_name}""
}
",resource,the block associated got renamed or deleted,,157,,13d859cd8db08b594a35bda795516a174c6dc6c1,58095783adeb0ca30ecdf094a7e8db7f17fb5919,https://github.com/pingcap/tidb-operator/blob/13d859cd8db08b594a35bda795516a174c6dc6c1/deploy/aws/main.tf#L157,https://github.com/pingcap/tidb-operator/blob/58095783adeb0ca30ecdf094a7e8db7f17fb5919/deploy/aws/main.tf,2019-05-01 21:36:07-07:00,2019-07-03 11:55:46+08:00,4,1,1,1,1,0,0,0,0,0
https://github.com/uyuni-project/sumaform,9,libvirt_host/main.tf,libvirt_host/main.tf,0,hack,// HACK: hostname is taken from VM metadata in order to,"// HACK: hostname is taken from VM metadata in order to 
 // establish dependencies with other modules","output ""hostname"" {
    // HACK: hostname is taken from VM metadata in order to
    // establish dependencies with other modules
    value = ""${libvirt_domain.domain.metadata}""
}
",output,"output ""hostname"" {
    // HACK: this output artificially depends on the domain id
    // any resource using this output will have to wait until domain is fully up
    value = ""${coalesce(""${var.name}.${var.domain}"", libvirt_domain.domain.id)}""
}
",output,68,,0cade995634cfcf24a1c0535c858f0711427ff3b,7b44a14d4d6a74a6e0387ceb7238c62df9765b92,https://github.com/uyuni-project/sumaform/blob/0cade995634cfcf24a1c0535c858f0711427ff3b/libvirt_host/main.tf#L68,https://github.com/uyuni-project/sumaform/blob/7b44a14d4d6a74a6e0387ceb7238c62df9765b92/libvirt_host/main.tf,2016-06-30 10:33:47+02:00,2016-09-05 14:18:52+02:00,11,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,804,fast/stages/00-bootstrap/cicd.tf,fast/stages/00-bootstrap/cicd.tf,0,# todo,# TODO: map null provider to Cloud Build once we add support for it,# TODO: map null provider to Cloud Build once we add support for it,"locals {
  # TODO: map null provider to Cloud Build once we add support for it
  cicd_repositories = {
    for k, v in coalesce(var.cicd_repositories, {}) : k => v
    if(
      v != null
      &&
      contains(keys(local.identity_providers), v.identity_provider)
      &&
      fileexists(""${path.module}/templates/workflow-${v.type}.yaml"")
    )
  }
  cicd_service_accounts = {
    for k, v in module.automation-tf-cicd-sa :
    k => v.iam_email
  }
}
",locals,"locals {
  cicd_repositories = {
    for k, v in coalesce(var.cicd_repositories, {}) : k => v
    if(
      v != null
      &&
      (
        v.type == ""sourcerepo""
        ||
        contains(keys(local.identity_providers), coalesce(v.identity_provider, "":""))
      )
      &&
      fileexists(""${path.module}/templates/workflow-${v.type}.yaml"")
    )
  }
  cicd_workflow_providers = {
    bootstrap = ""00-bootstrap-providers.tf""
    resman    = ""01-resman-providers.tf""
  }
  cicd_workflow_var_files = {
    bootstrap = []
    resman = [
      ""00-bootstrap.auto.tfvars.json"",
      ""globals.auto.tfvars.json""
    ]
  }
}
",locals,20,,725f7effce7bdb69522b8ad004b78cda31dcb7ce,44ae2671b0d1e8bbf8aca6ca815c68b56080a8cb,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/725f7effce7bdb69522b8ad004b78cda31dcb7ce/fast/stages/00-bootstrap/cicd.tf#L20,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/44ae2671b0d1e8bbf8aca6ca815c68b56080a8cb/fast/stages/00-bootstrap/cicd.tf,2022-04-12 08:17:27+02:00,2022-06-08 11:34:08+02:00,2,1,0,1,0,0,0,1,0,0
https://github.com/wireapp/wire-server-deploy,28,terraform/modules/aws-vpc-security-groups/main.tf,terraform/modules/aws-vpc-security-groups/main.tf,0,hack,# HACK: running out of security groups per instance.,"# HACK: running out of security groups per instance. 
 #       adding this here since the admin node needs to talk to S3. 
 # S3","resource ""aws_security_group"" ""talk_to_k8s"" {
  name        = ""talk_to_k8s""
  description = ""hosts that are allowed to speak to kubernetes.""
  vpc_id      = var.vpc_id

  # HACK: running out of security groups per instance.
  #       adding this here since the admin node needs to talk to S3.
  # S3
  egress {
    description = """"
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = var.s3_CIDRs
  }

  # kubectl
  egress {
    description = """"
    from_port   = 6443
    to_port     = 6443
    protocol    = ""tcp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  # the application itsself.
  egress {
    description = """"
    from_port   = 31772
    to_port     = 31773
    protocol    = ""tcp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  tags = {
    Name = ""talk_to_k8s""
  }
}
",resource,"resource ""aws_security_group"" ""talk_to_k8s"" {
  name        = ""talk_to_k8s""
  description = ""hosts that are allowed to speak to kubernetes.""
  vpc_id      = var.vpc_id

  # kubectl
  egress {
    description = """"
    from_port   = 6443
    to_port     = 6443
    protocol    = ""tcp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  # the application itsself.
  egress {
    description = """"
    from_port   = 31772
    to_port     = 31773
    protocol    = ""tcp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  tags = {
    Name = ""talk_to_k8s""
  }
}
",resource,187,,f239eeced44a73fb235171e9af52b1776e6ed6fc,e7f081ca744de60bf9d10cb037828b995883d2aa,https://github.com/wireapp/wire-server-deploy/blob/f239eeced44a73fb235171e9af52b1776e6ed6fc/terraform/modules/aws-vpc-security-groups/main.tf#L187,https://github.com/wireapp/wire-server-deploy/blob/e7f081ca744de60bf9d10cb037828b995883d2aa/terraform/modules/aws-vpc-security-groups/main.tf,2020-07-13 19:15:38+01:00,2020-08-26 16:29:39+02:00,2,1,1,1,0,1,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,15,eks.tf,eks.tf,0,#todo,#TODO Refer to internal AWS-IA module,"manage_aws_auth = false # Replaced by the auth.tf file  
 #TODO Refer to internal AWS-IA module","module ""eks"" {
  create_eks      = var.create_eks
  manage_aws_auth = false # Replaced by the auth.tf file

  #TODO Refer to internal AWS-IA module
  source  = ""terraform-aws-modules/eks/aws""
  version = ""17.1.0""

  cluster_name    = module.eks-label.id
  cluster_version = var.kubernetes_version

  # NETWORK CONFIG
  vpc_id  = var.create_vpc == false ? var.vpc_id : module.vpc.vpc_id
  subnets = var.create_vpc == false ? var.private_subnet_ids : module.vpc.private_subnets

  cluster_endpoint_private_access = var.endpoint_private_access
  cluster_endpoint_public_access  = var.endpoint_public_access

  # IRSA
  enable_irsa            = var.enable_irsa
  kubeconfig_output_path = ""./kubeconfig/""

  # TAGS
  tags = module.eks-label.tags

  # CLUSTER LOGGING
  cluster_enabled_log_types = var.enabled_cluster_log_types

  # CLUSTER ENCRYPTION
  cluster_encryption_config = [
    {
      provider_key_arn = aws_kms_key.eks.arn
      resources = [
      ""secrets""]
    }
  ]
}
",module,"module ""eks"" {
  source = ""git@github.com:maheshr-amzn/terraform-aws-eks_cluster?ref=feature/aws-eks_cluster-development""
  # version = ???

  create_eks      = var.create_eks
  manage_aws_auth = false     # Replaced by the auth.tf file

  eks_cluster_name    = module.eks-label.id
  eks_cluster_version = var.kubernetes_version

  # NETWORK CONFIG
  vpc_id  = var.create_vpc == false ? var.vpc_id : module.vpc.vpc_id
  subnets = var.create_vpc == false ? var.private_subnet_ids : module.vpc.private_subnets

  eks_cluster_endpoint_private_access = var.endpoint_private_access
  eks_cluster_endpoint_public_access  = var.endpoint_public_access

  # IRSA
  enable_irsa            = var.enable_irsa
//  kubeconfig_output_path = ""./kubeconfig/""

  # TAGS
  tags = module.eks-label.tags

  # CLUSTER LOGGING
  enabled_cluster_log_types = var.enabled_cluster_log_types

  # CLUSTER ENCRYPTION
  cluster_encryption_config = [
    {
      provider_key_arn = aws_kms_key.eks.arn
      resources = [
      ""secrets""]
    }
  ]
}
",module,43,,125390ed86df57dc9d8064df92b36e14cc8eb3e2,35137a9a2e6959bc4ce5ffb977c913914757bf1a,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/125390ed86df57dc9d8064df92b36e14cc8eb3e2/eks.tf#L43,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/35137a9a2e6959bc4ce5ffb977c913914757bf1a/eks.tf,2021-09-13 14:12:34+01:00,2021-09-21 10:41:45+01:00,2,1,1,1,0,0,0,0,0,0
https://github.com/Azure/terraform-azurerm-caf-enterprise-scale,4,archetypes/locals.role_definitions.tf,modules/archetypes/locals.role_definitions.tf,1,implemented,# Logic implemented to determine whether Role Definitions,"# Generate the Role Definition configurations for the specified archetype. 
 # Logic implemented to determine whether Role Definitions 
 # need to be loaded to save on compute and memory resources 
 # when none defined in archetype definition.","locals {
  archetype_role_definitions_list      = local.archetype_definition.role_definitions
  archetype_role_definitions_specified = try(length(local.archetype_role_definitions_list) > 0, false)
}
",locals,"locals {
  archetype_role_definitions_list      = local.archetype_definition.role_definitions
  archetype_role_definitions_specified = try(length(local.archetype_role_definitions_list) > 0, false)
}
",locals,2,2.0,a05be92e463f90e577936a84395ffb88cfd045e7,1b41f7062f286e790be9b2e16a74562936f691df,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/a05be92e463f90e577936a84395ffb88cfd045e7/archetypes/locals.role_definitions.tf#L2,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/1b41f7062f286e790be9b2e16a74562936f691df/modules/archetypes/locals.role_definitions.tf#L2,2020-09-25 20:39:19+01:00,2022-01-27 09:57:26+00:00,7,0,0,0,0,1,0,0,0,0
https://github.com/claranet/terraform-datadog-monitors,26,databases/elasticsearch/monitors-elasticsearch.tf,database/elasticsearch/monitors-elasticsearch.tf,1,todo,// TODO add tags to filter by node type and do not apply this monitor on non-data nodes,// TODO add tags to filter by node type and do not apply this monitor on non-data nodes,"resource ""datadog_monitor"" ""flush_latency"" {
  name    = ""[${var.environment}] Elasticsearch average index flushing to disk latency {{#is_alert}}{{{comparator}}} {{threshold}}ms ({{value}}ms){{/is_alert}}{{#is_warning}}{{{comparator}}} {{warn_threshold}}ms ({{value}}ms){{/is_warning}}""
  message = ""${coalesce(var.flush_latency_message, var.message)}""

  type = ""query alert""

  // TODO add tags to filter by node type and do not apply this monitor on non-data nodes
  query = <<EOF
  ${var.flush_latency_time_aggregator}(${var.flush_latency_timeframe}):
    avg:elasticsearch.flush.total.time{${data.template_file.filter.rendered}} by {node_name} / avg:elasticsearch.flush.total{${data.template_file.filter.rendered}} by {node_name} * 1000
  > ${var.flush_latency_threshold_critical}
EOF

  thresholds {
    warning  = ""${var.flush_latency_threshold_warning}""
    critical = ""${var.flush_latency_threshold_critical}""
  }

  notify_audit        = false
  locked              = false
  include_tags        = true
  require_full_window = true
  notify_no_data      = true

  evaluation_delay = ""${var.evaluation_delay}""

  silenced = ""${var.flush_latency_silenced}""

  tags = [
    ""resource:elasticsearch"",
    ""env:${var.environment}"",
    ""created_by:terraform"",
    ""${var.flush_latency_extra_tags}"",
  ]
}
",resource,"resource ""datadog_monitor"" ""flush_latency"" {
  count   = var.flush_latency_enabled == ""true"" ? 1 : 0
  name    = ""${var.prefix_slug == """" ? """" : ""[${var.prefix_slug}]""}[${var.environment}] Elasticsearch average index flushing to disk latency {{#is_alert}}{{{comparator}}} {{threshold}}ms ({{value}}ms){{/is_alert}}{{#is_warning}}{{{comparator}}} {{warn_threshold}}ms ({{value}}ms){{/is_warning}}""
  message = coalesce(var.flush_latency_message, var.message)
  type    = ""query alert""

  // TODO add tags to filter by node type and do not apply this monitor on non-data nodes
  query = <<EOQ
  ${var.flush_latency_time_aggregator}(${var.flush_latency_timeframe}):
    default(
      diff(avg:elasticsearch.flush.total.time${module.filter-tags.query_alert} by {node_name}) /
      diff(avg:elasticsearch.flush.total${module.filter-tags.query_alert} by {node_name})
    * 1000, 0)
  > ${var.flush_latency_threshold_critical}
EOQ

  monitor_thresholds {
    warning  = var.flush_latency_threshold_warning
    critical = var.flush_latency_threshold_critical
  }

  evaluation_delay    = var.evaluation_delay
  new_host_delay      = var.new_host_delay
  new_group_delay     = var.new_group_delay
  notify_audit        = false
  include_tags        = true
  require_full_window = true
  notify_no_data      = false

  tags = concat(local.common_tags, var.tags, var.flush_latency_extra_tags)
}
",resource,438,397.0,0500de1330cba0c19e4b4e39187cb8affe96452b,a449a1360a67ebf9abddf3e4e6ff6fa17c50b12a,https://github.com/claranet/terraform-datadog-monitors/blob/0500de1330cba0c19e4b4e39187cb8affe96452b/databases/elasticsearch/monitors-elasticsearch.tf#L438,https://github.com/claranet/terraform-datadog-monitors/blob/a449a1360a67ebf9abddf3e4e6ff6fa17c50b12a/database/elasticsearch/monitors-elasticsearch.tf#L397,2018-08-30 16:19:34+02:00,2023-12-12 16:00:34+01:00,31,0,0,1,0,0,0,0,1,0
https://github.com/cloudfoundry/bosh-bootloader,2,plan-patches/cfcr-azure/terraform/cfcr_sg_override.tf,plan-patches/cfcr-azure/terraform/cfcr_sg_override.tf,0,# todo,"# TODO: after the new cpi release, switch to cfcr resource group.","# TODO: after the new cpi release, switch to cfcr resource group.","resource ""azurerm_network_security_group"" ""cfcr-worker"" {
  name                = ""${var.env_id}-cfcr-worker-sg""
  location            = ""${var.region}""
  # TODO: after the new cpi release, switch to cfcr resource group.
  resource_group_name          = ""${azurerm_resource_group.bosh.name}""
}",resource,the block associated got renamed or deleted,,25,,5169911b36b56c3d7da1815cfc1b4b5e2c9e46eb,f098a6c6ca76caf96d25a1469be65f7cb7d5c514,https://github.com/cloudfoundry/bosh-bootloader/blob/5169911b36b56c3d7da1815cfc1b4b5e2c9e46eb/plan-patches/cfcr-azure/terraform/cfcr_sg_override.tf#L25,https://github.com/cloudfoundry/bosh-bootloader/blob/f098a6c6ca76caf96d25a1469be65f7cb7d5c514/plan-patches/cfcr-azure/terraform/cfcr_sg_override.tf,2018-10-09 15:43:19-07:00,2018-10-29 09:34:36-07:00,2,1,0,1,0,1,1,0,0,0
https://github.com/jenkins-x/terraform-google-jx,3,main.tf,main.tf,0,// todo,// TODO: replace with the following when parent_domain deprecations is complete: apex_domain  = var.apex_domain,// TODO: replace with the following when parent_domain deprecations is complete: apex_domain  = var.apex_domain,"locals {
  interpolated_content = templatefile(""${path.module}/modules/jx-requirements.yml.tpl"", {
    gcp_project                 = var.gcp_project
    zone                        = var.cluster_location
    cluster_name                = local.cluster_name
    git_owner_requirement_repos = var.git_owner_requirement_repos
    dev_env_approvers           = var.dev_env_approvers
    lets_encrypt_production     = var.lets_encrypt_production
    // Storage buckets
    log_storage_url        = module.cluster.log_storage_url
    report_storage_url     = module.cluster.report_storage_url
    repository_storage_url = module.cluster.repository_storage_url
    backup_bucket_url      = module.backup.backup_bucket_url
    // Vault
    external_vault = local.external_vault
    vault_bucket   = length(module.vault) > 0 ? module.vault[0].vault_bucket_name : """"
    vault_key      = length(module.vault) > 0 ? module.vault[0].vault_key : """"
    vault_keyring  = length(module.vault) > 0 ? module.vault[0].vault_keyring : """"
    vault_name     = length(module.vault) > 0 ? module.vault[0].vault_name : """"
    vault_sa       = length(module.vault) > 0 ? module.vault[0].vault_sa : """"
    vault_url      = var.vault_url
    // Velero
    enable_backup    = var.enable_backup
    velero_sa        = module.backup.velero_sa
    velero_namespace = module.backup.backup_bucket_url != """" ? var.velero_namespace : """"
    velero_schedule  = var.velero_schedule
    velero_ttl       = var.velero_ttl
    // DNS
    // TODO: remove parent_domain when its deprecations is complete: domain_enabled = var.apex_domain != """" ? true : false
    domain_enabled = var.apex_domain != """" ? true : (var.parent_domain != """" ? true : false)
    // TODO: replace with the following when parent_domain deprecations is complete: apex_domain  = var.apex_domain
    apex_domain    = var.apex_domain != """" ? var.apex_domain : var.parent_domain
    subdomain      = var.subdomain
    tls_email      = var.tls_email

    version_stream_ref = var.version_stream_ref
    version_stream_url = var.version_stream_url
    webhook            = var.webhook
  })

  split_content   = split(""\n"", local.interpolated_content)
  compact_content = compact(local.split_content)
  content         = join(""\n"", local.compact_content)
}
",locals,"locals {
  requirements_file = var.jx2 ? ""${path.module}/modules/jx-requirements.yml.tpl"" : ""${path.module}/modules/jx-requirements-v3.yml.tpl""
  interpolated_content = templatefile(local.requirements_file, {
    gcp_project                 = var.gcp_project
    zone                        = var.cluster_location
    cluster_name                = local.cluster_name
    git_owner_requirement_repos = var.git_owner_requirement_repos
    dev_env_approvers           = var.dev_env_approvers
    lets_encrypt_production     = var.lets_encrypt_production
    // GCP Artifact
    enable_artifact        = var.artifact_enable
    registry               = module.cluster.artifact_registry_repository
    docker_registry_org    = module.cluster.artifact_registry_repository_name
    // Storage buckets
    log_storage_url        = module.cluster.log_storage_url
    report_storage_url     = module.cluster.report_storage_url
    repository_storage_url = module.cluster.repository_storage_url
    backup_bucket_url      = module.backup.backup_bucket_url
    // Vault
    external_vault  = local.external_vault
    vault_bucket    = length(module.vault) > 0 ? module.vault[0].vault_bucket_name : """"
    vault_key       = length(module.vault) > 0 ? module.vault[0].vault_key : """"
    vault_keyring   = length(module.vault) > 0 ? module.vault[0].vault_keyring : """"
    vault_name      = length(module.vault) > 0 ? module.vault[0].vault_name : """"
    vault_sa        = length(module.vault) > 0 ? module.vault[0].vault_sa : """"
    vault_url       = var.vault_url
    vault_installed = !var.gsm ? true : false
    // Velero
    enable_backup    = var.enable_backup
    velero_sa        = module.backup.velero_sa
    velero_namespace = module.backup.backup_bucket_url != """" ? var.velero_namespace : """"
    velero_schedule  = var.velero_schedule
    velero_ttl       = var.velero_ttl
    // DNS
    // TODO: remove parent_domain when its deprecations is complete: domain_enabled = var.apex_domain != """" ? true : false
    domain_enabled = var.apex_domain != """" ? true : (var.parent_domain != """" ? true : false)
    // TODO: replace with the following when parent_domain deprecations is complete: apex_domain  = var.apex_domain
    apex_domain = var.apex_domain != """" ? var.apex_domain : var.parent_domain
    subdomain   = var.subdomain
    tls_email   = var.tls_email
    // Kuberhealthy
    kuberhealthy = var.kuberhealthy

    version_stream_ref = var.version_stream_ref
    version_stream_url = var.version_stream_url
    webhook            = var.webhook
  })

  split_content   = split(""\n"", local.interpolated_content)
  compact_content = compact(local.split_content)
  content         = join(""\n"", local.compact_content)
}
",locals,281,306.0,ffc0b68673f1e9341ef89e88f6af157631a9086d,6d61937b9d1a81a879246040a6f6e11ed5626a4e,https://github.com/jenkins-x/terraform-google-jx/blob/ffc0b68673f1e9341ef89e88f6af157631a9086d/main.tf#L281,https://github.com/jenkins-x/terraform-google-jx/blob/6d61937b9d1a81a879246040a6f6e11ed5626a4e/main.tf#L306,2020-12-09 11:35:14+10:00,2024-04-25 13:02:59+02:00,30,0,0,1,0,0,1,0,0,0
https://github.com/compiler-explorer/infra,116,terraform/ec2.tf,terraform/ec2.tf,0,todo,// TODO make 4xlarge or similar,// TODO make 4xlarge or similar,"resource ""aws_instance"" ""BuilderNode"" {
  ami                         = local.builder_image_id
  // TODO bring into the fold
  iam_instance_profile        = ""GccBuilder""
  ebs_optimized               = true
  // TODO make 4xlarge or similar
  instance_type               = ""c5d.large""
  monitoring                  = false
  key_name                    = ""mattgodbolt""
  subnet_id                   = aws_subnet.ce-1a.id
  // TODO reconsider, make an SG specifically for builder
  vpc_security_group_ids      = [aws_security_group.AdminNode.id]
  associate_public_ip_address = true
  source_dest_check           = false

  root_block_device {
    volume_type           = ""gp2""
    volume_size           = 24
    delete_on_termination = true
  }

  tags = {
    Name = ""Builder-New""
  }
}
",resource,"resource ""aws_instance"" ""BuilderNode"" {
  ami                         = local.builder_image_id
  // TODO bring into the fold
  iam_instance_profile        = ""GccBuilder""
  ebs_optimized               = true
  instance_type               = ""c5d.4xlarge""
  monitoring                  = false
  key_name                    = ""mattgodbolt""
  subnet_id                   = aws_subnet.ce-1a.id
  // TODO reconsider, make an SG specifically for builder
  vpc_security_group_ids      = [aws_security_group.AdminNode.id]
  associate_public_ip_address = true
  source_dest_check           = false

  root_block_device {
    volume_type           = ""gp2""
    volume_size           = 24
    delete_on_termination = true
  }


  tags = {
    Name = ""Builder""
  }
}
",resource,74,,b419a94ffc423c637e8722d79fb4f42770acf05e,0a4123656290c2ced43d947a97ad52ed6599be49,https://github.com/compiler-explorer/infra/blob/b419a94ffc423c637e8722d79fb4f42770acf05e/terraform/ec2.tf#L74,https://github.com/compiler-explorer/infra/blob/0a4123656290c2ced43d947a97ad52ed6599be49/terraform/ec2.tf,2021-08-30 22:47:49-05:00,2021-09-01 07:52:53-05:00,2,1,1,1,0,0,0,0,0,0
https://github.com/Azure/terraform-azurerm-caf-enterprise-scale,7,modules/terraform-azurerm-enterprise-scale-archetypes/locals.policy_assignments.tf,modules/archetypes/locals.policy_assignments.tf,1,implemented,# Logic implemented to determine whether Policy Assignments,"# Generate the Policy Assignment configurations for the specified archetype. 
 # Logic implemented to determine whether Policy Assignments 
 # need to be loaded to save on compute and memory resources 
 # when none defined in archetype definition.","locals {
  archetype_policy_assignments_list      = local.archetype_definition.policy_assignments
  archetype_policy_assignments_specified = try(length(local.archetype_policy_assignments_list) > 0, false)
}
",locals,"locals {
  archetype_policy_assignments_list      = local.archetype_definition.policy_assignments
  archetype_policy_assignments_specified = try(length(local.archetype_policy_assignments_list) > 0, false)
}
",locals,2,2.0,d5f8cbdd0d83352502d8fda485af2901e58464f5,644e61aedf55b4a033383669e70fb54096ee6b45,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/d5f8cbdd0d83352502d8fda485af2901e58464f5/modules/terraform-azurerm-enterprise-scale-archetypes/locals.policy_assignments.tf#L2,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/644e61aedf55b4a033383669e70fb54096ee6b45/modules/archetypes/locals.policy_assignments.tf#L2,2020-10-09 13:19:33+01:00,2023-07-21 14:05:10+01:00,12,0,0,0,0,1,0,0,0,0
https://github.com/compiler-explorer/infra,153,terraform/network.tf,terraform/network.tf,0,hack,// Hackily we inject this into the module instead of reading it out. Mainly due to frustration with a foreach(),// Hackily we inject this into the module instead of reading it out. Mainly due to frustration with a foreach(),"locals {
  // Hackily we inject this into the module instead of reading it out. Mainly due to frustration with a foreach()
  subnet_mappings = {
    ""1a"": ""0"",
    ""1b"": ""1"",
    ""1c"": ""4"",
    ""1d"": ""2"",
    ""1e"": ""6"",
    ""1f"": ""5"",
  }
  // All the subnet IDs, but not available until after planning, so can't be used in foreach. If you need this in
  // foreach, then you need to foreach over subnet_mappings and grab the ids from that.
  all_subnet_ids  = [for subnet, _ in local.subnet_mappings: module.ce_network.subnet[subnet].id]
}
",locals,"locals {
  // Hackily we inject this into the module instead of reading it out. Mainly due to frustration with a foreach()
  subnet_mappings = {
    ""1a"" : ""0"",
    ""1b"" : ""1"",
    ""1c"" : ""4"",
    ""1d"" : ""2"",
    ""1f"" : ""5"",
  }
  // All the subnet IDs, but not available until after planning, so can't be used in foreach. If you need this in
  // foreach, then you need to foreach over subnet_mappings and grab the ids from that.
  all_subnet_ids = [for subnet, _ in local.subnet_mappings : module.ce_network.subnet[subnet].id]
}
",locals,8,8.0,1d77db2c63d45cc8899aa9f1713ab6098e92595a,4b1e506ff9f009823bdf11e8f3dff9877c8527d8,https://github.com/compiler-explorer/infra/blob/1d77db2c63d45cc8899aa9f1713ab6098e92595a/terraform/network.tf#L8,https://github.com/compiler-explorer/infra/blob/4b1e506ff9f009823bdf11e8f3dff9877c8527d8/terraform/network.tf#L8,2021-09-20 07:44:56-05:00,2022-10-08 19:09:27-05:00,3,0,0,1,0,0,0,0,0,0
https://github.com/aws-observability/terraform-aws-observability-accelerator,8,locals.tf,locals.tf,0,todo,# TODO when tf resource for AMG api keys are supported,"# TODO when tf resource for AMG api keys are supported 
 # create a short-lived api key on the fly if api_key is not provided","locals {
  eks_oidc_issuer_url  = replace(data.aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer, ""https://"", """")
  eks_cluster_endpoint = data.aws_eks_cluster.eks_cluster.endpoint
  eks_cluster_version  = data.aws_eks_cluster.eks_cluster.version

  # if region is not passed, we assume the current one
  amp_ws_region   = coalesce(var.managed_prometheus_region, data.aws_region.current.name)
  amp_ws_id       = var.enable_managed_prometheus ? aws_prometheus_workspace.this[0].id : var.managed_prometheus_id
  amp_ws_endpoint = ""https://aps-workspaces.${local.amp_ws_region}.amazonaws.com/workspaces/${local.amp_ws_id}/""

  # if region is not passed, we assume the current one
  amg_ws_region = coalesce(var.managed_grafana_region, data.aws_region.current.name)

  # if grafana_workspace_id is supplied, we infer the endpoint from
  # computed region, else we create a new workspace
  amg_ws_endpoint = var.enable_managed_grafana ? ""https://${module.managed_grafana[0].workspace_endpoint}"" : ""https://${var.managed_grafana_workspace_id}.grafana-workspace.${local.amg_ws_region}.amazonaws.com""

  # TODO when tf resource for AMG api keys are supported
  # create a short-lived api key on the fly if api_key is not provided
  amg_api_key = var.grafana_api_key

  context = {
    aws_caller_identity_account_id = data.aws_caller_identity.current.account_id
    aws_caller_identity_arn        = data.aws_caller_identity.current.arn
    aws_eks_cluster_endpoint       = local.eks_cluster_endpoint
    aws_partition_id               = data.aws_partition.current.partition
    aws_region_name                = data.aws_region.current.name
    eks_cluster_id                 = var.eks_cluster_id
    eks_oidc_issuer_url            = local.eks_oidc_issuer_url
    eks_oidc_provider_arn          = ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/${local.eks_oidc_issuer_url}""
    tags                           = var.tags
    irsa_iam_role_path             = var.irsa_iam_role_path
    irsa_iam_permissions_boundary  = var.irsa_iam_permissions_boundary
  }

  name = ""aws-observability-accelerator""
}
",locals,"locals {
  eks_oidc_issuer_url  = replace(data.aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer, ""https://"", """")
  eks_cluster_endpoint = data.aws_eks_cluster.eks_cluster.endpoint
  eks_cluster_version  = data.aws_eks_cluster.eks_cluster.version

  # if region is not passed, we assume the current one
  amp_ws_region   = coalesce(var.managed_prometheus_workspace_region, data.aws_region.current.name)
  amp_ws_id       = var.enable_managed_prometheus ? aws_prometheus_workspace.this[0].id : var.managed_prometheus_workspace_id
  amp_ws_endpoint = ""https://aps-workspaces.${local.amp_ws_region}.amazonaws.com/workspaces/${local.amp_ws_id}/""

  # if region is not passed, we assume the current one
  amg_ws_region = coalesce(var.managed_grafana_region, data.aws_region.current.name)

  # if grafana_workspace_id is supplied, we infer the endpoint from
  # computed region, else we create a new workspace
  amg_ws_endpoint = var.managed_grafana_workspace_id == """" ? ""https://${module.managed_grafana[0].workspace_endpoint}"" : ""https://${data.aws_grafana_workspace.this[0].endpoint}""
  amg_ws_id       = var.managed_grafana_workspace_id == """" ? module.managed_grafana[0].workspace_ : data.aws_grafana_workspace.this[0].endpoint

  amg_api_key = var.grafana_api_key

  context = {
    aws_caller_identity_account_id = data.aws_caller_identity.current.account_id
    aws_caller_identity_arn        = data.aws_caller_identity.current.arn
    aws_eks_cluster_endpoint       = local.eks_cluster_endpoint
    aws_partition_id               = data.aws_partition.current.partition
    aws_region_name                = data.aws_region.current.name
    eks_cluster_id                 = var.eks_cluster_id
    eks_oidc_issuer_url            = local.eks_oidc_issuer_url
    eks_oidc_provider_arn          = ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/${local.eks_oidc_issuer_url}""
    tags                           = var.tags
    irsa_iam_role_path             = var.irsa_iam_role_path
    irsa_iam_permissions_boundary  = var.irsa_iam_permissions_boundary
  }

  name = ""aws-observability-accelerator""
}
",locals,52,,2a5564607491389ad1a87c16add9542d44d9ac20,05511992e9cc5b9fdd5c3ba59e30704d78fadda3,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/2a5564607491389ad1a87c16add9542d44d9ac20/locals.tf#L52,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/05511992e9cc5b9fdd5c3ba59e30704d78fadda3/locals.tf,2022-08-26 17:30:03+02:00,2022-08-30 19:59:34+02:00,5,1,0,1,1,1,1,0,1,0
https://github.com/kubernetes/k8s.io,163,infra/gcp/terraform/kubernetes-public/k8s-triage.tf,infra/gcp/terraform/kubernetes-public/k8s-triage.tf,0,// todo,// TODO(spiffxp): remove legacy serviceaccount when migration completed,// TODO(spiffxp): remove legacy serviceaccount when migration completed,"locals {
  // TODO(spiffxp): remove legacy serviceaccount when migration completed
  triage_legacy_sa_email = ""triage@k8s-gubernator.iam.gserviceaccount.com""
  triage_dataset               = ""k8s-triage""
  prow_owners                  = ""k8s-infra-prow-oncall@kubernetes.io""
}
",locals,"locals {
  // TODO(spiffxp): remove legacy serviceaccount when migration completed
  triage_legacy_sa_email = ""triage@k8s-gubernator.iam.gserviceaccount.com""
}
",locals,9,25.0,51dbc9a02e4c70248e2a7ea71fc6e32293a0666b,c6f1ce0ddb827eca6f416c363f96a1f948a5e33e,https://github.com/kubernetes/k8s.io/blob/51dbc9a02e4c70248e2a7ea71fc6e32293a0666b/infra/gcp/terraform/kubernetes-public/k8s-triage.tf#L9,https://github.com/kubernetes/k8s.io/blob/c6f1ce0ddb827eca6f416c363f96a1f948a5e33e/infra/gcp/terraform/kubernetes-public/k8s-triage.tf#L25,2021-08-12 11:12:49-07:00,2021-10-08 12:33:34+02:00,7,0,0,1,0,1,0,0,0,0
https://github.com/nasa/cumulus,145,tf-modules/archive/ingest-reporting.tf,tf-modules/archive/ingest-reporting.tf,0,fix,# TODO Re-enable once IAM permissions have been fixed,"# TODO Re-enable once IAM permissions have been fixed 
 # tags                 = local.default_tags","resource ""aws_iam_role"" ""report_granules_lambda_role"" {
  name                 = ""${var.prefix}-ReportGranulesLambda""
  assume_role_policy   = data.aws_iam_policy_document.lambda_assume_role_policy.json
  permissions_boundary = var.permissions_boundary_arn
  # TODO Re-enable once IAM permissions have been fixed
  # tags                 = local.default_tags
}
",resource,the block associated got renamed or deleted,,102,,60cef06b6a3c52c2993b0cb4da153e49f70c1c8c,c225c172d951fa95f466ed8c9ad82ff0a54e7542,https://github.com/nasa/cumulus/blob/60cef06b6a3c52c2993b0cb4da153e49f70c1c8c/tf-modules/archive/ingest-reporting.tf#L102,https://github.com/nasa/cumulus/blob/c225c172d951fa95f466ed8c9ad82ff0a54e7542/tf-modules/archive/ingest-reporting.tf,2019-10-10 13:45:06-04:00,2020-02-21 12:05:54-05:00,6,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,989,modules/project/service-accounts.tf,modules/project/service-accounts.tf,0,# todo,# TODO: jit?,# TODO: jit?,"locals {
  _service_accounts_cmek_service_dependencies = {
    ""composer"" : [
      ""composer"",
      ""artifactregistry"", ""container-engine"", ""compute"", ""pubsub"", ""storage""
    ]
    ""dataflow"" : [""dataflow"", ""compute""]
  }
  _service_accounts_robot_services = {
    artifactregistry  = ""service-%s@gcp-sa-artifactregistry""
    bq                = ""bq-%s@bigquery-encryption""
    cloudasset        = ""service-%s@gcp-sa-cloudasset""
    cloudbuild        = ""service-%s@gcp-sa-cloudbuild""
    cloudfunctions    = ""service-%s@gcf-admin-robot""
    cloudrun          = ""service-%s@serverless-robot-prod""
    composer          = ""service-%s@cloudcomposer-accounts""
    compute           = ""service-%s@compute-system""
    container-engine  = ""service-%s@container-engine-robot""
    containerregistry = ""service-%s@containerregistry""
    dataflow          = ""service-%s@dataflow-service-producer-prod""
    dataproc          = ""service-%s@dataproc-accounts""
    fleet             = ""service-%s@gcp-sa-gkehub""
    gae-flex          = ""service-%s@gae-api-prod""
    # TODO: deprecate gcf
    gcf = ""service-%s@gcf-admin-robot""
    # TODO: jit?
    gke-mcs                  = ""service-%s@gcp-sa-mcsd""
    monitoring-notifications = ""service-%s@gcp-sa-monitoring-notification""
    pubsub                   = ""service-%s@gcp-sa-pubsub""
    secretmanager            = ""service-%s@gcp-sa-secretmanager""
    sql                      = ""service-%s@gcp-sa-cloud-sql""
    sqladmin                 = ""service-%s@gcp-sa-cloud-sql""
    storage                  = ""service-%s@gs-project-accounts""
  }
  service_accounts_default = {
    compute = ""${local.project.number}-compute@developer.gserviceaccount.com""
    gae     = ""${local.project.project_id}@appspot.gserviceaccount.com""
  }
  service_account_cloud_services = (
    ""${local.project.number}@cloudservices.gserviceaccount.com""
  )
  service_accounts_robots = merge(
    {
      for k, v in local._service_accounts_robot_services :
      k => ""${format(v, local.project.number)}.iam.gserviceaccount.com""
    },
    {
      gke-mcs-importer = ""${local.project.project_id}.svc.id.goog[gke-mcs/gke-mcs-importer]""
    }
  )
  service_accounts_jit_services = [
    ""cloudasset.googleapis.com"",
    ""gkehub.googleapis.com"",
    ""pubsub.googleapis.com"",
    ""secretmanager.googleapis.com"",
    ""sqladmin.googleapis.com""
  ]
  service_accounts_cmek_service_keys = distinct(flatten([
    for s in keys(var.service_encryption_key_ids) : [
      for ss in try(local._service_accounts_cmek_service_dependencies[s], [s]) : [
        for key in var.service_encryption_key_ids[s] : {
          service = ss
          key     = key
        } if key != null
      ]
    ]
  ]))
}
",locals,"locals {
  _service_accounts_cmek_service_dependencies = {
    ""composer"" : [
      ""composer"",
      ""artifactregistry"", ""container-engine"", ""compute"", ""pubsub"", ""storage""
    ]
    ""dataflow"" : [""dataflow"", ""compute""]
  }
  _service_agents_data = yamldecode(file(""${path.module}/service-agents.yaml""))
  service_accounts_default = {
    compute      = ""${local.project.number}-compute@developer.gserviceaccount.com""
    gae          = ""${local.project.project_id}@appspot.gserviceaccount.com""
    workstations = ""service-${local.project.number}@gcp-sa-workstationsvm.iam.gserviceaccount.com""
  }
  service_account_cloud_services = (
    ""${local.project.number}@cloudservices.gserviceaccount.com""
  )
  service_accounts_robots = merge(
    {
      for agent in local._service_agents_data :
      agent.name => format(agent.service_agent, local.project.number)
    },
    {
      for agent in local._service_agents_data :
      agent.alias => format(agent.service_agent, local.project.number)
      if lookup(agent, ""alias"", null) != null
    },
    {
      gke-mcs-importer = ""${local.project.project_id}.svc.id.goog[gke-mcs/gke-mcs-importer]""
    }
  )
  service_accounts_jit_services = [
    for agent in local._service_agents_data :
    ""${agent.name}.googleapis.com""
    if lookup(agent, ""jit"", false)
  ]
  service_accounts_cmek_service_keys = distinct(flatten([
    for s in keys(var.service_encryption_key_ids) : [
      for ss in try(local._service_accounts_cmek_service_dependencies[s], [s]) : [
        for key in var.service_encryption_key_ids[s] : {
          service = ss
          key     = key
        } if key != null
      ]
    ]
  ]))
}
",locals,44,,133fd078232ef202140450d921bb8018b60e700f,b503bde544670d9acdd584a9798613dc84c0c0d5,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/133fd078232ef202140450d921bb8018b60e700f/modules/project/service-accounts.tf#L44,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b503bde544670d9acdd584a9798613dc84c0c0d5/modules/project/service-accounts.tf,2022-07-29 11:31:34+02:00,2023-03-30 09:36:14+03:00,16,1,0,1,0,0,0,0,0,0
https://github.com/Azure/sap-automation,5,deploy/terraform/run/sap_system/module.tf,deploy/terraform/run/sap_system/module.tf,0,workaround,// Workaround to create dependency from anchor to db to app,anchor_vm                                    = module.common_infrastructure.anchor_vm // Workaround to create dependency from anchor to db to app,"module ""anydb_node"" {
  source = ""../../terraform-units/modules/sap_system/anydb_node""
  providers = {
    azurerm.main     = azurerm
    azurerm.deployer = azurerm.deployer
  }
  depends_on = [module.common_infrastructure]
  order_deployment = local.db_zonal_deployment ? (
    module.app_tier.scs_vm_ids[0]
  ) : (null)
  databases                                    = local.databases
  infrastructure                               = local.infrastructure
  options                                      = local.options
  resource_group                               = module.common_infrastructure.resource_group
  storage_bootdiag_endpoint                    = module.common_infrastructure.storage_bootdiag_endpoint
  ppg                                          = module.common_infrastructure.ppg
  sid_kv_user_id                               = module.common_infrastructure.sid_kv_user_id
  naming                                       = module.sap_namegenerator.naming
  custom_disk_sizes_filename                   = var.db_disk_sizes_filename
  admin_subnet                                 = module.common_infrastructure.admin_subnet
  db_subnet                                    = module.common_infrastructure.db_subnet
  anchor_vm                                    = module.common_infrastructure.anchor_vm // Workaround to create dependency from anchor to db to app
  sid_password                                 = module.common_infrastructure.sid_password
  sid_username                                 = module.common_infrastructure.sid_username
  sdu_public_key                               = module.common_infrastructure.sdu_public_key
  sap_sid                                      = local.sap_sid
  db_asg_id                                    = module.common_infrastructure.db_asg_id
  terraform_template_version                   = var.terraform_template_version
  deployment                                   = var.deployment
  cloudinit_growpart_config                    = null # This needs more consideration module.common_infrastructure.cloudinit_growpart_config
  license_type                                 = var.license_type
  use_loadbalancers_for_standalone_deployments = var.use_loadbalancers_for_standalone_deployments
  database_vm_names                            = var.database_vm_names
  database_vm_db_nic_ips                       = var.database_vm_db_nic_ips
  database_vm_admin_nic_ips                    = var.database_vm_admin_nic_ips
  database_vm_storage_nic_ips                  = var.database_vm_storage_nic_ips
  database_server_count = upper(try(local.databases[0].platform, ""HANA"")) == ""HANA"" ? (
    0) : (
    local.databases[0].high_availability ? 2 * var.database_server_count : var.database_server_count
  )
}
",module,"module ""anydb_node"" {
  source                                        = ""../../terraform-units/modules/sap_system/anydb_node""
  providers                                     = {
                                                    azurerm.deployer       = azurerm
                                                    azurerm.main           = azurerm.system
                                                    azurerm.dnsmanagement  = azurerm.dnsmanagement
                                                    # azapi.api                                 = azapi.api
                                                  }

  depends_on                                    = [module.common_infrastructure]

  admin_subnet                                  = try(module.common_infrastructure.admin_subnet, null)
  anchor_vm                                     = module.common_infrastructure.anchor_vm // Workaround to create dependency from anchor to db to app
  cloudinit_growpart_config                     = null # This needs more consideration module.common_infrastructure.cloudinit_growpart_config
  custom_disk_sizes_filename                    = try(coalesce(var.custom_disk_sizes_filename, var.db_disk_sizes_filename), """")
  database                                      = local.database
  database_vm_db_nic_ips                        = var.database_vm_db_nic_ips
  database_vm_db_nic_secondary_ips              = var.database_vm_db_nic_secondary_ips
  database_vm_admin_nic_ips                     = var.database_vm_admin_nic_ips
  database_server_count                         = upper(try(local.database.platform, ""HANA"")) == ""HANA"" ? (
                                                  0) : (
                                                    local.database.high_availability ? 2 * var.database_server_count : var.database_server_count
                                                  )
  db_asg_id                                     = module.common_infrastructure.db_asg_id
  db_subnet                                     = module.common_infrastructure.db_subnet
  deploy_application_security_groups            = var.deploy_application_security_groups
  deployment                                    = var.deployment
  fencing_role_name                             = var.fencing_role_name
  infrastructure                                = local.infrastructure
  landscape_tfstate                             = data.terraform_remote_state.landscape.outputs
  license_type                                  = var.license_type
  management_dns_resourcegroup_name             = try(data.terraform_remote_state.landscape.outputs.management_dns_resourcegroup_name, local.saplib_resource_group_name)
  management_dns_subscription_id                = try(data.terraform_remote_state.landscape.outputs.management_dns_subscription_id, null)
  naming                                        = length(var.name_override_file) > 0 ? local.custom_names : module.sap_namegenerator.naming
  options                                       = local.options
  order_deployment                              = local.enable_db_deployment ? (
                                                    local.db_zonal_deployment && local.application_tier.enable_deployment ? (
                                                      try(module.app_tier.scs_vm_ids[0], null)
                                                    ) : (null)
                                                  ) : (null)
  ppg                                           = module.common_infrastructure.ppg
  register_virtual_network_to_dns               = try(data.terraform_remote_state.landscape.outputs.register_virtual_network_to_dns, true)
  register_endpoints_with_dns                   = var.register_endpoints_with_dns
  resource_group                                = module.common_infrastructure.resource_group
  sap_sid                                       = local.sap_sid
  scale_set_id                                  = try(module.common_infrastructure.scale_set_id, null)
  sdu_public_key                                = module.common_infrastructure.sdu_public_key
  sid_keyvault_user_id                          = module.common_infrastructure.sid_keyvault_user_id
  sid_password                                  = module.common_infrastructure.sid_password
  sid_username                                  = module.common_infrastructure.sid_username
  storage_bootdiag_endpoint                     = module.common_infrastructure.storage_bootdiag_endpoint
  tags                                          = var.tags
  terraform_template_version                    = var.terraform_template_version
  use_custom_dns_a_registration                 = data.terraform_remote_state.landscape.outputs.use_custom_dns_a_registration
  use_loadbalancers_for_standalone_deployments  = var.use_loadbalancers_for_standalone_deployments
  use_msi_for_clusters                          = var.use_msi_for_clusters
  use_observer                                  = var.use_observer
  use_scalesets_for_deployment                  = var.use_scalesets_for_deployment
  use_secondary_ips                             = var.use_secondary_ips
}
",module,164,242.0,6ff0b891114c36d3aeccb850d830b698cd1fe52a,df063c58945a9efa2cb2ba303762c43f0b9c1d8f,https://github.com/Azure/sap-automation/blob/6ff0b891114c36d3aeccb850d830b698cd1fe52a/deploy/terraform/run/sap_system/module.tf#L164,https://github.com/Azure/sap-automation/blob/df063c58945a9efa2cb2ba303762c43f0b9c1d8f/deploy/terraform/run/sap_system/module.tf#L242,2021-11-17 19:29:07+02:00,2024-05-17 12:37:17+03:00,110,0,1,1,0,0,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,357,main.tf,main.tf,0,todo,# TODO - hopefully this can be removed once the AWS endpoint is named properly in China,"# TODO - hopefully this can be removed once the AWS endpoint is named properly in China 
 # https://github.com/terraform-aws-modules/terraform-aws-eks/issues/1904","locals {
  iam_role_name     = coalesce(var.iam_role_name, ""${var.cluster_name}-cluster"")
  policy_arn_prefix = ""arn:${data.aws_partition.current.partition}:iam::aws:policy""

  # TODO - hopefully this can be removed once the AWS endpoint is named properly in China
  # https://github.com/terraform-aws-modules/terraform-aws-eks/issues/1904
  dns_suffix = coalesce(var.cluster_iam_role_dns_suffix, data.aws_partition.current.dns_suffix)
}
",locals,"locals {
  create = var.create && var.putin_khuylo

  partition = data.aws_partition.current.partition

  cluster_role = try(aws_iam_role.this[0].arn, var.iam_role_arn)

  create_outposts_local_cluster    = length(var.outpost_config) > 0
  enable_cluster_encryption_config = length(var.cluster_encryption_config) > 0 && !local.create_outposts_local_cluster
}
",locals,176,,9af0c2495a1fe7a02411ac436f48f6d9ca8b359f,6b40bdbb1d283d9259f43b03d24dca99cc1eceff,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/9af0c2495a1fe7a02411ac436f48f6d9ca8b359f/main.tf#L176,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/6b40bdbb1d283d9259f43b03d24dca99cc1eceff/main.tf,2022-03-02 18:26:20+01:00,2024-02-02 09:36:25-05:00,46,1,0,1,1,0,0,0,0,0
https://github.com/kubernetes/k8s.io,160,infra/gcp/terraform/kubernetes-public/k8s-infra-prow.tf,infra/gcp/terraform/kubernetes-public/k8s-infra-prow.tf,0,todo,// TODO (ameukam): move hardcoded value to terraform variables,"// Allow deck (component of k8s-infra-prow) service account to use GCP SA k8s-infra-prow via workload identity 
 // TODO (ameukam): move hardcoded value to terraform variables","resource ""google_service_account_iam_member"" ""aaa_cluster_sa_iam"" {
  role               = ""roles/iam.workloadIdentityUser""
  service_account_id = google_service_account.k8s_infra_prow.name
  member             = format(""serviceAccount:%s.svc.id.goog[%s/%s]"", ""kubernetes-public"", ""prow"", ""deck"")
}
",resource,"resource ""google_service_account_iam_member"" ""aaa_cluster_sa_iam"" {
  role               = ""roles/iam.workloadIdentityUser""
  service_account_id = google_service_account.k8s_infra_prow.name
  member             = format(""serviceAccount:%s.svc.id.goog[%s/%s]"", ""kubernetes-public"", ""prow"", ""deck"")
}
",resource,46,56.0,c7aa8b1685f1b9a8ab978639b28164e6cef305a2,5e69979eb5251d9a1ad6ca6ec8856c99a5927034,https://github.com/kubernetes/k8s.io/blob/c7aa8b1685f1b9a8ab978639b28164e6cef305a2/infra/gcp/terraform/kubernetes-public/k8s-infra-prow.tf#L46,https://github.com/kubernetes/k8s.io/blob/5e69979eb5251d9a1ad6ca6ec8856c99a5927034/infra/gcp/terraform/kubernetes-public/k8s-infra-prow.tf#L56,2021-08-10 16:09:26-07:00,2024-01-03 18:16:49+00:00,6,0,0,1,0,1,0,0,0,0
https://github.com/rust-lang/simpleinfra,4,terragrunt/modules/ecs-cluster/cluster.tf,terragrunt/modules/ecs-cluster/cluster.tf,0,# todo,# TODO: change this as it is deprecated,# TODO: change this as it is deprecated,"resource ""aws_ecs_cluster"" ""cluster"" {
  name = var.cluster_name
  # TODO: change this as it is deprecated
  capacity_providers = [""FARGATE"", ""FARGATE_SPOT""]

  setting {
    name  = ""containerInsights""
    value = ""enabled""
  }

  tags = {
    Name = var.cluster_name
  }
}
",resource,"resource ""aws_ecs_cluster"" ""cluster"" {
  name = var.cluster_name

  setting {
    name  = ""containerInsights""
    value = ""enabled""
  }

  tags = {
    Name = var.cluster_name
  }
}
",resource,18,,385329684454974c1801d3aecb0d7fa87bc066da,84eb7f700493ea666410e696a3c87073ea78198e,https://github.com/rust-lang/simpleinfra/blob/385329684454974c1801d3aecb0d7fa87bc066da/terragrunt/modules/ecs-cluster/cluster.tf#L18,https://github.com/rust-lang/simpleinfra/blob/84eb7f700493ea666410e696a3c87073ea78198e/terragrunt/modules/ecs-cluster/cluster.tf,2022-12-16 16:13:09+01:00,2022-12-16 16:13:09+01:00,2,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,481,fast/stages/02-security/core-dev.tf,fast/stages/2-security/core-dev.tf,1,# todo,# TODO(ludo): add support for conditions to Fabric modules,# TODO(ludo): add support for conditions to Fabric modules,"resource ""google_project_iam_member"" ""dev_key_admin_delegated"" {
  for_each = toset(try(var.kms_restricted_admins.dev, []))
  project  = module.dev-sec-project.project_id
  role     = ""roles/cloudkms.admin""
  member   = each.key
  condition {
    title       = ""kms_sa_delegated_grants""
    description = ""Automation service account delegated grants""
    expression = format(
      ""api.getAttribute('iam.googleapis.com/modifiedGrantsByRole', []).hasOnly([%s])"",
      join("","", formatlist(""'%s'"", [
        ""roles/cloudkms.cryptoKeyEncrypterDecrypter"",
        ""roles/cloudkms.cryptoKeyEncrypterDecrypterViaDelegation""
      ]))
    )
  }
  depends_on = [module.dev-sec-project]
}
",resource,the block associated got renamed or deleted,,45,,34e845fcdd4862f37409a843b3ffba52210e9a12,121598dbea2e72f9b4df57e5f0ca60c8bd7d0990,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/34e845fcdd4862f37409a843b3ffba52210e9a12/fast/stages/02-security/core-dev.tf#L45,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/121598dbea2e72f9b4df57e5f0ca60c8bd7d0990/fast/stages/2-security/core-dev.tf,2022-01-17 10:30:26+01:00,2023-09-17 00:21:36+02:00,17,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd,46,modules/secure-cd/main.tf,modules/secure-cd/main.tf,0,//todo,//TODO?,require_attestations_by = each.value.attestations //TODO?,"resource ""google_binary_authorization_policy"" ""deployment_policy"" {
  for_each = var.deploy_branch_clusters
  project  = each.value.project_id
  
  admission_whitelist_patterns {
    name_pattern = ""gcr.io/google_containers/*""
  }

  default_admission_rule {
    evaluation_mode  = ""ALWAYS_DENY""
    enforcement_mode = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
  }

  global_policy_evaluation_mode = ""ENABLE""

  cluster_admission_rules {
    cluster                 = ""${each.value.location}.${each.value.cluster}"" // TODO: customer config
    evaluation_mode         = ""REQUIRE_ATTESTATION""
    enforcement_mode        = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
    require_attestations_by = each.value.attestations //TODO?
  }
}
",resource,"resource ""google_binary_authorization_policy"" ""deployment_policy"" {
  for_each = local.binary_authorization_map
  project  = each.key

  default_admission_rule {
    evaluation_mode  = ""ALWAYS_DENY""
    enforcement_mode = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
  }

  global_policy_evaluation_mode = ""ENABLE""

  dynamic ""cluster_admission_rules"" {
    for_each = each.value
    content {
      cluster                 = ""${cluster_admission_rules.value.location}.${cluster_admission_rules.value.cluster}""
      evaluation_mode         = ""REQUIRE_ATTESTATION""
      enforcement_mode        = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
      require_attestations_by = cluster_admission_rules.value.required_attestations
    }
  }
}
",resource,77,,6249c4ca90692e593bc0c7bc6d603580150ff255,c8839114f847e80edd056da098f0489492f58e76,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/6249c4ca90692e593bc0c7bc6d603580150ff255/modules/secure-cd/main.tf#L77,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/c8839114f847e80edd056da098f0489492f58e76/modules/secure-cd/main.tf,2021-10-26 17:18:47-05:00,2021-12-01 11:14:01-06:00,18,1,0,1,0,0,0,0,0,0
https://github.com/compiler-explorer/infra,117,terraform/ec2.tf,terraform/ec2.tf,0,todo,"// TODO reconsider, make an SG specifically for builder","// TODO reconsider, make an SG specifically for builder","resource ""aws_instance"" ""BuilderNode"" {
  ami                         = local.builder_image_id
  // TODO bring into the fold
  iam_instance_profile        = ""GccBuilder""
  ebs_optimized               = true
  // TODO make 4xlarge or similar
  instance_type               = ""c5d.large""
  monitoring                  = false
  key_name                    = ""mattgodbolt""
  subnet_id                   = aws_subnet.ce-1a.id
  // TODO reconsider, make an SG specifically for builder
  vpc_security_group_ids      = [aws_security_group.AdminNode.id]
  associate_public_ip_address = true
  source_dest_check           = false

  root_block_device {
    volume_type           = ""gp2""
    volume_size           = 24
    delete_on_termination = true
  }

  tags = {
    Name = ""Builder-New""
  }
}
",resource,"resource ""aws_instance"" ""BuilderNode"" {
  ami                         = local.builder_image_id
  iam_instance_profile        = aws_iam_instance_profile.Builder.name
  ebs_optimized               = true
  instance_type               = ""c5d.4xlarge""
  monitoring                  = false
  key_name                    = ""mattgodbolt""
  subnet_id                   = aws_subnet.ce-1a.id
  vpc_security_group_ids      = [aws_security_group.Builder.id]
  associate_public_ip_address = true
  source_dest_check           = false

  root_block_device {
    volume_type           = ""gp2""
    volume_size           = 24
    delete_on_termination = true
  }

  lifecycle {
    ignore_changes = [
      // Seemingly needed to not replace stopped instances
      associate_public_ip_address
    ]
  }

  tags = {
    Name = ""Builder""
  }
}
",resource,79,,b419a94ffc423c637e8722d79fb4f42770acf05e,b95d2fa4883a28322df51001553dcf38d0638d09,https://github.com/compiler-explorer/infra/blob/b419a94ffc423c637e8722d79fb4f42770acf05e/terraform/ec2.tf#L79,https://github.com/compiler-explorer/infra/blob/b95d2fa4883a28322df51001553dcf38d0638d09/terraform/ec2.tf,2021-08-30 22:47:49-05:00,2021-09-02 18:02:04-05:00,5,1,1,1,0,1,0,0,0,0
https://github.com/deckhouse/deckhouse,1,candi/cloud-providers/yandex/terraform-modules/master-node/main.tf,candi/cloud-providers/yandex/terraform-modules/master-node/main.tf,0,todo,// TODO apply external_subnet_id_from_ids to external_subnet_id directly after remove externalSubnetID,// TODO apply external_subnet_id_from_ids to external_subnet_id directly after remove externalSubnetID,"locals {
  zone_to_subnet = {
    ""ru-central1-a"" = data.yandex_vpc_subnet.kube_a
    ""ru-central1-b"" = data.yandex_vpc_subnet.kube_b
    ""ru-central1-c"" = data.yandex_vpc_subnet.kube_c
  }

  actual_zones = lookup(var.providerClusterConfiguration, ""zones"", null) != null ? tolist(setintersection(keys(local.zone_to_subnet), var.providerClusterConfiguration.zones)) : keys(local.zone_to_subnet)
  zones = lookup(var.providerClusterConfiguration.masterNodeGroup, ""zones"", null) != null ? tolist(setintersection(local.actual_zones, var.providerClusterConfiguration.masterNodeGroup[""zones""])) : local.actual_zones
  subnets = length(local.zones) > 0 ? [for z in local.zones : local.zone_to_subnet[z]] : values(local.zone_to_subnet)
  internal_subnet = element(local.subnets, var.nodeIndex)

  // TODO apply external_subnet_id_from_ids to external_subnet_id directly after remove externalSubnetID
  external_subnet_id_from_ids = length(local.external_subnet_ids) > 0 ? local.external_subnet_ids[var.nodeIndex] : null

  external_subnet_id = local.external_subnet_id_from_ids == null ? local.external_subnet_id_deprecated : local.external_subnet_id_from_ids
  external_ip_address = length(local.external_ip_addresses) > 0 ? local.external_ip_addresses[var.nodeIndex] : null
  assign_external_ip_address = (local.external_subnet_id == null) && (local.external_ip_address != null) ? true : false

}
",locals,"locals {
  mapping = lookup(var.providerClusterConfiguration, ""existingZoneToSubnetIDMap"", {})

  zone_to_subnet = length(local.mapping) == 0 ? {
    ""ru-central1-a"" = length(data.yandex_vpc_subnet.kube_a) > 0 ? data.yandex_vpc_subnet.kube_a[0] : object({})
    ""ru-central1-b"" = length(data.yandex_vpc_subnet.kube_b) > 0 ? data.yandex_vpc_subnet.kube_b[0] : object({})
    ""ru-central1-c"" = length(data.yandex_vpc_subnet.kube_c) > 0 ? data.yandex_vpc_subnet.kube_c[0] : object({})
    ""ru-central1-d"" = length(data.yandex_vpc_subnet.kube_d) > 0 ? data.yandex_vpc_subnet.kube_d[0] : object({})
  } : data.yandex_vpc_subnet.existing

  actual_zones    = lookup(var.providerClusterConfiguration, ""zones"", null) != null ? tolist(setintersection(keys(local.zone_to_subnet), var.providerClusterConfiguration.zones)) : keys(local.zone_to_subnet)
  zones           = lookup(var.providerClusterConfiguration.masterNodeGroup, ""zones"", null) != null ? tolist(setintersection(local.actual_zones, var.providerClusterConfiguration.masterNodeGroup[""zones""])) : local.actual_zones
  subnets         = length(local.zones) > 0 ? [for z in local.zones : local.zone_to_subnet[z]] : values(local.zone_to_subnet)
  internal_subnet = element(local.subnets, var.nodeIndex)

  // TODO apply external_subnet_id_from_ids to external_subnet_id directly after remove externalSubnetID
  external_subnet_id_from_ids = length(local.external_subnet_ids) > 0 ? element(local.external_subnet_ids, var.nodeIndex) : null

  external_subnet_id         = local.external_subnet_id_from_ids == null ? local.external_subnet_id_deprecated : local.external_subnet_id_from_ids
  assign_external_ip_address = (local.external_subnet_id == null) && (local.external_ip_address != null) ? true : false

}
",locals,39,30.0,12c4247d312bc32bebf7b1393b3783e64f0ef3d8,31d3b87485eb8740318b216508ee703d1bff2ea8,https://github.com/deckhouse/deckhouse/blob/12c4247d312bc32bebf7b1393b3783e64f0ef3d8/candi/cloud-providers/yandex/terraform-modules/master-node/main.tf#L39,https://github.com/deckhouse/deckhouse/blob/31d3b87485eb8740318b216508ee703d1bff2ea8/candi/cloud-providers/yandex/terraform-modules/master-node/main.tf#L30,2021-06-28 18:47:36+00:00,2023-12-29 10:55:04+01:00,8,0,0,1,0,0,1,0,0,0
https://github.com/terraform-google-modules/terraform-google-log-export,1,test/setup/iam.tf,test/setup/iam.tf,0,workaround,# Adding a pause as a workaround for of the provider issue,"# Adding a pause as a workaround for of the provider issue 
 # https://github.com/terraform-providers/terraform-provider-google/issues/1131","resource ""null_resource"" ""wait_permissions"" {
  # Adding a pause as a workaround for of the provider issue
  # https://github.com/terraform-providers/terraform-provider-google/issues/1131
  provisioner ""local-exec"" {
    command = ""echo sleep 120s for permissions to get granted; sleep 120""
  }
  depends_on = [
    google_billing_account_iam_member.int_test,
    google_folder_iam_member.int_test,
    google_organization_iam_member.int_test,
    google_project_iam_member.int_test
  ]
}
",resource,"resource ""null_resource"" ""wait_permissions"" {
  # Adding a pause as a workaround for of the provider issue
  # https://github.com/terraform-providers/terraform-provider-google/issues/1131
  provisioner ""local-exec"" {
    command = ""echo sleep 120s for permissions to get granted; sleep 120""
  }
  depends_on = [
    google_billing_account_iam_member.int_test,
    google_folder_iam_member.int_test,
    google_organization_iam_member.int_test,
    google_project_iam_member.int_test,
    google_project_iam_member.int_test_logbkt
  ]
}
",resource,114,147.0,3b701bf57a515dc98c2e31c90e8c9a2b2815fa6b,44758c29c820d4c299e4c53e2ff08081d19e7f75,https://github.com/terraform-google-modules/terraform-google-log-export/blob/3b701bf57a515dc98c2e31c90e8c9a2b2815fa6b/test/setup/iam.tf#L114,https://github.com/terraform-google-modules/terraform-google-log-export/blob/44758c29c820d4c299e4c53e2ff08081d19e7f75/test/setup/iam.tf#L147,2019-09-30 19:00:53+03:00,2022-08-09 12:13:44-05:00,4,0,0,0,1,0,0,1,0,0
https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules,5,modules/vm-bootstrap/main.tf,modules/vm-bootstrap/main.tf,0,workaround,# Workaround: use random_id above to cause the full destroy/create of a file.,"# Live above is equivalent to:   `source = each.key`  but it re-creates the file every time the content changes. 
 # The replace() is not actually doing anything, except tricking Terraform to destroy a resource. 
 # There is a field content_md5 designed specifically for that. But I see a bug in the provider: 
 # When content_md5 is changed, the re-upload seemingly succeeds, result being however a totally empty file (size zero). 
 # Workaround: use random_id above to cause the full destroy/create of a file.","resource ""azurerm_storage_share_file"" ""this"" {
  for_each = var.files

  name             = regex(""[^/]*$"", each.value)
  path             = replace(each.value, ""/[/]*[^/]*$/"", """")
  storage_share_id = azurerm_storage_share.inbound-bootstrap-storage-share.id
  source           = replace(each.key, ""/CalculateMe[X]${random_id.file[each.key].id}/"", ""CalculateMeX${random_id.file[each.key].id}"")
  # Live above is equivalent to:   `source = each.key`  but it re-creates the file every time the content changes.
  # The replace() is not actually doing anything, except tricking Terraform to destroy a resource.
  # There is a field content_md5 designed specifically for that. But I see a bug in the provider: 
  # When content_md5 is changed, the re-upload seemingly succeeds, result being however a totally empty file (size zero).
  # Workaround: use random_id above to cause the full destroy/create of a file.
  depends_on = [
    azurerm_storage_share_directory.inbound-bootstrap-config-directory,
    azurerm_storage_share_directory.bootstrap-storage-directories,
  ]
}
",resource,,,49,0.0,2555bdd9f015bca2d6cda83130c3a365e694d677,451996d8a5926cb97e7d5c4db3534eade518bc84,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/2555bdd9f015bca2d6cda83130c3a365e694d677/modules/vm-bootstrap/main.tf#L49,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/451996d8a5926cb97e7d5c4db3534eade518bc84/modules/vm-bootstrap/main.tf#L0,2021-01-29 10:21:09+01:00,2021-03-31 16:20:24+02:00,8,2,1,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,18,community/modules/remote-desktop-linux/main.tf,community/modules/remote-desktop-linux/main.tf,0,todo,# todo change this to chrome install script & merge with xfce install script,# todo change this to chrome install script & merge with xfce install script,"locals {
  resource_prefix = var.name_prefix != null ? var.name_prefix : ""${var.deployment_name}-chrome-remote-desktop""

  /*
  #
  # if a machine type is a2-*-?g it will automatically fill in the guest_accelerator structure.
  #
  is_a2_vm = length(regexall(""a2-[a-z]+-\\d+g"", var.machine_type)) > 0
  accelerator_types = {
    ""highgpu""  = ""nvidia-tesla-a100""
    ""megagpu""  = ""nvidia-tesla-a100""
    ""ultragpu"" = ""nvidia-a100-80gb""
  }
  guest_accelerator = var.guest_accelerator == null && local.is_a2_vm ? [{
    type  = lookup(local.accelerator_types, regex(""a2-([A-Za-z]+)-"", var.machine_type)[0], """"),
    count = one(regex(""a2-[A-Za-z]+-(\\d+)"", var.machine_type)),
  }] : var.guest_accelerator

  gpu_count = length(local.guest_accelerator) > 0 ? 0 : local.guest_accelerator[0].count

*/
  user_startup_script_runners = var.startup_script == null ? [] : [
    {
      type        = ""shell""
      content     = var.startup_script
      destination = ""user_startup_script.sh""
    }
  ]

  ssh_args = join("""", [
    ""-e host_name_prefix=${local.resource_prefix}""
  ])

  configure_ssh_runners = [
    {
      type        = ""data""
      source      = ""${path.module}/scripts/setup-ssh-keys.sh""
      destination = ""/usr/local/ghpc/setup-ssh-keys.sh""
    },
    {
      type        = ""data""
      source      = ""${path.module}/scripts/setup-ssh-keys.yml""
      destination = ""/usr/local/ghpc/setup-ssh-keys.yml""
    },
    {
      type        = ""ansible-local""
      content     = file(""${path.module}/scripts/configure-ssh.yml"")
      destination = ""configure-ssh.yml""
      args        = local.ssh_args
    }
  ]
  # todo change this to driver install script
  configure_nvidia_driver_runners = var.install_nvidia_driver == false ? [] : [
    {
      type        = ""shell""
      content     = file(""${path.module}/scripts/configure-grid-drivers.sh"")
      destination = ""/usr/local/ghpc/configure-grid-drivers.yml""
    }
  ]
  # todo change this to chrome install script & merge with xfce install script
  configure_chrome_remote_desktop_runners = var.configure_chrome_remote_desktop == false ? [] : [
    {
      type        = ""shell""
      content     = file(""${path.module}/scripts/configure-chrome-desktop.sh"")
      destination = ""/usr/local/ghpc/configure-chrome-desktop.yml""
    }
  ]

  driver     = { install-nvidia-driver = var.install_nvidia_driver }
  logging    = var.enable_google_logging ? { google-logging-enable = 1 } : { google-logging-enable = 0 }
  monitoring = var.enable_google_monitoring ? { google-monitoring-enable = 1 } : { google-monitoring-enable = 0 }
  shutdown   = { shutdown-script = ""/opt/deeplearning/bin/shutdown_script.sh"" }
  metadata   = merge(local.driver, local.logging, local.monitoring, local.shutdown, var.metadata)
}
",locals,,,77,0.0,cb06601fe71434d7539ea073a9e1c3c03981e641,1fe9b4674767a89081bf6aa27b34e9e4338e8111,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/cb06601fe71434d7539ea073a9e1c3c03981e641/community/modules/remote-desktop-linux/main.tf#L77,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/1fe9b4674767a89081bf6aa27b34e9e4338e8111/community/modules/remote-desktop-linux/main.tf#L0,2023-01-11 06:57:26+00:00,2023-01-18 12:58:23+11:00,2,2,0,1,0,0,0,0,0,0
https://github.com/kubernetes/k8s.io,13,infra/gcp/clusters/modules/k8s-infra-gke-cluster/main.tf,infra/gcp/clusters/modules/gke-cluster/main.tf,1,workaround,// Workaround from https://github.com/hashicorp/terraform/issues/22544#issuecomment-582974372,"// BigQuery dataset for usage data 
 // Workaround from https://github.com/hashicorp/terraform/issues/22544#issuecomment-582974372 
 // to set delete_contents_on_destroy to false if is_prod_cluster 
 // keep prod_ and test_ identical except for ""unique to "" comments","resource ""google_bigquery_dataset"" ""prod_usage_metering"" {
  count       = var.is_prod_cluster == ""true"" ? 1 : 0
  dataset_id  = replace(""usage_metering_${var.cluster_name}"", ""-"", ""_"")
  project     = var.project_name
  description = ""GKE Usage Metering for cluster '${var.cluster_name}'""
  location    = var.bigquery_location

  access {
    role          = ""OWNER""
    special_group = ""projectOwners""
  }
  access {
    role          = ""WRITER""
    user_by_email = google_service_account.cluster_node_sa.email
  }

  // This restricts deletion of this dataset if there is data in it
  // unique to prod_usage_metering
  delete_contents_on_destroy = false
}
",resource,"resource ""google_bigquery_dataset"" ""prod_usage_metering"" {
  count       = var.is_prod_cluster == ""true"" ? 1 : 0
  dataset_id  = replace(""usage_metering_${var.cluster_name}"", ""-"", ""_"")
  project     = var.project_name
  description = ""GKE Usage Metering for cluster '${var.cluster_name}'""
  location    = var.bigquery_location

  access {
    role          = ""OWNER""
    special_group = ""projectOwners""
  }
  access {
    role          = ""WRITER""
    user_by_email = google_service_account.cluster_node_sa.email
  }

  // NOTE: unique to prod_usage_metering
  // This restricts deletion of this dataset if there is data in it
  delete_contents_on_destroy = false
}
",resource,42,,59c074f856b19f13b9e75479f2aaf09d9c95c7d6,0d83cf8820bd2d550c7032d8557aacb836bca743,https://github.com/kubernetes/k8s.io/blob/59c074f856b19f13b9e75479f2aaf09d9c95c7d6/infra/gcp/clusters/modules/k8s-infra-gke-cluster/main.tf#L42,https://github.com/kubernetes/k8s.io/blob/0d83cf8820bd2d550c7032d8557aacb836bca743/infra/gcp/clusters/modules/gke-cluster/main.tf,2020-05-06 14:16:38-07:00,2020-05-15 18:34:32-07:00,2,1,1,1,1,0,0,1,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,91,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,0,# todo,# TODO: is it used? should remove it?,client_install_runner = map(string) # TODO: is it used? should remove it?,"variable ""network_storage"" {
  description = ""An array of network attached storage mounts to be configured on all instances.""
  type = list(object({
    server_ip             = string,
    remote_mount          = string,
    local_mount           = string,
    fs_type               = string,
    mount_options         = string,
    client_install_runner = map(string) # TODO: is it used? should remove it?
    mount_runner          = map(string)
  }))
  default = []
}
",variable,"variable ""network_storage"" {
  description = ""An array of network attached storage mounts to be configured on all instances.""
  type = list(object({
    server_ip             = string,
    remote_mount          = string,
    local_mount           = string,
    fs_type               = string,
    mount_options         = string,
    client_install_runner = map(string) # TODO: is it used? should remove it?
    mount_runner          = map(string)
  }))
  default = []
}
",variable,351,405.0,33bf402eaa82607a027754c6048fb0dce6d7668c,367dbbc03d00a523c457d066229e6c9376bcd5c9,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/33bf402eaa82607a027754c6048fb0dce6d7668c/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf#L351,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/367dbbc03d00a523c457d066229e6c9376bcd5c9/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf#L405,2023-10-26 09:59:26-07:00,2024-05-01 19:59:12+00:00,24,0,1,1,0,0,1,0,0,0
https://github.com/Worklytics/psoxy,142,infra/modules/aws/main.tf,infra/modules/aws/main.tf,0,#todo,#TODO: add condition referencing GCP service account that will auth with Worklytics AWS account to call,"#TODO: add condition referencing GCP service account that will auth with Worklytics AWS account to call 
 #""Condition"" : { 
 #  ""StringEquals"" : { 
 #    ""sts:ExternalId"" : var.caller_aws_user_id 
 #  } 
 #}","resource ""aws_iam_role"" ""api-caller"" {
  name = ""PsoxyApiCaller""

  # who can assume this role
  assume_role_policy = jsonencode({
    ""Version"" : ""2012-10-17"",
    ""Statement"" : [
      {
        ""Action"" = ""sts:AssumeRole""
        ""Effect"" : ""Allow""
        ""Principal"" : {
          ""AWS"" : ""arn:aws:iam::${var.caller_aws_account_id}""
        }
        #TODO: add condition referencing GCP service account that will auth with Worklytics AWS account to call
        #""Condition"" : {
        #  ""StringEquals"" : {
        #    ""sts:ExternalId"" : var.caller_aws_user_id
        #  }
        #}
      }
    ]
  })

  # what this role can do (invoke anything in the API gateway )
  inline_policy {
    name = ""invoke""
    policy = jsonencode({
      ""Version"" : ""2012-10-17"",
      ""Statement"" : [
        {
          ""Effect"" : ""Allow""
          ""Action"" : ""execute-api:Invoke""
          ""Resource"" : [
            ""${aws_apigatewayv2_api.psoxy-api.arn}/*/*/*"",
          ]
        }
      ]
    })
  }
}
",resource,"resource ""aws_iam_role"" ""api-caller"" {
  name = ""PsoxyApiCaller""

  # who can assume this role
  assume_role_policy = jsonencode({
    ""Version"" : ""2012-10-17"",
    ""Statement"" : [
      {
        ""Action"" = ""sts:AssumeRole""
        ""Effect"" : ""Allow""
        ""Principal"" : {
          ""AWS"" : ""arn:aws:iam::${var.caller_aws_account_id}""
        }
      },
      # allows service account to assume role
      {
        ""Effect"": ""Allow"",
        ""Principal"": {
          ""Federated"": ""accounts.google.com""
        },
        ""Action"": ""sts:AssumeRoleWithWebIdentity"",
        ""Condition"": {
          ""StringEquals"": {
            ""accounts.google.com:aud"": ""${var.caller_external_user_id}""
          }
        }
      }
    ]
  })

  # what this role can do (invoke anything in the API gateway )
  inline_policy {
    name = ""lambda-invoker""
    policy = jsonencode({
      ""Version"" : ""2012-10-17"",
      ""Statement"" : [
        {
          ""Effect"": ""Allow"",
          ""Action"": ""execute-api:Invoke"",
          ""Resource"": ""arn:aws:execute-api:*:${var.aws_account_id}:*/*/GET/*"",
        }
      ]
    })
  }
}
",resource,38,,0695d5d70696539227a4054208cc7393683913a2,c408940e26fbaebc0c78e068c25a8e0fca3aa503,https://github.com/Worklytics/psoxy/blob/0695d5d70696539227a4054208cc7393683913a2/infra/modules/aws/main.tf#L38,https://github.com/Worklytics/psoxy/blob/c408940e26fbaebc0c78e068c25a8e0fca3aa503/infra/modules/aws/main.tf,2022-01-12 22:04:03-08:00,2022-01-21 14:29:14-08:00,8,1,0,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-kubernetes-engine,6,modules/safer-cluster/main.tf,modules/safer-cluster/main.tf,0,// todo,// TODO(mmontan): check whether we need to restrict these,"// TODO(mmontan): check whether we need to restrict these 
 // settings.","module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // Disable the dashboard. It creates risk by running as a very sensitive user.
  kubernetes_dashboard = false

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy          = true
  network_policy_provider = ""CALICO""

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  disable_legacy_metadata_endpoints = true

  node_pools        = var.node_pools
  node_pools_labels = var.node_pools_labels

  // TODO(mmontan): check whether we need to restrict these
  // settings.
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  // TODO(mmontan): we generally considered applying
  // just the cloud-platofrm scope and use Cloud IAM
  // If we have Workload Identity, are there advantages
  // in restricting scopes even more?
  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  // We should use IP Alias.
  configure_ip_masq = false

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = length(var.compute_engine_service_account) > 0 ? false : true
  service_account        = var.compute_engine_service_account

  // TODO(mmontan): define a registry_project parameter in the private_beta_cluster,
  // so that we can give GCS permissions to the service account on a project
  // that hosts only container-images and not data.
  grant_registry_access = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // TODO(mmontan): link to a couple of policies.
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id
  node_metadata                    = ""SECURE""

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // TODO(mmontan): investigate whether this should be a recommended setting
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group
}
",module,"module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy = true

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  node_pools          = var.node_pools
  node_pools_labels   = var.node_pools_labels
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = var.compute_engine_service_account == """" ? true : false
  service_account        = var.compute_engine_service_account
  registry_project_id    = var.registry_project_id
  grant_registry_access  = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // Example: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#example
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // Intranode Visibility enables you to capture flow logs for traffic between pods and create FW rules that apply to traffic between pods.
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group

  enable_shielded_nodes = var.enable_shielded_nodes
}
",module,74,,e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5,1a5eb6354dc0281e7cf4c1a0f4e2159afed08e48,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5/modules/safer-cluster/main.tf#L74,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/1a5eb6354dc0281e7cf4c1a0f4e2159afed08e48/modules/safer-cluster/main.tf,2019-10-04 15:21:33-07:00,2019-11-18 08:19:13-06:00,7,1,1,1,0,0,0,0,0,1
https://github.com/uyuni-project/sumaform,122,openstack_host/main.tf,modules/openstack/host/main.tf,1,hack,// HACK: this output artificially depends on the instance id,"// HACK: this output artificially depends on the instance id 
 // any resource using this output will have to wait until instance is fully up","output ""hostname"" {
  // HACK: this output artificially depends on the instance id
  // any resource using this output will have to wait until instance is fully up
  value = ""${coalesce(""${var.name}.${var.domain}"", openstack_compute_instance_v2.instance.id)}""
}
",output,the block associated got renamed or deleted,,66,,7b44a14d4d6a74a6e0387ceb7238c62df9765b92,a3b700ceca72ca9a01cf59626300698dd6776ada,https://github.com/uyuni-project/sumaform/blob/7b44a14d4d6a74a6e0387ceb7238c62df9765b92/openstack_host/main.tf#L66,https://github.com/uyuni-project/sumaform/blob/a3b700ceca72ca9a01cf59626300698dd6776ada/modules/openstack/host/main.tf,2016-09-05 14:18:52+02:00,2017-02-23 07:41:20+01:00,15,1,1,1,0,0,0,0,0,0
https://github.com/pivotal-cf/terraforming-azure,12,weblb.tf,weblb.tf,0,workaround,# Workaround until the backend_address_pool and probe resources output their own ids,# Workaround until the backend_address_pool and probe resources output their own ids,"resource ""azurerm_lb_rule"" ""web-http-rule"" {
  name                = ""web-http-rule""
  location            = ""${var.location}""
  resource_group_name = ""${azurerm_resource_group.pcf_resource_group.name}""
  loadbalancer_id     = ""${azurerm_lb.web.id}""

  frontend_ip_configuration_name = ""frontendip""
  protocol                       = ""Tcp""
  frontend_port                  = 80
  backend_port                   = 80

  # Workaround until the backend_address_pool and probe resources output their own ids
  backend_address_pool_id = ""${azurerm_lb.web.id}/backendAddressPools/${azurerm_lb_backend_address_pool.web-backend-pool.name}""
  probe_id                = ""${azurerm_lb.web.id}/probes/${azurerm_lb_probe.web-http-probe.name}""
}
",resource,"resource ""azurerm_lb_rule"" ""web-http-rule"" {
  name                = ""web-http-rule""
  location            = ""${var.location}""
  resource_group_name = ""${azurerm_resource_group.pcf_resource_group.name}""
  loadbalancer_id     = ""${azurerm_lb.web.id}""

  frontend_ip_configuration_name = ""frontendip""
  protocol                       = ""TCP""
  frontend_port                  = 80
  backend_port                   = 80

  backend_address_pool_id = ""${azurerm_lb_backend_address_pool.web-backend-pool.id}""
  probe_id                = ""${azurerm_lb_probe.web-http-probe.id}""
}
",resource,71,,033638f5608bf6bbec84abe9557692c1b431e719,50e14eaf3aeee1711cd2335ffa73774fec13c312,https://github.com/pivotal-cf/terraforming-azure/blob/033638f5608bf6bbec84abe9557692c1b431e719/weblb.tf#L71,https://github.com/pivotal-cf/terraforming-azure/blob/50e14eaf3aeee1711cd2335ffa73774fec13c312/weblb.tf,2016-10-17 13:49:20-07:00,2016-11-10 17:43:22-08:00,7,1,0,1,0,0,1,0,0,0
https://github.com/kbst/terraform-kubestack,2,azurerm/_modules/aks/cluster_services.tf,azurerm/_modules/aks/cluster_services.tf,0,hack,"# hack, because modules can't have depends_on","# hack, because modules can't have depends_on 
 # prevent a race between kubernetes provider and cluster services/kustomize 
 # creating the namespace and the provider erroring out during apply","module ""cluster_services"" {
  source = ""../../../common/cluster_services""

  cluster_type = ""aks""

  metadata_labels = ""${var.metadata_labels}""
  label_namespace = ""${var.metadata_label_namespace}""

  template_string = ""${file(""${path.module}/templates/kubeconfig.tpl"")}""

  template_vars = {
    cluster_name     = ""${azurerm_kubernetes_cluster.current.name}""
    cluster_endpoint = ""${azurerm_kubernetes_cluster.current.kube_config.0.host}""
    cluster_ca       = ""${azurerm_kubernetes_cluster.current.kube_config.0.cluster_ca_certificate}""
    client_cert      = ""${azurerm_kubernetes_cluster.current.kube_config.0.client_certificate}""
    client_key       = ""${azurerm_kubernetes_cluster.current.kube_config.0.client_key}""
    path_cwd         = ""${path.cwd}""

    # hack, because modules can't have depends_on
    # prevent a race between kubernetes provider and cluster services/kustomize
    # creating the namespace and the provider erroring out during apply
    not_used = ""${kubernetes_namespace.current.metadata.0.name}""
  }
}
",module,"module ""cluster_services"" {
  source = ""../../../common/cluster_services""

  cluster_type = ""aks""

  metadata_labels = var.metadata_labels
  label_namespace = var.metadata_label_namespace

  template_string = file(""${path.module}/templates/kubeconfig.tpl"")

  template_vars = {
    cluster_name     = azurerm_kubernetes_cluster.current.name
    cluster_endpoint = azurerm_kubernetes_cluster.current.kube_config[0].host
    cluster_ca       = azurerm_kubernetes_cluster.current.kube_config[0].cluster_ca_certificate
    client_cert      = azurerm_kubernetes_cluster.current.kube_config[0].client_certificate
    client_key       = azurerm_kubernetes_cluster.current.kube_config[0].client_key
    path_cwd         = path.cwd
  }
}
",module,19,,9dca6071a0ffed2973ecc836044700432e3a5af3,e5caa6d20926d546a045144ebe79c7cc8c0b4c8a,https://github.com/kbst/terraform-kubestack/blob/9dca6071a0ffed2973ecc836044700432e3a5af3/azurerm/_modules/aks/cluster_services.tf#L19,https://github.com/kbst/terraform-kubestack/blob/e5caa6d20926d546a045144ebe79c7cc8c0b4c8a/azurerm/_modules/aks/cluster_services.tf,2019-03-31 11:29:36+02:00,2020-01-26 13:45:22+01:00,3,1,1,1,1,0,0,0,0,0
https://github.com/kubernetes/k8s.io,445,infra/aws/terraform/prow-build-cluster/modules/cluster-autoscaler/cluster-autoscaler.tf,infra/aws/terraform/prow-build-cluster/modules/cluster-autoscaler/cluster-autoscaler.tf,0,# todo,# TODO(xmudrii-ubuntu): Temporary mount certificates from different known paths,# TODO(xmudrii-ubuntu): Temporary mount certificates from different known paths,"resource ""kubernetes_manifest"" ""deployment_kube_system_cluster_autoscaler"" {
  manifest = {
    ""apiVersion"" = ""apps/v1""
    ""kind""       = ""Deployment""
    ""metadata"" = {
      ""labels"" = {
        ""app"" = ""cluster-autoscaler""
      }
      ""name""      = ""cluster-autoscaler""
      ""namespace"" = ""kube-system""
    }
    ""spec"" = {
      ""replicas"" = 1
      ""selector"" = {
        ""matchLabels"" = {
          ""app"" = ""cluster-autoscaler""
        }
      }
      ""template"" = {
        ""metadata"" = {
          ""annotations"" = {
            ""cluster-autoscaler.kubernetes.io/safe-to-evict"" = ""false""
            ""prometheus.io/port""                             = ""8085""
            ""prometheus.io/scrape""                           = ""true""
          }
          ""labels"" = {
            ""app"" = ""cluster-autoscaler""
          }
        }
        ""spec"" = {
          ""containers"" = [
            {
              ""command"" = [
                ""./cluster-autoscaler"",
                ""--v=4"",
                ""--stderrthreshold=info"",
                ""--cloud-provider=aws"",
                ""--skip-nodes-with-local-storage=false"",
                ""--expander=least-waste"",
                ""--node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/${var.cluster_name}"",
                ""--balance-similar-node-groups"",
                ""--skip-nodes-with-system-pods=false"",
              ]
              ""image""           = ""registry.k8s.io/autoscaling/cluster-autoscaler:${var.cluster_autoscaler_version}""
              ""imagePullPolicy"" = ""Always""
              ""name""            = ""cluster-autoscaler""
              ""resources"" = {
                ""limits"" = {
                  ""cpu""    = ""100m""
                  ""memory"" = ""600Mi""
                }
                ""requests"" = {
                  ""cpu""    = ""100m""
                  ""memory"" = ""600Mi""
                }
              }
              ""volumeMounts"" = [
                # TODO(xmudrii-ubuntu): Temporary mount certificates from different known paths
                {
                  ""mountPath"" = ""/etc/ssl/certs""
                  ""name""      = ""ssl-certs""
                  ""readOnly""  = true
                },
                {
                  ""mountPath"" = ""/etc/pki""
                  ""name""      = ""pki-certs""
                  ""readOnly""  = true
                }
              ]
            },
          ]
          ""priorityClassName"" = ""system-cluster-critical""
          ""securityContext"" = {
            ""fsGroup""      = 65534
            ""runAsNonRoot"" = true
            ""runAsUser""    = 65534
          }
          ""serviceAccountName"" = ""cluster-autoscaler""
          ""volumes"" = [
            # TODO(xmudrii-ubuntu): Temporary mount certificates from different known paths
            {
              ""hostPath"" = {
                ""path"" = ""/etc/ssl/certs""
                ""type"" = ""Directory""
              }
              ""name"" = ""ssl-certs""
            },
            {
              ""hostPath"" = {
                ""path"" = ""/etc/pki""
                ""type"" = ""Directory""
              }
              ""name"" = ""pki-certs""
            }
          ]
        }
      }
    }
  }
}
",resource,,,415,0.0,ce20b9f5b43878cf51fd23a581829f7634efc97d,1f1eac3ac2ee01cae8e5fdbbe50e5270a11126d4,https://github.com/kubernetes/k8s.io/blob/ce20b9f5b43878cf51fd23a581829f7634efc97d/infra/aws/terraform/prow-build-cluster/modules/cluster-autoscaler/cluster-autoscaler.tf#L415,https://github.com/kubernetes/k8s.io/blob/1f1eac3ac2ee01cae8e5fdbbe50e5270a11126d4/infra/aws/terraform/prow-build-cluster/modules/cluster-autoscaler/cluster-autoscaler.tf#L0,2023-10-16 10:30:37+02:00,2024-02-19 18:06:16+03:00,3,2,1,1,0,1,0,0,0,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,225,init.tf,init.tf,0,todo,"# ToDo: See ToDo in locals.tf. Also: Refactor - e.g. do not install helm by pipe to bash, pin cilium chart version etc.","# Apply cilium and wait for it to initialize all nodes 
 # We can't use the HelmChart custom resource to install cilium due to the fact that it does not tolerate the cilium taint. 
 # ToDo: See ToDo in locals.tf. Also: Refactor - e.g. do not install helm by pipe to bash, pin cilium chart version etc. 
 # Use something like this (the example below is non working) after apply instead of sleep to check until cilium has removed the taint from all nodes 
 # Might not even be required due to pods from the remaining kustomization pending until taint is removed? Needs testing. 
 # timeout 300 bash <<EOF 
 #   until [[ -n ""\$(kubectl get nodes -o jsonpath={.items[].spec.taints[?(@.effect==NoExecute)].effect}{""\t""}{.items[].metadata.name})"" ]]; do 
 #     echo ""Waiting for the cluster to become ready..."" 
 #     sleep 2 
 #   done 
 # EOF","resource ""null_resource"" ""kustomization"" {
  connection {
    user           = ""root""
    private_key    = var.ssh_private_key
    agent_identity = local.ssh_agent_identity
    host           = module.control_planes[keys(module.control_planes)[0]].ipv4_address
    port           = var.ssh_port
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content = yamlencode({
      apiVersion = ""kustomize.config.k8s.io/v1beta1""
      kind       = ""Kustomization""

      resources = concat(
        [
          ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",
          ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",
          ""https://raw.githubusercontent.com/rancher/system-upgrade-controller/master/manifests/system-upgrade-controller.yaml"",
        ],
        var.disable_hetzner_csi ? [] : [
          ""https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml""
        ],
        var.traefik_enabled ? [""traefik_config.yaml""] : [],
        lookup(local.cni_install_resources, var.cni_plugin, []),
        var.enable_longhorn ? [""longhorn.yaml""] : [],
        var.enable_cert_manager || var.enable_rancher ? [""cert_manager.yaml""] : [],
        var.enable_rancher ? [""rancher.yaml""] : [],
        var.rancher_registration_manifest_url != """" ? [var.rancher_registration_manifest_url] : []
      ),
      patchesStrategicMerge = concat(
        [
          file(""${path.module}/kustomize/kured.yaml""),
          file(""${path.module}/kustomize/system-upgrade-controller.yaml""),
          ""ccm.yaml"",
        ],
        lookup(local.cni_install_resource_patches, var.cni_plugin, [])
      )
    })
    destination = ""/var/post_install/kustomization.yaml""
  }

  # Upload traefik config
  provisioner ""file"" {
    content = local.using_klipper_lb || var.traefik_enabled == false ? """" : templatefile(
      ""${path.module}/templates/traefik_config.yaml.tpl"",
      {
        name                       = ""${var.cluster_name}-traefik""
        load_balancer_disable_ipv6 = var.load_balancer_disable_ipv6
        load_balancer_type         = var.load_balancer_type
        location                   = var.load_balancer_location
        traefik_acme_tls           = var.traefik_acme_tls
        traefik_acme_email         = var.traefik_acme_email
        traefik_additional_options = var.traefik_additional_options
        using_hetzner_lb           = !local.using_klipper_lb
    })
    destination = ""/var/post_install/traefik_config.yaml""
  }

  # Upload the CCM patch config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/ccm.yaml.tpl"",
      {
        cluster_cidr_ipv4                 = local.cluster_cidr_ipv4
        allow_scheduling_on_control_plane = local.allow_scheduling_on_control_plane
        default_lb_location               = var.load_balancer_location
        using_hetzner_lb                  = !local.using_klipper_lb
    })
    destination = ""/var/post_install/ccm.yaml""
  }

  # Upload the calico patch config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/calico.yaml.tpl"",
      {
        cluster_cidr_ipv4 = local.cluster_cidr_ipv4
    })
    destination = ""/var/post_install/calico.yaml""
  }

  # Upload the cilium install file
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cilium.yaml.tpl"",
      {
        values = trimspace(fileexists(""cilium_values.yaml"") ? file(""cilium_values.yaml"") : local.default_cilium_values)
    })
    destination = ""/tmp/cilium.yaml""
  }

  # Upload the system upgrade controller plans config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/plans.yaml.tpl"",
      {
        channel = var.initial_k3s_channel
    })
    destination = ""/var/post_install/plans.yaml""
  }

  # Upload the Longhorn config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/longhorn.yaml.tpl"",
      {
        disable_hetzner_csi    = var.disable_hetzner_csi,
        longhorn_fstype        = var.longhorn_fstype,
        longhorn_replica_count = var.longhorn_replica_count
    })
    destination = ""/var/post_install/longhorn.yaml""
  }

  # Upload the cert-manager config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cert_manager.yaml.tpl"",
    {})
    destination = ""/var/post_install/cert_manager.yaml""
  }

  # Upload the Rancher config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/rancher.yaml.tpl"",
      {
        rancher_install_channel    = var.rancher_install_channel
        rancher_hostname           = var.rancher_hostname
        rancher_bootstrap_password = length(var.rancher_bootstrap_password) == 0 ? resource.random_password.rancher_bootstrap[0].result : var.rancher_bootstrap_password
        number_control_plane_nodes = length(local.control_plane_nodes)
    })
    destination = ""/var/post_install/rancher.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""set -ex"",
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name} --dry-run=client -o yaml | kubectl apply -f -"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token} --dry-run=client -o yaml | kubectl apply -f -"",
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = concat([
      ""set -ex"",

      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /var/post_install/kustomization.yaml"",

      # Wait for k3s to become ready (we check one more time) because in some edge cases,
      # the cluster had become unvailable for a few seconds, at this very instant.
      <<-EOT
      timeout 180 bash <<EOF
        until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do
          echo ""Waiting for the cluster to become ready...""
          sleep 2
        done
      EOF
      EOT
      ]
      ,

      # Apply cilium and wait for it to initialize all nodes
      # We can't use the HelmChart custom resource to install cilium due to the fact that it does not tolerate the cilium taint.
      # ToDo: See ToDo in locals.tf. Also: Refactor - e.g. do not install helm by pipe to bash, pin cilium chart version etc.
      # Use something like this (the example below is non working) after apply instead of sleep to check until cilium has removed the taint from all nodes
      # Might not even be required due to pods from the remaining kustomization pending until taint is removed? Needs testing.
      # timeout 300 bash <<EOF
      #   until [[ -n ""\$(kubectl get nodes -o jsonpath={.items[].spec.taints[?(@.effect==NoExecute)].effect}{""\t""}{.items[].metadata.name})"" ]]; do
      #     echo ""Waiting for the cluster to become ready...""
      #     sleep 2
      #   done
      # EOF
      var.cni_plugin == ""cilium"" ? [
        <<-EOT
        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
        helm repo add cilium https://helm.cilium.io/
        helm template cilium cilium/cilium --namespace kube-system --values /tmp/cilium.yaml --include-crds | kubectl apply -f -
        sleep 30
        rm -rf /tmp/cilium.yaml
        EOT
      ] : []
      ,

      [
        # Ready, set, go for the kustomization
        ""kubectl apply -k /var/post_install"",
        ""echo 'Waiting for the system-upgrade-controller deployment to become available...'"",
        ""kubectl -n system-upgrade wait --for=condition=available --timeout=180s deployment/system-upgrade-controller"",
        ""sleep 5"", # important as the system upgrade controller CRDs sometimes don't get ready right away, especially with Cilium.
        ""kubectl -n system-upgrade apply -f /var/post_install/plans.yaml""
      ],
      local.using_klipper_lb || var.traefik_enabled == false ? [] : [
        <<-EOT
      timeout 120 bash <<EOF
      until [ -n ""\$(kubectl get -n kube-system service/traefik --output=jsonpath='{.status.loadBalancer.ingress[0].ip}' 2> /dev/null)"" ]; do
          echo ""Waiting for load-balancer to get an IP...""
          sleep 2
      done
      EOF
      EOT
    ])
  }

  depends_on = [
    null_resource.first_control_plane,
    local_sensitive_file.kubeconfig,
    random_password.rancher_bootstrap
  ]
}
",resource,"resource ""null_resource"" ""kustomization"" {
  connection {
    user           = ""root""
    private_key    = var.ssh_private_key
    agent_identity = local.ssh_agent_identity
    host           = module.control_planes[keys(module.control_planes)[0]].ipv4_address
    port           = var.ssh_port
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content = yamlencode({
      apiVersion = ""kustomize.config.k8s.io/v1beta1""
      kind       = ""Kustomization""

      resources = concat(
        [
          ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",
          ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",
          ""https://raw.githubusercontent.com/rancher/system-upgrade-controller/master/manifests/system-upgrade-controller.yaml"",
        ],
        var.disable_hetzner_csi ? [] : [
          ""https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml""
        ],
        var.traefik_enabled ? [""traefik_config.yaml""] : [],
        lookup(local.cni_install_resources, var.cni_plugin, []),
        var.enable_longhorn ? [""longhorn.yaml""] : [],
        var.enable_cert_manager || var.enable_rancher ? [""cert_manager.yaml""] : [],
        var.enable_rancher ? [""rancher.yaml""] : [],
        var.rancher_registration_manifest_url != """" ? [var.rancher_registration_manifest_url] : []
      ),
      patchesStrategicMerge = concat(
        [
          file(""${path.module}/kustomize/kured.yaml""),
          file(""${path.module}/kustomize/system-upgrade-controller.yaml""),
          ""ccm.yaml"",
        ],
        lookup(local.cni_install_resource_patches, var.cni_plugin, [])
      )
    })
    destination = ""/var/post_install/kustomization.yaml""
  }

  # Upload traefik config
  provisioner ""file"" {
    content = local.using_klipper_lb || var.traefik_enabled == false ? """" : templatefile(
      ""${path.module}/templates/traefik_config.yaml.tpl"",
      {
        name                       = ""${var.cluster_name}-traefik""
        load_balancer_disable_ipv6 = var.load_balancer_disable_ipv6
        load_balancer_type         = var.load_balancer_type
        location                   = var.load_balancer_location
        traefik_acme_tls           = var.traefik_acme_tls
        traefik_acme_email         = var.traefik_acme_email
        traefik_additional_options = var.traefik_additional_options
        using_hetzner_lb           = !local.using_klipper_lb
    })
    destination = ""/var/post_install/traefik_config.yaml""
  }

  # Upload the CCM patch config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/ccm.yaml.tpl"",
      {
        cluster_cidr_ipv4                 = local.cluster_cidr_ipv4
        allow_scheduling_on_control_plane = local.allow_scheduling_on_control_plane
        default_lb_location               = var.load_balancer_location
        using_hetzner_lb                  = !local.using_klipper_lb
    })
    destination = ""/var/post_install/ccm.yaml""
  }

  # Upload the calico patch config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/calico.yaml.tpl"",
      {
        cluster_cidr_ipv4 = local.cluster_cidr_ipv4
    })
    destination = ""/var/post_install/calico.yaml""
  }

  # Upload the cilium install file
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cilium.yaml.tpl"",
      {
        values = indent(4, trimspace(fileexists(""cilium_values.yaml"") ? file(""cilium_values.yaml"") : local.default_cilium_values))
    })
    destination = ""/var/post_install/cilium.yaml""
  }

  # Upload the system upgrade controller plans config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/plans.yaml.tpl"",
      {
        channel = var.initial_k3s_channel
    })
    destination = ""/var/post_install/plans.yaml""
  }

  # Upload the Longhorn config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/longhorn.yaml.tpl"",
      {
        disable_hetzner_csi    = var.disable_hetzner_csi,
        longhorn_fstype        = var.longhorn_fstype,
        longhorn_replica_count = var.longhorn_replica_count
    })
    destination = ""/var/post_install/longhorn.yaml""
  }

  # Upload the cert-manager config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cert_manager.yaml.tpl"",
    {})
    destination = ""/var/post_install/cert_manager.yaml""
  }

  # Upload the Rancher config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/rancher.yaml.tpl"",
      {
        rancher_install_channel    = var.rancher_install_channel
        rancher_hostname           = var.rancher_hostname
        rancher_bootstrap_password = length(var.rancher_bootstrap_password) == 0 ? resource.random_password.rancher_bootstrap[0].result : var.rancher_bootstrap_password
        number_control_plane_nodes = length(local.control_plane_nodes)
    })
    destination = ""/var/post_install/rancher.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""set -ex"",
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name} --dry-run=client -o yaml | kubectl apply -f -"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token} --dry-run=client -o yaml | kubectl apply -f -"",
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = concat([
      ""set -ex"",

      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /var/post_install/kustomization.yaml"",

      # Wait for k3s to become ready (we check one more time) because in some edge cases,
      # the cluster had become unvailable for a few seconds, at this very instant.
      <<-EOT
      timeout 180 bash <<EOF
        until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do
          echo ""Waiting for the cluster to become ready...""
          sleep 2
        done
      EOF
      EOT
      ]
      ,

      [
        # Ready, set, go for the kustomization
        ""kubectl apply -k /var/post_install"",
        ""echo 'Waiting for the system-upgrade-controller deployment to become available...'"",
        ""kubectl -n system-upgrade wait --for=condition=available --timeout=180s deployment/system-upgrade-controller"",
        ""sleep 5"", # important as the system upgrade controller CRDs sometimes don't get ready right away, especially with Cilium.
        ""kubectl -n system-upgrade apply -f /var/post_install/plans.yaml""
      ],
      local.using_klipper_lb || var.traefik_enabled == false ? [] : [
        <<-EOT
      timeout 120 bash <<EOF
      until [ -n ""\$(kubectl get -n kube-system service/traefik --output=jsonpath='{.status.loadBalancer.ingress[0].ip}' 2> /dev/null)"" ]; do
          echo ""Waiting for load-balancer to get an IP...""
          sleep 2
      done
      EOF
      EOT
    ])
  }

  depends_on = [
    null_resource.first_control_plane,
    local_sensitive_file.kubeconfig,
    random_password.rancher_bootstrap
  ]
}
",resource,254,,85a20e91ca41fa82b34456dd3d82b84cd9495554,4d2ba124929af924e01d86c446fce37eb003d601,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/85a20e91ca41fa82b34456dd3d82b84cd9495554/init.tf#L254,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/4d2ba124929af924e01d86c446fce37eb003d601/init.tf,2022-08-28 13:43:57+02:00,2022-08-29 15:59:35+02:00,2,1,1,1,0,0,0,1,0,0
https://github.com/Worklytics/psoxy,1825,infra/modules/psoxy-constants/main.tf,infra/modules/psoxy-constants/main.tf,0,# todo,"# TODO: create IAM policy document, which installer could use to create their own policy as","# TODO: create IAM policy document, which installer could use to create their own policy as 
 # alternative to using AWS Managed policies ","locals {

  # AWS Managed polices
  # see: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#aws-managed-policies
  required_aws_roles_to_provision_host = {
    ""arn:aws:iam::aws:policy/IAMFullAccess""        = ""IAMFullAccess""
    ""arn:aws:iam::aws:policy/AmazonS3FullAccess""   = ""AmazonS3FullAccess""
    ""arn:aws:iam::aws:policy/CloudWatchFullAccess"" = ""CloudWatchFullAccess""
    ""arn:aws:iam::aws:policy/AmazonSSMFullAccess""  = ""AmazonSSMFullAccess""
    ""arn:aws:iam::aws:policy/AWSLambda_FullAccess"" = ""AWSLambda_FullAccess""
  }
  # TODO: create IAM policy document, which installer could use to create their own policy as
  # alternative to using AWS Managed policies

  required_gcp_roles_to_provision_host = {
    ""roles/storage.admin"" = {
      display_name    = ""Storage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#storage.admin""
    },
    ""roles/iam.roleAdmin"" = {
      display_name    = ""IAM Role Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.roleAdmin""
    },
    ""roles/secretmanager.admin"" = {
      display_name    = ""Secret Manager Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#secretmanager.admin""
    },
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    },
    ""roles/cloudfunctions.admin"" = {
      display_name    = ""Cloud Functions Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#cloudfunctions.admin""
    },
  }  # TODO: add list of permissions, which customer could use to create custom role as alternative

  required_gcp_roles_to_provision_google_workspace_source = {
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    }
  }
  # TODO: add list of permissions, which customer could use to create custom role as alternative

  required_azuread_roles_to_provision_msft_365_source = {
    ""7ab1d382-f21e-4acd-a863-ba3e13f7da61"" = ""Cloud Application Administrator"",
  }
}
",locals,"locals {

  # AWS Managed polices
  # see: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#aws-managed-policies
  required_aws_roles_to_provision_host = {
    ""arn:aws:iam::aws:policy/IAMFullAccess""        = ""IAMFullAccess""
    ""arn:aws:iam::aws:policy/AmazonS3FullAccess""   = ""AmazonS3FullAccess"" # only if using bulk sources, although 95% do
    ""arn:aws:iam::aws:policy/CloudWatchFullAccess"" = ""CloudWatchFullAccess""
    ""arn:aws:iam::aws:policy/AmazonSSMFullAccess""  = ""AmazonSSMFullAccess""
    ""arn:aws:iam::aws:policy/AWSLambda_FullAccess"" = ""AWSLambda_FullAccess""
  }
  # AWS managed policy required to consume Microsoft 365 data
  # (in addition to above)
  required_aws_managed_policies_to_consume_msft_365_source = {
    ""arn:aws:iam::aws:policy/AmazonCognitoPowerUser"" = ""AmazonCognitoPowerUser""
  }

  # subset of https://docs.aws.amazon.com/aws-managed-policy/latest/reference/SecretsManagerReadWrite.html
  # as that seems like overkill
  #  - if you're going to use KMS to encrypt the secrets, then you'll need to add the KMS permissions
  #    on the key you intend to use.
  #  - you can/should modify the Resource part of this to limit to a subset of secrets, if this
  #    is being deployed to an AWS account that's used for purposes beyond this proxy deployment
  required_aws_policy_to_use_secrets_manager = {
    ""Version"" : ""2012-10-17"",
    ""Statement"" : [
      {
        ""Effect"" : ""Allow"",
        ""Action"" : [
          ""secretsmanager:*"",
          ""tag:GetResources""
        ],
        ""Resource"" : ""*""
      }
    ]
  }


  # TODO: create IAM policy document, which installer could use to create their own policy as
  # alternative to using AWS Managed policies

  # initial GCP APIs that must be enabled in projects that will host the proxy.
  # (Terraform apply will enabled additional ones)
  required_gcp_apis_to_host = {
    # https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com
    ""iamcredentials.googleapis.com"" = ""IAM Service Account Credentials API"",
    # https://console.cloud.google.com/apis/library/serviceusage.googleapis.com
    ""serviceusage.googleapis.com"" = ""Service Usage API"",
  }

  required_gcp_roles_to_provision_host = {
    ""roles/storage.admin"" = {
      display_name    = ""Storage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#storage.admin""
    },
    ""roles/iam.roleAdmin"" = {
      display_name    = ""IAM Role Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.roleAdmin""
    },
    ""roles/secretmanager.admin"" = {
      display_name    = ""Secret Manager Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#secretmanager.admin""
    },
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    },
    ""roles/cloudfunctions.admin"" = {
      display_name    = ""Cloud Functions Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#cloudfunctions.admin""
    },
  }
  # TODO: add list of permissions, which customer could use to create custom role as alternative


  # TODO: confirm that this is indeed the same list (believe it is)
  required_gcp_apis_to_provision_google_workspace_source = local.required_gcp_apis_to_host

  required_gcp_roles_to_provision_google_workspace_source = {
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    }
  }
  # TODO: add list of permissions, which customer could use to create custom role as alternative

  required_azuread_roles_to_provision_msft_365_source = {
    ""7ab1d382-f21e-4acd-a863-ba3e13f7da61"" = ""Cloud Application Administrator"",
  }
}
",locals,12,39.0,afe4f8e792fc412e4d03b347a7e566d47a7fa0d2,005e1fed5f46b4310d81d41d29862bb1c4f360b0,https://github.com/Worklytics/psoxy/blob/afe4f8e792fc412e4d03b347a7e566d47a7fa0d2/infra/modules/psoxy-constants/main.tf#L12,https://github.com/Worklytics/psoxy/blob/005e1fed5f46b4310d81d41d29862bb1c4f360b0/infra/modules/psoxy-constants/main.tf#L39,2023-06-23 19:10:00+00:00,2024-02-06 19:07:07+00:00,5,0,0,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,4294,infra/modules/aws/main.tf,infra/modules/aws/main.tf,0,# todo,"# TODO: it would maximize granularity of policy to push this into `aws-psoxy-rest` module, and","# TODO: it would maximize granularity of policy to push this into `aws-psoxy-rest` module, and 
 # do the statements based on configured list of http methods; but cost of that is policy + attachment 
 # for each instance, instead of one per deployment","resource ""aws_iam_policy"" ""invoke_api"" {
  count = var.use_api_gateway_v2 ? 1 : 0

  name_prefix = ""${var.deployment_id}InvokeAPI""

  policy = jsonencode({
    ""Version"" : ""2012-10-17"",
    ""Statement"" : [
      {
        ""Effect"" : ""Allow"",
        ""Action"" : ""execute-api:Invoke"",
        ""Resource"" : ""arn:aws:execute-api:*:${var.aws_account_id}:${aws_apigatewayv2_api.proxy_api[0].id}/*/GET/*"",
      },
      {
        ""Effect"" : ""Allow"",
        ""Action"" : ""execute-api:Invoke"",
        ""Resource"" : ""arn:aws:execute-api:*:${var.aws_account_id}:${aws_apigatewayv2_api.proxy_api[0].id}/*/HEAD/*"",
      },
      {
        ""Effect"" : ""Allow"",
        ""Action"" : ""execute-api:Invoke"",
        ""Resource"" : ""arn:aws:execute-api:*:${var.aws_account_id}:${aws_apigatewayv2_api.proxy_api[0].id}/*/POST/*"",
      },
    ]
  })
}
",resource,"resource ""aws_iam_policy"" ""invoke_api"" {
  count = var.use_api_gateway_v2 ? 1 : 0

  name_prefix = ""${var.deployment_id}InvokeAPI""

  policy = jsonencode({
    ""Version"" : ""2012-10-17"",
    ""Statement"" : [
      {
        ""Effect"" : ""Allow"",
        ""Action"" : ""execute-api:Invoke"",
        ""Resource"" : ""arn:aws:execute-api:*:${var.aws_account_id}:${aws_apigatewayv2_api.proxy_api[0].id}/*/GET/*"",
      },
      {
        ""Effect"" : ""Allow"",
        ""Action"" : ""execute-api:Invoke"",
        ""Resource"" : ""arn:aws:execute-api:*:${var.aws_account_id}:${aws_apigatewayv2_api.proxy_api[0].id}/*/HEAD/*"",
      },
      {
        ""Effect"" : ""Allow"",
        ""Action"" : ""execute-api:Invoke"",
        ""Resource"" : ""arn:aws:execute-api:*:${var.aws_account_id}:${aws_apigatewayv2_api.proxy_api[0].id}/*/POST/*"",
      },
    ]
  })
}
",resource,172,172.0,5ed7bc9ef2e9108a02f4d42e7d0d156eb5c7184d,e07a69ceca80240af2462aa09465535cc795d0b6,https://github.com/Worklytics/psoxy/blob/5ed7bc9ef2e9108a02f4d42e7d0d156eb5c7184d/infra/modules/aws/main.tf#L172,https://github.com/Worklytics/psoxy/blob/e07a69ceca80240af2462aa09465535cc795d0b6/infra/modules/aws/main.tf#L172,2024-01-31 10:34:59-08:00,2024-03-05 12:38:07-08:00,2,0,0,1,0,1,0,0,0,0
https://github.com/kubernetes-sigs/kubespray,1,contrib/terraform/aws/modules/vpc/main.tf,contrib/terraform/aws/modules/vpc/main.tf,0,#todo,#TODO: Do we need two routing tables for each subnet for redundancy or is one enough?,"#Routing in VPC  
 #TODO: Do we need two routing tables for each subnet for redundancy or is one enough? ","resource ""aws_route_table"" ""kubernetes-public"" {
    vpc_id = ""${aws_vpc.cluster-vpc.id}""
    route {
        cidr_block = ""0.0.0.0/0""
        gateway_id = ""${aws_internet_gateway.cluster-vpc-internetgw.id}""
    }
    tags {
        Name = ""kubernetes-${var.aws_cluster_name}-routetable-public""
    }
}
",resource,"resource ""aws_route_table"" ""kubernetes-public"" {
  vpc_id = aws_vpc.cluster-vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.cluster-vpc-internetgw.id
  }

  tags = merge(var.default_tags, tomap({
    Name = ""kubernetes-${var.aws_cluster_name}-routetable-public""
  }))
}
",resource,61,60.0,3c6b1480b806bcf8ac77e87a9cba0a637c04fe6d,3d4baea01c2af2f8174fc64a24a2768c4b2dbb96,https://github.com/kubernetes-sigs/kubespray/blob/3c6b1480b806bcf8ac77e87a9cba0a637c04fe6d/contrib/terraform/aws/modules/vpc/main.tf#L61,https://github.com/kubernetes-sigs/kubespray/blob/3d4baea01c2af2f8174fc64a24a2768c4b2dbb96/contrib/terraform/aws/modules/vpc/main.tf#L60,2017-03-06 12:52:02+01:00,2022-04-12 10:05:23-07:00,10,0,0,1,0,0,1,0,0,0
https://github.com/Worklytics/psoxy,695,infra/modules/worklytics-psoxy-connection-generic/main.tf,infra/modules/worklytics-psoxy-connection-generic/main.tf,0,todo,"# TODO try to avoid repetition of ""per_setting"" instructions (manual vs. deep linking)","# TODO try to avoid repetition of ""per_setting"" instructions (manual vs. deep linking) 
 # use 4 whitespace indentation for sub-lists","locals {
  # for backwards compatibility < 0.4.6
  instance_id = coalesce(var.psoxy_instance_id, var.display_name)

  # build TODO

  worklytics_add_connection_url = ""https://intl.worklytics.co/analytics/connect/""

  # map of Worklytics setting key --> display name
  autofilled_settings = {
    PROXY_AWS_ROLE_ARN = ""AWS Psoxy Role ARN"",
    PROXY_AWS_REGION   = ""AWS Psoxy Region""
    PROXY_ENDPOINT     = ""Psoxy Base URL""
    PROXY_BUCKET_NAME  = ""Bucket Name""
  }

  query_params = [for k, v in local.autofilled_settings : ""${k}=${urlencode(var.settings_to_provide[(v)])}""
  if contains(keys(var.settings_to_provide), v)]
  query_param_string = join(""&"", local.query_params)

  # TODO try to avoid repetition of ""per_setting"" instructions (manual vs. deep linking)
  # use 4 whitespace indentation for sub-lists
  per_setting_instructions      = [for k, v in var.settings_to_provide : ""    - Copy and paste `${v}` as the value for \""${k}\""."" if !contains(values(local.autofilled_settings), k)]
  per_setting_instructions_text = length(var.settings_to_provide) > 0 ? ""\n${join(""\n"", tolist(local.per_setting_instructions))}"" : """"

  per_setting_instructions_manual      = [for k, v in var.settings_to_provide : ""    - Copy and paste `${v}` as the value for \""${k}\"".""]
  per_setting_instructions_manual_text = length(var.settings_to_provide) > 0 ? ""\n${join(""\n"", tolist(local.per_setting_instructions_manual))}"" : """"

  deep_link_base = ""${local.worklytics_add_connection_url}${var.connector_id}/settings?PROXY_DEPLOYMENT_KIND=${var.psoxy_host_platform_id}&${local.query_param_string}""

  manual_instructions = <<EOT
1. Visit https://intl.worklytics.co/analytics/integrations (or login into Worklytics, and navigate to
   Manage --> Data Connections)
2. Click on the 'Add new connection' in the upper right.
3. Find the connector named ""${var.display_name}"" and click 'Connect'.
    - If presented with a further screen with several options, choose the 'via Psoxy' one.
4. Review instructions and click 'Connect' again.
5. Select `${var.psoxy_host_platform_id}` for ""Proxy Instance Type"".${local.per_setting_instructions_manual_text}
6. Review any additional settings that connector supports, adjusting values as you see fit, then
   click ""Connect"".
EOT


  deep_link_instructions = <<EOT
1. Ensure you're authenticated with Worklytics. Either sign-in at `https://app.worklytics.co` with
   your organization's SSO provider *or* request OTP link from your Worklytics support team.
2. Visit `${local.deep_link_base}`.${local.per_setting_instructions_text}
3. Review any additional settings that connector supports, adjusting values as you see fit, then
   click ""Connect"".

Alternatively, you may follow the manual instructions below:
${local.manual_instructions}
EOT

}
",locals,"locals {
  # for backwards compatibility < 0.4.6
  instance_id = coalesce(var.psoxy_instance_id, var.display_name)

  # build TODO

  worklytics_add_connection_url = ""https://${var.worklytics_host}/analytics/connect/""

  # map of Worklytics setting key --> display name (matches `settings_to_provide` keys)
  autofilled_settings = {
    PROXY_AWS_ROLE_ARN  = ""AWS Psoxy Role ARN"",
    PROXY_AWS_REGION    = ""AWS Psoxy Region""
    PROXY_ENDPOINT      = ""Psoxy Base URL""
    PROXY_BUCKET_NAME   = ""Bucket Name""
    parserId            = ""Parser""
    CLOUD_ID            = ""Jira Cloud Id""
    GITHUB_ORGANIZATION = ""GitHub Organization""
  }

  query_params = [for param_name, ux_name in local.autofilled_settings : ""${param_name}=${urlencode(var.settings_to_provide[ux_name])}""
  if contains(keys(var.settings_to_provide), ux_name) && try(var.settings_to_provide[ux_name] != null, false)]
  query_param_string = join(""&"", local.query_params)

  # TODO try to avoid repetition of ""per_setting"" instructions (manual vs. deep linking)
  # use 4 whitespace indentation for sub-lists
  per_setting_instructions      = [for ux_name, value_to_provide in var.settings_to_provide : ""    - Copy and paste `${value_to_provide}` as the value for \""${ux_name}\""."" if !contains(values(local.autofilled_settings), ux_name)]
  per_setting_instructions_text = length(var.settings_to_provide) > 0 ? ""\n${join(""\n"", tolist(local.per_setting_instructions))}"" : """"

  per_setting_instructions_manual      = [for ux_name, value_to_provide in var.settings_to_provide : ""    - Copy and paste `${value_to_provide}` as the value for \""${ux_name}\""."" if value_to_provide != null]
  per_setting_instructions_manual_text = length(var.settings_to_provide) > 0 ? ""\n${join(""\n"", tolist(local.per_setting_instructions_manual))}"" : """"

  deep_link_base = ""${local.worklytics_add_connection_url}${var.connector_id}/settings?PROXY_DEPLOYMENT_KIND=${var.psoxy_host_platform_id}&${local.query_param_string}""

  manual_instructions = <<EOT
1. Visit https://intl.worklytics.co/analytics/integrations (or login into Worklytics, and navigate to
   Manage --> Data Connections)
2. Click on the 'Add new connection' in the upper right.
3. Find the connector named ""${var.display_name}"" and click 'Connect'.
    - If presented with a further screen with several options, choose the 'via Psoxy' one.
4. Review instructions and click 'Connect' again.
5. Select `${var.psoxy_host_platform_id}` for ""Proxy Instance Type"".${local.per_setting_instructions_manual_text}
6. Review any additional settings that connector supports, adjusting values as you see fit, then
   click ""Connect"".
EOT


  deep_link_instructions = <<EOT
1. Ensure you're authenticated with Worklytics. Either sign-in at `https://app.worklytics.co` with
   your organization's SSO provider *or* request OTP link from your Worklytics support team.
2. Visit `${local.deep_link_base}`.${local.per_setting_instructions_text}
3. Review any additional settings that connector supports, adjusting values as you see fit, then
   click ""Connect"".

Alternatively, you may follow the manual instructions below:
${local.manual_instructions}
EOT

  todo_content = <<EOT
Complete the following steps in Worklytics AFTER you have deployed the Psoxy instance for your connection:

${var.connector_id == """" ? local.manual_instructions : local.deep_link_instructions}

Worklytics will attempt some basic health checks to ensure your Psoxy instance is reachable and
configured correctly. If this fails, contact support@worklytics.co for guidance.

EOT

}
",locals,22,25.0,04b1de74258a32bef06b0e6329fb54b07154259a,5937e6a45055a94ff2b493f96f21863d91699825,https://github.com/Worklytics/psoxy/blob/04b1de74258a32bef06b0e6329fb54b07154259a/infra/modules/worklytics-psoxy-connection-generic/main.tf#L22,https://github.com/Worklytics/psoxy/blob/5937e6a45055a94ff2b493f96f21863d91699825/infra/modules/worklytics-psoxy-connection-generic/main.tf#L25,2023-02-15 12:28:37-08:00,2024-03-06 18:11:21+00:00,9,0,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd,122,modules/gke-cloudbuild-private-pool/main.tf,modules/gke-cloudbuild-private-pool/main.tf,0,# todo,## TODO: GKE network self_link,"network    = ""https://www.googleapis.com/compute/v1/projects/<PROJECT_ID>/global/networks/local-network"" ## TODO: GKE network self_link","module ""vpn_ha-2"" {
  source     = ""terraform-google-modules/vpn/google//modules/vpn_ha""
  version    = ""~> 1.3.0""
  project_id = var.project_id
  region     = var.location
  network    = ""https://www.googleapis.com/compute/v1/projects/<PROJECT_ID>/global/networks/local-network"" ## TODO: GKE network self_link
  name       = ""gke-to-cloudbuild""
  router_asn = 64513
  peer_gcp_gateway = module.vpn_ha-1.self_link
  tunnels = {
    remote-0 = {
      bgp_peer = {
        address = ""169.254.1.2""
        asn     = 64514
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.1.1/30""
      ike_version       = 2
      vpn_gateway_interface = 0
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1.random_secret
    }
    remote-1 = {
      bgp_peer = {
        address = ""169.254.2.2""
        asn     = 64514
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.2.1/30""
      ike_version       = 2
      vpn_gateway_interface = 1
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1.random_secret
    }
  }
}",module,"module ""vpn_ha-2"" {
  count = length(local.gke_networks)

  source     = ""terraform-google-modules/vpn/google//modules/vpn_ha""
  version    = ""~> 1.3.0""
  project_id = local.gke_networks[count.index].project_id
  region     = local.gke_networks[count.index].location
  network    = local.gke_networks[count.index].network 
  name       = ""${local.gke_networks[count.index].network}-to-cloudbuild""
  router_asn = 65002+(count.index*2)
  peer_gcp_gateway = module.vpn_ha-1[count.index].self_link
  tunnels = {
    remote-0 = {
      bgp_peer = {
        address = ""169.254.${1+(count.index*2)}.1""
        asn     = 65001+(count.index*2)
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.${1+(count.index*2)}.2/30""
      ike_version       = 2
      vpn_gateway_interface = 0
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1[count.index].random_secret
    }
    remote-1 = {
      bgp_peer = {
        address = ""169.254.${2+(count.index*2)}.1""
        asn     = 65001+(count.index*2)
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.${2+(count.index*2)}.2/30""
      ike_version       = 2
      vpn_gateway_interface = 1
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1[count.index].random_secret
    }
  }
}",module,106,,a24b05ad924b0f7de098ce76711bc8cf32d12dbd,e29ac91f2eedf8a48e82065434b81010d298a423,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/a24b05ad924b0f7de098ce76711bc8cf32d12dbd/modules/gke-cloudbuild-private-pool/main.tf#L106,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/e29ac91f2eedf8a48e82065434b81010d298a423/modules/gke-cloudbuild-private-pool/main.tf,2021-11-24 15:09:55-06:00,2021-12-06 17:46:36-06:00,3,1,0,1,0,0,1,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-vpc,1,main.tf,main.tf,0,workaround,"# Workaround for interpolation not being able to ""short-circuit"" the evaluation of the conditional branch that doesn't end up being used","############## 
 # NAT Gateway 
 ############## 
 # Workaround for interpolation not being able to ""short-circuit"" the evaluation of the conditional branch that doesn't end up being used 
 # Source: https://github.com/hashicorp/terraform/issues/11566#issuecomment-289417805 
 # 
 # The logical expression would be 
 # 
 #    nat_gateway_ips = var.reuse_nat_ips ? var.external_nat_ip_ids : aws_eip.nat.*.id 
 # 
 # but then when count of aws_eip.nat.*.id is zero, this would throw a resource not found error on aws_eip.nat.*.id.","locals {
  nat_gateway_ips = ""${split("","", (var.reuse_nat_ips ? join("","", var.external_nat_ip_ids) : join("","", aws_eip.nat.*.id)))}""
}
",locals,"locals {
  max_subnet_length = max(
    length(var.private_subnets),
    length(var.elasticache_subnets),
    length(var.database_subnets),
    length(var.redshift_subnets),
  )
  nat_gateway_count = var.single_nat_gateway ? 1 : var.one_nat_gateway_per_az ? length(var.azs) : local.max_subnet_length

  # Use `local.vpc_id` to give a hint to Terraform that subnets should be deleted before secondary CIDR blocks can be free!
  vpc_id = try(aws_vpc_ipv4_cidr_block_association.this[0].vpc_id, aws_vpc.this[0].id, """")
}
",locals,159,,5c111ad16c93b4982621b91dacdc120f7b4faa6c,d1adf743b27ef131b559ec15c7aadc37466a74b9,https://github.com/terraform-aws-modules/terraform-aws-vpc/blob/5c111ad16c93b4982621b91dacdc120f7b4faa6c/main.tf#L159,https://github.com/terraform-aws-modules/terraform-aws-vpc/blob/d1adf743b27ef131b559ec15c7aadc37466a74b9/main.tf,2017-12-07 16:32:22+01:00,2022-01-13 15:10:46+01:00,95,1,0,1,1,0,1,0,0,0
https://github.com/alphagov/govuk-aws,146,terraform/projects/infra-security-groups/whitehall-backend.tf,terraform/projects/infra-security-groups/whitehall-backend.tf,0,# todo,# TODO: replace this with ingress from the LBs when we build them.,# TODO: replace this with ingress from the LBs when we build them.,"resource ""aws_security_group_rule"" ""allow_management_to_whitehall-backend_elb"" {
  type      = ""ingress""
  from_port = 443
  to_port   = 443
  protocol  = ""tcp""

  security_group_id        = ""${aws_security_group.whitehall-backend_elb.id}""
  source_security_group_id = ""${aws_security_group.management.id}""
}
",resource,the block associated got renamed or deleted,,47,,b4e9a049cc7f645b739d9b021ed7d4fe9f929a8c,7fd13330f8d6238e108f76ce76a76a28a99caaaf,https://github.com/alphagov/govuk-aws/blob/b4e9a049cc7f645b739d9b021ed7d4fe9f929a8c/terraform/projects/infra-security-groups/whitehall-backend.tf#L47,https://github.com/alphagov/govuk-aws/blob/7fd13330f8d6238e108f76ce76a76a28a99caaaf/terraform/projects/infra-security-groups/whitehall-backend.tf,2017-07-25 12:48:54+01:00,2018-01-02 17:41:32+00:00,4,1,0,1,0,1,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1835,modules/vpc-sc/factory.tf,modules/vpc-sc/factory.tf,0,# todo,# TODO: add checks that emulate the variable validations,# TODO: add checks that emulate the variable validations,"locals {
  _data = {
    for k, v in local._data_paths : k => {
      for f in try(fileset(v, ""**/*.yaml""), []) :
      trimsuffix(f, "".yaml"") => yamldecode(file(""${v}/${f}""))
    }
  }
  _data_paths = {
    for k in [""access_levels"", ""egress_policies"", ""ingress_policies""] : k => (
      var.factories_config[k] == null
      ? null
      : pathexpand(var.factories_config[k])
    )
  }
  data = {
    access_levels = {
      for k, v in local._data.access_levels : k => {
        combining_function = try(v.combining_function, null)
        description        = try(v.description, null)
        conditions = [
          for c in try(v.conditions, []) : merge({
            device_policy          = null
            ip_subnetworks         = []
            members                = []
            negate                 = null
            regions                = []
            required_access_levels = []
          }, c)
        ]
      }
    }
    egress_policies = {
      for k, v in local._data.egress_policies : k => {
        from = merge({
          identity_type = null
          identities    = null
        }, try(v.from, {}))
        to = {
          operations = [
            for o in try(v.to.operations, []) : merge({
              method_selectors     = []
              permission_selectors = []
              service_name         = null
            }, o)
          ]
          resources              = try(v.to.resources, [])
          resource_type_external = try(v.to.resource_type_external, false)
        }
      }
    }
    ingress_policies = {
      for k, v in local._data.ingress_policies : k => {
        from = merge({
          access_levels = []
          identity_type = null
          identities    = null
          resources     = []
        }, try(v.from, {}))
        to = {
          operations = [
            for o in try(v.operations, []) : merge({
              method_selectors     = []
              permission_selectors = []
              service_name         = null
            }, o)
          ]
          resources = try(v.to.resources, [])
        }
      }
    }
  }
  # TODO: add checks that emulate the variable validations
}
",locals,"locals {
  _data = {
    for k, v in local._data_paths : k => {
      for f in try(fileset(v, ""**/*.yaml""), []) :
      trimsuffix(f, "".yaml"") => yamldecode(file(""${v}/${f}""))
    }
  }
  _data_paths = {
    for k in [""access_levels"", ""egress_policies"", ""ingress_policies""] : k => (
      var.factories_config[k] == null
      ? null
      : pathexpand(var.factories_config[k])
    )
  }
  data = {
    access_levels = {
      for k, v in local._data.access_levels : k => {
        combining_function = try(v.combining_function, null)
        description        = try(v.description, null)
        conditions = [
          for c in try(v.conditions, []) : merge({
            device_policy          = null
            ip_subnetworks         = []
            members                = []
            negate                 = null
            regions                = []
            required_access_levels = []
          }, c)
        ]
      }
    }
    egress_policies = {
      for k, v in local._data.egress_policies : k => {
        from = merge({
          identity_type = null
          identities    = null
        }, try(v.from, {}))
        to = {
          operations = [
            for o in try(v.to.operations, []) : merge({
              method_selectors     = []
              permission_selectors = []
              service_name         = null
            }, o)
          ]
          resources              = try(v.to.resources, [])
          resource_type_external = try(v.to.resource_type_external, false)
        }
      }
    }
    ingress_policies = {
      for k, v in local._data.ingress_policies : k => {
        from = merge({
          access_levels = []
          identity_type = null
          identities    = null
          resources     = []
        }, try(v.from, {}))
        to = {
          operations = [
            for o in try(v.to.operations, []) : merge({
              method_selectors     = []
              permission_selectors = []
              service_name         = null
            }, o)
          ]
          resources = try(v.to.resources, [])
        }
      }
    }
  }
  # TODO: add checks that emulate the variable validations
}
",locals,88,88.0,91615e014054ada63899da558f3b1c6cac5c8000,27a055a9cbffda216250af736ec2a68241bec12e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/91615e014054ada63899da558f3b1c6cac5c8000/modules/vpc-sc/factory.tf#L88,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/27a055a9cbffda216250af736ec2a68241bec12e/modules/vpc-sc/factory.tf#L88,2024-02-17 08:02:16+01:00,2024-05-01 18:50:30+02:00,2,0,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,853,fast/stages/03-gke-multitenant/prod/variables.tf,fast/stages/03-gke-multitenant/dev/variables.tf,1,# todo,# TODO: review defaults,# TODO: review defaults,"variable ""cluster_defaults"" {
  description = ""Default values for optional cluster configurations.""
  type = object({
    cloudrun_config             = bool
    database_encryption_key     = string
    enable_binary_authorization = bool
    master_authorized_ranges    = map(string)
    max_pods_per_node           = number
    pod_security_policy         = bool
    release_channel             = string
    vertical_pod_autoscaling    = bool
  })
  default = {
    # TODO: review defaults
    cloudrun_config             = false
    database_encryption_key     = null
    enable_binary_authorization = false
    master_authorized_ranges = {
      rfc1918_1 = ""10.0.0.0/8""
      rfc1918_2 = ""172.16.0.0/12""
      rfc1918_3 = ""192.168.0.0/16""
    }
    max_pods_per_node        = 110
    pod_security_policy      = false
    release_channel          = ""STABLE""
    vertical_pod_autoscaling = false
  }
}
",variable,"variable ""cluster_defaults"" {
  description = ""Default values for optional cluster configurations.""
  type = object({
    cloudrun_config                 = bool
    database_encryption_key         = string
    master_authorized_ranges        = map(string)
    max_pods_per_node               = number
    pod_security_policy             = bool
    release_channel                 = string
    vertical_pod_autoscaling        = bool
    gcp_filestore_csi_driver_config = bool
  })
  default = {
    cloudrun_config         = false
    database_encryption_key = null
    # binary_authorization    = false
    master_authorized_ranges = {
      rfc1918_1 = ""10.0.0.0/8""
      rfc1918_2 = ""172.16.0.0/12""
      rfc1918_3 = ""192.168.0.0/16""
    }
    max_pods_per_node               = 110
    pod_security_policy             = false
    release_channel                 = ""STABLE""
    vertical_pod_autoscaling        = false
    gcp_filestore_csi_driver_config = false
  }
}
",variable,46,,f3f9a4a88cedd64f6fc91b64666046aa6726a2f3,a16cf9e2a8ec54f2ca83184acd851cfd60eb81ad,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/f3f9a4a88cedd64f6fc91b64666046aa6726a2f3/fast/stages/03-gke-multitenant/prod/variables.tf#L46,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/a16cf9e2a8ec54f2ca83184acd851cfd60eb81ad/fast/stages/03-gke-multitenant/dev/variables.tf,2022-06-08 11:41:50+02:00,2022-08-08 13:54:06+02:00,15,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,100,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf,0,# todo,# TODO: resolve cyclic dependency and use,"# TODO: resolve cyclic dependency and use 
 # try(module.slurm_controller_instance.cloudsql_secret, null)","module ""slurm_files"" {
  source = ""github.com/SchedMD/slurm-gcp.git//terraform/slurm_cluster/modules/slurm_files?ref=6.2.0""

  project_id         = var.project_id
  slurm_cluster_name = local.slurm_cluster_name
  bucket_dir         = var.bucket_dir
  bucket_name        = local.bucket_name

  slurmdbd_conf_tpl = var.slurmdbd_conf_tpl
  slurm_conf_tpl    = var.slurm_conf_tpl
  cgroup_conf_tpl   = var.cgroup_conf_tpl
  cloud_parameters  = var.cloud_parameters
  # TODO: resolve cyclic dependency and use 
  # try(module.slurm_controller_instance.cloudsql_secret, null)
  cloudsql_secret = null

  controller_startup_scripts         = local.ghpc_startup_script_controller
  controller_startup_scripts_timeout = var.controller_startup_scripts_timeout
  compute_startup_scripts            = local.ghpc_startup_script_compute
  compute_startup_scripts_timeout    = var.compute_startup_scripts_timeout
  login_startup_scripts              = local.ghpc_startup_script_login
  login_startup_scripts_timeout      = var.login_startup_scripts_timeout

  enable_devel         = var.enable_devel
  enable_debug_logging = var.enable_debug_logging
  extra_logging_flags  = var.extra_logging_flags

  enable_bigquery_load = var.enable_bigquery_load
  epilog_scripts       = var.epilog_scripts
  prolog_scripts       = var.prolog_scripts

  disable_default_mounts = var.disable_default_mounts
  network_storage        = var.network_storage
  login_network_storage  = var.network_storage # TODO: reconsider duplication

  partitions  = values(module.slurm_partition)[*]
  nodeset     = values(module.slurm_nodeset)[*]
  nodeset_tpu = values(module.slurm_nodeset_tpu)[*]

  depends_on = [module.bucket]
}
",module,"module ""slurm_files"" {
  source = ""github.com/SchedMD/slurm-gcp.git//terraform/slurm_cluster/modules/slurm_files?ref=6.2.0""

  project_id         = var.project_id
  slurm_cluster_name = local.slurm_cluster_name
  bucket_dir         = var.bucket_dir
  bucket_name        = local.bucket_name

  slurmdbd_conf_tpl = var.slurmdbd_conf_tpl
  slurm_conf_tpl    = var.slurm_conf_tpl
  cgroup_conf_tpl   = var.cgroup_conf_tpl
  cloud_parameters  = var.cloud_parameters
  cloudsql_secret = try(
    one(google_secret_manager_secret_version.cloudsql_version[*].id),
  null)

  controller_startup_scripts         = local.ghpc_startup_script_controller
  controller_startup_scripts_timeout = var.controller_startup_scripts_timeout
  compute_startup_scripts            = local.ghpc_startup_script_compute
  compute_startup_scripts_timeout    = var.compute_startup_scripts_timeout
  login_startup_scripts              = local.ghpc_startup_script_login
  login_startup_scripts_timeout      = var.login_startup_scripts_timeout

  enable_devel         = var.enable_devel
  enable_debug_logging = var.enable_debug_logging
  extra_logging_flags  = var.extra_logging_flags

  enable_bigquery_load = var.enable_bigquery_load
  epilog_scripts       = var.epilog_scripts
  prolog_scripts       = var.prolog_scripts

  disable_default_mounts = var.disable_default_mounts
  network_storage        = var.network_storage
  login_network_storage  = var.login_network_storage

  partitions  = values(module.slurm_partition)[*]
  nodeset     = values(module.slurm_nodeset)[*]
  nodeset_tpu = values(module.slurm_nodeset_tpu)[*]

  depends_on = [module.bucket]
}
",module,101,,82199854e24ac10e7293605c5c4eae45748e8c99,c47d346676eb04eb46385f018a745fb865c081a0,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/82199854e24ac10e7293605c5c4eae45748e8c99/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf#L101,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/c47d346676eb04eb46385f018a745fb865c081a0/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf,2023-11-09 19:50:03+00:00,2023-11-15 01:15:19+00:00,3,1,1,1,0,0,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,1,worker_nodes.tf,worker_nodes.tf,0,# todo,"# TODO: create this policy as a data source
","# EKS Worker Nodes Resources 
 #  * IAM role allowing Kubernetes actions to access other AWS services 
 #  * EC2 Security Group to allow networking traffic 
 #  * Data source to fetch latest EKS worker AMI 
 #  * AutoScaling Launch Configuration to configure worker instances 
 #  * AutoScaling Group to launch worker instances 
 #  
 # TODO: create this policy as a data source","resource ""aws_iam_role"" ""demo-node"" {
  name = ""terraform-eks-demo-node""

  assume_role_policy = <<POLICY
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""Service"": ""ec2.amazonaws.com""
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}
POLICY
}
",resource,,,9,0.0,07aba1b7667da1c6df0540564d3bcd011764d4aa,309e7f70832270f7a40bcaf751380242515c2989,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/07aba1b7667da1c6df0540564d3bcd011764d4aa/worker_nodes.tf#L9,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/309e7f70832270f7a40bcaf751380242515c2989/worker_nodes.tf#L0,2018-06-06 20:55:23-07:00,2018-06-06 20:55:44-07:00,2,2,0,1,0,1,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,49,kubernetes-addons.tf,kubernetes-addons.tf,0,todo,# TODO Upgrade,# TODO Upgrade,"module ""cert_manager"" {
  count  = var.create_eks && var.cert_manager_enable ? 1 : 0
  source = ""./kubernetes-addons/cert-manager""

  private_container_repo_url      = var.private_container_repo_url
  public_docker_repo              = var.public_docker_repo
  cert_manager_helm_chart_version = var.cert_manager_helm_chart_version
  cert_manager_image_tag          = var.cert_manager_image_tag
  cert_manager_install_crds       = var.cert_manager_install_crds
  cert_manager_helm_chart_name    = var.cert_manager_helm_chart_name
  cert_manager_helm_chart_url     = var.cert_manager_helm_chart_url
  cert_manager_image_repo_name    = var.cert_manager_image_repo_name

  depends_on = [module.aws_eks]
}
",module,"module ""opentelemetry_collector"" {
  count  = var.create_eks && var.opentelemetry_enable ? 1 : 0
  source = ""./kubernetes-addons/opentelemetry-collector""

  private_container_repo_url                            = var.private_container_repo_url
  public_docker_repo                                    = var.public_docker_repo
  opentelemetry_command_name                            = var.opentelemetry_command_name
  opentelemetry_helm_chart                              = var.opentelemetry_helm_chart
  opentelemetry_helm_chart_url                          = var.opentelemetry_helm_chart_url
  opentelemetry_image                                   = var.opentelemetry_image
  opentelemetry_image_tag                               = var.opentelemetry_image_tag
  opentelemetry_helm_chart_version                      = var.opentelemetry_helm_chart_version
  opentelemetry_enable_agent_collector                  = var.opentelemetry_enable_agent_collector
  opentelemetry_enable_standalone_collector             = var.opentelemetry_enable_standalone_collector
  opentelemetry_enable_autoscaling_standalone_collector = var.opentelemetry_enable_autoscaling_standalone_collector
  opentelemetry_enable_container_logs                   = var.opentelemetry_enable_container_logs
  opentelemetry_min_standalone_collectors               = var.opentelemetry_min_standalone_collectors
  opentelemetry_max_standalone_collectors               = var.opentelemetry_max_standalone_collectors

  depends_on = [module.aws_eks]
}
",module,132,144.0,074bc8c69e3bab74a8642dfd8f32d8ca5afa9aa9,d9986b66484e6905c7ace374fae2c6fe45fa70c7,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/074bc8c69e3bab74a8642dfd8f32d8ca5afa9aa9/kubernetes-addons.tf#L132,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/d9986b66484e6905c7ace374fae2c6fe45fa70c7/kubernetes-addons.tf#L144,2021-10-13 14:49:03+01:00,2021-10-18 13:22:40-04:00,6,0,1,1,0,1,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,18,locals.tf,locals.tf,0,#todo,#TODO Allow this to be specified in TFVARS file,#TODO Allow this to be specified in TFVARS file,"locals {

  tags                = tomap({ ""created-by"" = var.terraform_version })
  private_subnet_tags = merge(tomap({ ""kubernetes.io/role/internal-elb"" = ""1"" }), tomap({ ""created-by"" = var.terraform_version }))
  public_subnet_tags  = merge(tomap({ ""kubernetes.io/role/elb"" = ""1"" }), tomap({ ""created-by"" = var.terraform_version }))

  service_account_amp_ingest_name = format(""%s-%s-%s-%s"", var.tenant, var.environment, var.zone, ""amp-ingest-account"")
  service_account_amp_query_name  = format(""%s-%s-%s-%s"", var.tenant, var.environment, var.zone, ""amp-query-account"")
  #TODO Allow this to be specified in TFVARS file
  amp_workspace_name = format(""%s-%s-%s-%s"", var.tenant, var.environment, var.zone, ""EKS-Metrics-Workspace"")

  image_repo = ""${data.aws_caller_identity.current.account_id}.dkr.ecr.${data.aws_region.current.id}.amazonaws.com/""

  self_managed_node_platform = var.enable_windows_support ? ""windows"" : ""linux""

  # Managed node IAM Roles for aws-auth
  managed_node_group_aws_auth_config_map = var.enable_managed_nodegroups == true ? [
    for key, node in var.managed_node_groups : {
      rolearn : ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/${module.eks.cluster_id}-${node.node_group_name}""
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes""
      ]
    }
  ] : []

  # Self Managed node IAM Roles for aws-auth
  self_managed_node_group_aws_auth_config_map = var.enable_self_managed_nodegroups == true ? [
    for key, node in var.self_managed_node_groups : {
      rolearn : ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/${module.eks.cluster_id}-${node.node_group_name}""
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes""
      ]
    }
  ] : []

  # Fargate node IAM Roles for aws-auth
  fargate_profiles_aws_auth_config_map = var.enable_fargate == true ? [
    for key, node in var.fargate_profiles : {
      rolearn : ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/${module.eks.cluster_id}-${node.fargate_profile_name}""
      username : ""system:node:{{SessionName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes"",
        ""system:node-proxier""
      ]
    }
  ] : []

}",locals,"locals {

  tags                = tomap({ ""created-by"" = var.terraform_version })
  private_subnet_tags = merge(tomap({ ""kubernetes.io/role/internal-elb"" = ""1"" }), tomap({ ""created-by"" = var.terraform_version }))
  public_subnet_tags  = merge(tomap({ ""kubernetes.io/role/elb"" = ""1"" }), tomap({ ""created-by"" = var.terraform_version }))

  ecr_image_repo_url = ""${data.aws_caller_identity.current.account_id}.dkr.ecr.${data.aws_region.current.id}.amazonaws.com""

  # Managed node IAM Roles for aws-auth
  managed_node_group_aws_auth_config_map = var.enable_managed_nodegroups == true ? [
    for key, node in var.managed_node_groups : {
      rolearn : ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/${module.eks.eks_cluster_id}-${node.node_group_name}""
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes""
      ]
    }
  ] : []

  # Self Managed node IAM Roles for aws-auth
  self_managed_node_group_aws_auth_config_map = var.enable_self_managed_nodegroups ? [
    for key, node in var.self_managed_node_groups : {
      rolearn : ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/${module.eks.eks_cluster_id}-${node.node_group_name}""
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes""
      ]
    } if node.custom_ami_type != ""windows""
  ] : []

  # Self Managed Windows node IAM Roles for aws-auth
  windows_node_group_aws_auth_config_map = var.enable_self_managed_nodegroups && var.enable_windows_support ? [
    for key, node in var.self_managed_node_groups : {
      rolearn : ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/${module.eks.eks_cluster_id}-${node.node_group_name}""
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes"",
        ""eks:kube-proxy-windows""
      ]
    } if node.custom_ami_type == ""windows""
  ] : []

  # Fargate node IAM Roles for aws-auth
  fargate_profiles_aws_auth_config_map = var.enable_fargate == true ? [
    for key, node in var.fargate_profiles : {
      rolearn : ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/${module.eks.eks_cluster_id}-${node.fargate_profile_name}""
      username : ""system:node:{{SessionName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes"",
        ""system:node-proxier""
      ]
    }
  ] : []

}",locals,27,,125390ed86df57dc9d8064df92b36e14cc8eb3e2,0f757b1a24ba147292e99c66667737dd57bb661e,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/125390ed86df57dc9d8064df92b36e14cc8eb3e2/locals.tf#L27,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/0f757b1a24ba147292e99c66667737dd57bb661e/locals.tf,2021-09-13 14:12:34+01:00,2021-09-24 23:30:52+01:00,6,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,966,modules/organization-policy/experimental.tf,modules/organization-policy/experimental.tf,0,# todo,# TODO: Remove once Terraform 1.3 is released https://github.com/hashicorp/terraform/releases/tag/v1.3.0-alpha20220622,# TODO: Remove once Terraform 1.3 is released https://github.com/hashicorp/terraform/releases/tag/v1.3.0-alpha20220622,"terraform {
  # TODO: Remove once Terraform 1.3 is released https://github.com/hashicorp/terraform/releases/tag/v1.3.0-alpha20220622
  experiments = [module_variable_optional_attrs]  
}
",terraform,,,17,0.0,9c942a68d6511184dae15cb2c9711de398db3867,ac835b6d50f7889b6f71fcaf0089a7ecceff5154,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9c942a68d6511184dae15cb2c9711de398db3867/modules/organization-policy/experimental.tf#L17,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/ac835b6d50f7889b6f71fcaf0089a7ecceff5154/modules/organization-policy/experimental.tf#L0,2022-07-08 15:19:47+02:00,2022-09-28 11:28:05+02:00,3,2,0,1,1,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-iam,23,test/fixtures/additive/outputs.tf,test/fixtures/additive/outputs.tf,0,workaround,# workaround InSpec lack of support for integer,# workaround InSpec lack of support for integer,"output ""roles"" {
  # workaround InSpec lack of support for integer
  value       = tostring(var.roles)
  description = ""Amount of roles assigned. Useful for testing how the module behaves on updates.""
}
",output,"output ""roles"" {
  # workaround InSpec lack of support for integer
  value       = tostring(var.roles)
  description = ""Amount of roles assigned. Useful for testing how the module behaves on updates.""
}
",output,119,131.0,2529fe6173f7540994a346066d5ab141144861e9,4f6e19d1e561853dd55106bcb2bcc1c4edc96d45,https://github.com/terraform-google-modules/terraform-google-iam/blob/2529fe6173f7540994a346066d5ab141144861e9/test/fixtures/additive/outputs.tf#L119,https://github.com/terraform-google-modules/terraform-google-iam/blob/4f6e19d1e561853dd55106bcb2bcc1c4edc96d45/test/fixtures/additive/outputs.tf#L131,2019-10-23 19:31:30+03:00,2023-02-15 13:45:50-06:00,4,0,0,1,0,0,0,0,0,1
https://github.com/kbst/terraform-kubestack,54,aws/cluster/elb-dns/ingress.tf,aws/cluster/elb-dns/ingress.tf,0,workaround,# this is a workaround as aws_elb_hosted_zone_id doesn't support NLBs,"# this is a workaround as aws_elb_hosted_zone_id doesn't support NLBs 
 # ref: https://github.com/hashicorp/terraform-provider-aws/issues/7988","data ""aws_lb"" ""current"" {
  name = split(""-"", data.kubernetes_service.current.status[0].load_balancer[0].ingress[0].hostname).0
}
",data,"data ""aws_lb"" ""current"" {
  count = var.using_nlb ? 1 : 0

  name = split(""-"", data.kubernetes_service.current.status[0].load_balancer[0].ingress[0].hostname).0
}
",data,15,16.0,dc6dff74e9f4b68bf9ef5d0b8366f664a52cbe55,5594ec823bd41d2d4b09ca8fa83e57eb9a536eb0,https://github.com/kbst/terraform-kubestack/blob/dc6dff74e9f4b68bf9ef5d0b8366f664a52cbe55/aws/cluster/elb-dns/ingress.tf#L15,https://github.com/kbst/terraform-kubestack/blob/5594ec823bd41d2d4b09ca8fa83e57eb9a536eb0/aws/cluster/elb-dns/ingress.tf#L16,2021-11-05 09:56:54+09:00,2021-11-08 19:12:28+01:00,2,0,0,0,1,0,1,0,0,0
https://github.com/zenml-io/mlstacks,3,gcp-kubeflow-kserve/kserve-module/istio.tf,gcp-kubeflow-kserve/kserve-module/istio.tf,0,hack,# repeating the same command as a hack to prevent errors in subsequent commands,"# repeating the same command as a hack to prevent errors in subsequent commands 
 # running it only once would have caused the next command to fail with no envoyfilter CRDs.","resource ""null_resource"" ""create-istio-kserve"" {
  provisioner ""local-exec"" {
    command = ""kubectl apply -l knative.dev/crd-install=true -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  # repeating the same command as a hack to prevent errors in subsequent commands
  # running it only once would have caused the next command to fail with no envoyfilter CRDs.
  provisioner ""local-exec"" {
    command = ""kubectl apply -l knative.dev/crd-install=true -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    command = ""kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    command = ""kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/net-istio.yaml""
  }

  # destroy-time provisioners
  provisioner ""local-exec"" {
    when    = destroy
    command = ""kubectl delete -l knative.dev/crd-install=true -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    when    = destroy
    command = ""kubectl delete -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    when    = destroy
    command = ""kubectl delete -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/net-istio.yaml""
  }

  depends_on = [
    null_resource.create-knative-serving,
  ]
}",resource,"resource ""null_resource"" ""create-istio-kserve"" {
  provisioner ""local-exec"" {
    command = ""kubectl apply -l knative.dev/crd-install=true -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  # repeating the same command as a hack to prevent errors in subsequent commands
  # running it only once would have caused the next command to fail with no envoyfilter CRDs.
  provisioner ""local-exec"" {
    command = ""kubectl apply -l knative.dev/crd-install=true -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    command = ""kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    command = ""kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/net-istio.yaml""
  }

  # destroy-time provisioners
  provisioner ""local-exec"" {
    when    = destroy
    command = ""kubectl delete -l knative.dev/crd-install=true -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    when    = destroy
    command = ""kubectl delete -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    when    = destroy
    command = ""kubectl delete -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/net-istio.yaml""
  }

  depends_on = [
    null_resource.create-knative-serving,
  ]
}",resource,6,6.0,e6fee7c554adb678d7c35f29cefcfb37e37ed0c0,e6fee7c554adb678d7c35f29cefcfb37e37ed0c0,https://github.com/zenml-io/mlstacks/blob/e6fee7c554adb678d7c35f29cefcfb37e37ed0c0/gcp-kubeflow-kserve/kserve-module/istio.tf#L6,https://github.com/zenml-io/mlstacks/blob/e6fee7c554adb678d7c35f29cefcfb37e37ed0c0/gcp-kubeflow-kserve/kserve-module/istio.tf#L6,2022-08-15 22:40:55+05:30,2022-08-15 22:40:55+05:30,1,0,0,1,0,0,0,1,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,534,fast/stages/02-security/core-dev.tf,fast/stages/02-security/core-dev.tf,0,# todo,# TODO(ludo): grant delegated role at key instead of project level,"# TODO(ludo): add support for conditions to Fabric modules 
 # TODO(ludo): grant delegated role at key instead of project level ","resource ""google_project_iam_member"" ""dev_key_admin_delegated"" {
  for_each = toset(try(var.kms_restricted_admins.dev, []))
  project  = module.dev-sec-project.project_id
  role     = ""roles/cloudkms.admin""
  member   = each.key
  condition {
    title       = ""kms_sa_delegated_grants""
    description = ""Automation service account delegated grants""
    expression = format(
      ""api.getAttribute('iam.googleapis.com/modifiedGrantsByRole', []).hasOnly([%s])"",
      join("","", formatlist(""'%s'"", [
        ""roles/cloudkms.cryptoKeyEncrypterDecrypter"",
        ""roles/cloudkms.cryptoKeyEncrypterDecrypterViaDelegation""
      ]))
    )
  }
  depends_on = [module.dev-sec-project]
}
",resource,"resource ""google_project_iam_member"" ""dev_key_admin_delegated"" {
  for_each = toset(local.dev_kms_restricted_admins)
  project  = module.dev-sec-project.project_id
  role     = ""roles/cloudkms.admin""
  member   = each.key
  condition {
    title       = ""kms_sa_delegated_grants""
    description = ""Automation service account delegated grants.""
    expression = format(
      ""api.getAttribute('iam.googleapis.com/modifiedGrantsByRole', []).hasOnly([%s]) && resource.type == 'cloudkms.googleapis.com/CryptoKey'"",
      join("","", formatlist(""'%s'"", [
        ""roles/cloudkms.cryptoKeyEncrypterDecrypter"",
        ""roles/cloudkms.cryptoKeyEncrypterDecrypterViaDelegation""
      ]))
    )
  }
  depends_on = [module.dev-sec-project]
}
",resource,46,,4e02f4475a6140e0baab53867d847345a28d4b3b,ceb611bb819cced9f35ee19031584197caafddd1,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/4e02f4475a6140e0baab53867d847345a28d4b3b/fast/stages/02-security/core-dev.tf#L46,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/ceb611bb819cced9f35ee19031584197caafddd1/fast/stages/02-security/core-dev.tf,2022-01-19 17:03:58+01:00,2022-06-23 07:04:35+02:00,6,1,0,1,0,1,0,0,0,0
https://github.com/nasa/cumulus,28,example/cumulus-tf/main.tf,example/cumulus-tf/main.tf,0,todo,# TODO This should be coming from the ingest module,# TODO This should be coming from the ingest module,"module ""cumulus"" {
  source = ""../../tf-modules/cumulus""

  prefix = var.prefix

  vpc_id            = var.vpc_id
  lambda_subnet_ids = var.subnet_ids

  ecs_cluster_instance_subnet_ids = var.subnet_ids
  ecs_cluster_min_size            = 1
  ecs_cluster_desired_size        = 1
  ecs_cluster_max_size            = 2

  urs_url             = ""https://uat.urs.earthdata.nasa.gov""
  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password

  cmr_client_id   = var.cmr_client_id
  cmr_environment = ""UAT""
  cmr_username    = var.cmr_username
  cmr_password    = var.cmr_password
  cmr_provider    = var.cmr_provider

  permissions_boundary_arn = var.permissions_boundary_arn

  system_bucket     = var.system_bucket
  public_buckets    = var.public_buckets
  protected_buckets = var.protected_buckets
  private_buckets   = var.private_buckets

  elasticsearch_domain_arn        = data.terraform_remote_state.data_persistence.outputs.elasticsearch_domain_arn
  elasticsearch_hostname          = data.terraform_remote_state.data_persistence.outputs.elasticsearch_hostname
  elasticsearch_security_group_id = data.terraform_remote_state.data_persistence.outputs.elasticsearch_security_group_id

  dynamo_tables = data.terraform_remote_state.data_persistence.outputs.dynamo_tables

  archive_api_port = 8000

  token_secret = var.token_secret

  archive_api_users = [
    ""jennyhliu"",
    ""jmcampbell"",
    ""jnorton1"",
    ""kbaynes"",
    ""kkelly"",
    ""kovarik"",
    ""lfrederick"",
    ""matthewsavoie"",
    ""mboyd"",
    ""menno.vandiermen"",
    ""mhuffnagle2"",
    ""pquinn1""
  ]

  # TODO This should be coming from the ingest module
  kinesis_inbound_event_logger = ""${var.prefix}-KinesisInboundEventLogger""

  distribution_url = var.distribution_url
}
",module,"module ""cumulus"" {
  source = ""../../tf-modules/cumulus""

  prefix = var.prefix

  vpc_id            = var.vpc_id
  lambda_subnet_ids = var.subnet_ids

  ecs_cluster_instance_subnet_ids = var.subnet_ids
  ecs_cluster_min_size            = 1
  ecs_cluster_desired_size        = 1
  ecs_cluster_max_size            = 2

  urs_url             = ""https://uat.urs.earthdata.nasa.gov""
  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password

  cmr_client_id   = var.cmr_client_id
  cmr_environment = ""UAT""
  cmr_username    = var.cmr_username
  cmr_password    = var.cmr_password
  cmr_provider    = var.cmr_provider

  permissions_boundary_arn = var.permissions_boundary_arn

  system_bucket     = var.system_bucket
  public_buckets    = var.public_buckets
  protected_buckets = var.protected_buckets
  private_buckets   = var.private_buckets

  elasticsearch_domain_arn        = data.terraform_remote_state.data_persistence.outputs.elasticsearch_domain_arn
  elasticsearch_hostname          = data.terraform_remote_state.data_persistence.outputs.elasticsearch_hostname
  elasticsearch_security_group_id = data.terraform_remote_state.data_persistence.outputs.elasticsearch_security_group_id

  dynamo_tables = data.terraform_remote_state.data_persistence.outputs.dynamo_tables

  archive_api_port = 8000

  token_secret = var.token_secret

  archive_api_users = [
    ""jennyhliu"",
    ""jmcampbell"",
    ""jnorton1"",
    ""kbaynes"",
    ""kkelly"",
    ""kovarik"",
    ""lfrederick"",
    ""matthewsavoie"",
    ""mboyd"",
    ""menno.vandiermen"",
    ""mhuffnagle2"",
    ""pquinn1""
  ]

  distribution_url = var.distribution_url
}
",module,66,,ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7,59283cd10c3891258816b76160848a494ffc35b7,https://github.com/nasa/cumulus/blob/ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7/example/cumulus-tf/main.tf#L66,https://github.com/nasa/cumulus/blob/59283cd10c3891258816b76160848a494ffc35b7/example/cumulus-tf/main.tf,2019-08-14 14:23:38-04:00,2019-08-16 13:47:55-04:00,2,1,1,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1595,blueprints/factories/project-factory/outputs.tf,modules/project-factory/outputs.tf,1,# todo,# TODO: group by project,# TODO: group by project,"output ""service_accounts"" {
  description = ""Service account emails.""
  # TODO: group by project
  value = {
    for k, v in module.service-accounts : k => v.email
  }
}
",output,"output ""service_accounts"" {
  description = ""Service account emails.""
  value = {
    for k, v in module.service-accounts : k => v.email
  }
}
",output,24,,819894d2bab4b440f1b52b1ac8035912fb107004,39139e2fa19df5e42df781979c7c60bc24bb156c,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/819894d2bab4b440f1b52b1ac8035912fb107004/blueprints/factories/project-factory/outputs.tf#L24,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/39139e2fa19df5e42df781979c7c60bc24bb156c/modules/project-factory/outputs.tf,2023-08-20 09:44:20+02:00,2024-03-05 13:13:02+01:00,2,1,0,1,0,1,0,0,0,0
https://github.com/nasa/cumulus,31,tf-modules/cumulus/archive.tf,tf-modules/cumulus/archive.tf,0,todo,# TODO This should eventually come from the ingest module,# TODO This should eventually come from the ingest module,"module ""archive"" {
  source = ""../archive""

  prefix = var.prefix

  permissions_boundary_arn = var.permissions_boundary_arn

  ecs_cluster_name = aws_ecs_cluster.default.name

  elasticsearch_domain_arn        = var.elasticsearch_domain_arn
  elasticsearch_hostname          = var.elasticsearch_hostname
  elasticsearch_security_group_id = var.elasticsearch_security_group_id

  ems_host = ""change-ems-host""

  system_bucket     = var.system_bucket
  public_buckets    = var.public_buckets
  protected_buckets = var.protected_buckets
  private_buckets   = var.private_buckets

  vpc_id            = var.vpc_id
  lambda_subnet_ids = var.lambda_subnet_ids

  cmr_client_id   = var.cmr_client_id
  cmr_environment = var.cmr_environment
  cmr_provider    = var.cmr_provider
  cmr_username    = var.cmr_username
  cmr_password    = var.cmr_password

  urs_url             = ""https://uat.urs.earthdata.nasa.gov""
  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password

  token_secret = var.token_secret

  dynamo_tables = var.dynamo_tables

  api_port = var.archive_api_port

  schedule_sf_function_arn      = data.aws_lambda_function.schedule_sf.arn
  message_consumer_function_arn = data.aws_lambda_function.message_consumer.arn
  # TODO This should eventually come from the ingest module
  kinesis_inbound_event_logger = var.kinesis_inbound_event_logger

  # TODO We need to figure out how to make this dynamic
  background_queue_name = ""backgroundProcessing""

  distribution_api_id = module.distribution.rest_api_id
  distribution_url    = module.distribution.distribution_url

  users = var.archive_api_users
}
",module,"module ""archive"" {
  source = ""../archive""

  prefix = var.prefix

  permissions_boundary_arn = var.permissions_boundary_arn

  lambda_processing_role_arn = aws_iam_role.lambda_processing.arn

  ecs_cluster_name = aws_ecs_cluster.default.name

  elasticsearch_domain_arn        = var.elasticsearch_domain_arn
  elasticsearch_hostname          = var.elasticsearch_hostname
  elasticsearch_security_group_id = var.elasticsearch_security_group_id

  ems_host = ""change-ems-host""

  system_bucket     = var.system_bucket
  public_buckets    = var.public_buckets
  protected_buckets = var.protected_buckets
  private_buckets   = var.private_buckets

  vpc_id            = var.vpc_id
  lambda_subnet_ids = var.lambda_subnet_ids

  cmr_client_id   = var.cmr_client_id
  cmr_environment = var.cmr_environment
  cmr_provider    = var.cmr_provider
  cmr_username    = var.cmr_username
  cmr_password    = var.cmr_password

  urs_url             = ""https://uat.urs.earthdata.nasa.gov""
  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password

  token_secret = var.token_secret

  dynamo_tables = var.dynamo_tables

  api_port = var.archive_api_port

  schedule_sf_function_arn                   = data.aws_lambda_function.schedule_sf.arn
  message_consumer_function_arn              = data.aws_lambda_function.message_consumer.arn
  kinesis_inbound_event_logger_function_name = module.ingest.kinesis_inbound_event_logger_function_name

  # TODO We need to figure out how to make this dynamic
  background_queue_name = ""backgroundProcessing""

  distribution_api_id = module.distribution.rest_api_id
  distribution_url    = module.distribution.distribution_url

  users = var.archive_api_users
}
",module,52,,ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7,59283cd10c3891258816b76160848a494ffc35b7,https://github.com/nasa/cumulus/blob/ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7/tf-modules/cumulus/archive.tf#L52,https://github.com/nasa/cumulus/blob/59283cd10c3891258816b76160848a494ffc35b7/tf-modules/cumulus/archive.tf,2019-08-14 14:23:38-04:00,2019-08-16 13:47:55-04:00,2,1,0,1,0,0,1,0,0,0
https://github.com/cookpad/terraform-aws-eks,1,modules/asg_node_group/variables.tf,modules/asg_node_group/variables.tf,0,# todo,# TODO: add custom validation rule once the feature is stable https://www.terraform.io/docs/configuration/variables.html#custom-validation-rules,# TODO: add custom validation rule once the feature is stable https://www.terraform.io/docs/configuration/variables.html#custom-validation-rules,"variable ""labels"" {
  type        = map(string)
  default     = {}
  description = ""Labels that will be added to the kubernetes node. A qualified name must consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyName',  or 'my.name',  or '123-abc', regex used for validation is '([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9]') with an optional DNS subdomain prefix and '/' (e.g. 'example.com/MyName')""
  # TODO: add custom validation rule once the feature is stable https://www.terraform.io/docs/configuration/variables.html#custom-validation-rules
}
",variable,,,70,0.0,e8c595caffa0772127b635ed600e65e4fac97800,f5918bacf2ee295f233c047cb71488a47c248e52,https://github.com/cookpad/terraform-aws-eks/blob/e8c595caffa0772127b635ed600e65e4fac97800/modules/asg_node_group/variables.tf#L70,https://github.com/cookpad/terraform-aws-eks/blob/f5918bacf2ee295f233c047cb71488a47c248e52/modules/asg_node_group/variables.tf#L0,2020-02-21 15:19:07+00:00,2023-06-21 09:58:23+02:00,43,2,0,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd,131,modules/secure-cd/main.tf,modules/secure-cd/main.tf,0,#todo,#TODO parameterize,"description = ""Pipeline for application"" #TODO parameterize","resource ""google_clouddeploy_delivery_pipeline"" ""pipeline"" {
  name        = var.clouddeploy_pipeline_name
  description = ""Pipeline for application"" #TODO parameterize
  project     = var.project_id
  location    = var.primary_location

  serial_pipeline {
    dynamic ""stages"" {
      for_each = var.deploy_branch_clusters
      content {
        target_id = google_clouddeploy_target.deploy_target[stages.key].name
      }
    }
  }
}
",resource,"resource ""google_clouddeploy_delivery_pipeline"" ""pipeline"" {
  name        = var.clouddeploy_pipeline_name
  description = ""Pipeline for application"" #TODO parameterize
  project     = var.project_id
  location    = var.primary_location

  serial_pipeline {
    dynamic ""stages"" {
      for_each = var.deploy_branch_clusters
      content {
        target_id = google_clouddeploy_target.deploy_target[stages.key].name
      }
    }
  }
}
",resource,57,76.0,49402b46010b20e3afcaeec200cc2b64db409d01,771f462ca788d02a284fbaf58dc2cd4072b355a2,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/49402b46010b20e3afcaeec200cc2b64db409d01/modules/secure-cd/main.tf#L57,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/771f462ca788d02a284fbaf58dc2cd4072b355a2/modules/secure-cd/main.tf#L76,2022-08-23 16:20:45-04:00,2023-03-28 11:20:32-05:00,3,0,0,1,0,0,0,1,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,485,tests/fast/stages/s03_project_factory/fixture/variables.tf,tests/fast/stages/s03_project_factory/fixture/variables.tf,0,#todo,#TODO(sruffilli): is this really required?,#TODO(sruffilli): is this really required?,"variable ""environment"" {
  description = ""Environment where projects will be created (e.g. prod, dev, ...).""
  type        = string
  default     = ""prod""
}
",variable,,,44,0.0,cee207b4544cfe2bc2eb517fd91c79952e3052b3,dc3a2ad7be032c093ae4d82d3abfeb628c16dfe6,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/cee207b4544cfe2bc2eb517fd91c79952e3052b3/tests/fast/stages/s03_project_factory/fixture/variables.tf#L44,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/dc3a2ad7be032c093ae4d82d3abfeb628c16dfe6/tests/fast/stages/s03_project_factory/fixture/variables.tf#L0,2022-01-17 10:36:38+01:00,2022-02-24 15:05:18+01:00,5,2,0,1,0,0,0,0,0,1
https://github.com/uyuni-project/sumaform,1385,backend_modules/aws/host/main.tf,backend_modules/aws/host/main.tf,0,workaround,# WORKAROUND,"# WORKAROUND 
 # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner""). 
 # After the first `apply`, terraform removes those tags. The following block avoids this behavior. 
 # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider 
 # See github:terraform-providers/terraform-provider-aws#10689","resource ""aws_ebs_volume"" ""data_disk"" {
  count = var.additional_disk_size == null ? 0 : var.additional_disk_size > 0 ? var.quantity : 0

  availability_zone = local.availability_zone
  size              = var.additional_disk_size == null ? 0 : var.additional_disk_size
  type              = lookup(var.volume_provider_settings, ""type"", ""sc1"")
  snapshot_id       = lookup(var.volume_provider_settings, ""volume_snapshot_id"", null)
  tags = {
    Name = ""${local.resource_name_prefix}-data-volume${var.quantity > 1 ? ""-${count.index + 1}"" : """"}""
  }
  # WORKAROUND
  # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner"").
  # After the first `apply`, terraform removes those tags. The following block avoids this behavior.
  # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider
  # See github:terraform-providers/terraform-provider-aws#10689
  lifecycle {
    ignore_changes = [tags]
  }
}
",resource,"resource ""aws_ebs_volume"" ""data_disk"" {
  count = var.additional_disk_size == null ? 0 : var.additional_disk_size > 0 ? var.quantity : 0

  availability_zone = local.availability_zone
  size              = var.additional_disk_size == null ? 0 : var.additional_disk_size
  type              = lookup(var.volume_provider_settings, ""type"", ""sc1"")
  snapshot_id       = lookup(var.volume_provider_settings, ""volume_snapshot_id"", null)
  tags = {
    Name = ""${local.resource_name_prefix}-data-volume${var.quantity > 1 ? ""-${count.index + 1}"" : """"}""
  }
  # WORKAROUND
  # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner"").
  # After the first `apply`, terraform removes those tags. The following block avoids this behavior.
  # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider
  # See github:terraform-providers/terraform-provider-aws#10689
  lifecycle {
    ignore_changes = [tags]
  }
}
",resource,114,160.0,12fc857978857ebd94f9b0906480004ca9b88c22,3a05fda9be0240b065bb2031b5812c595b77f973,https://github.com/uyuni-project/sumaform/blob/12fc857978857ebd94f9b0906480004ca9b88c22/backend_modules/aws/host/main.tf#L114,https://github.com/uyuni-project/sumaform/blob/3a05fda9be0240b065bb2031b5812c595b77f973/backend_modules/aws/host/main.tf#L160,2021-01-26 15:58:29+01:00,2024-04-12 13:39:31+12:00,35,0,1,1,1,1,0,0,0,0
https://github.com/nasa/cumulus,3,example/main.tf,example/main.tf,0,todo,# TODO Don't hard-code,# TODO Don't hard-code,"module ""s3_credentials_endpoint"" {
  source = ""../packages/s3-credentials-endpoint""

  prefix               = ""mth-2""
  permissions_boundary = var.permissions_boundary

  rest_api   = module.thin_egress_app.rest_api
  stage_name = module.thin_egress_app.rest_api_stage_name

  sts_credentials_lambda_arn = ""asdf""

  # TODO Don't hard-code
  urs_client_id = ""asdf""
  # TODO Don't hard-code
  urs_client_password = ""asdf""
  # TODO Don't hard-code
  urs_url = ""https://uat.urs.earthdata.nasa.gov""
}
",module,"module ""s3_credentials_endpoint"" {
  source = ""../packages/s3-credentials-endpoint""

  prefix               = var.prefix
  permissions_boundary = var.permissions_boundary
  subnet_ids           = var.tea_subnet_ids
  ngap_sgs             = var.ngap_sgs

  rest_api   = module.thin_egress_app.rest_api
  stage_name = module.thin_egress_app.rest_api_stage_name

  sts_credentials_lambda_arn = var.sts_credentials_lambda_arn

  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password
  urs_url             = var.urs_url
}
",module,43,,b37630397418a4cb61e428974577b0e21a636fae,3342da9920c9b6d6b98498b5edda515c84c59a24,https://github.com/nasa/cumulus/blob/b37630397418a4cb61e428974577b0e21a636fae/example/main.tf#L43,https://github.com/nasa/cumulus/blob/3342da9920c9b6d6b98498b5edda515c84c59a24/example/main.tf,2019-07-02 10:24:14-04:00,2019-07-07 22:31:31-05:00,2,1,1,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,415,infra/modules/aws-psoxy-lambda/main.tf,infra/modules/aws-psoxy-lambda/main.tf,0,# todo,"# TODO: this for 'writable' case requires TWO applies to show up. Eg, on first run, the ACCESS_TOKEN","# q: creates implicit dependency on parameters being created, which may not be case in first run??  
 # TODO: this for 'writable' case requires TWO applies to show up. Eg, on first run, the ACCESS_TOKEN 
 # params aren't there, so not visible via 'data'; and not added to write. 
 # ""PSOXY_${upper(replace(each.value.connector_name, ""-"", ""_""))}_${upper(each.value.secret_name)}""","data ""aws_ssm_parameters_by_path"" ""psoxy_parameters"" {
  path            = ""/""
  with_decryption = false # we just want the arns, not the values
}
",data,the block associated got renamed or deleted,,82,,5d8f4928978c96878368db88bc908047217bcda8,b4d53a1c6321a4b354b8703f9105a67cc872c310,https://github.com/Worklytics/psoxy/blob/5d8f4928978c96878368db88bc908047217bcda8/infra/modules/aws-psoxy-lambda/main.tf#L82,https://github.com/Worklytics/psoxy/blob/b4d53a1c6321a4b354b8703f9105a67cc872c310/infra/modules/aws-psoxy-lambda/main.tf,2022-10-06 13:05:00-07:00,2022-10-06 13:06:47-07:00,2,1,0,0,0,1,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,29,modules/aws-eks-managed-node-groups/iam.tf,modules/aws-eks-managed-node-groups/iam.tf,0,#todo,#TODO Allow IAM policies can be passed from tfvars file,#TODO Allow IAM policies can be passed from tfvars file,"resource ""aws_iam_role_policy_attachment"" ""managed_ng_AmazonEKSWorkerNodePolicy"" {
  policy_arn = ""${local.policy_arn_prefix}/AmazonEKSWorkerNodePolicy""
  role       = aws_iam_role.managed_ng.name
}
",resource,"resource ""aws_iam_role_policy_attachment"" ""managed_ng_AmazonEKSWorkerNodePolicy"" {
  policy_arn = ""${local.policy_arn_prefix}/AmazonEKSWorkerNodePolicy""
  role       = aws_iam_role.managed_ng.name
}
",resource,21,,836f6ff008667e880f7591aa5c3d5d755d8da8e6,b80759c98848fb1b79fffc9db9d9c6606fafd320,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/836f6ff008667e880f7591aa5c3d5d755d8da8e6/modules/aws-eks-managed-node-groups/iam.tf#L21,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/b80759c98848fb1b79fffc9db9d9c6606fafd320/modules/aws-eks-managed-node-groups/iam.tf,2021-09-30 15:35:25+01:00,2021-11-24 18:47:05+00:00,3,1,0,1,0,1,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-iam,4,modules/iam-role-for-service-accounts-eks/policies.tf,modules/iam-role-for-service-accounts-eks/policies.tf,0,todo,# TODO - remove *_ids at next breaking change,# TODO - remove *_ids at next breaking change,"data ""aws_iam_policy_document"" ""cluster_autoscaler"" {
  count = var.create_role && var.attach_cluster_autoscaler_policy ? 1 : 0

  statement {
    actions = [
      ""autoscaling:DescribeAutoScalingGroups"",
      ""autoscaling:DescribeAutoScalingInstances"",
      ""autoscaling:DescribeLaunchConfigurations"",
      ""autoscaling:DescribeScalingActivities"",
      ""autoscaling:DescribeTags"",
      ""ec2:DescribeLaunchTemplateVersions"",
      ""ec2:DescribeInstanceTypes"",
      ""eks:DescribeNodegroup"",
      ""ec2:DescribeImages"",
      ""ec2:GetInstanceTypesFromInstanceRequirements""
    ]

    resources = [""*""]
  }

  dynamic ""statement"" {
    # TODO - remove *_ids at next breaking change
    for_each = toset(coalescelist(var.cluster_autoscaler_cluster_ids, var.cluster_autoscaler_cluster_names))
    content {
      actions = [
        ""autoscaling:SetDesiredCapacity"",
        ""autoscaling:TerminateInstanceInAutoScalingGroup""
      ]

      resources = [""*""]

      condition {
        test     = ""StringEquals""
        variable = ""autoscaling:ResourceTag/kubernetes.io/cluster/${statement.value}""
        values   = [""owned""]
      }
    }
  }
}
",data,"data ""aws_iam_policy_document"" ""cluster_autoscaler"" {
  count = var.create_role && var.attach_cluster_autoscaler_policy ? 1 : 0

  statement {
    actions = [
      ""autoscaling:DescribeAutoScalingGroups"",
      ""autoscaling:DescribeAutoScalingInstances"",
      ""autoscaling:DescribeLaunchConfigurations"",
      ""autoscaling:DescribeScalingActivities"",
      ""autoscaling:DescribeTags"",
      ""ec2:DescribeLaunchTemplateVersions"",
      ""ec2:DescribeInstanceTypes"",
      ""eks:DescribeNodegroup"",
      ""ec2:DescribeImages"",
      ""ec2:GetInstanceTypesFromInstanceRequirements""
    ]

    resources = [""*""]
  }

  dynamic ""statement"" {
    # TODO - remove *_ids at next breaking change
    for_each = toset(coalescelist(var.cluster_autoscaler_cluster_ids, var.cluster_autoscaler_cluster_names))
    content {
      actions = [
        ""autoscaling:SetDesiredCapacity"",
        ""autoscaling:TerminateInstanceInAutoScalingGroup""
      ]

      resources = [""*""]

      condition {
        test     = ""StringEquals""
        variable = ""autoscaling:ResourceTag/kubernetes.io/cluster/${statement.value}""
        values   = [""owned""]
      }
    }
  }
}
",data,111,111.0,fdee003477c5f86c4236be08ef6a69dffbcc39fd,f9d5e28996ca282af4c09cb97f6291cf77ac03ea,https://github.com/terraform-aws-modules/terraform-aws-iam/blob/fdee003477c5f86c4236be08ef6a69dffbcc39fd/modules/iam-role-for-service-accounts-eks/policies.tf#L111,https://github.com/terraform-aws-modules/terraform-aws-iam/blob/f9d5e28996ca282af4c09cb97f6291cf77ac03ea/modules/iam-role-for-service-accounts-eks/policies.tf#L111,2023-05-22 15:55:09-04:00,2024-04-08 18:43:26-04:00,13,0,0,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,536,infra/modules/worklytics-psoxy-connection-generic/variables.tf,infra/modules/worklytics-psoxy-connection-generic/variables.tf,0,fix,"# TODO: fix these validations; logically correct, but Terraform doesn't allow validation conditions","# TODO: fix these validations; logically correct, but Terraform doesn't allow validation conditions 
 # to depend on values of other variables 
 #  validation { 
 #    condition     = var.psoxy_host_platform_id != ""AWS"" || contains(keys(var.settings_to_provide), ""AWS Psoxy Region"") 
 #    error_message = ""For connections to AWS deployments, must provide 'AWS Psoxy Region' in settings_to_provide."" 
 #  } 
 # 
 #  validation { 
 #    condition     = var.psoxy_host_platform_id != ""AWS"" || contains(keys(var.settings_to_provide), ""AWS Psoxy Role ARN"") 
 #    error_message = ""For connections to AWS deployments, must provide 'AWS Psoxy Role ARN' in settings_to_provide."" 
 #  }","variable ""settings_to_provide"" {
  type        = map(string)
  description = ""map of values for installer to copy-paste into connection settings in Worklytics UX""
  default     = {}

  # TODO: fix these validations; logically correct, but Terraform doesn't allow validation conditions
  # to depend on values of other variables
  #  validation {
  #    condition     = var.psoxy_host_platform_id != ""AWS"" || contains(keys(var.settings_to_provide), ""AWS Psoxy Region"")
  #    error_message = ""For connections to AWS deployments, must provide 'AWS Psoxy Region' in settings_to_provide.""
  #  }
  #
  #  validation {
  #    condition     = var.psoxy_host_platform_id != ""AWS"" || contains(keys(var.settings_to_provide), ""AWS Psoxy Role ARN"")
  #    error_message = ""For connections to AWS deployments, must provide 'AWS Psoxy Role ARN' in settings_to_provide.""
  #  }
}
",variable,"variable ""settings_to_provide"" {
  type        = map(string)
  description = ""map of values for installer to copy-paste into connection settings in Worklytics UX""
  default     = {}

  # TODO: fix these validations; logically correct, but Terraform doesn't allow validation conditions
  # to depend on values of other variables
  #  validation {
  #    condition     = var.psoxy_host_platform_id != ""AWS"" || contains(keys(var.settings_to_provide), ""AWS Psoxy Region"")
  #    error_message = ""For connections to AWS deployments, must provide 'AWS Psoxy Region' in settings_to_provide.""
  #  }
  #
  #  validation {
  #    condition     = var.psoxy_host_platform_id != ""AWS"" || contains(keys(var.settings_to_provide), ""AWS Psoxy Role ARN"")
  #    error_message = ""For connections to AWS deployments, must provide 'AWS Psoxy Role ARN' in settings_to_provide.""
  #  }
}
",variable,34,48.0,fc966f9714e332add194d2aacc9c7e328b714c1c,5937e6a45055a94ff2b493f96f21863d91699825,https://github.com/Worklytics/psoxy/blob/fc966f9714e332add194d2aacc9c7e328b714c1c/infra/modules/worklytics-psoxy-connection-generic/variables.tf#L34,https://github.com/Worklytics/psoxy/blob/5937e6a45055a94ff2b493f96f21863d91699825/infra/modules/worklytics-psoxy-connection-generic/variables.tf#L48,2022-12-21 09:04:05-08:00,2024-03-06 18:11:21+00:00,4,0,0,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,980,fast/stages/03-gke-multitenant/module/main.tf,fast/stages/03-gke-multitenant/module/main.tf,0,# todo,# TODO: depend GKE hub services on GKE hub variable/activation,# TODO: depend GKE hub services on GKE hub variable/activation,"module ""gke-project-0"" {
  source          = ""../../../../modules/project""
  billing_account = var.billing_account.id
  name            = ""dev-gke-clusters-0""
  parent          = var.folder_ids.gke-dev
  prefix          = var.prefix
  group_iam       = var.group_iam
  labels          = local.labels
  # TODO: depend GKE hub services on GKE hub variable/activation
  services = [
    # ""anthosconfigmanagement.googleapis.com"",
    # ""anthos.googleapis.com"",
    ""cloudresourcemanager.googleapis.com"",
    ""container.googleapis.com"",
    ""dns.googleapis.com"",
    # ""gkeconnect.googleapis.com"",
    # ""gkehub.googleapis.com"",
    ""iam.googleapis.com"",
    # ""multiclusteringress.googleapis.com"",
    # ""multiclusterservicediscovery.googleapis.com"",
    ""stackdriver.googleapis.com"",
    # ""trafficdirector.googleapis.com""
  ]
  shared_vpc_service_config = {
    attach       = true
    host_project = var.host_project_ids.dev-spoke-0
    service_identity_iam = {
      ""roles/compute.networkUser"" = [
        ""cloudservices"", ""container-engine""
      ]
      ""roles/container.hostServiceAgentUser"" = [
        ""container-engine""
      ]
    }
  }
  # specify project-level org policies here if you need them
  # policy_boolean = {
  #   ""constraints/compute.disableGuestAttributesAccess"" = true
  # }
  # policy_list = {
  #   ""constraints/compute.trustedImageProjects"" = {
  #     inherit_from_parent = null
  #     suggested_value     = null
  #     status              = true
  #     values              = [""projects/fl01-prod-iac-core-0""]
  #   }
  # }
}
",module,"module ""gke-project-0"" {
  source          = ""../../../../modules/project""
  billing_account = var.billing_account.id
  name            = ""dev-gke-clusters-0""
  parent          = var.folder_ids.gke-dev
  prefix          = var.prefix
  group_iam       = var.group_iam
  labels          = local.labels
  services = concat(
    [
      ""cloudresourcemanager.googleapis.com"",
      ""container.googleapis.com"",
      ""dns.googleapis.com"",
      ""iam.googleapis.com"",
      ""stackdriver.googleapis.com"",
    ],
    !local.fleet_enabled ? [] : [
      ""anthosconfigmanagement.googleapis.com"",
      ""anthos.googleapis.com"",
      ""gkeconnect.googleapis.com"",
      ""gkehub.googleapis.com"",
      ""multiclusteringress.googleapis.com"",
      ""multiclusterservicediscovery.googleapis.com"",
      ""trafficdirector.googleapis.com""
    ]
  )
  shared_vpc_service_config = {
    attach       = true
    host_project = var.host_project_ids.dev-spoke-0
    service_identity_iam = merge({
      ""roles/compute.networkUser"" = [
        ""cloudservices"", ""container-engine""
      ]
      ""roles/container.hostServiceAgentUser"" = [
        ""container-engine""
      ]
      },
      !local.fleet_mcs_enabled ? {} : {
        ""roles/multiclusterservicediscovery.serviceAgent"" = [""gke-mcs""]
        ""roles/compute.networkViewer""                     = [""gke-mcs-importer""]
    })
  }
  # specify project-level org policies here if you need them
  # policy_boolean = {
  #   ""constraints/compute.disableGuestAttributesAccess"" = true
  # }
  # policy_list = {
  #   ""constraints/compute.trustedImageProjects"" = {
  #     inherit_from_parent = null
  #     suggested_value     = null
  #     status              = true
  #     values              = [""projects/fl01-prod-iac-core-0""]
  #   }
  # }
}
",module,29,,1260db923e8179de72cf63b79f1381791b30885f,c24e66138339bb5c599e52cd88236267acac4611,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/1260db923e8179de72cf63b79f1381791b30885f/fast/stages/03-gke-multitenant/module/main.tf#L29,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c24e66138339bb5c599e52cd88236267acac4611/fast/stages/03-gke-multitenant/module/main.tf,2022-07-29 10:49:50+02:00,2022-07-29 14:01:35+02:00,3,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,288,modules/compute-vm/main.tf,modules/compute-vm/main.tf,0,# todo,# TODO: check if self link works here,# TODO: check if self link works here,"resource ""google_compute_region_disk"" ""disks"" {
  provider = google-beta
  for_each = var.create_template ? {} : {
    for k, v in local.attached_disks_regional :
    k => v if v.source_type != ""attach""
  }
  project       = var.project_id
  region        = local.region
  replica_zones = [var.zone, each.value.options.replica_zone]
  name          = ""${var.name}-${each.key}""
  type          = each.value.options.type
  size          = each.value.size
  # image         = each.value.source_type == ""image"" ? each.value.source : null
  snapshot = each.value.source_type == ""snapshot"" ? each.value.source : null
  labels = merge(var.labels, {
    disk_name = each.value.name
    disk_type = each.value.options.type
  })
  dynamic ""disk_encryption_key"" {
    for_each = var.encryption != null ? [""""] : []
    content {
      raw_key = var.encryption.disk_encryption_key_raw
      # TODO: check if self link works here
      kms_key_name = var.encryption.kms_key_self_link
    }
  }
}
",resource,"resource ""google_compute_region_disk"" ""disks"" {
  provider = google-beta
  for_each = var.create_template ? {} : {
    for k, v in local.attached_disks_regional :
    k => v if v.source_type != ""attach""
  }
  project       = var.project_id
  region        = local.region
  replica_zones = [var.zone, each.value.options.replica_zone]
  name          = ""${var.name}-${each.key}""
  type          = each.value.options.type
  size          = each.value.size
  # image         = each.value.source_type == ""image"" ? each.value.source : null
  snapshot = each.value.source_type == ""snapshot"" ? each.value.source : null
  labels = merge(var.labels, {
    disk_name = each.value.name
    disk_type = each.value.options.type
  })
  dynamic ""disk_encryption_key"" {
    for_each = var.encryption != null ? [""""] : []
    content {
      raw_key = var.encryption.disk_encryption_key_raw
      # TODO: check if self link works here
      kms_key_name = var.encryption.kms_key_self_link
    }
  }
}
",resource,112,142.0,262f823464462dfd7c75f8917835e829e2f88bf7,3af7e257d21f889ffaf7b32a3bab974fdbfda6e4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/262f823464462dfd7c75f8917835e829e2f88bf7/modules/compute-vm/main.tf#L112,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3af7e257d21f889ffaf7b32a3bab974fdbfda6e4/modules/compute-vm/main.tf#L142,2021-10-04 10:46:44+02:00,2024-04-17 10:23:48+02:00,24,0,0,1,0,1,0,0,0,1
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,406,modules/vpc-sc/service_perimeters_bridge.tf,modules/vpc-sc/service-perimeters-bridge.tf,1,implement,"# this code implements ""additive"" service perimeters, if ""authoritative""","/** 
 * Copyright 2021 Google LLC 
 * 
 * Licensed under the Apache License, Version 2.0 (the ""License""); 
 * you may not use this file except in compliance with the License. 
 * You may obtain a copy of the License at 
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, software 
 * distributed under the License is distributed on an ""AS IS"" BASIS, 
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 * See the License for the specific language governing permissions and 
 * limitations under the License. 
 */  
 # this code implements ""additive"" service perimeters, if ""authoritative"" 
 # service perimeters are needed, switch to the 
 # google_access_context_manager_service_perimeters resource ","resource ""google_access_context_manager_service_perimeter"" ""bridge"" {
  for_each                  = var.service_perimeters_bridge
  parent                    = ""accessPolicies/${local.access_policy}""
  name                      = ""accessPolicies/${local.access_policy}/servicePerimeters/${each.key}""
  title                     = each.key
  perimeter_type            = ""PERIMETER_TYPE_BRIDGE""
  use_explicit_dry_run_spec = each.value.use_explicit_dry_run_spec
  spec {
    resources = each.value.spec_resources
  }
  status {
    resources = each.value.status_resources
  }
  lifecycle {
    ignore_changes = [spec[0].resources, status[0].resources]
  }
  depends_on = [
    google_access_context_manager_access_policy.default,
    google_access_context_manager_access_level.basic
  ]
}
",resource,"resource ""google_access_context_manager_service_perimeter"" ""bridge"" {
  for_each                  = var.service_perimeters_bridge
  parent                    = ""accessPolicies/${local.access_policy}""
  name                      = ""accessPolicies/${local.access_policy}/servicePerimeters/${each.key}""
  title                     = each.key
  perimeter_type            = ""PERIMETER_TYPE_BRIDGE""
  use_explicit_dry_run_spec = each.value.use_explicit_dry_run_spec

  dynamic ""spec"" {
    for_each = each.value.spec_resources == null ? [] : [""""]
    content {
      resources = each.value.spec_resources
    }
  }

  status {
    resources = each.value.status_resources == null ? [] : each.value.status_resources
  }

  # lifecycle {
  #   ignore_changes = [spec[0].resources, status[0].resources]
  # }

  depends_on = [
    google_access_context_manager_access_policy.default,
    google_access_context_manager_access_level.basic,
    google_access_context_manager_service_perimeter.regular
  ]
}
",resource,17,19.0,2c7dab3bb2b572c4d60bc91ba48eb9e4ffbb8913,aecb6fd543f1f2e9cc852a19996aef2922ecd145,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2c7dab3bb2b572c4d60bc91ba48eb9e4ffbb8913/modules/vpc-sc/service_perimeters_bridge.tf#L17,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/aecb6fd543f1f2e9cc852a19996aef2922ecd145/modules/vpc-sc/service-perimeters-bridge.tf#L19,2021-12-31 13:29:22+01:00,2023-02-25 16:04:19+00:00,7,0,0,1,0,1,0,0,0,0
https://github.com/uyuni-project/sumaform,488,modules/openstack/host/main.tf,modules/openstack/host/main.tf,0,hack,# HACK: it should be possible to use 0 in case var.floating_ips has a value,"# HACK: it should be possible to use 0 in case var.floating_ips has a value 
 # this is not possible though, because 1) ?: is not short-circuiting 
 # 2) ?: can't return a list 3) element() does not operate on empty lists 
 # and 4) expressions like list.*.attribute will fail if list is empty","resource ""openstack_networking_floatingip_v2"" ""floating_ip"" {
  pool = ""floating""
  # HACK: it should be possible to use 0 in case var.floating_ips has a value
  # this is not possible though, because 1) ?: is not short-circuiting
  # 2) ?: can't return a list 3) element() does not operate on empty lists
  # and 4) expressions like list.*.attribute will fail if list is empty
  count = ""${var.count}""
}
",resource,,,61,0.0,519968238b16c7e6c42f0d516d2c590f0a788b9c,1c35cccee18a9803abaf1dc824c6ea13e068d344,https://github.com/uyuni-project/sumaform/blob/519968238b16c7e6c42f0d516d2c590f0a788b9c/modules/openstack/host/main.tf#L61,https://github.com/uyuni-project/sumaform/blob/1c35cccee18a9803abaf1dc824c6ea13e068d344/modules/openstack/host/main.tf#L0,2018-02-23 16:14:43+01:00,2019-11-08 15:50:33+01:00,18,2,0,1,0,0,1,0,0,0
