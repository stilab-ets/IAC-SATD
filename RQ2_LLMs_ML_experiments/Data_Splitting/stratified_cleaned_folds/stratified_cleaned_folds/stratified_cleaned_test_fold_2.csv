Repo URL,Satd Comment Id,File Path Of First Occurence,File Path Of Last Occurence,renamed,Keyword,SATD Comment,context,bloc of first occurrence,bloc type of first occurrence,bloc of last occurrence,bloc type of last occurrence,SATD Comment Line Of First Occurence,SATD Comment Line Of Last Occurence,first Commit Hash,last Commit Hash,Link To The File Of First Occurence,Link To The File Of Last Occurence/When Adressed,Introduction Time,Last Occurence (even solved or not),number of commits,adressed ?,Computing Management Debt,IaC Code Debt,Dependency Management,Security Debt,Networking Debt,Environment-Based Configuration Debt,Monitoring and Logging Debt,Test Debt
https://github.com/oracle-terraform-modules/terraform-oci-oke,138,modules/oke/autoscaler.tf,modules/oke/autoscaler.tf,0,fix,"# image fixed to E4.flex, no need to lookup","# image fixed to E4.flex, no need to lookup","resource ""oci_containerengine_node_pool"" ""autoscaler_pool"" {
  for_each       = var.enable_cluster_autoscaler == true ? var.autoscaler_pools : {}
  cluster_id     = oci_containerengine_cluster.k8s_cluster.id
  compartment_id = var.compartment_id
  depends_on     = [oci_containerengine_cluster.k8s_cluster]

  kubernetes_version = var.cluster_kubernetes_version
  name               = var.label_prefix == ""none"" ? each.key : ""${var.label_prefix}-${each.key}""

  freeform_tags = merge(var.freeform_tags[""node_pool""], { app = ""cluster-autoscaler"", pool = ""autoscaler"" })
  defined_tags  = merge(var.defined_tags[""node_pool""], { ""oke.pool"" = ""autoscaler"" })

  node_config_details {

    is_pv_encryption_in_transit_enabled = var.enable_pv_encryption_in_transit

    kms_key_id = var.node_pool_volume_kms_key_id

    # iterating over ADs
    dynamic ""placement_configs"" {
      iterator = ad_iterator
      for_each = [for n in lookup(each.value, ""placement_ads"", local.ad_numbers) :
      local.ad_number_to_name[n]]
      content {
        availability_domain = ad_iterator.value
        subnet_id           = var.cluster_subnets[""workers""]
      }
    }

    nsg_ids = var.worker_nsgs

    # flannel requires cni type only
    dynamic ""node_pool_pod_network_option_details"" {
      for_each = var.cni_type == ""flannel"" ? [1] : []
      content {
        cni_type = ""FLANNEL_OVERLAY""
      }
    }

    # native requires max pods/node, nsg ids, subnet ids
    dynamic ""node_pool_pod_network_option_details"" {
      for_each = var.cni_type == ""npn"" ? [1] : []
      content {
        cni_type          = ""OCI_VCN_IP_NATIVE""
        max_pods_per_node = var.max_pods_per_node
        # pick the 1st pod nsg here until https://github.com/oracle/terraform-provider-oci/issues/1662 is clarified and resolved
        pod_nsg_ids    = element(var.pod_nsgs, 0)
        pod_subnet_ids = tolist([var.cluster_subnets[""pods""]])
      }
    }

    size = 1

    freeform_tags = merge(var.freeform_tags[""node""], { app = ""cluster-autoscaler"", pool = ""autoscaler"" })

    # hardcoded defined tags are used to determine dynamic group membership and permissions
    defined_tags = merge(var.defined_tags[""node""], { ""oke.pool"" = ""autoscaler"" })
  }

  # setting shape
  node_shape = ""VM.Standard.E4.Flex""

  node_shape_config {
    ocpus         = 2
    memory_in_gbs = 32
  }

  # cloud-init
  node_metadata = {
    user_data = var.cloudinit_nodepool_common == """" && lookup(var.cloudinit_nodepool, each.key, null) == null ? data.cloudinit_config.worker.rendered : lookup(var.cloudinit_nodepool, each.key, null) != null ? filebase64(lookup(var.cloudinit_nodepool, each.key, null)) : filebase64(var.cloudinit_nodepool_common)
  }

  # optimized OKE images
  dynamic ""node_source_details"" {
    for_each = var.node_pool_image_type == ""oke"" ? [1] : []
    content {
      boot_volume_size_in_gbs = lookup(each.value, ""boot_volume_size"", 50)

      # image fixed to E4.flex, no need to lookup
      image_id    = (element([for source in local.node_pool_image_ids : source.image_id if length(regexall(""Oracle-Linux-${var.node_pool_os_version}-20[0-9]*.*-OKE-${local.k8s_version_only}"", source.source_name)) > 0], 0))
      source_type = data.oci_containerengine_node_pool_option.node_pool_options.sources[0].source_type
    }
  }

  # OCI platform images
  dynamic ""node_source_details"" {
    for_each = var.node_pool_image_type == ""platform"" ? [1] : []
    content {
      boot_volume_size_in_gbs = lookup(each.value, ""boot_volume_size"", 50)
      # image fixed to E4.flex, no need to lookup
      image_id    = element([for source in local.node_pool_image_ids : source.image_id if length(regexall(""^(Oracle-Linux-${var.node_pool_os_version}-\\d{4}.\\d{2}.\\d{2}-[0-9]*)$"", source.source_name)) > 0], 0)
      source_type = data.oci_containerengine_node_pool_option.node_pool_options.sources[0].source_type
    }
  }

  # custom images 
  dynamic ""node_source_details"" {
    for_each = var.node_pool_image_type == ""custom"" ? [1] : []
    content {
      boot_volume_size_in_gbs = lookup(each.value, ""boot_volume_size"", 50)
      image_id                = var.node_pool_image_id
      source_type             = data.oci_containerengine_node_pool_option.node_pool_options.sources[0].source_type
    }
  }

  ssh_public_key = (var.ssh_public_key != """") ? var.ssh_public_key : (var.ssh_public_key_path != ""none"") ? file(var.ssh_public_key_path) : """"

  # do not destroy the node pool if the kubernetes version has changed as part of the upgrade
  lifecycle {
    ignore_changes = [kubernetes_version]
  }

  # initial node labels for the autoscaler pool
  initial_node_labels {
    key   = ""app""
    value = ""cluster-autoscaler""
  }
}
",resource,,,94,0.0,788f33fd036093de6591157ca5c9f96f3f990e41,9910c5654ac76c0235b0e7ff2e98137cc90226d3,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/788f33fd036093de6591157ca5c9f96f3f990e41/modules/oke/autoscaler.tf#L94,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/9910c5654ac76c0235b0e7ff2e98137cc90226d3/modules/oke/autoscaler.tf#L0,2023-02-16 14:25:03+11:00,2023-10-25 16:40:02+11:00,2,2,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,483,tests/fast/stages/s03_project_factory/fixture/main.tf,tests/fast/stages/s03_project_factory/fixture/main.tf,0,#todo,#TODO(sruffilli): Pin to release,#TODO(sruffilli): Pin to release,"module ""projects"" {
  #TODO(sruffilli): Pin to release
  source             = ""github.com/terraform-google-modules/cloud-foundation-fabric/examples/factories/project-factory""
  for_each           = local.projects
  defaults           = local.defaults
  project_id         = each.key
  billing_account_id = try(each.value.billing_account_id, null)
  billing_alert      = try(each.value.billing_alert, null)
  dns_zones          = try(each.value.dns_zones, [])
  essential_contacts = try(each.value.essential_contacts, [])
  folder_id          = each.value.folder_id
  group_iam          = try(each.value.group_iam, {})
  iam                = try(each.value.iam, {})
  kms_service_agents = try(each.value.kms, {})
  labels             = try(each.value.labels, {})
  org_policies       = try(each.value.org_policies, null)
  service_accounts   = try(each.value.service_accounts, {})
  services           = try(each.value.services, [])
  services_iam       = try(each.value.services_iam, {})
  vpc                = try(each.value.vpc, null)
}
",module,"module ""projects"" {
  source               = ""../../../../../fast/stages/03-project-factory/dev""
  data_dir             = ""./data/projects/""
  defaults_file        = ""./data/defaults.yaml""
  prefix               = ""test""
  billing_account_id   = ""12345-67890A-BCDEF0""
  environment_dns_zone = ""dev""
  shared_vpc_self_link = ""fake_link""
  vpc_host_project     = ""host_project""
}
",module,36,,cee207b4544cfe2bc2eb517fd91c79952e3052b3,1d187ddd236a0f522528139204f0b64d71d74d9e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/cee207b4544cfe2bc2eb517fd91c79952e3052b3/tests/fast/stages/s03_project_factory/fixture/main.tf#L36,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/1d187ddd236a0f522528139204f0b64d71d74d9e/tests/fast/stages/s03_project_factory/fixture/main.tf,2022-01-17 10:36:38+01:00,2022-02-15 12:22:08+01:00,5,1,0,1,0,0,0,0,0,0
https://github.com/alphagov/govuk-aws,1446,terraform/projects/infra-assets/main.tf,terraform/projects/infra-assets/main.tf,0,# todo,"# TODO: check if still used; clean up and remove (app_user, s3_writer).","# TODO: check if still used; clean up and remove (app_user, s3_writer).","resource ""aws_iam_user"" ""app_user"" {
  name = ""govuk-assets-${var.aws_environment}-user""
}
",resource,"resource ""aws_iam_user"" ""app_user"" {
  name = ""govuk-assets-${var.aws_environment}-user""
}
",resource,51,51.0,c1d29a6caabcd4f041d87d76a10f2deef06685ef,c1d29a6caabcd4f041d87d76a10f2deef06685ef,https://github.com/alphagov/govuk-aws/blob/c1d29a6caabcd4f041d87d76a10f2deef06685ef/terraform/projects/infra-assets/main.tf#L51,https://github.com/alphagov/govuk-aws/blob/c1d29a6caabcd4f041d87d76a10f2deef06685ef/terraform/projects/infra-assets/main.tf#L51,2023-06-02 10:34:23+01:00,2023-06-02 10:34:23+01:00,1,0,0,1,0,0,0,0,0,1
https://github.com/aws-ia/terraform-aws-eks-blueprints,306,modules/kubernetes-addons/external-dns/data.tf,modules/kubernetes-addons/external-dns/data.tf,0,todo,# TODO - remove at next breaking change,# TODO - remove at next breaking change,"data ""aws_route53_zone"" ""selected"" {
  name         = var.domain_name
  private_zone = var.private_zone
}
",data,,,1,0.0,e6c597ecd70be756fd5be142a46519555e448066,85a0fca86ea422a2273a5e2845ce4e4a30413f37,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/e6c597ecd70be756fd5be142a46519555e448066/modules/kubernetes-addons/external-dns/data.tf#L1,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/85a0fca86ea422a2273a5e2845ce4e4a30413f37/modules/kubernetes-addons/external-dns/data.tf#L0,2022-08-10 11:46:16-04:00,2022-10-28 18:55:20-04:00,2,2,0,1,0,0,1,0,0,0
https://github.com/ManagedKube/kubernetes-ops,2,terraform-modules/aws/cloudposse/aws-cloudtrail-cloudwatch-alarms/main.tf,terraform-modules/aws/cloudposse/aws-cloudtrail-cloudwatch-alarms/main.tf,0,/*todo,"/*ToDo: We are collaborating with cloudposse to bring this solution to your project, we have the task of following up this pr to integrate it            and return to the direct version of cloudposse.                      Cloudposse' issue: New input variable s3_object_ownership cloudposse/terraform-aws-cloudtrail-s3-bucket#62           Cloudposse' pr: add input var s3_object_ownership cloudposse/terraform-aws-cloudtrail-s3-bucket#63*/","/*ToDo: We are collaborating with cloudposse to bring this solution to your project, we have the task of following up this pr to integrate it 
 and return to the direct version of cloudposse. 
  
 Cloudposse' issue: New input variable s3_object_ownership cloudposse/terraform-aws-cloudtrail-s3-bucket#62 
 Cloudposse' pr: add input var s3_object_ownership cloudposse/terraform-aws-cloudtrail-s3-bucket#63 
 */","module ""cloudtrail_s3_bucket"" {
  source  = ""github.com/ManagedKube/terraform-aws-cloudtrail-s3-bucket.git//?ref=0.24.0""
  #version = ""master""
  force_destroy          = var.force_destroy
  versioning_enabled     = var.versioning_enabled
  access_log_bucket_name = var.access_log_bucket_name
  allow_ssl_requests_only= var.allow_ssl_requests_only
  acl                    = var.acl
  s3_object_ownership    = var.s3_object_ownership
  sse_algorithm          = ""aws:kms""
  context = module.this.context
}
",module,"module ""cloudtrail_s3_bucket"" {
  source  = ""github.com/ManagedKube/terraform-aws-cloudtrail-s3-bucket.git//?ref=0.24.0""
  #version = ""master""
  force_destroy          = var.force_destroy
  versioning_enabled     = var.versioning_enabled
  access_log_bucket_name = var.access_log_bucket_name
  allow_ssl_requests_only= var.allow_ssl_requests_only
  acl                    = var.acl
  s3_object_ownership    = var.s3_object_ownership
  sse_algorithm          = ""aws:kms""
  context = module.this.context
}
",module,4,,9370126da6d0e1ff99a7a6cedbdd8e1a696f8a65,666deaf5b451460ad4b7253ba6b82bf6a64a018a,https://github.com/ManagedKube/kubernetes-ops/blob/9370126da6d0e1ff99a7a6cedbdd8e1a696f8a65/terraform-modules/aws/cloudposse/aws-cloudtrail-cloudwatch-alarms/main.tf#L4,https://github.com/ManagedKube/kubernetes-ops/blob/666deaf5b451460ad4b7253ba6b82bf6a64a018a/terraform-modules/aws/cloudposse/aws-cloudtrail-cloudwatch-alarms/main.tf,2022-06-24 10:30:24-07:00,2022-07-18 14:43:06-07:00,3,1,1,0,0,0,0,0,0,0
https://github.com/alphagov/govuk-aws,1274,terraform/projects/app-amazonmq/main.tf,terraform/projects/app-publishing-amazonmq/main.tf,1,# todo,"# TODO: this version will only work with a single instance, as on integration. ","# TODO: this version will only work with a single instance, as on integration. 
 # For staging/production, we'll have a highly-available cluster, at which point 
 # we'll need to repoint this Route53 record at a Network Load Balancer that balances 
 # between the instances. See Amazon's article about how to do that here: 
 # https://aws.amazon.com/blogs/compute/creating-static-custom-domain-endpoints-with-amazon-mq-for-rabbitmq/","resource ""aws_route53_record"" ""amazonmq_internal_root_domain_name"" {
  zone_id = data.terraform_remote_state.infra_root_dns_zones.outputs.internal_root_zone_id
  name    = ""${aws_mq_broker.publishing_amazonmq.broker_name}.${data.terraform_remote_state.infra_root_dns_zones.outputs.internal_root_domain_name}""
  type    = ""CNAME""
  ttl     = 300
  # TODO: this version will only work with a single instance, as on integration. 
  # For staging/production, we'll have a highly-available cluster, at which point
  # we'll need to repoint this Route53 record at a Network Load Balancer that balances
  # between the instances. See Amazon's article about how to do that here:
  # https://aws.amazon.com/blogs/compute/creating-static-custom-domain-endpoints-with-amazon-mq-for-rabbitmq/
  records = [regex(""://([^/:]+)"", aws_mq_broker.publishing_amazonmq.instances.0.console_url)[0]]

}
",resource,the block associated got renamed or deleted,,126,,7bc6821c6e0ca7490ebc9a6b9388a1e3565d7cdf,7562838d07e1e9808599970c19e0c5589a58c83c,https://github.com/alphagov/govuk-aws/blob/7bc6821c6e0ca7490ebc9a6b9388a1e3565d7cdf/terraform/projects/app-amazonmq/main.tf#L126,https://github.com/alphagov/govuk-aws/blob/7562838d07e1e9808599970c19e0c5589a58c83c/terraform/projects/app-publishing-amazonmq/main.tf,2022-11-29 10:57:17+00:00,2023-01-20 10:56:58+00:00,18,1,0,1,0,0,1,1,0,0
https://github.com/wireapp/wire-server-deploy,19,terraform/modules/aws-vpc-security-groups/main.tf,terraform/modules/aws-vpc-security-groups/main.tf,0,fix,"# we go with the CIDR for now, which is hard-coded and needs fixing","# NOTE: NLBs dont allow security groups to be set on them, which is why 
 # we go with the CIDR for now, which is hard-coded and needs fixing","resource ""aws_security_group"" ""k8s_node"" {
  name        = ""k8s_node""
  description = ""hosts that have kubernetes.""
  vpc_id      = var.vpc_id

  # incoming from the admin node (kubectl)
  ingress {
    from_port       = 6443
    to_port         = 6443
    protocol        = ""tcp""
    security_groups = [""${aws_security_group.talk_to_k8s.id}""]
  }

  # FIXME: tighten this up.
  ingress {
    from_port       = 0
    to_port         = 65535
    protocol        = ""tcp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # FIXME: tighten this up. need UDP for flannel.
  ingress {
    from_port       = 0
    to_port         = 65535
    protocol        = ""udp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # incoming traffic to the application.
  ingress {
    from_port   = 31772
    to_port     = 31773
    protocol    = ""tcp""
    # NOTE: NLBs dont allow security groups to be set on them, which is why
    # we go with the CIDR for now, which is hard-coded and needs fixing
    cidr_blocks = [""172.17.0.0/20""]
  }

  tags = {
    Name = ""k8s_node""
  }
}
",resource,"resource ""aws_security_group"" ""k8s_node"" {
  name        = ""k8s_node""
  description = ""hosts that have kubernetes.""
  vpc_id      = var.vpc_id

  # incoming from the admin node (kubectl)
  ingress {
    description     = """"
    from_port       = 6443
    to_port         = 6443
    protocol        = ""tcp""
    security_groups = [""${aws_security_group.talk_to_k8s.id}""]
  }

  # FIXME: tighten this up.
  ingress {
    description     = """"
    from_port       = 0
    to_port         = 65535
    protocol        = ""tcp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # FIXME: tighten this up. need UDP for flannel.
  ingress {
    description     = """"
    from_port       = 0
    to_port         = 65535
    protocol        = ""udp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # incoming traffic to the application.
  ingress {
    from_port   = 31772
    to_port     = 31773
    protocol    = ""tcp""
    # NOTE: NLBs dont allow security groups to be set on them, which is why
    # we go with the CIDR for now, which is hard-coded and needs fixing
    cidr_blocks = [""172.17.0.0/20""]
  }

  tags = {
    Name = ""k8s_node""
  }
}
",resource,244,249.0,1e8874480a2745e81a973d5e2eee84c58baa983e,e7f081ca744de60bf9d10cb037828b995883d2aa,https://github.com/wireapp/wire-server-deploy/blob/1e8874480a2745e81a973d5e2eee84c58baa983e/terraform/modules/aws-vpc-security-groups/main.tf#L244,https://github.com/wireapp/wire-server-deploy/blob/e7f081ca744de60bf9d10cb037828b995883d2aa/terraform/modules/aws-vpc-security-groups/main.tf#L249,2020-06-24 14:14:54+01:00,2020-08-26 16:29:39+02:00,3,0,0,1,0,0,1,0,0,0
https://github.com/jenkins-x/terraform-google-jx,2,main.tf,main.tf,0,// todo,"// TODO: remove parent_domain when its deprecations is complete: domain_enabled = var.apex_domain != """" ? true : false","// DNS 
 // TODO: remove parent_domain when its deprecations is complete: domain_enabled = var.apex_domain != """" ? true : false","locals {
  interpolated_content = templatefile(""${path.module}/modules/jx-requirements.yml.tpl"", {
    gcp_project                 = var.gcp_project
    zone                        = var.cluster_location
    cluster_name                = local.cluster_name
    git_owner_requirement_repos = var.git_owner_requirement_repos
    dev_env_approvers           = var.dev_env_approvers
    lets_encrypt_production     = var.lets_encrypt_production
    // Storage buckets
    log_storage_url        = module.cluster.log_storage_url
    report_storage_url     = module.cluster.report_storage_url
    repository_storage_url = module.cluster.repository_storage_url
    backup_bucket_url      = module.backup.backup_bucket_url
    // Vault
    external_vault = local.external_vault
    vault_bucket   = length(module.vault) > 0 ? module.vault[0].vault_bucket_name : """"
    vault_key      = length(module.vault) > 0 ? module.vault[0].vault_key : """"
    vault_keyring  = length(module.vault) > 0 ? module.vault[0].vault_keyring : """"
    vault_name     = length(module.vault) > 0 ? module.vault[0].vault_name : """"
    vault_sa       = length(module.vault) > 0 ? module.vault[0].vault_sa : """"
    vault_url      = var.vault_url
    // Velero
    enable_backup    = var.enable_backup
    velero_sa        = module.backup.velero_sa
    velero_namespace = module.backup.backup_bucket_url != """" ? var.velero_namespace : """"
    velero_schedule  = var.velero_schedule
    velero_ttl       = var.velero_ttl
    // DNS
    // TODO: remove parent_domain when its deprecations is complete: domain_enabled = var.apex_domain != """" ? true : false
    domain_enabled = var.apex_domain != """" ? true : (var.parent_domain != """" ? true : false)
    // TODO: replace with the following when parent_domain deprecations is complete: apex_domain  = var.apex_domain
    apex_domain    = var.apex_domain != """" ? var.apex_domain : var.parent_domain
    subdomain      = var.subdomain
    tls_email      = var.tls_email

    version_stream_ref = var.version_stream_ref
    version_stream_url = var.version_stream_url
    webhook            = var.webhook
  })

  split_content   = split(""\n"", local.interpolated_content)
  compact_content = compact(local.split_content)
  content         = join(""\n"", local.compact_content)
}
",locals,"locals {
  requirements_file = var.jx2 ? ""${path.module}/modules/jx-requirements.yml.tpl"" : ""${path.module}/modules/jx-requirements-v3.yml.tpl""
  interpolated_content = templatefile(local.requirements_file, {
    gcp_project                 = var.gcp_project
    zone                        = var.cluster_location
    cluster_name                = local.cluster_name
    git_owner_requirement_repos = var.git_owner_requirement_repos
    dev_env_approvers           = var.dev_env_approvers
    lets_encrypt_production     = var.lets_encrypt_production
    // GCP Artifact
    enable_artifact        = var.artifact_enable
    registry               = module.cluster.artifact_registry_repository
    docker_registry_org    = module.cluster.artifact_registry_repository_name
    // Storage buckets
    log_storage_url        = module.cluster.log_storage_url
    report_storage_url     = module.cluster.report_storage_url
    repository_storage_url = module.cluster.repository_storage_url
    backup_bucket_url      = module.backup.backup_bucket_url
    // Vault
    external_vault  = local.external_vault
    vault_bucket    = length(module.vault) > 0 ? module.vault[0].vault_bucket_name : """"
    vault_key       = length(module.vault) > 0 ? module.vault[0].vault_key : """"
    vault_keyring   = length(module.vault) > 0 ? module.vault[0].vault_keyring : """"
    vault_name      = length(module.vault) > 0 ? module.vault[0].vault_name : """"
    vault_sa        = length(module.vault) > 0 ? module.vault[0].vault_sa : """"
    vault_url       = var.vault_url
    vault_installed = !var.gsm ? true : false
    // Velero
    enable_backup    = var.enable_backup
    velero_sa        = module.backup.velero_sa
    velero_namespace = module.backup.backup_bucket_url != """" ? var.velero_namespace : """"
    velero_schedule  = var.velero_schedule
    velero_ttl       = var.velero_ttl
    // DNS
    // TODO: remove parent_domain when its deprecations is complete: domain_enabled = var.apex_domain != """" ? true : false
    domain_enabled = var.apex_domain != """" ? true : (var.parent_domain != """" ? true : false)
    // TODO: replace with the following when parent_domain deprecations is complete: apex_domain  = var.apex_domain
    apex_domain = var.apex_domain != """" ? var.apex_domain : var.parent_domain
    subdomain   = var.subdomain
    tls_email   = var.tls_email
    // Kuberhealthy
    kuberhealthy = var.kuberhealthy

    version_stream_ref = var.version_stream_ref
    version_stream_url = var.version_stream_url
    webhook            = var.webhook
  })

  split_content   = split(""\n"", local.interpolated_content)
  compact_content = compact(local.split_content)
  content         = join(""\n"", local.compact_content)
}
",locals,279,304.0,ffc0b68673f1e9341ef89e88f6af157631a9086d,6d61937b9d1a81a879246040a6f6e11ed5626a4e,https://github.com/jenkins-x/terraform-google-jx/blob/ffc0b68673f1e9341ef89e88f6af157631a9086d/main.tf#L279,https://github.com/jenkins-x/terraform-google-jx/blob/6d61937b9d1a81a879246040a6f6e11ed5626a4e/main.tf#L304,2020-12-09 11:35:14+10:00,2024-04-25 13:02:59+02:00,30,0,0,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,588,examples/data-solutions/dp-foundation/07-exposure.tf,examples/data-solutions/data-platform-foundations/07-exposure.tf,1,#todo,#TODO add group => role mapping to asign on exposure project,#TODO add group => role mapping to asign on exposure project,"locals {
  group_iam_exp = {
    #TODO add group => role mapping to asign on exposure project
  }
  iam_exp = {
    #TODO add role => service account mapping to assign roles on exposure project
  }
  prefix_exp = ""${var.prefix}-exp""
}
",locals,the block associated got renamed or deleted,,19,,3c99074b3ff652827d277217e4f84b48a713b224,4f4a9cd7ac2cee3a1b2e413cd19f6b6b07c35622,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3c99074b3ff652827d277217e4f84b48a713b224/examples/data-solutions/dp-foundation/07-exposure.tf#L19,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/4f4a9cd7ac2cee3a1b2e413cd19f6b6b07c35622/examples/data-solutions/data-platform-foundations/07-exposure.tf,2022-02-02 15:31:54+01:00,2022-02-09 17:01:25+01:00,3,1,0,1,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,12,infra/gcp/clusters/kubernetes-public/prow-build-test/k8s-infra-gke-nodepool/main.tf,infra/gcp/terraform/modules/gke-nodepool/main.tf,1,todo,// TODO ??,"// TODO ?? 
 // name = var.name","resource ""google_container_node_pool"" ""node_pool"" {
  provider = google-beta

  // TODO ??
  // name = var.name
  name_prefix = ""${var.name}-""

  project     = var.project_name
  location    = var.location
  cluster     = var.cluster_name

  // Auto repair, and auto upgrade nodes to match the master version
  management {
    auto_repair  = true
    auto_upgrade = true
  }

  // Autoscale the cluster as needed. Note if using a regional cluster these values will be multiplied by 3
  initial_node_count = var.min_count
  autoscaling {
    min_node_count = var.min_count
    max_node_count = var.max_count
  }

  // Set machine type, and enable all oauth scopes tied to the service account
  node_config {
    machine_type = var.machine_type
    disk_size_gb = var.disk_size_gb
    disk_type    = var.disk_type

    service_account = var.service_account
    oauth_scopes    = [""https://www.googleapis.com/auth/cloud-platform""]

    // Needed for workload identity
    workload_metadata_config {
      node_metadata = ""GKE_METADATA_SERVER""
    }
    metadata = {
      disable-legacy-endpoints = ""true""
    }
  }

  // If we need to destroy the node pool, create the new one before destroying
  // the old one
  lifecycle {
    create_before_destroy = true
  }
}
",resource,"resource ""google_container_node_pool"" ""node_pool"" {
  provider = google-beta

  // TODO ??
  // name = var.name
  name_prefix = ""${var.name}-""

  project     = var.project_name
  location    = var.location
  cluster     = var.cluster_name

  // Auto repair, and auto upgrade nodes to match the master version
  management {
    auto_repair  = true
    auto_upgrade = true
  }

  // Autoscale the cluster as needed. Note if using a regional cluster these values will be multiplied by 3
  initial_node_count = var.initial_count
  autoscaling {
    min_node_count = var.min_count
    max_node_count = var.max_count
  }

  // Set machine type, and enable all oauth scopes tied to the service account
  node_config {
    image_type   = var.image_type
    machine_type = var.machine_type
    disk_size_gb = var.disk_size_gb
    disk_type    = var.disk_type
    labels       = var.labels
    taint        = var.taints

    service_account = var.service_account
    oauth_scopes    = [""https://www.googleapis.com/auth/cloud-platform""]

    dynamic ""ephemeral_storage_config"" {
      for_each = var.ephemeral_local_ssd_count > 0 ? [var.ephemeral_local_ssd_count] : [] 
      content {
        local_ssd_count = ephemeral_storage_config.value
      }
    }

    // Needed for workload identity
    workload_metadata_config {
      mode = ""GKE_METADATA""
    }
    metadata = {
      disable-legacy-endpoints = ""true""
    }
  }

  // If we need to destroy the node pool, create the new one before destroying
  // the old one
  lifecycle {
    create_before_destroy = true
    # https://www.terraform.io/docs/providers/google/r/container_cluster.html#taint
    ignore_changes = [
      node_config[0].taint,
    ]
  }
}
",resource,20,20.0,c21384be39d30f4b0b0d7ecbc52a8d1109bc36c9,8e2d286dbeb61acbe598ae083b9e246518c0b78e,https://github.com/kubernetes/k8s.io/blob/c21384be39d30f4b0b0d7ecbc52a8d1109bc36c9/infra/gcp/clusters/kubernetes-public/prow-build-test/k8s-infra-gke-nodepool/main.tf#L20,https://github.com/kubernetes/k8s.io/blob/8e2d286dbeb61acbe598ae083b9e246518c0b78e/infra/gcp/terraform/modules/gke-nodepool/main.tf#L20,2020-04-29 16:44:08-07:00,2021-11-01 15:53:37-07:00,8,0,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1838,modules/cloud-run-v2/variables-vpcconnector.tf,modules/cloud-run-v2/variables-vpcconnector.tf,0,workaround,# workaround for a wrong default in provider,"max = optional(number, 1000) # workaround for a wrong default in provider","variable ""vpc_connector_create"" {
  description = ""Populate this to create a Serverless VPC Access connector.""
  type = object({
    ip_cidr_range = optional(string)
    machine_type  = optional(string)
    name          = optional(string)
    network       = optional(string)
    instances = optional(object({
      max = optional(number)
      min = optional(number)
    }), {})
    throughput = optional(object({
      max = optional(number, 1000) # workaround for a wrong default in provider
      min = optional(number)
    }), {})
    subnet = optional(object({
      name       = optional(string)
      project_id = optional(string)
    }), {})
  })
  default = null
}
",variable,"variable ""vpc_connector_create"" {
  description = ""Populate this to create a Serverless VPC Access connector.""
  type = object({
    ip_cidr_range = optional(string)
    machine_type  = optional(string)
    name          = optional(string)
    network       = optional(string)
    instances = optional(object({
      max = optional(number)
      min = optional(number)
    }), {})
    throughput = optional(object({
      max = optional(number, 1000) # workaround for a wrong default in provider
      min = optional(number)
    }), {})
    subnet = optional(object({
      name       = optional(string)
      project_id = optional(string)
    }), {})
  })
  default = null
}
",variable,29,29.0,bee307256877ca5f44ba0829ee160f6341fb3ca4,bee307256877ca5f44ba0829ee160f6341fb3ca4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/bee307256877ca5f44ba0829ee160f6341fb3ca4/modules/cloud-run-v2/variables-vpcconnector.tf#L29,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/bee307256877ca5f44ba0829ee160f6341fb3ca4/modules/cloud-run-v2/variables-vpcconnector.tf#L29,2024-02-18 14:57:34+01:00,2024-02-18 14:57:34+01:00,1,0,0,0,1,0,1,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,167,modules/workergroup/instancepools.tf,modules/workers/instancepools.tf,1,todo,# TODO Support w/ named secondary VNICs,"      vnic_selection = lookup(lb, ""vnic_selection"", ""PrimaryVnic"") # TODO Support w/ named secondary VNICs","resource ""oci_core_instance_pool"" ""instance_pools"" {
  # Create an OCI Instance Pool resource for each enabled entry of the worker_groups map with that mode.
  for_each                  = local.enabled_instance_pools
  compartment_id            = each.value.compartment_id
  display_name              = ""${each.value.label_prefix}-${each.key}""
  size                      = each.value.size
  instance_configuration_id = oci_core_instance_configuration.instance_configuration[each.key].id
  defined_tags              = merge(local.defined_tags, contains(keys(each.value), ""defined_tags"") ? each.value.defined_tags : {})
  freeform_tags             = merge(local.freeform_tags, contains(keys(each.value), ""freeform_tags"") ? each.value.freeform_tags : { worker_group = each.key })

  dynamic ""placement_configurations"" {
    # Define each configured availability domain for placement, with bounds on # available
    # Configured AD numbers e.g. [1,2,3] are converted into tenancy/compartment-specific names
    iterator = ad_number
    for_each = (contains(keys(each.value), ""placement_ads"")
      ? tolist(setintersection(each.value.placement_ads, local.ad_numbers))
      : local.ad_numbers
    )

    content {
      availability_domain = lookup(local.ad_number_to_name, ad_number.value, local.first_ad_name)
      primary_subnet_id   = each.value.subnet_id
    }
  }

  lifecycle {
    ignore_changes = [
      display_name, defined_tags, freeform_tags,
      placement_configurations,
    ]
  }

  dynamic ""load_balancers"" {
    # Associate the instance pool with 0+ load balancers for ingress traffic
    # TODO Accept full definition to create
    for_each = contains(keys(each.value), ""load_balancers"") ? each.value.load_balancers : {}

    content {
      # TODO From dynamic creation when no lb_id provided; introspected fields when present
      backend_set_name = lookup(lb, ""backend_set_name"", display_name)
      load_balancer_id = lookup(lb, ""lb_id"", lb_id)
      port             = lookup(lb, ""port"", 8080)

      // Possible values are ""PrimaryVnic"" or the displayName of
      // one of the secondary VNICs on the instance configuration
      // that is associated with the instance pool.
      vnic_selection = lookup(lb, ""vnic_selection"", ""PrimaryVnic"") # TODO Support w/ named secondary VNICs
    }
  }

  depends_on = [
    oci_core_instance_configuration.instance_configuration,
  ]
}",resource,the block associated got renamed or deleted,,51,,4d2b3f3d672a8f41655da3a7c58fded42c6858f3,f49f1da39d79cf260d80dcb10ee8e399828e6e1c,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/4d2b3f3d672a8f41655da3a7c58fded42c6858f3/modules/workergroup/instancepools.tf#L51,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/f49f1da39d79cf260d80dcb10ee8e399828e6e1c/modules/workers/instancepools.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,4,1,1,1,0,0,1,0,0,0
https://github.com/pingcap/tidb-operator,5,deploy/alicloud/ack/main.tf,deploy/aliyun/ack/main.tf,1,fix,"# FIXME: currently update vswitch_ids will force will recreate, allow updating when upstream support in-place","# FIXME: currently update vswitch_ids will force will recreate, allow updating when upstream support in-place 
 # vswitch id update","resource ""alicloud_ess_scaling_group"" ""workers"" {
  count              = ""${length(var.worker_groups)}""
  scaling_group_name = ""${alicloud_cs_managed_kubernetes.k8s.name}-${lookup(var.worker_groups[count.index], ""name"", count.index)}""
  vswitch_ids        = [""${split("","", var.vpc_id != """" ? join("","", data.template_file.vswitch_id.*.rendered) : join("","", alicloud_vswitch.all.*.id))}""]
  min_size           = ""${lookup(var.worker_groups[count.index], ""min_size"", var.group_default[""min_size""])}""
  max_size           = ""${lookup(var.worker_groups[count.index], ""max_size"", var.group_default[""max_size""])}""
  default_cooldown   = ""${lookup(var.worker_groups[count.index], ""default_cooldown"", var.group_default[""default_cooldown""])}""
  multi_az_policy    = ""${lookup(var.worker_groups[count.index], ""multi_az_policy"", var.group_default[""multi_az_policy""])}""

  # Remove the newest instance in the oldest scaling configuration
  removal_policies = [
    ""OldestScalingConfiguration"",
    ""NewestInstance""
  ]

  lifecycle {
    # FIXME: currently update vswitch_ids will force will recreate, allow updating when upstream support in-place
    # vswitch id update
    ignore_changes = [""vswitch_ids""]

    create_before_destroy = true
  }
}
",resource,,,104,0.0,eebd686956c0b2adf67a10e1a376659c95c571a3,042b1a97fbbdf342297002990564828e6644a3f0,https://github.com/pingcap/tidb-operator/blob/eebd686956c0b2adf67a10e1a376659c95c571a3/deploy/alicloud/ack/main.tf#L104,https://github.com/pingcap/tidb-operator/blob/042b1a97fbbdf342297002990564828e6644a3f0/deploy/aliyun/ack/main.tf#L0,2019-05-06 19:59:43+08:00,2019-07-23 19:44:58+08:00,3,2,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,848,fast/stages/01-resman/branch-gke.tf,fast/stages/01-resman/branch-gke.tf,0,fix,# FIXME(jccb): who should we use here?,# FIXME(jccb): who should we use here?,"module ""branch-gke-multitenant-prod-sa"" {
  source      = ""../../../modules/iam-service-account""
  project_id  = var.automation_project_id
  name        = ""gke-prod-0""
  description = ""Terraform gke multitenant prod service account.""
  prefix      = var.prefix
  iam = {
    # FIXME(jccb): who should we use here?
    ""roles/iam.serviceAccountTokenCreator"" = [""group:${local.groups.gcp-devops}""]
  }
}
",module,the block associated got renamed or deleted,,56,,f3f9a4a88cedd64f6fc91b64666046aa6726a2f3,7b5ced7e15a537b1760bfe822780d9429e94f182,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/f3f9a4a88cedd64f6fc91b64666046aa6726a2f3/fast/stages/01-resman/branch-gke.tf#L56,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/7b5ced7e15a537b1760bfe822780d9429e94f182/fast/stages/01-resman/branch-gke.tf,2022-06-08 11:41:50+02:00,2022-06-30 18:22:57+02:00,6,1,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,8,community/modules/scheduler/cloud-batch-login-node/main.tf,community/modules/scheduler/cloud-batch-login-node/main.tf,0,# todo,# TODO: This workaround should be removed once startup-script supports Bash syntax,{ # TODO: This workaround should be removed once startup-script supports Bash syntax,"module ""login_startup_script"" {
  source          = ""github.com/GoogleCloudPlatform/hpc-toolkit//modules/scripts/startup-script?ref=v1.0.0""
  labels          = var.labels
  project_id      = var.project_id
  deployment_name = var.deployment_name
  region          = var.region
  runners = [
    {
      content     = local.batch_startup_script
      destination = ""/tmp/startup-scripts/batch_startup_script.sh""
      type        = ""data""
    },
    { # TODO: This workaround should be removed once startup-script supports Bash syntax
      content     = ""bash /tmp/startup-scripts/batch_startup_script.sh""
      destination = ""/tmp/startup-scripts/invoke_batch_startup_script.sh""
      type        = ""shell""
    },
    {
      content     = var.job_template_contents
      destination = ""${var.batch_job_directory}/${var.job_filename}""
      type        = ""data""
    }
  ]
}
",module,"module ""login_startup_script"" {
  source          = ""github.com/GoogleCloudPlatform/hpc-toolkit//modules/scripts/startup-script?ref=v1.6.0""
  labels          = var.labels
  project_id      = var.project_id
  deployment_name = var.deployment_name
  region          = var.region
  runners = [
    {
      content     = local.batch_startup_script
      destination = ""batch_startup_script.sh""
      type        = ""shell""
    },
    {
      content     = var.job_template_contents
      destination = ""${var.batch_job_directory}/${var.job_filename}""
      type        = ""data""
    }
  ]
}
",module,39,,9af8e32cdaabe4e30b03f245061414230e11d930,061ab8ccf926ba56d1fb97e2fd97cfaa4fdcaf88,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/9af8e32cdaabe4e30b03f245061414230e11d930/community/modules/scheduler/cloud-batch-login-node/main.tf#L39,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/061ab8ccf926ba56d1fb97e2fd97cfaa4fdcaf88/community/modules/scheduler/cloud-batch-login-node/main.tf,2022-07-08 10:56:43-07:00,2022-10-10 15:37:34-07:00,3,1,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1910,fast/stages/2-security/vpc-sc.tf,fast/stages/2-security/vpc-sc.tf,0,# todo,# TODO(ludomagno): allow passing in restricted services via variable and factory file,"# TODO(ludomagno): allow passing in restricted services via variable and factory file 
 # TODO(ludomagno): implement vpc accessible services via variable or factory file ","module ""vpc-sc"" {
  source = ""../../../modules/vpc-sc""
  # only enable if the default perimeter is defined
  count         = var.vpc_sc.perimeter_default == null ? 0 : 1
  access_policy = null
  access_policy_create = {
    parent = ""organizations/${var.organization.id}""
    title  = ""default""
  }
  access_levels   = var.vpc_sc.access_levels
  egress_policies = var.vpc_sc.egress_policies
  ingress_policies = merge(
    var.vpc_sc.ingress_policies,
    local.vpc_sc_ingress_policies
  )
  factories_config = var.factories_config.vpc_sc
  service_perimeters_regular = {
    default = {
      spec = (
        var.vpc_sc.perimeter_default.dry_run ? local.vpc_sc_perimeter : null
      )
      status = (
        !var.vpc_sc.perimeter_default.dry_run ? local.vpc_sc_perimeter : null
      )
      use_explicit_dry_run_spec = var.vpc_sc.perimeter_default.dry_run
    }
  }
}
",module,"module ""vpc-sc"" {
  source = ""../../../modules/vpc-sc""
  # only enable if the default perimeter is defined
  count         = var.vpc_sc.perimeter_default == null ? 0 : 1
  access_policy = var.access_policy
  access_policy_create = var.access_policy != null ? null : {
    parent = ""organizations/${var.organization.id}""
    title  = ""default""
  }
  access_levels   = var.vpc_sc.access_levels
  egress_policies = var.vpc_sc.egress_policies
  ingress_policies = merge(
    var.vpc_sc.ingress_policies,
    local.vpc_sc_ingress_policies
  )
  factories_config = var.factories_config.vpc_sc
  service_perimeters_regular = {
    default = {
      spec = (
        var.vpc_sc.perimeter_default.dry_run ? local.vpc_sc_perimeter : null
      )
      status = (
        !var.vpc_sc.perimeter_default.dry_run ? local.vpc_sc_perimeter : null
      )
      use_explicit_dry_run_spec = var.vpc_sc.perimeter_default.dry_run
    }
  }
}
",module,61,61.0,8511170412355efd6c5995431f2e5617d28d604e,7a5dd4e6db197daa52da8a8d877ce86b5c93182e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/8511170412355efd6c5995431f2e5617d28d604e/fast/stages/2-security/vpc-sc.tf#L61,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/7a5dd4e6db197daa52da8a8d877ce86b5c93182e/fast/stages/2-security/vpc-sc.tf#L61,2024-04-07 20:14:39-07:00,2024-05-15 09:17:13+00:00,3,0,0,1,0,1,1,0,0,0
https://github.com/Azure/Avere,15,src/terraform/modules/vmss_mountable/variables.tf,src/terraform/modules/vmss_mountable/variables.tf,0,implement,// depends on technique described here: https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2,// depends on technique described here: https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2,"variable ""vmss_depends_on"" {
  description = ""used to establish dependency between objects""
  type = any
  default = null
}
",variable,the block associated got renamed or deleted,,82,,3a982a8a0eab051c6b1bc553c5d9406b8a3d53c6,feb88e973389482312720d4be85a13cac2973006,https://github.com/Azure/Avere/blob/3a982a8a0eab051c6b1bc553c5d9406b8a3d53c6/src/terraform/modules/vmss_mountable/variables.tf#L82,https://github.com/Azure/Avere/blob/feb88e973389482312720d4be85a13cac2973006/src/terraform/modules/vmss_mountable/variables.tf,2020-06-25 12:01:00+01:00,2021-04-23 18:58:31-04:00,4,1,0,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,978,infra/modular-examples/gcp/main.tf,infra/modular-examples/gcp/main.tf,0,# todo,# TODO: what if multiple lookups from same source??,"bucket_name_suffix             = ""-lookup"" # TODO: what if multiple lookups from same source??","module ""lookup_output"" {
  for_each = var.lookup_tables

  source = ""../../modules/gcp-output-bucket""

  bucket_write_role_id           = module.psoxy.bucket_write_role_id
  function_service_account_email = module.psoxy-bulk[each.value.source_connector_id].instance_sa_email
  project_id                     = var.gcp_project_id
  region                         = var.gcp_region
  bucket_name_prefix             = module.psoxy-bulk[each.value.source_connector_id].bucket_prefix
  bucket_name_suffix             = ""-lookup"" # TODO: what if multiple lookups from same source??
  expiration_days                = each.value.expiration_days
  sanitizer_accessor_principals  = each.value.sanitized_accessor_principals
}
",module,,,334,0.0,4f41d480721ffa94079bbfb0cdcc181e22423300,f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e,https://github.com/Worklytics/psoxy/blob/4f41d480721ffa94079bbfb0cdcc181e22423300/infra/modular-examples/gcp/main.tf#L334,https://github.com/Worklytics/psoxy/blob/f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e/infra/modular-examples/gcp/main.tf#L0,2023-04-18 16:24:15-07:00,2023-06-16 14:08:45-07:00,13,2,1,1,0,0,0,0,0,0
https://github.com/ministryofjustice/aws-root-account,1,terraform/opg-roles.tf,terraform/opg-roles.tf,0,implemented,# Once SSO has been implemented we can get rid of these anyway,"# I've decided to hard code our ARNs, I don't want to make this repo depend on our accounts. 
 # Once SSO has been implemented we can get rid of these anyway","locals {
  # I've decided to hard code our ARNs, I don't want to make this repo depend on our accounts.
  # Once SSO has been implemented we can get rid of these anyway
  opg_engineers = [
    ""arn:aws:iam::631181914621:user/thomas.withers"",
    ""arn:aws:iam::631181914621:user/andrew.pearce"",
  ]
}
",locals,,,26,0.0,b0d4ef88fceb7d448a684c4480e756911c47f87d,432bff65375d36954c647b7a17f259addc2ccc40,https://github.com/ministryofjustice/aws-root-account/blob/b0d4ef88fceb7d448a684c4480e756911c47f87d/terraform/opg-roles.tf#L26,https://github.com/ministryofjustice/aws-root-account/blob/432bff65375d36954c647b7a17f259addc2ccc40/terraform/opg-roles.tf#L0,2020-09-30 14:06:38+01:00,2020-12-07 09:01:03+00:00,4,2,0,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,551,infra/modules/vault-psoxy/main.tf,infra/modules/vault-psoxy/main.tf,0,# todo,"# TODO: as of 4 Jan 2023, secret/* case needed here; please scope better for your prod use!!","# TODO: as of 4 Jan 2023, secret/* case needed here; please scope better for your prod use!!","resource ""vault_policy"" ""psoxy_instance"" {
  name   = var.instance_id

  # TODO: as of 4 Jan 2023, secret/* case needed here; please scope better for your prod use!!
  policy = <<EOT
path ""secret/*""
{
	capabilities = [""read""]
}
path ""${var.path_to_global_secrets}*""
{
	capabilities = [""read""]
}

path ""${local.path_to_instance_secrets}*""
{
    capabilities = [""create"", ""read"", ""update""]
}
EOT
}
",resource,"resource ""vault_policy"" ""psoxy_instance"" {
  name = var.instance_id

  # TODO: as of 4 Jan 2023, secret/* case needed here; please scope better for your prod use!!
  policy = <<EOT
path ""secret/*""
{
	capabilities = [""read""]
}
path ""${var.path_to_global_secrets}*""
{
	capabilities = [""read""]
}

path ""${local.path_to_instance_secrets}*""
{
    capabilities = [""create"", ""read"", ""update""]
}
EOT
}
",resource,12,12.0,516c6c050368e211634d3dd35047ab0e2fdc6b52,a71ebbfe78973520324010427ee3857040e087ee,https://github.com/Worklytics/psoxy/blob/516c6c050368e211634d3dd35047ab0e2fdc6b52/infra/modules/vault-psoxy/main.tf#L12,https://github.com/Worklytics/psoxy/blob/a71ebbfe78973520324010427ee3857040e087ee/infra/modules/vault-psoxy/main.tf#L12,2023-01-04 19:42:55-08:00,2023-01-13 11:00:08-08:00,2,0,0,1,0,1,0,1,0,0
https://github.com/compiler-explorer/infra,22,terraform/dynamodb.tf,terraform/dynamodb.tf,0,// todo,// TODO: change once terraform supports on-demand pricing. We are currently set to use,"// TODO: change once terraform supports on-demand pricing. We are currently set to use 
 // on-demand in the UI only.","resource ""aws_dynamodb_table"" ""links"" {
  name = ""links""
  lifecycle {
    ignore_changes = [
      ""read_capacity"",
      ""write_capacity""
    ]
  }
  // TODO: change once terraform supports on-demand pricing. We are currently set to use
  // on-demand in the UI only.
  read_capacity = 1
  write_capacity = 1
  hash_key = ""prefix""
  range_key = ""unique_subhash""

  attribute = [
    {
      name = ""prefix""
      type = ""S""
    },
    {
      name = ""unique_subhash""
      type = ""S""
    }
  ]

  point_in_time_recovery {
    enabled = true
  }

  tags {
    key = ""Site""
    value = ""CompilerExplorer""
  }
}
",resource,"resource ""aws_dynamodb_table"" ""links"" {
  name = ""links""
  lifecycle {
    ignore_changes = [
      ""read_capacity"",
      ""write_capacity""
    ]
  }
  billing_mode = ""PAY_PER_REQUEST""
  read_capacity = 1
  write_capacity = 1
  hash_key = ""prefix""
  range_key = ""unique_subhash""

  attribute = [
    {
      name = ""prefix""
      type = ""S""
    },
    {
      name = ""unique_subhash""
      type = ""S""
    }
  ]

  point_in_time_recovery {
    enabled = true
  }

  tags {
    key = ""Site""
    value = ""CompilerExplorer""
  }
}
",resource,9,,6701d62eda6ae70b38f4faa908ae23ce2662103f,79bb0fbfeb42154db0aa14f7ecb1d69b60197545,https://github.com/compiler-explorer/infra/blob/6701d62eda6ae70b38f4faa908ae23ce2662103f/terraform/dynamodb.tf#L9,https://github.com/compiler-explorer/infra/blob/79bb0fbfeb42154db0aa14f7ecb1d69b60197545/terraform/dynamodb.tf,2018-12-01 08:04:08-06:00,2019-03-04 09:56:50-06:00,2,1,1,1,1,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,199,module-iam.tf,module-iam.tf,0,implementation,# Default IAM sub-module implementation for OKE cluster,# Default IAM sub-module implementation for OKE cluster,"module ""iam"" {
  source                   = ""./modules/iam""
  compartment_id           = local.compartment_id
  state_id                 = random_id.state_id.id
  tenancy_id               = local.tenancy_id
  cluster_id               = local.cluster_id
  create_autoscaler_policy = local.create_autoscaler_policy
  create_kms_policy        = local.create_kms_policy
  create_operator_policy   = local.create_operator_policy
  create_worker_policy     = local.create_worker_policy

  create_tag_namespace = var.create_tag_namespace
  create_defined_tags  = var.create_defined_tags
  defined_tags         = lookup(var.defined_tags, ""policy"", {})
  freeform_tags        = lookup(var.freeform_tags, ""policy"", {})
  tag_namespace        = var.tag_namespace
  use_defined_tags     = var.use_defined_tags

  cluster_kms_key_id         = var.cluster_kms_key_id
  operator_volume_kms_key_id = var.operator_volume_kms_key_id
  worker_volume_kms_key_id   = var.worker_volume_kms_key_id

  autoscaler_compartments = local.autoscaler_compartments
  worker_compartments     = local.worker_compartments

  providers = {
    oci.home = oci.home
  }
}
",module,"module ""iam"" {
  source                       = ""./modules/iam""
  compartment_id               = local.compartment_id
  state_id                     = local.state_id
  tenancy_id                   = local.tenancy_id
  cluster_id                   = local.cluster_id
  create_iam_resources         = var.create_iam_resources
  create_iam_autoscaler_policy = local.create_iam_autoscaler_policy
  create_iam_kms_policy        = local.create_iam_kms_policy
  create_iam_operator_policy   = local.create_iam_operator_policy
  create_iam_worker_policy     = local.create_iam_worker_policy

  create_iam_tag_namespace = var.create_iam_tag_namespace
  create_iam_defined_tags  = var.create_iam_defined_tags
  defined_tags             = local.iam_defined_tags
  freeform_tags            = local.iam_freeform_tags
  tag_namespace            = var.tag_namespace
  use_defined_tags         = var.use_defined_tags

  cluster_kms_key_id         = var.cluster_kms_key_id
  operator_volume_kms_key_id = var.operator_volume_kms_key_id
  worker_volume_kms_key_id   = var.worker_volume_kms_key_id

  autoscaler_compartments = local.autoscaler_compartments
  worker_compartments     = local.worker_compartments

  providers = {
    oci.home = oci.home
  }
}
",module,35,53.0,6c867cd8e9cbf559742f56658989bcded0d1fd89,e2ac866a96bd7171c980727c46078cc438643225,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/module-iam.tf#L35,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/e2ac866a96bd7171c980727c46078cc438643225/module-iam.tf#L53,2023-10-25 16:40:02+11:00,2024-03-28 20:16:45+11:00,9,0,0,0,0,1,0,0,0,0
https://github.com/rust-lang/simpleinfra,20,terragrunt/modules/docs-rs/web-server.tf,terragrunt/modules/docs-rs/web-server.tf,0,# todo,# TODO: ensure that this is a secret in the SSM store,"# TODO: ensure that this is a secret in the SSM store 
 # DOCSRS_GITHUB_ACCESSTOKEN = ""/prod/docs-rs/github-access-token""","module ""web"" {
  source         = ""../ecs-app""
  cluster_config = var.cluster_config

  env  = ""dev""
  name = ""docs-rs-web""
  repo = ""rust-lang/docs.rs""

  cpu                  = 256
  memory               = 512
  tasks_count          = 1
  platform_version     = ""1.4.0""
  ephemeral_storage_gb = 40

  environment = {
    DOCSRS_PREFIX               = ""/tmp""
    DOCSRS_STORAGE_BACKEND      = ""s3""
    DOCSRS_LOG                  = ""docs_rs=debug,rustwide=info""
    RUST_BACKTRACE              = ""1""
    DOCSRS_STATIC_CLOUDFRONT_ID = ""${aws_cloudfront_distribution.static.id}""
  }

  secrets = {
    # TODO: ensure that this is a secret in the SSM store
    # DOCSRS_GITHUB_ACCESSTOKEN = ""/prod/docs-rs/github-access-token""
  }

  computed_secrets = {
    DOCSRS_DATABASE_URL = aws_ssm_parameter.connection_url.arn
  }

  expose_http = {
    container_port = 80
    prometheus     = ""/about/metrics""
    domains        = [local.web_domain]
    zone_id        = var.zone_id

    health_check_path     = ""/""
    health_check_interval = 5
    health_check_timeout  = 2
  }

  # Allow database access
  additional_security_group_ids = [aws_security_group.web.id]
}
",module,"module ""web"" {
  source         = ""../ecs-app""
  cluster_config = var.cluster_config

  env  = ""dev""
  name = ""docs-rs-web""
  repo = ""rust-lang/docs.rs""

  cpu                  = 256
  memory               = 512
  tasks_count          = 1
  platform_version     = ""1.4.0""
  ephemeral_storage_gb = 40

  environment = {
    DOCSRS_PREFIX               = ""/tmp""
    DOCSRS_STORAGE_BACKEND      = ""s3""
    DOCSRS_LOG                  = ""docs_rs=debug,rustwide=info""
    RUST_BACKTRACE              = ""1""
    DOCSRS_STATIC_CLOUDFRONT_ID = ""${aws_cloudfront_distribution.static.id}""
  }

  secrets = {
    DOCSRS_GITHUB_ACCESSTOKEN = ""/prod/docs-rs/github-access-token""
  }

  computed_secrets = {
    DOCSRS_DATABASE_URL = aws_ssm_parameter.connection_url.arn
  }

  expose_http = {
    container_port = 80
    prometheus     = ""/about/metrics""
    domains        = [local.web_domain]
    zone_id        = var.zone_id

    health_check_path     = ""/""
    health_check_interval = 5
    health_check_timeout  = 2
  }

  # Allow database access
  additional_security_group_ids = [aws_security_group.web.id]
}
",module,33,,8d588e18da3b236eaa53a9920a5af7219450cf75,cbd55946ef1f6ed122e73fa26ef14ec814f100c0,https://github.com/rust-lang/simpleinfra/blob/8d588e18da3b236eaa53a9920a5af7219450cf75/terragrunt/modules/docs-rs/web-server.tf#L33,https://github.com/rust-lang/simpleinfra/blob/cbd55946ef1f6ed122e73fa26ef14ec814f100c0/terragrunt/modules/docs-rs/web-server.tf,2023-01-10 20:16:01+01:00,2023-01-11 16:41:27+01:00,2,1,0,1,0,1,0,0,0,1
https://github.com/alphagov/govuk-aws,926,terraform/projects/infra-public-services/waf.tf,terraform/projects/infra-public-services/waf.tf,0,fix,# FIXME: Change this to BLOCK after 25th July 2019,"type = ""ALLOW"" # FIXME: Change this to BLOCK after 25th July 2019","resource ""aws_wafregional_web_acl"" ""default"" {
  name        = ""CachePublicWebACL""
  metric_name = ""CachePublicWebACL""

  default_action {
    type = ""ALLOW""
  }

  rule {
    action {
      type = ""BLOCK""
    }

    priority = 2
    rule_id  = ""${aws_wafregional_rule.x_always_block.id}""
  }

  rule {
    action {
      type = ""ALLOW"" # FIXME: Change this to BLOCK after 25th July 2019
    }

    priority = 3
    rule_id  = ""${aws_wafregional_rule.sqli.id}""
  }

  logging_configuration {
    log_destination = ""${aws_kinesis_firehose_delivery_stream.splunk.arn}""

    redacted_fields {
      field_to_match {
        type = ""URI""
      }

      field_to_match {
        data = ""referer""
        type = ""HEADER""
      }
    }
  }

  depends_on = [
    ""aws_wafregional_rule.x_always_block"",
    ""aws_wafregional_rule.sqli"",
  ]
}
",resource,"resource ""aws_wafregional_web_acl"" ""default"" {
  name        = ""CachePublicWebACL""
  metric_name = ""CachePublicWebACL""

  default_action {
    type = ""ALLOW""
  }

  rule {
    action {
      type = ""BLOCK""
    }

    priority = 2
    rule_id  = ""${aws_wafregional_rule.x_always_block.id}""
  }

  logging_configuration {
    log_destination = ""${aws_kinesis_firehose_delivery_stream.splunk.arn}""
  }

  depends_on = [
    ""aws_wafregional_rule.x_always_block"",
  ]
}
",resource,20,,8c2729fb92978bd4a778234d756943d31428c1e3,c1ecdd9a87ba22fcb5c0585636e4ebbe527f0137,https://github.com/alphagov/govuk-aws/blob/8c2729fb92978bd4a778234d756943d31428c1e3/terraform/projects/infra-public-services/waf.tf#L20,https://github.com/alphagov/govuk-aws/blob/c1ecdd9a87ba22fcb5c0585636e4ebbe527f0137/terraform/projects/infra-public-services/waf.tf,2019-07-22 16:00:21+01:00,2020-12-02 18:07:36+00:00,7,1,0,1,0,1,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,24,modules/extensions/locals.tf,modules/extensions/locals.tf,0,workaround,# workaround for summing a list of numbers: https://github.com/hashicorp/terraform/issues/17239,# workaround for summing a list of numbers: https://github.com/hashicorp/terraform/issues/17239,"locals {

  node_pools_size_list = [
    for node_pool in data.oci_containerengine_node_pools.all_node_pools.node_pools :
    node_pool.node_config_details[0].size
  ]

  # workaround for summing a list of numbers: https://github.com/hashicorp/terraform/issues/17239
  total_nodes = length(flatten([
    for nodes in local.node_pools_size_list : range(nodes)
  ]))

  service_account_cluster_role_binding_name = var.service_account_cluster_role_binding == """" ? ""${var.service_account_name}-crb"" : var.service_account_cluster_role_binding

  # 1. get a list of available images for this cluster
  # 2. filter by version
  # 3. if more than 1 image found for this version, pick the latest
  node_pool_image_ids = data.oci_containerengine_node_pool_option.node_pool_options.sources

  # determine if post provisioning operations are possible
  # requires:
  ## 1. bastion to be enabled and in a running state
  ## 2. operation to be enabled and instance_principal to be enabled

  post_provisioning_ops = var.create_bastion_host == true && var.bastion_state == ""RUNNING"" && var.create_operator == true && var.operator_instance_principal == true ? true : false

  dynamic_group_rule_this_cluster = (var.use_encryption == true) ? ""ALL {resource.type = 'cluster', resource.id = '${var.cluster_id}'}"" : ""null""

  # scripting templates
  update_dynamic_group_template = templatefile(""${path.module}/scripts/update_dynamic_group.template.sh"",
    {
      dynamic_group_id   = var.use_encryption == true ? var.kms_dynamic_group_id : ""null""
      dynamic_group_rule = local.dynamic_group_rule_this_cluster
      home_region        = data.oci_identity_regions.home_region.regions[0].name
    }
  )
  
  check_active_worker_template = templatefile(""${path.module}/scripts/check_worker_active.template.sh"",
    {
      check_node_active = var.check_node_active
      total_nodes       = local.total_nodes
    }
  )

  install_calico_template = templatefile(""${path.module}/scripts/install_calico.template.sh"",
    {
      calico_version     = var.calico_version
      number_of_nodes    = local.total_nodes
      pod_cidr           = var.pods_cidr
      number_of_replicas = min(20, max((local.total_nodes) / 200, 3))
    }
  )

  drain_template = templatefile(""${path.module}/scripts/drain.template.sh"", {})

  drain_list_template = templatefile(""${path.module}/scripts/drainlist.py"",
    {
      cluster_id     = var.cluster_id
      compartment_id = var.compartment_id
      region         = var.region
      pools_to_drain = var.label_prefix == ""none"" ? trim(join("","", formatlist(""'%s'"", var.node_pools_to_drain)), ""'"") : trim(join("","", formatlist(""'%s-%s'"", var.label_prefix, var.node_pools_to_drain)), ""'"")
    }
  )

  install_kubectl_template = templatefile(""${path.module}/scripts/install_kubectl.template.sh"",
    {
      ol = var.operator_os_version
    }
  )

  install_helm_template = templatefile(""${path.module}/scripts/install_helm.template.sh"", {})

  metric_server_template = templatefile(""${path.module}/scripts/install_metricserver.template.sh"",
    {
      enable_vpa  = var.enable_vpa
      vpa_version = var.vpa_version
    }
  )

  secret_template = templatefile(""${path.module}/scripts/secret.py"",
    {
      compartment_id = var.compartment_id
      region         = var.region

      email_address     = var.email_address
      region_registry   = var.ocir_urls[var.region]
      secret_id         = var.secret_id
      secret_name       = var.secret_name
      secret_namespace  = var.secret_namespace
      tenancy_namespace = data.oci_objectstorage_namespace.object_storage_namespace.namespace
      username          = var.username
    }
  )

  create_service_account_template = templatefile(""${path.module}/scripts/create_service_account.template.sh"",
    {
      service_account_name                 = var.service_account_name
      service_account_namespace            = var.service_account_namespace
      service_account_cluster_role_binding = local.service_account_cluster_role_binding_name
    }
  )
}
",locals,"locals {
  ssh_private_key = (
    var.ssh_private_key != """"
    ? try(base64decode(var.ssh_private_key), var.ssh_private_key)
    : var.ssh_private_key_path != ""none""
    ? file(var.ssh_private_key_path)
  : null)

  service_account_cluster_role_binding_name = var.service_account_cluster_role_binding == """" ? ""${var.service_account_name}-crb"" : var.service_account_cluster_role_binding

  # 1. get a list of available images for this cluster
  # 2. filter by version
  # 3. if more than 1 image found for this version, pick the latest
  node_pool_image_ids = data.oci_containerengine_node_pool_option.node_pool_options.sources

  # determine if post provisioning operations are possible
  # requires:
  ## 1. bastion to be enabled and in a running state
  ## 2. operation to be enabled and instance_principal to be enabled

  post_provisioning_ops = var.create_bastion_host == true && var.bastion_state == ""RUNNING"" && var.create_operator == true && var.operator_state == ""RUNNING"" && var.enable_operator_instance_principal == true ? true : false

  dynamic_group_rule_this_cluster = (var.use_cluster_encryption == true) ? ""ALL {resource.type = 'cluster', resource.id = '${var.cluster_id}'}"" : ""null""

  dynamic_group_prefix = (var.label_prefix == ""none"") ? """" : ""${var.label_prefix}""
}
",locals,11,,1c53f2ae31789274bfe4ded6cf312f265e011307,4d2b3f3d672a8f41655da3a7c58fded42c6858f3,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/1c53f2ae31789274bfe4ded6cf312f265e011307/modules/extensions/locals.tf#L11,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/4d2b3f3d672a8f41655da3a7c58fded42c6858f3/modules/extensions/locals.tf,2021-10-26 10:35:29+11:00,2023-10-25 16:40:02+11:00,10,1,0,1,1,0,0,0,0,0
https://github.com/alphagov/govuk-aws,1463,terraform/projects/infra-database-backups-bucket/reader.tf,terraform/projects/infra-database-backups-bucket/reader.tf,0,# todo,# TODO: Remove once Content Data API is using the content_data_api_production database.,# TODO: Remove once Content Data API is using the content_data_api_production database.,"data ""aws_iam_policy_document"" ""production_content_data_api_dbadmin_database_backups_reader"" {
  statement {
    sid       = ""ContentDataAPIDBAdminListBucket""
    actions   = [""s3:ListBucket""]
    resources = [""arn:aws:s3:::govuk-production-database-backups""]
  }

  statement {
    sid     = ""ContentDataAPIDBAdminGetObject""
    actions = [""s3:GetObject""]
    resources = [
      ""arn:aws:s3:::govuk-production-database-backups/content-data-api-postgresql/*-content_data_api_production.gz"",
      # TODO: Remove once Content Data API is using the content_data_api_production database.
      ""arn:aws:s3:::govuk-production-database-backups/content-data-api-postgresql/*-content_performance_manager_production.gz"",
    ]
  }
}
",data,the block associated got renamed or deleted,,325,,04b817e5e25eb8691b316964e87f01ab15d339aa,20bc67be4a7d62f92027f56128eeabe46679e8a2,https://github.com/alphagov/govuk-aws/blob/04b817e5e25eb8691b316964e87f01ab15d339aa/terraform/projects/infra-database-backups-bucket/reader.tf#L325,https://github.com/alphagov/govuk-aws/blob/20bc67be4a7d62f92027f56128eeabe46679e8a2/terraform/projects/infra-database-backups-bucket/reader.tf,2023-10-31 20:03:46+00:00,2023-11-22 15:00:38+00:00,2,1,1,1,0,0,0,1,0,0
https://github.com/Worklytics/psoxy,1706,infra/modules/gcp/main.tf,infra/modules/gcp/main.tf,0,# todo,"# TODO: name ends in .jar, but it's actually a zipped JAR","# TODO: name ends in .jar, but it's actually a zipped JAR 
 # (gcp doesn't like straight JAR)","resource ""google_storage_bucket_object"" ""function"" {
  # TODO: name ends in .jar, but it's actually a zipped JAR
  # (gcp doesn't like straight JAR)
  name           = ""${var.environment_id_prefix}${local.file_name_with_sha1}""
  content_type   = ""application/zip""
  bucket         = google_storage_bucket.artifacts.name
  # source         = module.psoxy_package.path_to_deployment_jar
  source         = data.archive_file.source.output_path
  detect_md5hash = true
}
",resource,"resource ""google_storage_bucket_object"" ""function"" {
  name           = ""${var.environment_id_prefix}${local.file_name_with_sha1}""
  content_type   = ""application/zip""
  bucket         = google_storage_bucket.artifacts.name
  source         = coalesce(var.deployment_bundle, data.archive_file.source[0].output_path)
  detect_md5hash = true
}
",resource,173,,658db10204a6a35dc5a3583d24a86c7c8f10c0af,b9eeb4eb92b5bf9e1fe52487f2ac918fe04e0808,https://github.com/Worklytics/psoxy/blob/658db10204a6a35dc5a3583d24a86c7c8f10c0af/infra/modules/gcp/main.tf#L173,https://github.com/Worklytics/psoxy/blob/b9eeb4eb92b5bf9e1fe52487f2ac918fe04e0808/infra/modules/gcp/main.tf,2023-06-21 15:21:51-07:00,2023-06-21 15:44:40-07:00,2,1,1,1,0,0,0,0,0,0
https://github.com/uyuni-project/sumaform,268,modules/libvirt/mirror/variables.tf,modules/libvirt/mirror/variables.tf,0,hack,"# HACK: """" cannot be used as a default because of https://github\.com/hashicorp/hil/issues/50","# HACK: """" cannot be used as a default because of https://github\.com/hashicorp/hil/issues/50","variable ""ssh_key_path"" {
  description = ""path of additional pub ssh key you want to use to access VMs, see libvirt/README.md""
  default = ""/dev/null""
  # HACK: """" cannot be used as a default because of https://github\.com/hashicorp/hil/issues/50
}
",variable,"variable ""ssh_key_path"" {
  description = ""path of additional pub ssh key you want to use to access VMs, see libvirt/README.md""
  default = ""/dev/null""
  # HACK: """" cannot be used as a default because of https://github.com/hashicorp/hil/issues/50
}
",variable,35,,3b7407fd3d6f1265ceb845c4fd21a9f0f0a76beb,141dd30bb2100f8edfd74b0ad149662dcdeb03ea,https://github.com/uyuni-project/sumaform/blob/3b7407fd3d6f1265ceb845c4fd21a9f0f0a76beb/modules/libvirt/mirror/variables.tf#L35,https://github.com/uyuni-project/sumaform/blob/141dd30bb2100f8edfd74b0ad149662dcdeb03ea/modules/libvirt/mirror/variables.tf,2017-08-04 15:28:57+02:00,2017-08-22 16:07:20+02:00,2,1,0,1,1,1,0,0,0,0
https://github.com/wireapp/wire-server-deploy,2,terraform/modules/aws_vpc_security_groups/main.tf,terraform/modules/aws-vpc-security-groups/main.tf,1,fix,# FIXME: tighten this up.,# FIXME: tighten this up.,"resource ""aws_security_group"" ""k8s_node"" {
  name        = ""k8s_node""
  description = ""hosts that have kubernetes.""
  vpc_id      = var.vpc_id

  # incoming from the admin node (kubectl)
  ingress {
    from_port   = 6443
    to_port     = 6443
    protocol    = ""tcp""
    security_groups = [""${aws_security_group.talk_to_k8s.id}""]
  }

  # FIXME: tighten this up.
  ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = ""tcp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # FIXME: tighten this up. need UDP for flannel.
  ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = ""udp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  tags = {
    Name = ""k8s_node""
  }
}
",resource,"resource ""aws_security_group"" ""k8s_node"" {
  name        = ""k8s_node""
  description = ""hosts that have kubernetes.""
  vpc_id      = var.vpc_id

  # incoming from the admin node (kubectl)
  ingress {
    description     = """"
    from_port       = 6443
    to_port         = 6443
    protocol        = ""tcp""
    security_groups = [""${aws_security_group.talk_to_k8s.id}""]
  }

  # FIXME: tighten this up.
  ingress {
    description     = """"
    from_port       = 0
    to_port         = 65535
    protocol        = ""tcp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # FIXME: tighten this up. need UDP for flannel.
  ingress {
    description     = """"
    from_port       = 0
    to_port         = 65535
    protocol        = ""udp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # incoming traffic to the application.
  ingress {
    from_port   = 31772
    to_port     = 31773
    protocol    = ""tcp""
    # NOTE: NLBs dont allow security groups to be set on them, which is why
    # we go with the CIDR for now, which is hard-coded and needs fixing
    cidr_blocks = [""172.17.0.0/20""]
  }

  tags = {
    Name = ""k8s_node""
  }
}
",resource,213,225.0,cb61b733457d5a1ccdb5c9dcb5fde4cab9e5d7e0,e7f081ca744de60bf9d10cb037828b995883d2aa,https://github.com/wireapp/wire-server-deploy/blob/cb61b733457d5a1ccdb5c9dcb5fde4cab9e5d7e0/terraform/modules/aws_vpc_security_groups/main.tf#L213,https://github.com/wireapp/wire-server-deploy/blob/e7f081ca744de60bf9d10cb037828b995883d2aa/terraform/modules/aws-vpc-security-groups/main.tf#L225,2020-04-23 17:54:17+01:00,2020-08-26 16:29:39+02:00,5,0,0,0,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,650,examples/data-solutions/data-platform-foundations/02-load.tf,examples/data-solutions/data-platform-foundations/02-load.tf,0,# todo,# TODO: these are needed on the shared VPC?,"# TODO: these are needed on the shared VPC? 
 # ""roles/compute.serviceAgent"" = [ 
 #   ""serviceAccount:${module.load-project.service_accounts.robots.compute}"" 
 # ] 
 # ""roles/dataflow.serviceAgent"" = [ 
 #   ""serviceAccount:${module.load-project.service_accounts.robots.dataflow}"" 
 # ]","module ""load-project"" {
  source          = ""../../../modules/project""
  parent          = var.folder_id
  billing_account = var.billing_account_id
  prefix          = var.prefix
  name            = ""lod""
  group_iam = {
    (local.groups.data-engineers) = [
      ""roles/compute.viewer"",
      ""roles/dataflow.admin"",
      ""roles/dataflow.developer"",
      ""roles/viewer"",
    ]
  }
  iam = {
    ""roles/bigquery.jobUser"" = [module.load-sa-df-0.iam_email]
    ""roles/dataflow.admin"" = [
      module.orch-sa-cmp-0.iam_email, module.load-sa-df-0.iam_email
    ]
    ""roles/dataflow.worker""     = [module.load-sa-df-0.iam_email]
    ""roles/storage.objectAdmin"" = local.load_service_accounts
    # TODO: these are needed on the shared VPC?
    # ""roles/compute.serviceAgent"" = [
    #   ""serviceAccount:${module.load-project.service_accounts.robots.compute}""
    # ]
    # ""roles/dataflow.serviceAgent"" = [
    #   ""serviceAccount:${module.load-project.service_accounts.robots.dataflow}""
    # ]
  }
  services = concat(var.project_services, [
    ""bigquery.googleapis.com"",
    ""bigqueryreservation.googleapis.com"",
    ""bigquerystorage.googleapis.com"",
    ""cloudkms.googleapis.com"",
    ""compute.googleapis.com"",
    ""dataflow.googleapis.com"",
    ""dlp.googleapis.com"",
    ""pubsub.googleapis.com"",
    ""servicenetworking.googleapis.com"",
    ""storage.googleapis.com"",
    ""storage-component.googleapis.com""
  ])
  service_encryption_key_ids = {
    pubsub   = [try(local.service_encryption_keys.pubsub, null)]
    dataflow = [try(local.service_encryption_keys.dataflow, null)]
    storage  = [try(local.service_encryption_keys.storage, null)]
  }
  shared_vpc_service_config = local.shared_vpc_project == null ? null : {
    attach       = true
    host_project = local.shared_vpc_project
    service_identity_iam = {
      # TODO: worker service account
      ""compute.networkUser"" = [""dataflow""]
    }
  }
}
",module,"module ""load-project"" {
  source          = ""../../../modules/project""
  parent          = var.folder_id
  billing_account = var.billing_account_id
  prefix          = var.prefix
  name            = ""lod""
  group_iam = {
    (local.groups.data-engineers) = [
      ""roles/compute.viewer"",
      ""roles/dataflow.admin"",
      ""roles/dataflow.developer"",
      ""roles/viewer"",
    ]
  }
  iam = {
    ""roles/bigquery.jobUser"" = [module.load-sa-df-0.iam_email]
    ""roles/dataflow.admin"" = [
      module.orch-sa-cmp-0.iam_email, module.load-sa-df-0.iam_email
    ]
    ""roles/dataflow.worker""     = [module.load-sa-df-0.iam_email]
    ""roles/storage.objectAdmin"" = local.load_service_accounts
  }
  services = concat(var.project_services, [
    ""bigquery.googleapis.com"",
    ""bigqueryreservation.googleapis.com"",
    ""bigquerystorage.googleapis.com"",
    ""cloudkms.googleapis.com"",
    ""compute.googleapis.com"",
    ""dataflow.googleapis.com"",
    ""dlp.googleapis.com"",
    ""pubsub.googleapis.com"",
    ""servicenetworking.googleapis.com"",
    ""storage.googleapis.com"",
    ""storage-component.googleapis.com""
  ])
  service_encryption_key_ids = {
    pubsub   = [try(local.service_encryption_keys.pubsub, null)]
    dataflow = [try(local.service_encryption_keys.dataflow, null)]
    storage  = [try(local.service_encryption_keys.storage, null)]
  }
  shared_vpc_service_config = local.shared_vpc_project == null ? null : {
    attach               = true
    host_project         = local.shared_vpc_project
    service_identity_iam = {}
  }
}
",module,57,,4f4a9cd7ac2cee3a1b2e413cd19f6b6b07c35622,12383ae72df0082cb52ef70803aa2d293a66d674,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/4f4a9cd7ac2cee3a1b2e413cd19f6b6b07c35622/examples/data-solutions/data-platform-foundations/02-load.tf#L57,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/12383ae72df0082cb52ef70803aa2d293a66d674/examples/data-solutions/data-platform-foundations/02-load.tf,2022-02-09 17:01:25+01:00,2022-02-12 09:48:16+01:00,7,1,0,1,0,1,1,0,0,1
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,153,modules/organization/main.tf,modules/organization/main.tf,0,# todo,# TODO: add logging buckets support,"# TODO: add logging buckets support 
 # logging  = ""logging.googleapis.com""","locals {
  organization_id_numeric = split(""/"", var.organization_id)[1]
  iam_additive_pairs = flatten([
    for role, members in var.iam_additive : [
      for member in members : { role = role, member = member }
    ]
  ])
  iam_additive_member_pairs = flatten([
    for member, roles in var.iam_additive_members : [
      for role in roles : { role = role, member = member }
    ]
  ])
  iam_additive = {
    for pair in concat(local.iam_additive_pairs, local.iam_additive_member_pairs) :
    ""${pair.role}-${pair.member}"" => pair
  }
  extended_rules = flatten([
    for policy, rules in var.firewall_policies : [
      for rule_name, rule in rules :
      merge(rule, { policy = policy, name = rule_name })
    ]
  ])
  rules_map = {
    for rule in local.extended_rules :
    ""${rule.policy}-${rule.name}"" => rule
  }
  logging_sinks = coalesce(var.logging_sinks, {})
  sink_type_destination = {
    gcs      = ""storage.googleapis.com""
    bigquery = ""bigquery.googleapis.com""
    pubsub   = ""pubsub.googleapis.com""
    # TODO: add logging buckets support
    # logging  = ""logging.googleapis.com""
  }
  sink_bindings = {
    for type in [""gcs"", ""bigquery"", ""pubsub"", ""logging""] :
    type => {
      for name, sink in local.logging_sinks :
      name => sink
      if sink.grant && sink.type == type
    }
  }
}
",locals,"locals {
  organization_id_numeric = split(""/"", var.organization_id)[1]
  iam_additive_pairs = flatten([
    for role, members in var.iam_additive : [
      for member in members : { role = role, member = member }
    ]
  ])
  iam_additive_member_pairs = flatten([
    for member, roles in var.iam_additive_members : [
      for role in roles : { role = role, member = member }
    ]
  ])
  iam_additive = {
    for pair in concat(local.iam_additive_pairs, local.iam_additive_member_pairs) :
    ""${pair.role}-${pair.member}"" => pair
  }
  extended_rules = flatten([
    for policy, rules in var.firewall_policies : [
      for rule_name, rule in rules :
      merge(rule, { policy = policy, name = rule_name })
    ]
  ])
  rules_map = {
    for rule in local.extended_rules :
    ""${rule.policy}-${rule.name}"" => rule
  }
  logging_sinks = coalesce(var.logging_sinks, {})
  sink_type_destination = {
    gcs      = ""storage.googleapis.com""
    bigquery = ""bigquery.googleapis.com""
    pubsub   = ""pubsub.googleapis.com""
    logging  = ""logging.googleapis.com""
  }
  sink_bindings = {
    for type in [""gcs"", ""bigquery"", ""pubsub"", ""logging""] :
    type => {
      for name, sink in local.logging_sinks :
      name => sink
      if sink.iam && sink.type == type
    }
  }
}
",locals,48,,2c0f949f07d0563b621cb433c36cf13e117ea561,ad68fc4dfa576624a7e2caa1b96499161d8b0937,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2c0f949f07d0563b621cb433c36cf13e117ea561/modules/organization/main.tf#L48,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/ad68fc4dfa576624a7e2caa1b96499161d8b0937/modules/organization/main.tf,2020-12-05 08:31:35+01:00,2021-03-03 14:23:59+01:00,7,1,1,1,0,0,0,0,1,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1307,blueprints/data-solutions/shielded-folder/variables.tf,blueprints/data-solutions/shielded-folder/variables.tf,0,#todo,"#TODO data-analysts  = ""gcp-data-analysts""","#TODO data-analysts  = ""gcp-data-analysts""","variable ""groups"" {
  description = ""User groups.""
  type        = map(string)
  default = {
    #TODO data-analysts  = ""gcp-data-analysts""
    data-engineers = ""gcp-data-engineers""
    #TODO data-security  = ""gcp-data-security""
  }
}
",variable,"variable ""groups"" {
  description = ""User groups.""
  type        = map(string)
  default = {
    workload-engineers = ""gcp-data-engineers""
    workload-security  = ""gcp-data-security""
  }
}
",variable,57,,84be665172b21220938ee702c4654e1a0cd0a584,840fc86b3e7a0b9f0734d7cb41abca813b84ab40,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/84be665172b21220938ee702c4654e1a0cd0a584/blueprints/data-solutions/shielded-folder/variables.tf#L57,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/840fc86b3e7a0b9f0734d7cb41abca813b84ab40/blueprints/data-solutions/shielded-folder/variables.tf,2023-01-17 08:49:04+01:00,2023-02-01 08:55:33+01:00,10,1,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,21,modules/gke-cluster/main.tf,modules/gke-cluster/main.tf,0,# todo,# TODO(ludomagno): support setting address ranges instead of range names,"# TODO(ludomagno): support setting address ranges instead of range names 
 # https://www.terraform.io/docs/providers/google/r/container_cluster.html#cluster_ipv4_cidr_block","resource ""google_container_cluster"" ""cluster"" {
  provider                    = google-beta
  project                     = var.project_id
  name                        = var.name
  description                 = var.description
  location                    = var.location
  node_locations              = length(var.node_locations) == 0 ? null : var.node_locations
  min_master_version          = var.min_master_version
  network                     = var.network
  subnetwork                  = var.subnetwork
  logging_service             = var.logging_service
  monitoring_service          = var.monitoring_service
  resource_labels             = var.labels
  default_max_pods_per_node   = var.default_max_pods_per_node
  enable_binary_authorization = var.enable_binary_authorization
  enable_intranode_visibility = var.enable_intranode_visibility
  enable_shielded_nodes       = var.enable_shielded_nodes
  enable_tpu                  = var.enable_tpu
  initial_node_count          = 1
  remove_default_node_pool    = true

  # node_config

  addons_config {
    http_load_balancing {
      disabled = ! var.addons.http_load_balancing
    }
    horizontal_pod_autoscaling {
      disabled = ! var.addons.horizontal_pod_autoscaling
    }
    network_policy_config {
      disabled = ! var.addons.network_policy_config
    }
    # beta addons
    # cloudrun is dynamic as it tends to trigger cluster recreation on change
    dynamic cloudrun_config {
      for_each = var.addons.istio_config.enabled && var.addons.cloudrun_config ? [""""] : []
      content {
        disabled = false
      }
    }
    istio_config {
      disabled = ! var.addons.istio_config.enabled
      auth     = var.addons.istio_config.tls ? ""AUTH_MUTUAL_TLS"" : ""AUTH_NONE""
    }
  }

  # TODO(ludomagno): support setting address ranges instead of range names
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#cluster_ipv4_cidr_block
  ip_allocation_policy {
    cluster_secondary_range_name  = var.secondary_range_pods
    services_secondary_range_name = var.secondary_range_services
  }

  # TODO(ludomagno): make optional, and support beta feature
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#daily_maintenance_window
  maintenance_policy {
    daily_maintenance_window {
      start_time = var.maintenance_start_time
    }
  }

  master_auth {
    client_certificate_config {
      issue_client_certificate = false
    }
  }

  dynamic master_authorized_networks_config {
    for_each = length(var.master_authorized_ranges) == 0 ? [] : list(var.master_authorized_ranges)
    iterator = ranges
    content {
      dynamic cidr_blocks {
        for_each = ranges.value
        iterator = range
        content {
          cidr_block   = range.value
          display_name = range.key
        }
      }
    }
  }

  dynamic network_policy {
    for_each = var.addons.network_policy_config ? [""""] : []
    content {
      enabled  = true
      provider = ""CALICO""
    }
  }

  dynamic private_cluster_config {
    for_each = var.private_cluster_config != null ? [var.private_cluster_config] : []
    iterator = config
    content {
      enable_private_nodes    = config.value.enable_private_nodes
      enable_private_endpoint = config.value.enable_private_endpoint
      master_ipv4_cidr_block  = config.value.master_ipv4_cidr_block
    }
  }

  # beta features

  dynamic authenticator_groups_config {
    for_each = var.authenticator_security_group == null ? [] : [""""]
    content {
      security_group = var.authenticator_security_group
    }
  }

  dynamic cluster_autoscaling {
    for_each = var.cluster_autoscaling.enabled ? [var.cluster_autoscaling] : []
    iterator = config
    content {
      enabled = true
      resource_limits {
        resource_type = ""cpu""
        minimum       = config.cpu_min
        maximum       = config.cpu_max
      }
      resource_limits {
        resource_type = ""memory""
        minimum       = config.memory_min
        maximum       = config.memory_max
      }
    }
  }

  dynamic database_encryption {
    for_each = var.database_encryption.enabled ? [var.database_encryption] : []
    iterator = config
    content {
      state    = config.value.state
      key_name = config.value.key_name
    }
  }

  dynamic pod_security_policy_config {
    for_each = var.pod_security_policy != null ? [""""] : []
    content {
      enabled = var.pod_security_policy
    }
  }

  dynamic release_channel {
    for_each = var.release_channel != null ? [""""] : []
    content {
      channel = var.release_channel
    }
  }

  dynamic resource_usage_export_config {
    for_each = (
      var.resource_usage_export_config.enabled != null
      &&
      var.resource_usage_export_config.dataset != null
      ? [""""] : []
    )
    content {
      enable_network_egress_metering = var.resource_usage_export_config.enabled
      bigquery_destination {
        dataset_id = var.resource_usage_export_config.dataset
      }
    }
  }

  dynamic vertical_pod_autoscaling {
    for_each = var.vertical_pod_autoscaling == null ? [] : [""""]
    content {
      enabled = var.vertical_pod_autoscaling
    }
  }

  dynamic workload_identity_config {
    for_each = var.workload_identity ? [""""] : []
    content {
      identity_namespace = ""${var.project_id}.svc.id.goog""
    }
  }

}
",resource,"resource ""google_container_cluster"" ""cluster"" {
  provider    = google-beta
  project     = var.project_id
  name        = var.name
  description = var.description
  location    = var.location
  node_locations = (
    length(var.node_locations) == 0 ? null : var.node_locations
  )
  min_master_version = var.min_master_version
  network            = var.vpc_config.network
  subnetwork         = var.vpc_config.subnetwork
  resource_labels    = var.labels
  default_max_pods_per_node = (
    var.enable_features.autopilot ? null : var.max_pods_per_node
  )
  enable_intranode_visibility = (
    var.enable_features.autopilot ? null : var.enable_features.intranode_visibility
  )
  enable_l4_ilb_subsetting = var.enable_features.l4_ilb_subsetting
  enable_shielded_nodes = (
    var.enable_features.autopilot ? null : var.enable_features.shielded_nodes
  )
  enable_tpu               = var.enable_features.tpu
  initial_node_count       = 1
  remove_default_node_pool = var.enable_features.autopilot ? null : true
  datapath_provider = (
    var.enable_features.dataplane_v2
    ? ""ADVANCED_DATAPATH""
    : ""DATAPATH_PROVIDER_UNSPECIFIED""
  )
  enable_autopilot = var.enable_features.autopilot ? true : null

  # the default nodepool is deleted here, use the gke-nodepool module instead
  # node_config {}

  addons_config {
    dynamic ""dns_cache_config"" {
      for_each = !var.enable_features.autopilot ? [""""] : []
      content {
        enabled = var.enable_addons.dns_cache
      }
    }
    http_load_balancing {
      disabled = !var.enable_addons.http_load_balancing
    }
    horizontal_pod_autoscaling {
      disabled = !var.enable_addons.horizontal_pod_autoscaling
    }
    dynamic ""network_policy_config"" {
      for_each = !var.enable_features.autopilot ? [""""] : []
      content {
        disabled = !var.enable_addons.network_policy
      }
    }
    cloudrun_config {
      disabled = !var.enable_addons.cloudrun
    }
    istio_config {
      disabled = var.enable_addons.istio == null
      auth = (
        try(var.enable_addons.istio.enable_tls, false) ? ""AUTH_MUTUAL_TLS"" : ""AUTH_NONE""
      )
    }
    gce_persistent_disk_csi_driver_config {
      enabled = var.enable_addons.gce_persistent_disk_csi_driver
    }
    dynamic ""gcp_filestore_csi_driver_config"" {
      for_each = !var.enable_features.autopilot ? [""""] : []
      content {
        enabled = var.enable_addons.gcp_filestore_csi_driver
      }
    }
    kalm_config {
      enabled = var.enable_addons.kalm
    }
    config_connector_config {
      enabled = var.enable_addons.config_connector
    }
    gke_backup_agent_config {
      enabled = var.enable_addons.gke_backup_agent
    }
  }

  dynamic ""authenticator_groups_config"" {
    for_each = var.enable_features.groups_for_rbac != null ? [""""] : []
    content {
      security_group = var.enable_features.groups_for_rbac
    }
  }

  dynamic ""binary_authorization"" {
    for_each = var.enable_features.binary_authorization ? [""""] : []
    content {
      evaluation_mode = ""PROJECT_SINGLETON_POLICY_ENFORCE""
    }
  }

  dynamic ""cluster_autoscaling"" {
    for_each = var.cluster_autoscaling == null ? [] : [""""]
    content {
      enabled = true
      dynamic ""resource_limits"" {
        for_each = var.cluster_autoscaling.cpu_limits != null ? [""""] : []
        content {
          resource_type = ""cpu""
          minimum       = var.cluster_autoscaling.cpu_limits.min
          maximum       = var.cluster_autoscaling.cpu_limits.max
        }
      }
      dynamic ""resource_limits"" {
        for_each = var.cluster_autoscaling.mem_limits != null ? [""""] : []
        content {
          resource_type = ""cpu""
          minimum       = var.cluster_autoscaling.mem_limits.min
          maximum       = var.cluster_autoscaling.mem_limits.max
        }
      }
      // TODO: support GPUs too
    }
  }

  dynamic ""database_encryption"" {
    for_each = var.enable_features.database_encryption != null ? [""""] : []
    content {
      state    = var.enable_features.database_encryption.state
      key_name = var.enable_features.database_encryption.key_name
    }
  }

  dynamic ""dns_config"" {
    for_each = var.enable_features.cloud_dns != null ? [""""] : []
    content {
      cluster_dns        = enable_features.cloud_dns.cluster_dns
      cluster_dns_scope  = enable_features.cloud_dns.cluster_dns_scope
      cluster_dns_domain = enable_features.cloud_dns.cluster_dns_domain
    }
  }

  dynamic ""ip_allocation_policy"" {
    for_each = var.vpc_config.secondary_range_blocks != null ? [""""] : []
    content {
      cluster_ipv4_cidr_block  = var.vpc_config.secondary_range_blocks.pods
      services_ipv4_cidr_block = var.vpc_config.secondary_range_blocks.services
    }
  }
  dynamic ""ip_allocation_policy"" {
    for_each = var.vpc_config.secondary_range_names != null ? [""""] : []
    content {
      cluster_secondary_range_name  = var.vpc_config.secondary_range_names.pods
      services_secondary_range_name = var.vpc_config.secondary_range_names.services
    }
  }

  dynamic ""logging_config"" {
    for_each = var.logging_config != null ? [""""] : []
    content {
      enable_components = var.logging_config
    }
  }

  maintenance_policy {
    dynamic ""daily_maintenance_window"" {
      for_each = (
        try(var.maintenance_config.daily_window_start_time, null) != null
        ? [""""]
        : []
      )
      content {
        start_time = var.maintenance_config.daily_window_start_time
      }
    }
    dynamic ""recurring_window"" {
      for_each = (
        try(var.maintenance_config.recurring_window, null) != null
        ? [""""]
        : []
      )
      content {
        start_time = var.maintenance_config.recurring_window.start_time
        end_time   = var.maintenance_config.recurring_window.end_time
        recurrence = var.maintenance_config.recurring_window.recurrence
      }
    }
    dynamic ""maintenance_exclusion"" {
      for_each = (
        try(var.maintenance_config.maintenance_exclusions, null) == null
        ? []
        : var.maintenance_config.maintenance_exclusions
      )
      iterator = exclusion
      content {
        exclusion_name = exclusion.value.name
        start_time     = exclusion.value.start_time
        end_time       = exclusion.value.end_time
      }
    }
  }

  master_auth {
    client_certificate_config {
      issue_client_certificate = var.issue_client_certificate
    }
  }

  dynamic ""master_authorized_networks_config"" {
    for_each = var.vpc_config.master_authorized_ranges != null ? [""""] : []
    content {
      dynamic ""cidr_blocks"" {
        for_each = var.vpc_config.master_authorized_ranges
        iterator = range
        content {
          cidr_block   = range.value
          display_name = range.key
        }
      }
    }
  }

  dynamic ""monitoring_config"" {
    for_each = var.monitoring_config != null ? [""""] : []
    content {
      enable_components = var.monitoring_config
    }
  }

  # dataplane v2 has bult-in network policies
  dynamic ""network_policy"" {
    for_each = (
      var.enable_addons.network_policy && !var.enable_features.dataplane_v2
      ? [""""]
      : []
    )
    content {
      enabled  = true
      provider = ""CALICO""
    }
  }

  dynamic ""notification_config"" {
    for_each = var.enable_features.upgrade_notifications != null ? [""""] : []
    content {
      pubsub {
        enabled = true
        topic = (
          try(var.enable_features.upgrade_notifications.topic_id, null) != null
          ? var.enable_features.upgrade_notifications.topic_id
          : google_pubsub_topic.notifications[0].id
        )
      }
    }
  }

  dynamic ""private_cluster_config"" {
    for_each = (
      var.private_cluster_config != null ? [""""] : []
    )
    content {
      enable_private_nodes    = true
      enable_private_endpoint = var.private_cluster_config.enable_private_endpoint
      master_ipv4_cidr_block  = var.private_cluster_config.master_ipv4_cidr_block
      master_global_access_config {
        enabled = var.private_cluster_config.master_global_access
      }
    }
  }

  dynamic ""pod_security_policy_config"" {
    for_each = var.enable_features.pod_security_policy ? [""""] : []
    content {
      enabled = var.enable_features.pod_security_policy
    }
  }

  dynamic ""release_channel"" {
    for_each = var.release_channel != null ? [""""] : []
    content {
      channel = var.release_channel
    }
  }

  dynamic ""resource_usage_export_config"" {
    for_each = (
      try(var.enable_features.resource_usage_export.dataset, null) != null
      ? [""""]
      : []
    )
    content {
      enable_network_egress_metering = (
        var.enable_features.resource_usage_export.enable_network_egress_metering
      )
      enable_resource_consumption_metering = (
        var.enable_features.resource_usage_export.enable_resource_consumption_metering
      )
      bigquery_destination {
        dataset_id = var.enable_features.resource_usage_export.dataset
      }
    }
  }

  dynamic ""vertical_pod_autoscaling"" {
    for_each = var.enable_features.vertical_pod_autoscaling ? [""""] : []
    content {
      enabled = var.enable_features.vertical_pod_autoscaling
    }
  }

  dynamic ""workload_identity_config"" {
    for_each = var.enable_features.workload_identity ? [""""] : []
    content {
      workload_pool = ""${var.project_id}.svc.id.goog""
    }
  }
}
",resource,64,,c486bfc66f9814e33b410602cb557a5e4d532912,16822e94ab70d75099214b9db786affcb231fbf6,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c486bfc66f9814e33b410602cb557a5e4d532912/modules/gke-cluster/main.tf#L64,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/16822e94ab70d75099214b9db786affcb231fbf6/modules/gke-cluster/main.tf,2020-04-03 14:06:48+02:00,2022-10-10 09:38:21+02:00,35,1,0,1,1,0,1,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,200,modules/aws-eks-managed-node-groups/iam.tf,modules/aws-eks-managed-node-groups/iam.tf,0,fix,# TODO - fix at next breaking change,"# TODO - fix at next breaking change 
 # tflint-ignore: terraform_naming_convention","resource ""aws_iam_role_policy_attachment"" ""managed_ng_AmazonEKSWorkerNodePolicy"" {
  policy_arn = ""${local.policy_arn_prefix}/AmazonEKSWorkerNodePolicy""
  role       = aws_iam_role.managed_ng.name
}
",resource,the block associated got renamed or deleted,,23,,deec7d5caea47c06dea48fa616ad2c56e52c3cce,5010ec9551e21fee27bda888d98fc106b1e54f8a,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/deec7d5caea47c06dea48fa616ad2c56e52c3cce/modules/aws-eks-managed-node-groups/iam.tf#L23,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/5010ec9551e21fee27bda888d98fc106b1e54f8a/modules/aws-eks-managed-node-groups/iam.tf,2022-04-29 14:37:13-07:00,2022-05-19 09:22:58+01:00,2,1,0,1,0,1,0,0,0,0
https://github.com/alphagov/govuk-aws,1447,terraform/projects/infra-assets/replication.tf,terraform/projects/infra-assets/replication.tf,0,# todo,"# TODO: govuk-assets-backup-production should really have timelock (AWS ""Object","# govuk-assets-production replicates to: 
 #   govuk-assets-backup-production (same account, different region, supposed to be a backup) 
 #   govuk-assets-staging (different account, objects owned by destination account) 
 #   govuk-assets-integration (different account, objects owned by destination account) 
 # 
 # See: 
 # https://docs.aws.amazon.com/AmazonS3/latest/userguide/setting-repl-config-perm-overview 
 # https://docs.aws.amazon.com/AmazonS3/latest/userguide/delete-marker-replication 
 # https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-change-owner 
 # 
 # TODO: govuk-assets-backup-production should really have timelock (AWS ""Object 
 # Lock"") or similar to properly serve as a backup. ","locals {
  replication_role_name                    = ""govuk-production-assets-s3-replication""
  replication_service_role_in_prod_account = ""arn:aws:iam::172025368201:role/${local.replication_role_name}""
}
",locals,"locals {
  replication_role_name                    = ""govuk-production-assets-s3-replication""
  replication_service_role_in_prod_account = ""arn:aws:iam::172025368201:role/${local.replication_role_name}""
}
",locals,11,11.0,c1d29a6caabcd4f041d87d76a10f2deef06685ef,c1d29a6caabcd4f041d87d76a10f2deef06685ef,https://github.com/alphagov/govuk-aws/blob/c1d29a6caabcd4f041d87d76a10f2deef06685ef/terraform/projects/infra-assets/replication.tf#L11,https://github.com/alphagov/govuk-aws/blob/c1d29a6caabcd4f041d87d76a10f2deef06685ef/terraform/projects/infra-assets/replication.tf#L11,2023-06-02 10:34:23+01:00,2023-06-02 10:34:23+01:00,1,0,1,1,0,0,0,1,0,0
https://github.com/Worklytics/psoxy,232,infra/modules/azuread-local-cert/main.tf,infra/modules/azuread-local-cert/main.tf,0,hack,# hackery to translate output of openssl fingerprint --> string of hex chars equivalent to how MSFT does,"# hackery to translate output of openssl fingerprint --> string of hex chars equivalent to how MSFT does 
 # (eg, MSFT will compute fingerprint server-side of the certificate value posted above)","output ""private_key_id"" {
  # hackery to translate output of openssl fingerprint --> string of hex chars equivalent to how MSFT does
  # (eg, MSFT will compute fingerprint server-side of the certificate value posted above)
  value = replace(replace(data.external.certificate.result.fingerprint, ""SHA1 Fingerprint="", """"), "":"", """")
}
",output,,,35,0.0,44a2da80a65900db472a11c8a598e4df58a339e6,242232edab4d3fd867c0b2833ded96cacf95ef21,https://github.com/Worklytics/psoxy/blob/44a2da80a65900db472a11c8a598e4df58a339e6/infra/modules/azuread-local-cert/main.tf#L35,https://github.com/Worklytics/psoxy/blob/242232edab4d3fd867c0b2833ded96cacf95ef21/infra/modules/azuread-local-cert/main.tf#L0,2022-01-28 21:48:32-08:00,2023-02-01 09:14:32-08:00,6,2,0,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,482,infra/modules/aws-psoxy-bulk-existing/main.tf,infra/modules/aws-psoxy-bulk-existing/main.tf,0,# todo,"# TODO: highly duplicative with regular `aws-psoxy-bulk` case, and could likely be unified in future","# creates a Bulk processing instance of Psoxy, with existing S3 bucket as the input 
 # TODO: highly duplicative with regular `aws-psoxy-bulk` case, and could likely be unified in future 
 # version ","terraform {
  required_providers {
    # for the infra that will host Psoxy instances
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.29""
    }
  }
}
",terraform,"module ""psoxy_lambda"" {
  source = ""../aws-psoxy-lambda""

  environment_name                     = var.environment_name
  instance_id                          = var.instance_id
  handler_class                        = ""co.worklytics.psoxy.S3Handler""
  timeout_seconds                      = 600 # 10 minutes
  memory_size_mb                       = var.memory_size_mb
  path_to_function_zip                 = var.path_to_function_zip
  function_zip_hash                    = var.function_zip_hash
  global_parameter_arns                = var.global_parameter_arns
  global_secrets_manager_secrets_arns  = var.global_secrets_manager_secret_arns
  path_to_instance_ssm_parameters      = var.path_to_instance_ssm_parameters
  path_to_shared_ssm_parameters        = var.path_to_shared_ssm_parameters
  function_env_kms_key_arn             = var.function_env_kms_key_arn
  logs_kms_key_arn                     = var.logs_kms_key_arn
  ssm_kms_key_ids                      = var.ssm_kms_key_ids
  vpc_config                           = var.vpc_config
  secrets_store_implementation         = var.secrets_store_implementation
  aws_lambda_execution_role_policy_arn = var.aws_lambda_execution_role_policy_arn

  environment_variables = merge(
    var.environment_variables,
    {
      INPUT_BUCKET  = var.input_bucket
      OUTPUT_BUCKET = module.sanitized_output_bucket.output_bucket
    }
  )
}
",module,2,2.0,d1d2ef9b4a358d71a7d8e228a8291a4569a6f3cf,a6fe806adeb28bbf5edc920030d9019c837df209,https://github.com/Worklytics/psoxy/blob/d1d2ef9b4a358d71a7d8e228a8291a4569a6f3cf/infra/modules/aws-psoxy-bulk-existing/main.tf#L2,https://github.com/Worklytics/psoxy/blob/a6fe806adeb28bbf5edc920030d9019c837df209/infra/modules/aws-psoxy-bulk-existing/main.tf#L2,2022-10-27 14:12:45-07:00,2024-03-15 08:20:09-07:00,17,0,0,1,0,0,0,0,0,0
https://github.com/kubernetes/k8s.io,307,infra/gcp/terraform/k8s-infra-oci-proxy-prod/oci-proxy.tf,infra/gcp/terraform/k8s-infra-oci-proxy-prod/oci-proxy.tf,0,// todo,// TODO: adjust to control costs,"""autoscaling.knative.dev/maxScale"" = ""10"" // TODO: adjust to control costs","resource ""google_cloud_run_service"" ""oci-proxy"" {
  project  = google_project.project.project_id
  for_each = var.cloud_run_config
  name     = ""${var.project_id}-${each.key}""
  location = each.key

  template {
    metadata {
      annotations = {
        ""autoscaling.knative.dev/maxScale"" = ""10"" // TODO: adjust to control costs
        ""run.googleapis.com/launch-stage""  = ""BETA""
      }
    }
    spec {
      service_account_name = google_service_account.oci-proxy.email
      containers {
        image = ""us.gcr.io/k8s-artifacts-prod/infra-tools/archeio:${var.tag}""
        args  = [""-v=3""]

        dynamic ""env"" {
          for_each = each.value.environment_variables
          content {
            name  = env.value[""name""]
            value = env.value[""value""]
          }
        }

        // ensure this macth the value for template.spec.containers.resources.limits
        env {
          name = ""GOMAXPROCS""
          value = ""1""
        }

        resources {
          limits = {
            ""cpu"" = ""1000m""
          }
        }
      }

      container_concurrency = 1000

      // 30 seconds less than cloud scheduler maximum.
      timeout_seconds = 570
    }
  }

  traffic {
    percent         = 100
    latest_revision = true
  }

  depends_on = [
    google_project_service.project[""run.googleapis.com""]
  ]

  lifecycle {
    ignore_changes = [
      // This gets added by the Cloud Run API post deploy and causes diffs, can be ignored...
      template[0].metadata[0].annotations[""client.knative.dev/sandbox""],
      template[0].metadata[0].annotations[""run.googleapis.com/user-image""],
      template[0].metadata[0].annotations[""run.googleapis.com/client-name""],
      template[0].metadata[0].annotations[""run.googleapis.com/client-version""],
    ]
  }
}
",resource,the block associated got renamed or deleted,,82,,819ad39b6bb53e044619abde05a99cd11383c15a,ffbaa4eb3b652a9cc7520593cae83a8004ef88a3,https://github.com/kubernetes/k8s.io/blob/819ad39b6bb53e044619abde05a99cd11383c15a/infra/gcp/terraform/k8s-infra-oci-proxy-prod/oci-proxy.tf#L82,https://github.com/kubernetes/k8s.io/blob/ffbaa4eb3b652a9cc7520593cae83a8004ef88a3/infra/gcp/terraform/k8s-infra-oci-proxy-prod/oci-proxy.tf,2022-10-03 19:00:50+02:00,2023-04-02 19:38:59-07:00,10,1,1,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,606,infra/modules/aws-psoxy-lambda/main.tf,infra/modules/aws-psoxy-lambda/main.tf,0,hack,# parse PATH_TO_SHARED_CONFIG in super-hacky way,"# parse PATH_TO_SHARED_CONFIG in super-hacky way 
 # expect something like: 
 # arn:aws:ssm:us-east-1:123123123123:parameter/PSOXY_SALT","locals {
  instance_ssm_prefix = coalesce(var.path_to_instance_ssm_parameters, ""${upper(replace(var.function_name, ""-"", ""_""))}_"")

  # parse PATH_TO_SHARED_CONFIG in super-hacky way
  # expect something like:
  # arn:aws:ssm:us-east-1:123123123123:parameter/PSOXY_SALT
  salt_arn              = [for l in var.global_parameter_arns : l if endswith(l, ""PSOXY_SALT"")][0]
  path_to_shared_config = regex(""arn.+parameter/(.*)PSOXY_SALT"", local.salt_arn)[0]
}
",locals,"locals {
  salt_parameter_name_suffix = ""PSOXY_SALT""
  function_name              = ""${module.env_id.id}-${var.instance_id}""

  kms_key_ids_to_allow = merge(
    var.ssm_kms_key_ids,
    var.kms_keys_to_allow
  )
}
",locals,14,,59ed477a2b92552a81c734405b9d0faf43f10330,005e1fed5f46b4310d81d41d29862bb1c4f360b0,https://github.com/Worklytics/psoxy/blob/59ed477a2b92552a81c734405b9d0faf43f10330/infra/modules/aws-psoxy-lambda/main.tf#L14,https://github.com/Worklytics/psoxy/blob/005e1fed5f46b4310d81d41d29862bb1c4f360b0/infra/modules/aws-psoxy-lambda/main.tf,2023-01-13 11:04:05-08:00,2024-02-06 19:07:07+00:00,14,1,0,1,0,0,1,0,0,0
https://github.com/pingcap/tidb-operator,6,deploy/alicloud/main.tf,deploy/aliyun/main.tf,1,# todo,# TODO: support non-public apiserver,# TODO: support non-public apiserver,"module ""ack"" {
  source  = ""./ack""
  version = ""1.0.2""

  providers = {
    alicloud = ""alicloud.this""
  }

  # TODO: support non-public apiserver
  region           = ""${var.ALICLOUD_REGION}""
  cluster_name     = ""${var.cluster_name}""
  public_apiserver = true
  kubeconfig_file  = ""${local.kubeconfig}""
  key_file         = ""${local.key_file}""
  vpc_cidr         = ""${var.vpc_cidr}""
  k8s_pod_cidr     = ""${var.k8s_pod_cidr}""
  k8s_service_cidr = ""${var.k8s_service_cidr}""
  vpc_cidr_newbits = ""${var.vpc_cidr_newbits}""
  vpc_id           = ""${var.vpc_id}""
  group_id         = ""${var.group_id}""

  worker_groups = [
    {
      name          = ""pd_worker_group""
      instance_type = ""${data.alicloud_instance_types.pd.instance_types.0.id}""
      min_size      = ""${var.pd_count}""
      max_size      = ""${var.pd_count}""
      node_taints   = ""dedicated=pd:NoSchedule""
      node_labels   = ""dedicated=pd""
      post_userdata = ""${file(""userdata/pd-userdata.sh"")}""
    },
    {
      name          = ""tikv_worker_group""
      instance_type = ""${data.alicloud_instance_types.tikv.instance_types.0.id}""
      min_size      = ""${var.tikv_count}""
      max_size      = ""${var.tikv_count}""
      node_taints   = ""dedicated=tikv:NoSchedule""
      node_labels   = ""dedicated=tikv""
      post_userdata = ""${file(""userdata/tikv-userdata.sh"")}""
    },
    {
      name          = ""tidb_worker_group""
      instance_type = ""${var.tidb_instance_type != """" ? var.tidb_instance_type : data.alicloud_instance_types.tidb.instance_types.0.id}""
      min_size      = ""${var.tidb_count}""
      max_size      = ""${var.tidb_count}""
      node_taints   = ""dedicated=tidb:NoSchedule""
      node_labels   = ""dedicated=tidb""
    },
    {
      name          = ""monitor_worker_group""
      instance_type = ""${var.monitor_intance_type != """" ? var.monitor_intance_type : data.alicloud_instance_types.monitor.instance_types.0.id}""
      min_size      = 1
      max_size      = 1
    },
  ]
}
",module,the block associated got renamed or deleted,,36,,eebd686956c0b2adf67a10e1a376659c95c571a3,042b1a97fbbdf342297002990564828e6644a3f0,https://github.com/pingcap/tidb-operator/blob/eebd686956c0b2adf67a10e1a376659c95c571a3/deploy/alicloud/main.tf#L36,https://github.com/pingcap/tidb-operator/blob/042b1a97fbbdf342297002990564828e6644a3f0/deploy/aliyun/main.tf,2019-05-06 19:59:43+08:00,2019-07-23 19:44:58+08:00,4,1,1,1,0,0,1,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,4,main.tf,main.tf,0,//todo,//TODO: array of users or groups needs to be added to have access. Need to figure out the best method of customers to allocate users or groups.,"//TODO: array of users or groups needs to be added to have access. Need to figure out the best method of customers to allocate users or groups. 
 # access { 
 #   role   = ""READER"" 
 #   domain = ""adigangi.com"" 
 # } 
 # 
 # access { 
 #   role           = ""WRITER"" 
 #   user_by_email = ""adigangi@adigangi.com"" 
 # } 
 # 
 # access { 
 #   role           = ""OWNER"" 
 #   special_group  = ""projectOwners"" 
 # }","resource ""google_bigquery_dataset"" ""default"" {
  dataset_id                  = ""${var.dataset_id}""
  friendly_name               = ""${var.dataset_name}""
  description                 = ""${var.description}""
  #TODO: add if condition to validate if neither US or EU are supplied
  location                    = ""${var.region}""
  #TODO: format this ne excluded by default but can optionally be defined if the user wishes
  default_table_expiration_ms = ""${var.expiration}""
  project                     = ""${var.project_id}""

  #TODO: Need to find a way to dynamically assign a dict object(s)
  labels {
    env = ""default""
    foo = ""bar""
    tonyd = ""tonyd""
  }

  //TODO: array of users or groups needs to be added to have access. Need to figure out the best method of customers to allocate users or groups.
  # access {
  #   role   = ""READER""
  #   domain = ""adigangi.com""
  # }
  #
  # access {
  #   role           = ""WRITER""
  #   user_by_email = ""adigangi@adigangi.com""
  # }
  #
  # access {
  #   role           = ""OWNER""
  #   special_group  = ""projectOwners""
  # }
}
",resource,the block associated got renamed or deleted,,39,,d56aa2c9a80343d60eed3e1a7d24962be31ee0b6,7f922f7e9df197df38c9b09dfa6e3614d71f19f5,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/d56aa2c9a80343d60eed3e1a7d24962be31ee0b6/main.tf#L39,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/7f922f7e9df197df38c9b09dfa6e3614d71f19f5/main.tf,2018-11-20 10:30:15-05:00,2019-01-16 18:10:54-05:00,3,1,0,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,1547,infra/modules/google-workspace-dwd-connection/main.tf,infra/modules/google-workspace-dwd-connection/main.tf,0,# todo,"# TODO: md5 here is 32 chars of hex, so some risk of collision by truncating","# hash if too long 
 # TODO: md5 here is 32 chars of hex, so some risk of collision by truncating","locals {
  # sa_account_ids must be 6-30 chars, and must start with a letter, use only lowercase letters,
  # numbers and - (inside)
  # see https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_service_account

  # trim trailing replaces, replace enclosed spaces with dashes, and everything as lower case
  trimmed_id = lower(replace(trim(var.connector_service_account_id, "" ""), "" "", ""-""))

  # pad if too short
  padded_id = length(local.trimmed_id) < 6 ? ""psoxy-${local.trimmed_id}"" : local.trimmed_id

  # hash if too long
  # TODO: md5 here is 32 chars of hex, so some risk of collision by truncating
  sa_account_id = length(local.padded_id) < 31 ? local.padded_id : substr(md5(local.padded_id), 0, 30)

  instance_id = coalesce(var.instance_id, var.display_name)
}
",locals,"locals {
  # sa_account_ids must be 6-30 chars, and must start with a letter, use only lowercase letters,
  # numbers and - (inside)
  # see https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_service_account

  # trim trailing replaces, replace enclosed spaces with dashes, and everything as lower case
  trimmed_id = lower(replace(trim(var.connector_service_account_id, "" ""), "" "", ""-""))

  # pad if too short
  padded_id = length(local.trimmed_id) < 6 ? ""psoxy-${local.trimmed_id}"" : local.trimmed_id

  # hash if too long
  # TODO: md5 here is 32 chars of hex, so some risk of collision by truncating
  sa_account_id = length(local.padded_id) < 31 ? local.padded_id : substr(md5(local.padded_id), 0, 30)

  instance_id = coalesce(var.instance_id, var.display_name)
}
",locals,19,19.0,2f240146ca64d11927aac88550c4df4b094c45ee,419ab7426298f38d950186bd64303ef628cc2fc5,https://github.com/Worklytics/psoxy/blob/2f240146ca64d11927aac88550c4df4b094c45ee/infra/modules/google-workspace-dwd-connection/main.tf#L19,https://github.com/Worklytics/psoxy/blob/419ab7426298f38d950186bd64303ef628cc2fc5/infra/modules/google-workspace-dwd-connection/main.tf#L19,2023-06-20 14:50:05+00:00,2023-12-20 09:36:51-08:00,7,0,0,1,0,0,0,0,0,0
https://github.com/kubernetes/k8s.io,373,infra/aws/terraform/prow-build-cluster/prow.tf,infra/aws/terraform/prow-build-cluster/prow.tf,0,# todo,# TODO(xmudrii): This is a temporary condition. To be deleted after making canary cluster a build cluster.,# TODO(xmudrii): This is a temporary condition. To be deleted after making canary cluster a build cluster.,"resource ""aws_iam_openid_connect_provider"" ""k8s_prow"" {
  # TODO(xmudrii): This is a temporary condition. To be deleted after making canary cluster a build cluster.
  count = var.cluster_name == ""prow-build-cluster"" ? 1 : 0

  url             = ""https://container.googleapis.com/v1/projects/k8s-prow/locations/us-central1-f/clusters/prow""
  client_id_list  = [""sts.amazonaws.com""]
  thumbprint_list = [""08745487e891c19e3078c1f2a07e452950ef36f6""]
}
",resource,"resource ""aws_iam_openid_connect_provider"" ""k8s_prow"" {
  count = local.configure_prow ? 1 : 0

  url             = ""https://container.googleapis.com/v1/projects/k8s-prow/locations/us-central1-f/clusters/prow""
  client_id_list  = [""sts.amazonaws.com""]
  thumbprint_list = [""08745487e891c19e3078c1f2a07e452950ef36f6""]
}
",resource,22,,db6b5a5b1de7cac3d8f08c4b44a6a19b36855b58,3cf0ef275d51659e041a4663921016a73d4eb7b8,https://github.com/kubernetes/k8s.io/blob/db6b5a5b1de7cac3d8f08c4b44a6a19b36855b58/infra/aws/terraform/prow-build-cluster/prow.tf#L22,https://github.com/kubernetes/k8s.io/blob/3cf0ef275d51659e041a4663921016a73d4eb7b8/infra/aws/terraform/prow-build-cluster/prow.tf,2023-04-26 13:27:36+02:00,2023-04-26 14:58:43+02:00,2,1,1,1,0,1,0,0,0,0
https://github.com/clong/DetectionLab,1,Azure/Terraform/main.tf,Azure/Terraform/main.tf,0,fix,# FIXME!,"# terraform init, plan, apply, destroy 
 # Note: does not support idempotence, don't execute twice with same scope. 
 # https://www.terraform.io/docs/providers/azurerm/index.html 
 # latest test: terraform 0.12.18 
 # 
 # FIXME! 
 # * apply: provisioning not working on Windows 
 # Error: Unsupported argument [...] An argument named ""connection"" is not expected here. 
 #    apply => Error: timeout - last error: SSH authentication failed (root@:22): ssh: handshake failed: ssh: unable to authenticate, attempted methods [none publickey], no supported methods remain 
 # * apply: linux provisioning 
 #        => works but script ends with error code for some reason (post bro install and splunk restart)  
 # Specify the provider and access details","provider ""azurerm"" {
  version = ""=2.12.0""
  features {}
}
",provider,"provider ""azurerm"" {
  features {}
}
",provider,6,6.0,5791b99c8fb8b6de6552e91e5ed6da5607e90401,0a61dc1d44e9a4c4331f054ebf0b225e065739ac,https://github.com/clong/DetectionLab/blob/5791b99c8fb8b6de6552e91e5ed6da5607e90401/Azure/Terraform/main.tf#L6,https://github.com/clong/DetectionLab/blob/0a61dc1d44e9a4c4331f054ebf0b225e065739ac/Azure/Terraform/main.tf#L6,2020-06-14 18:45:18-07:00,2022-04-21 13:40:51-05:00,19,0,0,1,1,0,0,0,0,0
https://github.com/Worklytics/psoxy,287,infra/modules/aws-psoxy-rest/main.tf,infra/modules/aws-psoxy-rest/main.tf,0,# todo,# TODO: configurable? not all require POST,"allow_methods     = [""POST"", ""GET"", ""HEAD""] # TODO: configurable? not all require POST","resource ""aws_lambda_function_url"" ""lambda_url"" {
  function_name      = var.function_name
  authorization_type = ""AWS_IAM""

  cors {
    allow_credentials = true
    allow_origins     = [""*""]
    allow_methods     = [""POST"", ""GET"", ""HEAD""] # TODO: configurable? not all require POST
    allow_headers     = [""date"", ""keep-alive""]
    expose_headers    = [""keep-alive"", ""date""]
    max_age           = 86400
  }
}
",resource,"resource ""aws_lambda_function_url"" ""lambda_url"" {
  function_name      = module.psoxy_lambda.function_name
  authorization_type = ""AWS_IAM""

  depends_on = [
    module.psoxy_lambda
  ]
}
",resource,40,,83172700daa197caa267997675d3ec6acb23c229,1b9ca4eaa917e0f3fce28bafc58d33529de22312,https://github.com/Worklytics/psoxy/blob/83172700daa197caa267997675d3ec6acb23c229/infra/modules/aws-psoxy-rest/main.tf#L40,https://github.com/Worklytics/psoxy/blob/1b9ca4eaa917e0f3fce28bafc58d33529de22312/infra/modules/aws-psoxy-rest/main.tf,2022-05-09 21:48:24-07:00,2023-08-09 09:57:11-07:00,51,1,1,1,0,0,0,0,0,0
https://github.com/CDCgov/prime-simplereport,13,ops/prod/app_gateway_url_redirects.tf,ops/prod/app_gateway_url_redirects.tf,0,todo,# TODO = extant but not yet managed by Terraform,"# Production has three special App Gateways that perform global redirects instead of routing 
 # to apps: simplereport-gateway and simplereport-internal-gw 
 # 
 # simplereport-gateway:       redirects simplereport.cdc.gov -> www.simplereport.gov (TODO) 
 # simplereport-internal-gw:   redirects simplereport.org     -> www.simplereport.gov (TODO) 
 # simple-report-www-redirect: redirects simplereport.gov     -> www.simplereport.gov 
 # 
 # TODO = extant but not yet managed by Terraform ","locals {
  static_backend_pool          = ""${local.name}-${local.env}-fe-static""
  static_backend_http_setting  = ""${local.name}-${local.env}-fe-static-http""
  static_backend_https_setting = ""${local.name}-${local.env}-fe-static-https""
  http_listener                = ""${local.name}-http""
  https_listener               = ""${local.name}-https""
  frontend_config              = ""${local.name}-config""
}
",locals,"locals {
  static_backend_pool          = ""${local.name}-${local.env}-fe-static""
  static_backend_http_setting  = ""${local.name}-${local.env}-fe-static-http""
  static_backend_https_setting = ""${local.name}-${local.env}-fe-static-https""
  http_listener                = ""${local.name}-http""
  https_listener               = ""${local.name}-https""
  frontend_config              = ""${local.name}-config""
}
",locals,8,,40834da4606790f5a553f9e2e9bc300869aeacda,4f6c71db585cbb2bd83553aeb21c8eb21fe5773e,https://github.com/CDCgov/prime-simplereport/blob/40834da4606790f5a553f9e2e9bc300869aeacda/ops/prod/app_gateway_url_redirects.tf#L8,https://github.com/CDCgov/prime-simplereport/blob/4f6c71db585cbb2bd83553aeb21c8eb21fe5773e/ops/prod/app_gateway_url_redirects.tf,2021-10-15 23:06:17-04:00,2021-11-23 00:12:13-05:00,3,1,0,0,1,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,108,modules/extensions/iam.tf,modules/extensions/iam.tf,0,todo,# TODO Move to Operator module,# TODO Move to Operator module,"resource ""oci_identity_policy"" ""operator_use_dynamic_group_policy"" {
  provider       = oci.home
  compartment_id = random_id.dynamic_group_suffix.keepers.tenancy_id
  description    = ""policy to allow operator host to manage dynamic group""
  name           = join(""-"", compact([
    random_id.dynamic_group_suffix.keepers.label_prefix,
    ""operator-instance-principal-dynamic-group"",
    random_id.dynamic_group_suffix.hex
  ]))
  statements     = [""Allow dynamic-group ${var.operator_dynamic_group} to use dynamic-groups in tenancy""]
  count          = (local.create_operator_dynamic_group_policy == true) ? 1 : 0
}
",resource,,,30,0.0,269d3fdd896309157e667558bb885f0c54a3e11d,6c867cd8e9cbf559742f56658989bcded0d1fd89,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/269d3fdd896309157e667558bb885f0c54a3e11d/modules/extensions/iam.tf#L30,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/modules/extensions/iam.tf#L0,2022-10-06 16:42:51+11:00,2023-10-25 16:40:02+11:00,3,2,0,1,0,1,0,0,0,0
https://github.com/aws-observability/terraform-aws-observability-accelerator,4,main.tf,main.tf,0,# todo,# TODO: support custom alert manager config,# TODO: support custom alert manager config,"resource ""aws_prometheus_alert_manager_definition"" ""this"" {
  count = var.enable_alertmanager ? 1 : 0

  workspace_id = local.amp_ws_id

  # TODO: support custom alert manager config
  definition = <<EOF
alertmanager_config: |
    route:
      receiver: 'default'
    receivers:
      - name: 'default'
EOF
}
",resource,"resource ""aws_prometheus_alert_manager_definition"" ""this"" {
  count = var.enable_alertmanager ? 1 : 0

  workspace_id = local.amp_ws_id

  definition = <<EOF
alertmanager_config: |
    route:
      receiver: 'default'
    receivers:
      - name: 'default'
EOF
}
",resource,42,,333d46cccca511ba7ec2b83f695324b4c03c342e,c93e2cffc1d180af9d5c9a3623f0a18b4d6130b2,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/333d46cccca511ba7ec2b83f695324b4c03c342e/main.tf#L42,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/c93e2cffc1d180af9d5c9a3623f0a18b4d6130b2/main.tf,2022-08-26 17:30:03+02:00,2022-08-30 10:47:21+02:00,9,1,0,1,0,0,0,0,1,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,7,frontend/tf/main.tf,community/front-end/ofe/tf/main.tf,1,# todo,# TODO:  SSH Keys,# TODO:  SSH Keys,"resource ""google_compute_instance"" ""server_vm"" {

    name = ""${var.deployment_name}-server""
    machine_type = var.server_instance_type
    zone = var.zone

    hostname = length(trimspace(var.webserver_hostname)) > 0 ? var.webserver_hostname : null
    
    metadata = {
        startup-script-url = ""${module.control_bucket.bucket.url}/webserver/startup.sh"",
        webserver-config-bucket = module.control_bucket.bucket.name,
        ghpcfe-c2-topic = module.pubsub.topic,
        hostname = var.webserver_hostname
        deploy_mode = var.deployment_mode
        # TODO:  SSH Keys
    }

    service_account {
        email = module.service_account.email
        scopes = [
            ""storage-full"",
            ""logging-write"",
            ""monitoring-write"",
            ""trace"",
            ""service-control"",
            ""service-management"",
            ""pubsub""
        ]
    }
    scheduling {
        on_host_maintenance = ""MIGRATE""
    }

    labels = local.labels
    tags = [""http-server"", ""https-server"", ""ssh-server""]

    boot_disk {
        initialize_params {
            image = ""projects/rocky-linux-cloud/global/images/rocky-linux-8-v20220126""
            size = 30
            type = ""pd-ssd""
        }
    }

    network_interface {
        subnetwork = length(trimspace(var.subnet)) > 0 ? var.subnet : module.network[0].subnet_name
        access_config {
            nat_ip = length(trimspace(var.static_ip)) > 0 ? var.static_ip : null
        }
    }

}
",resource,"resource ""google_compute_instance"" ""server_vm"" {

  name         = ""${var.deployment_name}-server""
  machine_type = var.server_instance_type
  zone         = var.zone

  hostname = length(trimspace(var.webserver_hostname)) > 0 ? var.webserver_hostname : null

  metadata = {
    startup-script-url      = ""${module.control_bucket.bucket.url}/webserver/startup.sh"",
    webserver-config-bucket = module.control_bucket.bucket.name,
    ghpcfe-c2-topic         = module.pubsub.topic,
    hostname                = var.webserver_hostname
    deploy_mode             = var.deployment_mode
  }

  service_account {
    email = module.service_account.email
    scopes = [
      ""storage-full"",
      ""logging-write"",
      ""monitoring-write"",
      ""trace"",
      ""service-control"",
      ""service-management"",
      ""pubsub""
    ]
  }
  scheduling {
    on_host_maintenance = ""MIGRATE""
  }

  labels = local.labels
  tags   = [""http-server"", ""https-server"", ""ssh-server""]

  boot_disk {
    initialize_params {
      image = ""projects/rocky-linux-cloud/global/images/rocky-linux-8-v20220126""
      size  = 30
      type  = ""pd-ssd""
    }
  }

  network_interface {
    subnetwork = length(trimspace(var.subnet)) > 0 ? var.subnet : module.network[0].subnet_name
    access_config {
      nat_ip = length(trimspace(var.static_ip)) > 0 ? var.static_ip : null
    }
  }

}
",resource,136,,67c9341c36b25f7c4fb6bc3b8aef45fddff8c2cc,d0b943737fd33716875d6b7e24bce99dda940918,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/67c9341c36b25f7c4fb6bc3b8aef45fddff8c2cc/frontend/tf/main.tf#L136,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/d0b943737fd33716875d6b7e24bce99dda940918/community/front-end/ofe/tf/main.tf,2022-03-31 13:08:42-05:00,2022-11-10 17:18:21+00:00,2,1,1,1,0,1,0,0,0,0
https://github.com/pingcap/tidb-operator,3,deploy/alicloud/ack/data.tf,deploy/aliyun/ack/data.tf,1,workaround,"# Workaround map to list transformation, see stackoverflow.com/questions/43893295","# Workaround map to list transformation, see stackoverflow.com/questions/43893295","data ""template_file"" ""vswitch_id"" {
  count    = ""${var.vpc_id == """" ? 0 : length(data.alicloud_vswitches.default.vswitches)}""
  template = ""${lookup(data.alicloud_vswitches.default.0.vswitches[count.index], ""id"")}""
}
",data,,,14,0.0,eebd686956c0b2adf67a10e1a376659c95c571a3,042b1a97fbbdf342297002990564828e6644a3f0,https://github.com/pingcap/tidb-operator/blob/eebd686956c0b2adf67a10e1a376659c95c571a3/deploy/alicloud/ack/data.tf#L14,https://github.com/pingcap/tidb-operator/blob/042b1a97fbbdf342297002990564828e6644a3f0/deploy/aliyun/ack/data.tf#L0,2019-05-06 19:59:43+08:00,2019-07-23 19:44:58+08:00,2,2,0,1,0,0,0,0,0,0
https://github.com/alphagov/govuk-aws,1,terraform/projects/govuk-networking/main.tf,terraform/projects/infra-networking/main.tf,1,workaround,"# There are a few workarounds to get around this limitation,","# Intermediate variables in Terraform are not supported. 
 # There are a few workarounds to get around this limitation, 
 # https://github.com/hashicorp/terraform/issues/4084 
 # The template_file resources allow us to use a private_subnet_nat_gateway_association 
 # variable to select which NAT gateway, if any, each private 
 # subnet must use to route public traffic.","data ""template_file"" ""nat_gateway_association_subnet_id"" {
  count    = ""${length(keys(var.private_subnet_nat_gateway_association))}""
  template = ""$${subnet_id}""

  vars {
    subnet_id = ""${lookup(module.govuk_public_subnet.subnet_names_ids_map, element(values(var.private_subnet_nat_gateway_association), count.index))}""
  }
}
",data,"data ""template_file"" ""nat_gateway_association_subnet_id"" {
  count    = length(keys(var.private_subnet_nat_gateway_association))
  template = ""$${subnet_id}""

  vars = {
    subnet_id = ""${lookup(module.infra_public_subnet.subnet_names_ids_map, element(values(var.private_subnet_nat_gateway_association), count.index))}""
  }
}
",data,114,167.0,7770005f878b16b548c854f6a22e817b5438d3ca,7a0cb9b14717825fe20ec66dde2591851d2de47b,https://github.com/alphagov/govuk-aws/blob/7770005f878b16b548c854f6a22e817b5438d3ca/terraform/projects/govuk-networking/main.tf#L114,https://github.com/alphagov/govuk-aws/blob/7a0cb9b14717825fe20ec66dde2591851d2de47b/terraform/projects/infra-networking/main.tf#L167,2017-07-05 16:12:02+01:00,2023-12-01 16:46:15+00:00,45,0,0,0,1,0,1,0,0,0
https://github.com/magma/magma,1,orc8r/cloud/deploy/terraform/main.tf,orc8r/cloud/deploy/terraform/main.tf,0,# todo,# TODO: custom userdata to claim and mount the EBS volume,"# TODO: custom userdata to claim and mount the EBS volume 
 # for now, you'll have to mount the volume to the node manually  
 # Have to specify this here otherwise it forces a new resource","module ""eks"" {
  source       = ""terraform-aws-modules/eks/aws""
  cluster_name = var.cluster_name
  vpc_id       = module.vpc.vpc_id
  subnets      = module.vpc.public_subnets

  worker_additional_security_group_ids = [aws_security_group.default.id]

  # asg max capacity is 3
  # 1 worker group for orc8r (3 boxes total)
  # 1 worker group for metrics (1 box)
  worker_groups = [
    {
      name                 = ""wg-1""
      instance_type        = ""m4.xlarge""
      asg_desired_capacity = 3
      key_name             = var.key_name

      # Have to specify this here otherwise it forces a new resource
      ami_id = ""ami-08716b70cac884aaa""

      tags = [
        {
          key                 = ""orc8r-node-type""
          value               = ""orc8r-worker-node""
          propagate_at_launch = true
        },
      ]
    },
    {
      name                 = ""wg-metrics""
      instance_type        = ""t2.xlarge""
      asg_desired_capacity = 1
      key_name             = var.key_name

      # we put the metrics nodes into 1 specific subnet because EBS volumes
      # can only be mounted into the same AZ
      subnets = [module.vpc.public_subnets[0]]

      # TODO: custom userdata to claim and mount the EBS volume
      # for now, you'll have to mount the volume to the node manually

      # Have to specify this here otherwise it forces a new resource
      ami_id = ""ami-08716b70cac884aaa""

      tags = [
        {
          key                 = ""orc8r-node-type""
          value               = ""orc8r-prometheus-node""
          propagate_at_launch = true
        },
      ]
    },
  ]

  map_users = var.map_users

  write_kubeconfig      = true
  write_aws_auth_config = true
}
",module,"module ""eks"" {
  source       = ""terraform-aws-modules/eks/aws""
  cluster_name = var.cluster_name
  vpc_id       = module.vpc.vpc_id
  subnets      = module.vpc.public_subnets

  worker_additional_security_group_ids = [aws_security_group.default.id]
  workers_additional_policies          = [""${aws_iam_policy.worker_node_policy.arn}""]

  # asg max capacity is 3
  # 1 worker group for orc8r (3 boxes total)
  # 1 worker group for metrics (1 box)
  worker_groups = [
    {
      name                 = ""wg-1""
      instance_type        = ""m4.xlarge""
      asg_desired_capacity = 3
      key_name             = var.key_name

      # Have to specify this here otherwise it forces a new resource
      ami_id = ""ami-08716b70cac884aaa""

      tags = [
        {
          key                 = ""orc8r-node-type""
          value               = ""orc8r-worker-node""
          propagate_at_launch = true
        },
      ]
    },
    {
      name                 = ""wg-metrics""
      instance_type        = ""t2.xlarge""
      asg_desired_capacity = 1
      key_name             = var.key_name

      # we put the metrics nodes into 1 specific subnet because EBS volumes
      # can only be mounted into the same AZ
      subnets = [module.vpc.public_subnets[0]]

      additional_userdata  = ""${data.template_file.metrics_userdata.rendered}""

      # Have to specify this here otherwise it forces a new resource
      ami_id = ""ami-08716b70cac884aaa""

      tags = [
        {
          key                 = ""orc8r-node-type""
          value               = ""orc8r-prometheus-node""
          propagate_at_launch = true
        },
      ]
    },
  ]

  map_users = var.map_users

  write_kubeconfig      = true
  write_aws_auth_config = true
}
",module,73,,3f09622541c736c040f6f9b91841cdbf40b7680a,fb12ebd0afc6e7cfb1e8a1339eb39b9fbdce6b63,https://github.com/magma/magma/blob/3f09622541c736c040f6f9b91841cdbf40b7680a/orc8r/cloud/deploy/terraform/main.tf#L73,https://github.com/magma/magma/blob/fb12ebd0afc6e7cfb1e8a1339eb39b9fbdce6b63/orc8r/cloud/deploy/terraform/main.tf,2019-06-28 14:42:33-07:00,2019-06-28 18:17:17-07:00,2,1,1,1,0,0,0,0,0,0
https://github.com/kubernetes/k8s.io,375,infra/aws/terraform/prow-build-cluster/variables.tf,infra/aws/terraform/prow-build-cluster/variables.tf,0,# todo,# TODO: remove once applied on prod,"# TODO: remove once applied on prod 
 # This variable is required in the installation process as we cannot 
 # assume a role that is yet to be created.","variable ""assume_role"" {
  type        = bool
  description = ""Assumes role to get access to EKS cluster after provisioning.""
  default     = true
}
",variable,"variable ""assume_role"" {
  type        = bool
  description = ""Assumes role to get access to EKS cluster after provisioning.""
  default     = true
}
",variable,29,,db6b5a5b1de7cac3d8f08c4b44a6a19b36855b58,ea61b21c1c9188ea5df32e9b08d1a51fe706715a,https://github.com/kubernetes/k8s.io/blob/db6b5a5b1de7cac3d8f08c4b44a6a19b36855b58/infra/aws/terraform/prow-build-cluster/variables.tf#L29,https://github.com/kubernetes/k8s.io/blob/ea61b21c1c9188ea5df32e9b08d1a51fe706715a/infra/aws/terraform/prow-build-cluster/variables.tf,2023-04-26 13:27:36+02:00,2023-04-26 15:07:51+02:00,3,1,0,1,0,0,0,1,0,0
https://github.com/compiler-explorer/infra,73,terraform/audit.tf,terraform/audit.tf,0,fix,# until https://github.com/terraform-providers/terraform-provider-aws/pull/5448 is fixed,# until https://github.com/terraform-providers/terraform-provider-aws/pull/5448 is fixed,"resource ""aws_cloudtrail"" ""audit"" {
  name                          = ""ce-audit""
  s3_bucket_name                = aws_s3_bucket.cloudtrail.id
  include_global_service_events = true
  tags                          = {
    Site = ""CompilerExplorer""
  }
  event_selector {
    include_management_events = true
    read_write_type = ""All""
  }
  is_multi_region_trail         = true
  enable_log_file_validation    = true
  depends_on = [aws_s3_bucket_policy.cloudtrail-bucket-policy]
  # until https://github.com/terraform-providers/terraform-provider-aws/pull/5448 is fixed
  lifecycle {
    ignore_changes = [event_selector]
  }
}
",resource,"resource ""aws_cloudtrail"" ""audit"" {
  name                          = ""ce-audit""
  s3_bucket_name                = aws_s3_bucket.cloudtrail.id
  include_global_service_events = true
  event_selector {
    include_management_events = true
    read_write_type = ""All""
  }
  is_multi_region_trail         = true
  enable_log_file_validation    = true
  depends_on = [aws_s3_bucket_policy.cloudtrail-bucket-policy]
}
",resource,17,,65e8fa11ba2143385880fac5c502a533ecd740b8,5e1f4cf94354e86ad45b271dcfb7be3423e20a3c,https://github.com/compiler-explorer/infra/blob/65e8fa11ba2143385880fac5c502a533ecd740b8/terraform/audit.tf#L17,https://github.com/compiler-explorer/infra/blob/5e1f4cf94354e86ad45b271dcfb7be3423e20a3c/terraform/audit.tf,2020-06-22 22:30:12-05:00,2022-10-08 16:41:47-05:00,5,1,0,0,1,0,0,0,1,0
https://github.com/terraform-google-modules/terraform-google-project-factory,258,modules/core_project_factory/main.tf,modules/core_project_factory/main.tf,0,#todo,#TODO rename resource in the next breaking change.,"resource ""time_sleep"" ""wait_5_seconds"" { #TODO rename resource in the next breaking change.","resource ""time_sleep"" ""wait_5_seconds"" { #TODO rename resource in the next breaking change.
  count           = var.vpc_service_control_attach_enabled ? 1 : 0
  depends_on      = [google_access_context_manager_service_perimeter_resource.service_perimeter_attachment[0], google_project_service.enable_access_context_manager[0]]
  create_duration = var.vpc_service_control_sleep_duration
}
",resource,"resource ""time_sleep"" ""wait_5_seconds"" { #TODO rename resource in the next breaking change.
  count           = var.vpc_service_control_attach_enabled || var.vpc_service_control_attach_dry_run ? 1 : 0
  depends_on      = [google_access_context_manager_service_perimeter_resource.service_perimeter_attachment[0], google_project_service.enable_access_context_manager[0]]
  create_duration = var.vpc_service_control_sleep_duration
}
",resource,111,111.0,086210b2b4cacfe1ab0463a9cfb065da8f902f41,cfd7f3f15e0866fe09cc5ec4a2f8e94398c773d9,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/086210b2b4cacfe1ab0463a9cfb065da8f902f41/modules/core_project_factory/main.tf#L111,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/cfd7f3f15e0866fe09cc5ec4a2f8e94398c773d9/modules/core_project_factory/main.tf#L111,2022-11-15 17:28:37-06:00,2024-05-17 18:11:28+00:00,5,0,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,101,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf,0,# todo,# TODO: reconsider duplication,login_network_storage  = var.network_storage # TODO: reconsider duplication,"module ""slurm_files"" {
  source = ""github.com/SchedMD/slurm-gcp.git//terraform/slurm_cluster/modules/slurm_files?ref=6.2.0""

  project_id         = var.project_id
  slurm_cluster_name = local.slurm_cluster_name
  bucket_dir         = var.bucket_dir
  bucket_name        = local.bucket_name

  slurmdbd_conf_tpl = var.slurmdbd_conf_tpl
  slurm_conf_tpl    = var.slurm_conf_tpl
  cgroup_conf_tpl   = var.cgroup_conf_tpl
  cloud_parameters  = var.cloud_parameters
  # TODO: resolve cyclic dependency and use 
  # try(module.slurm_controller_instance.cloudsql_secret, null)
  cloudsql_secret = null

  controller_startup_scripts         = local.ghpc_startup_script_controller
  controller_startup_scripts_timeout = var.controller_startup_scripts_timeout
  compute_startup_scripts            = local.ghpc_startup_script_compute
  compute_startup_scripts_timeout    = var.compute_startup_scripts_timeout
  login_startup_scripts              = local.ghpc_startup_script_login
  login_startup_scripts_timeout      = var.login_startup_scripts_timeout

  enable_devel         = var.enable_devel
  enable_debug_logging = var.enable_debug_logging
  extra_logging_flags  = var.extra_logging_flags

  enable_bigquery_load = var.enable_bigquery_load
  epilog_scripts       = var.epilog_scripts
  prolog_scripts       = var.prolog_scripts

  disable_default_mounts = var.disable_default_mounts
  network_storage        = var.network_storage
  login_network_storage  = var.network_storage # TODO: reconsider duplication

  partitions  = values(module.slurm_partition)[*]
  nodeset     = values(module.slurm_nodeset)[*]
  nodeset_tpu = values(module.slurm_nodeset_tpu)[*]

  depends_on = [module.bucket]
}
",module,"module ""slurm_files"" {
  source = ""github.com/SchedMD/slurm-gcp.git//terraform/slurm_cluster/modules/slurm_files?ref=6.2.0""

  project_id         = var.project_id
  slurm_cluster_name = local.slurm_cluster_name
  bucket_dir         = var.bucket_dir
  bucket_name        = local.bucket_name

  slurmdbd_conf_tpl = var.slurmdbd_conf_tpl
  slurm_conf_tpl    = var.slurm_conf_tpl
  cgroup_conf_tpl   = var.cgroup_conf_tpl
  cloud_parameters  = var.cloud_parameters
  # TODO: resolve cyclic dependency and use 
  # try(module.slurm_controller_instance.cloudsql_secret, null)
  cloudsql_secret = null

  controller_startup_scripts         = local.ghpc_startup_script_controller
  controller_startup_scripts_timeout = var.controller_startup_scripts_timeout
  compute_startup_scripts            = local.ghpc_startup_script_compute
  compute_startup_scripts_timeout    = var.compute_startup_scripts_timeout
  login_startup_scripts              = local.ghpc_startup_script_login
  login_startup_scripts_timeout      = var.login_startup_scripts_timeout

  enable_devel         = var.enable_devel
  enable_debug_logging = var.enable_debug_logging
  extra_logging_flags  = var.extra_logging_flags

  enable_bigquery_load = var.enable_bigquery_load
  epilog_scripts       = var.epilog_scripts
  prolog_scripts       = var.prolog_scripts

  disable_default_mounts = var.disable_default_mounts
  network_storage        = var.network_storage
  login_network_storage  = var.login_network_storage

  partitions  = values(module.slurm_partition)[*]
  nodeset     = values(module.slurm_nodeset)[*]
  nodeset_tpu = values(module.slurm_nodeset_tpu)[*]

  depends_on = [module.bucket]
}
",module,122,,82199854e24ac10e7293605c5c4eae45748e8c99,6b26fd7c86bc829718789d9530cf861e1e130e21,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/82199854e24ac10e7293605c5c4eae45748e8c99/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf#L122,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/6b26fd7c86bc829718789d9530cf861e1e130e21/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf,2023-11-09 19:50:03+00:00,2023-11-14 01:21:30+00:00,2,1,1,1,0,0,1,0,0,0
https://github.com/uyuni-project/sumaform,1418,backend_modules/libvirt/base/main.tf,backend_modules/libvirt/base/main.tf,0,fix,"# WORKAROUND: temporary fix ""cannot write to stream: No space left on device"", which is in fact caused by a 404 (https://progress.opensuse.org/issues/95512)","opensuse153-ci-pr  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.opensuse.org""}/repositories/systemsmanagement:/sumaform:/images:/libvirt/images/opensuse153-ci-pr.x86_64.qcow2"" 
 # opensuse153o = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.opensuse.org""}/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2"" 
 # WORKAROUND: temporary fix ""cannot write to stream: No space left on device"", which is in fact caused by a 404 (https://progress.opensuse.org/issues/95512)","locals {
  images_used = var.use_shared_resources ? [] : var.images
  image_urls = {
    centos6o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.centos.org""}/centos/6/images/CentOS-6-x86_64-GenericCloud.qcow2""
    centos7      = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://github.com""}/moio/sumaform-images/releases/download/4.3.0/centos7.qcow2""
    centos7o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.centos.org""}/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2""
    centos8o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.centos.org""}/centos/8/x86_64/images/CentOS-8-GenericCloud-8.2.2004-20200611.2.x86_64.qcow2""
    opensuse152-ci-pr  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.opensuse.org""}/repositories/systemsmanagement:/sumaform:/images:/libvirt/images/opensuse152-ci-pr.x86_64.qcow2""
    opensuse152o = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.opensuse.org""}/distribution/leap/15.2/appliances/openSUSE-Leap-15.2-JeOS.x86_64-OpenStack-Cloud.qcow2""
    opensuse153-ci-pr  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.opensuse.org""}/repositories/systemsmanagement:/sumaform:/images:/libvirt/images/opensuse153-ci-pr.x86_64.qcow2""
    # opensuse153o = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.opensuse.org""}/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2""
    # WORKAROUND: temporary fix ""cannot write to stream: No space left on device"", which is in fact caused by a 404 (https://progress.opensuse.org/issues/95512)
    opensuse153o = ""https://www.mirrorservice.org/sites/download.opensuse.org/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2""
    sles15       = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles15.x86_64.qcow2""
    sles15o      = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-15-JeOS-GM/SLES15-JeOS.x86_64-15.0-OpenStack-Cloud-GM.qcow2""
    sles15sp1    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles15sp1.x86_64.qcow2""
    sles15sp1o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.suse.de""}/install/SLE-15-SP1-JeOS-QU4/SLES15-SP1-JeOS.x86_64-15.1-OpenStack-Cloud-QU4.qcow2""
    sles15sp2    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles15sp2.x86_64.qcow2""
    sles15sp2o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.suse.de""}/install/SLE-15-SP2-JeOS-GM/SLES15-SP2-JeOS.x86_64-15.2-OpenStack-Cloud-GM.qcow2""
    sles15sp3o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.suse.de""}/install/SLE-15-SP3-JeOS-GM/SLES15-SP3-JeOS.x86_64-15.3-OpenStack-Cloud-GM.qcow2""
    sles11sp4    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles11sp4.x86_64.qcow2""
    sles12       = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12.x86_64.qcow2""
    sles12sp1    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp1.x86_64.qcow2""
    sles12sp2    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp2.x86_64.qcow2""
    sles12sp3    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp3.x86_64.qcow2""
    sles12sp4    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp4.x86_64.qcow2""
    sles12sp4o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-12-SP4-JeOS-GM/SLES12-SP4-JeOS.x86_64-12.4-OpenStack-Cloud-GM.qcow2""
    sles12sp5o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.suse.de""}/install/SLE-12-SP5-JeOS-GM/SLES12-SP5-JeOS.x86_64-12.5-OpenStack-Cloud-GM.qcow2""
    ubuntu1604o  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud-images.ubuntu.com""}/xenial/current/xenial-server-cloudimg-amd64-disk1.img""
    ubuntu1804   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://github.com""}/moio/sumaform-images/releases/download/4.4.0/ubuntu1804.qcow2""
    ubuntu1804o  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud-images.ubuntu.com""}/bionic/current/bionic-server-cloudimg-amd64.img""
    ubuntu2004o  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud-images.ubuntu.com""}/focal/current/focal-server-cloudimg-amd64.img""
    debian9o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.debian.org""}/images/cloud/OpenStack/current-9/debian-9-openstack-amd64.qcow2""
    debian10o    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.debian.org""}/images/cloud/OpenStack/current-10/debian-10-openstack-amd64.qcow2""
  }
  pool               = lookup(var.provider_settings, ""pool"", ""default"")
  network_name       = lookup(var.provider_settings, ""network_name"", ""default"")
  bridge             = lookup(var.provider_settings, ""bridge"", null)
  additional_network = lookup(var.provider_settings, ""additional_network"", null)
}
",locals,"locals {
  images_used = var.use_shared_resources ? [] : var.images
  image_urls = {
    centos6o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.centos.org""}/centos/6/images/CentOS-6-x86_64-GenericCloud.qcow2""
    centos7      = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://github.com""}/moio/sumaform-images/releases/download/4.3.0/centos7.qcow2""
    centos7o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.centos.org""}/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2""
    centos8o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.centos.org""}/centos/8/x86_64/images/CentOS-8-GenericCloud-8.2.2004-20200611.2.x86_64.qcow2""
    opensuse152-ci-pr  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.opensuse.org""}/repositories/systemsmanagement:/sumaform:/images:/libvirt/images/opensuse152-ci-pr.x86_64.qcow2""
    opensuse152o = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.opensuse.org""}/distribution/leap/15.2/appliances/openSUSE-Leap-15.2-JeOS.x86_64-OpenStack-Cloud.qcow2""
    opensuse153-ci-pr  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.opensuse.org""}/repositories/systemsmanagement:/sumaform:/images:/libvirt/images/opensuse153-ci-pr.x86_64.qcow2""
    opensuse153o = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.opensuse.org""}/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2""
    sles15       = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles15.x86_64.qcow2""
    sles15o      = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-15-JeOS-GM/SLES15-JeOS.x86_64-15.0-OpenStack-Cloud-GM.qcow2""
    sles15sp1    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles15sp1.x86_64.qcow2""
    sles15sp1o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-15-SP1-JeOS-QU4/SLES15-SP1-JeOS.x86_64-15.1-OpenStack-Cloud-QU4.qcow2""
    sles15sp2    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles15sp2.x86_64.qcow2""
    sles15sp2o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-15-SP2-JeOS-GM/SLES15-SP2-JeOS.x86_64-15.2-OpenStack-Cloud-GM.qcow2""
    sles15sp3o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-15-SP3-JeOS-GM/SLES15-SP3-JeOS.x86_64-15.3-OpenStack-Cloud-GM.qcow2""
    sles11sp4    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles11sp4.x86_64.qcow2""
    sles12       = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12.x86_64.qcow2""
    sles12sp1    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp1.x86_64.qcow2""
    sles12sp2    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp2.x86_64.qcow2""
    sles12sp3    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp3.x86_64.qcow2""
    sles12sp4    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp4.x86_64.qcow2""
    sles12sp4o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-12-SP4-JeOS-GM/SLES12-SP4-JeOS.x86_64-12.4-OpenStack-Cloud-GM.qcow2""
    sles12sp5o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-12-SP5-JeOS-GM/SLES12-SP5-JeOS.x86_64-12.5-OpenStack-Cloud-GM.qcow2""
    ubuntu1604o  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud-images.ubuntu.com""}/xenial/current/xenial-server-cloudimg-amd64-disk1.img""
    ubuntu1804   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://github.com""}/moio/sumaform-images/releases/download/4.4.0/ubuntu1804.qcow2""
    ubuntu1804o  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud-images.ubuntu.com""}/bionic/current/bionic-server-cloudimg-amd64.img""
    ubuntu2004o  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud-images.ubuntu.com""}/focal/current/focal-server-cloudimg-amd64.img""
    debian9o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.debian.org""}/images/cloud/OpenStack/current-9/debian-9-openstack-amd64.qcow2""
    debian10o    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.debian.org""}/images/cloud/OpenStack/current-10/debian-10-openstack-amd64.qcow2""
  }
  pool               = lookup(var.provider_settings, ""pool"", ""default"")
  network_name       = lookup(var.provider_settings, ""network_name"", ""default"")
  bridge             = lookup(var.provider_settings, ""bridge"", null)
  additional_network = lookup(var.provider_settings, ""additional_network"", null)
}
",locals,13,,a79ce678f697c52365b7a5a485e482ef843c6bbe,5d343d967b407b5f1645e34090646066c7fe2fed,https://github.com/uyuni-project/sumaform/blob/a79ce678f697c52365b7a5a485e482ef843c6bbe/backend_modules/libvirt/base/main.tf#L13,https://github.com/uyuni-project/sumaform/blob/5d343d967b407b5f1645e34090646066c7fe2fed/backend_modules/libvirt/base/main.tf,2021-07-21 16:20:47+02:00,2021-08-10 12:11:54+02:00,2,1,1,0,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1531,blueprints/data-solutions/vertex-mlops/vertex.tf,blueprints/data-solutions/vertex-mlops/vertex.tf,0,fix,# Remove once terraform-provider-google/issues/9164 is fixed,# Remove once terraform-provider-google/issues/9164 is fixed,"resource ""google_notebooks_instance"" ""playground"" {
  for_each     = { for k, v in var.notebooks : k => v if v.type == ""USER_MANAGED"" }
  name         = ""${var.prefix}-${each.key}""
  location     = format(""%s-%s"", var.region, ""b"")
  machine_type = var.notebooks[each.key].machine_type
  project      = module.project.project_id

  container_image {
    repository = ""gcr.io/deeplearning-platform-release/base-cpu""
    tag        = ""latest""
  }

  install_gpu_driver = true
  boot_disk_type     = ""PD_SSD""
  boot_disk_size_gb  = 110
  disk_encryption    = try(local.service_encryption_keys.notebooks != null, false) ? ""CMEK"" : null
  kms_key            = try(local.service_encryption_keys.notebooks, null)

  no_public_ip    = var.notebooks[each.key].internal_ip_only
  no_proxy_access = false

  network = local.vpc
  subnet  = local.subnet

  instance_owners = try(tolist(var.notebooks[each.key].owner), null)
  service_account = module.service-account-notebook.email

  metadata = {
    notebook-disable-nbconvert = ""false""
    notebook-disable-downloads = ""false""
    notebook-disable-terminal  = ""false""
    notebook-disable-root      = ""true""
  }

  # Remove once terraform-provider-google/issues/9164 is fixed
  lifecycle {
    ignore_changes = [disk_encryption, kms_key]
  }

  #TODO Uncomment once terraform-provider-google/issues/9273 is fixed
  # tags = [""ssh""]
  depends_on = [
    google_project_iam_member.shared_vpc,
  ]
}
",resource,"resource ""google_notebooks_instance"" ""playground"" {
  for_each     = { for k, v in var.notebooks : k => v if v.type == ""USER_MANAGED"" }
  name         = ""${var.prefix}-${each.key}""
  location     = ""${var.region}-b""
  machine_type = var.notebooks[each.key].machine_type
  project      = module.project.project_id

  container_image {
    repository = ""gcr.io/deeplearning-platform-release/base-cpu""
    tag        = ""latest""
  }

  install_gpu_driver = true
  boot_disk_type     = ""PD_SSD""
  boot_disk_size_gb  = 110
  disk_encryption    = var.service_encryption_keys.notebooks != null ? ""CMEK"" : null
  kms_key            = var.service_encryption_keys.notebooks

  no_public_ip    = var.notebooks[each.key].internal_ip_only
  no_proxy_access = false

  network = local.vpc
  subnet  = local.subnet

  instance_owners = try(tolist(var.notebooks[each.key].owner), null)
  service_account = module.service-account-notebook.email
  service_account_scopes = [
    ""https://www.googleapis.com/auth/cloud-platform"",
    ""https://www.googleapis.com/auth/userinfo.email"",
  ]


  metadata = {
    notebook-disable-nbconvert = ""false""
    notebook-disable-downloads = ""false""
    notebook-disable-terminal  = ""false""
    notebook-disable-root      = ""true""
  }

  # Remove once terraform-provider-google/issues/9164 is fixed
  lifecycle {
    ignore_changes = [disk_encryption, kms_key]
  }

  #TODO Uncomment once terraform-provider-google/issues/9273 is fixed
  # tags = [""ssh""]
  depends_on = [
    google_project_iam_member.shared_vpc,
  ]
}
",resource,117,121.0,edf67fc5d040acfcd852a24a62d04acd2e4038f2,b902b1dab98fbe1d3cedc0b341838e827de5897b,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/edf67fc5d040acfcd852a24a62d04acd2e4038f2/blueprints/data-solutions/vertex-mlops/vertex.tf#L117,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b902b1dab98fbe1d3cedc0b341838e827de5897b/blueprints/data-solutions/vertex-mlops/vertex.tf#L121,2023-04-18 17:32:15+02:00,2024-02-13 07:40:31+01:00,3,0,1,1,1,0,0,0,0,0
https://github.com/alphagov/govuk-aws,136,terraform/projects/infra-security-groups/calculators-frontend.tf,terraform/projects/infra-security-groups/calculators-frontend.tf,0,# todo,# TODO: replace this with ingress from the frontend LBs when we build them.,# TODO: replace this with ingress from the frontend LBs when we build them.,"resource ""aws_security_group_rule"" ""allow_management_to_calculators-frontend_elb"" {
  type      = ""ingress""
  from_port = 443
  to_port   = 443
  protocol  = ""tcp""

  security_group_id        = ""${aws_security_group.calculators-frontend_elb.id}""
  source_security_group_id = ""${aws_security_group.management.id}""
}
",resource,the block associated got renamed or deleted,,47,,ffc6c7e1ab1ea8954c750636cab89d66e6f3f213,7fd13330f8d6238e108f76ce76a76a28a99caaaf,https://github.com/alphagov/govuk-aws/blob/ffc6c7e1ab1ea8954c750636cab89d66e6f3f213/terraform/projects/infra-security-groups/calculators-frontend.tf#L47,https://github.com/alphagov/govuk-aws/blob/7fd13330f8d6238e108f76ce76a76a28a99caaaf/terraform/projects/infra-security-groups/calculators-frontend.tf,2017-07-20 14:43:28+01:00,2018-01-02 17:41:32+00:00,3,1,0,1,0,1,1,0,0,0
https://github.com/kubernetes/k8s.io,280,infra/gcp/terraform/k8s-infra-oci-proxy-prod/network.tf,infra/gcp/terraform/modules/oci-proxy/network.tf,1,#todo,#TODO(ameukam): current the TF resource google_compute_global_address don't have,"#TODO(ameukam): current the TF resource google_compute_global_address don't have 
 #the value of IP in his attribute. But it's accessible with the data source: 
 #https://registry.terraform.io/providers/hashicorp/google/latest/docs/data-sources/compute_global_address","module ""lb-http"" {
  source  = ""GoogleCloudPlatform/lb-http/google//modules/serverless_negs""
  version = ""~> 6.2.0""

  project = google_project.project.project_id
  name    = local.project_id

  # ...
  backends = {
    default = {
      description = null
      groups = [
        for neg in google_compute_region_network_endpoint_group.oci-proxy :
        {
          group = neg.id
        }
      ]
      enable_cdn              = true
      security_policy         = null
      custom_request_headers  = null
      custom_response_headers = null

      iap_config = {
        enable               = false
        oauth2_client_id     = """"
        oauth2_client_secret = """"
      }

      log_config = {
        enable      = true
        sample_rate = ""1.0""
      }
    }
  }

  create_address      = false
  create_ipv6_address = false
  enable_ipv6         = true
  https_redirect      = true
  #TODO(ameukam): current the TF resource google_compute_global_address don't have
  #the value of IP in his attribute. But it's accessible with the data source:
  #https://registry.terraform.io/providers/hashicorp/google/latest/docs/data-sources/compute_global_address
  address      = data.google_compute_global_address.default_ipv4.address
  ipv6_address = data.google_compute_global_address.default_ipv6.address
  managed_ssl_certificate_domains = [
    local.domain
  ]
  random_certificate_suffix = true
  ssl                       = true
  use_ssl_certificates      = false
  security_policy = google_compute_security_policy.cloud-armor.self_link
}
",module,the block associated got renamed or deleted,,101,,5b8b742861f36e67e787ab5dc1f1df1fead4cb60,ee945e07553d74e12218d1057477e5d2f3fee8f7,https://github.com/kubernetes/k8s.io/blob/5b8b742861f36e67e787ab5dc1f1df1fead4cb60/infra/gcp/terraform/k8s-infra-oci-proxy-prod/network.tf#L101,https://github.com/kubernetes/k8s.io/blob/ee945e07553d74e12218d1057477e5d2f3fee8f7/infra/gcp/terraform/modules/oci-proxy/network.tf,2022-06-15 23:24:49-07:00,2023-05-09 22:52:37-07:00,9,1,0,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd,22,modules/secure-cd/main.tf,modules/secure-cd/main.tf,0,// todo,// TODO: customer config,"    cluster                 = ""${var.deploy_branch_clusters[qa].location}.${var.deploy_branch_clusters[qa].cluster}"" // TODO: customer config","resource ""google_binary_authorization_policy"" ""deployment_policy"" {
  project = var.project_id
  
  admission_whitelist_patterns {
    name_pattern = ""gcr.io/google_containers/*""
  }

  default_admission_rule {
    evaluation_mode  = ""ALWAYS_DENY""
    enforcement_mode = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
  }

  global_policy_evaluation_mode = ""ENABLE""

  // Prod Cluster Policy
  cluster_admission_rules {
    cluster                 = ""${var.deploy_branch_clusters[prod].location}.${var.deploy_branch_clusters[prod].cluster}"" // TODO: customer config
    evaluation_mode         = ""REQUIRE_ATTESTATION""
    enforcement_mode        = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
    require_attestations_by = [""security-attestor"", ""quality-attestor"", ""build-attestor""] //TODO
  }

  // QA Cluster Policy
  cluster_admission_rules {
    cluster                 = ""${var.deploy_branch_clusters[qa].location}.${var.deploy_branch_clusters[qa].cluster}"" // TODO: customer config
    evaluation_mode         = ""REQUIRE_ATTESTATION""
    enforcement_mode        = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
    require_attestations_by = [""security-attestor"", ""build-attestor""] //TODO
  }

  // Dev Cluster Policy
  cluster_admission_rules {
    cluster                 = ""${var.deploy_branch_clusters[dev].location}.${var.deploy_branch_clusters[dev].cluster}"" // TODO: customer config
    evaluation_mode         = ""REQUIRE_ATTESTATION""
    enforcement_mode        = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
    require_attestations_by = [""security-attestor""] //TODO
  }
}",resource,"resource ""google_binary_authorization_policy"" ""deployment_policy"" {
  for_each = var.deploy_branch_clusters
  project  = each.value.project_id
  
  admission_whitelist_patterns {
    name_pattern = ""gcr.io/google_containers/*""
  }

  default_admission_rule {
    evaluation_mode  = ""ALWAYS_DENY""
    enforcement_mode = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
  }

  global_policy_evaluation_mode = ""ENABLE""

  cluster_admission_rules {
    cluster                 = ""${each.value.location}.${each.value.cluster}"" // TODO: customer config
    evaluation_mode         = ""REQUIRE_ATTESTATION""
    enforcement_mode        = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
    require_attestations_by = each.value.attestations //TODO?
  }
}
",resource,73,,f9a05b119561ffabc9659fcfae59ac85a250d10e,6249c4ca90692e593bc0c7bc6d603580150ff255,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/f9a05b119561ffabc9659fcfae59ac85a250d10e/modules/secure-cd/main.tf#L73,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/6249c4ca90692e593bc0c7bc6d603580150ff255/modules/secure-cd/main.tf,2021-10-15 12:55:01-07:00,2021-10-26 17:18:47-05:00,3,1,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1588,modules/compute-vm/variables.tf,modules/compute-vm/variables.tf,0,# todo,# TODO: size can be null when source_type is attach,# TODO: size can be null when source_type is attach,"variable ""attached_disks"" {
  description = ""Additional disks, if options is null defaults will be used in its place. Source type is one of 'image' (zonal disks in vms and template), 'snapshot' (vm), 'existing', and null.""
  type = list(object({
    name        = string
    device_name = optional(string)
    # TODO: size can be null when source_type is attach
    size              = string
    snapshot_schedule = optional(string)
    source            = optional(string)
    source_type       = optional(string)
    options = optional(
      object({
        auto_delete  = optional(bool, false)
        mode         = optional(string, ""READ_WRITE"")
        replica_zone = optional(string)
        type         = optional(string, ""pd-balanced"")
      }),
      {
        auto_delete  = true
        mode         = ""READ_WRITE""
        replica_zone = null
        type         = ""pd-balanced""
      }
    )
  }))
  default = []
  validation {
    condition = length([
      for d in var.attached_disks : d if(
        d.source_type == null
        ||
        contains([""image"", ""snapshot"", ""attach""], coalesce(d.source_type, ""1""))
      )
    ]) == length(var.attached_disks)
    error_message = ""Source type must be one of 'image', 'snapshot', 'attach', null.""
  }

  validation {
    condition = length([
      for d in var.attached_disks : d if d.options == null ||
      d.options.mode == ""READ_WRITE"" || !d.options.auto_delete
    ]) == length(var.attached_disks)
    error_message = ""auto_delete can only be specified on READ_WRITE disks.""
  }
}
",variable,"variable ""attached_disks"" {
  description = ""Additional disks, if options is null defaults will be used in its place. Source type is one of 'image' (zonal disks in vms and template), 'snapshot' (vm), 'existing', and null.""
  type = list(object({
    name        = string
    device_name = optional(string)
    # TODO: size can be null when source_type is attach
    size              = string
    snapshot_schedule = optional(string)
    source            = optional(string)
    source_type       = optional(string)
    options = optional(
      object({
        auto_delete  = optional(bool, false)
        mode         = optional(string, ""READ_WRITE"")
        replica_zone = optional(string)
        type         = optional(string, ""pd-balanced"")
      }),
      {
        auto_delete  = true
        mode         = ""READ_WRITE""
        replica_zone = null
        type         = ""pd-balanced""
      }
    )
  }))
  default = []
  validation {
    condition = length([
      for d in var.attached_disks : d if(
        d.source_type == null
        ||
        contains([""image"", ""snapshot"", ""attach""], coalesce(d.source_type, ""1""))
      )
    ]) == length(var.attached_disks)
    error_message = ""Source type must be one of 'image', 'snapshot', 'attach', null.""
  }

  validation {
    condition = length([
      for d in var.attached_disks : d if d.options == null ||
      d.options.mode == ""READ_WRITE"" || !d.options.auto_delete
    ]) == length(var.attached_disks)
    error_message = ""auto_delete can only be specified on READ_WRITE disks.""
  }
}
",variable,42,42.0,b1679ad21aebd6de752526c19d8ba1ff2579bc98,da68d3cfc4a0a90ea7d3358bc32f7d3bad27e98e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b1679ad21aebd6de752526c19d8ba1ff2579bc98/modules/compute-vm/variables.tf#L42,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/da68d3cfc4a0a90ea7d3358bc32f7d3bad27e98e/modules/compute-vm/variables.tf#L42,2023-08-11 15:25:17+00:00,2024-03-04 10:12:11+01:00,7,0,1,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-sql-db,1,examples/mysql-backup-create-service-account/main.tf,examples/mysql-backup-create-service-account/main.tf,0,# todo,# TODO: don't use force_destroy for production this is just required for testing,# TODO: don't use force_destroy for production this is just required for testing,"resource ""google_storage_bucket"" ""backup"" {
  name     = ""${module.mysql.instance_name}-backup""
  location = ""us-central1""
  # TODO: don't use force_destroy for production this is just required for testing
  force_destroy = true
  project       = var.project_id
}
",resource,"resource ""google_storage_bucket"" ""backup"" {
  name     = ""${module.mysql.instance_name}-backup""
  location = ""us-central1""
  # TODO: don't use force_destroy for production this is just required for testing
  force_destroy = true
  project       = var.project_id
}
",resource,39,41.0,c51bf296e392fca246aae1c9ba4299a5a97ef274,af54237e8c6a00c73052615abf1d82df65d115f4,https://github.com/terraform-google-modules/terraform-google-sql-db/blob/c51bf296e392fca246aae1c9ba4299a5a97ef274/examples/mysql-backup-create-service-account/main.tf#L39,https://github.com/terraform-google-modules/terraform-google-sql-db/blob/af54237e8c6a00c73052615abf1d82df65d115f4/examples/mysql-backup-create-service-account/main.tf#L41,2022-05-13 10:59:53-05:00,2024-04-25 10:17:30-07:00,5,0,1,0,0,0,0,1,1,1
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,747,fast/stages/03-data-platform/dev/main.tf,fast/stages/3-data-platform/dev/main.tf,1,# todo,# TODO: align example variable,# TODO: align example variable,"module ""data-platform"" {
  source             = ""../../../../examples/data-solutions/data-platform-foundations""
  billing_account_id = var.billing_account.id
  composer_config    = var.composer_config
  data_force_destroy = var.data_force_destroy
  folder_id          = var.folder_ids.data-platform
  groups             = var.groups
  network_config = {
    host_project      = var.host_project_ids.dev-spoke-0
    network_self_link = var.vpc_self_links.dev-spoke-0
    subnet_self_links = {
      load           = var.subnet_self_links.dev-spoke-0[""europe-west1/dev-dataplatform-ew1""]
      transformation = var.subnet_self_links.dev-spoke-0[""europe-west1/dev-dataplatform-ew1""]
      orchestration  = var.subnet_self_links.dev-spoke-0[""europe-west1/dev-dataplatform-ew1""]
    }
    # TODO: align example variable
    composer_ip_ranges = {
      cloudsql   = var.network_config_composer.cloudsql_range
      gke_master = var.network_config_composer.gke_master_range
      web_server = var.network_config_composer.web_server_range
    }
    composer_secondary_ranges = {
      pods     = var.network_config_composer.gke_pods_name
      services = var.network_config_composer.gke_services_name
    }
  }
  organization_domain     = var.organization.domain
  prefix                  = var.prefix
  project_services        = var.project_services
  region                  = var.region
  service_encryption_keys = var.service_encryption_keys
}
",module,"module ""data-platform"" {
  source              = ""../../../../blueprints/data-solutions/data-platform-foundations""
  composer_config     = var.composer_config
  deletion_protection = var.deletion_protection
  data_catalog_tags   = var.data_catalog_tags
  project_config = {
    billing_account_id = var.billing_account.id
    project_create     = var.project_config.project_create
    parent             = var.folder_ids.data-platform-dev
    project_ids        = var.project_config.project_ids
  }
  groups   = var.groups_dp
  location = var.location
  network_config = {
    host_project      = var.host_project_ids.dev-spoke-0
    network_self_link = var.vpc_self_links.dev-spoke-0
    subnet_self_links = {
      load           = var.subnet_self_links.dev-spoke-0[""europe-west1/dev-dataplatform-ew1""]
      transformation = var.subnet_self_links.dev-spoke-0[""europe-west1/dev-dataplatform-ew1""]
      orchestration  = var.subnet_self_links.dev-spoke-0[""europe-west1/dev-dataplatform-ew1""]
    }
    # TODO: align example variable
    composer_ip_ranges = {
      cloudsql   = var.network_config_composer.cloudsql_range
      gke_master = var.network_config_composer.gke_master_range
    }
    composer_secondary_ranges = {
      pods     = var.network_config_composer.gke_pods_name
      services = var.network_config_composer.gke_services_name
    }
  }
  organization_domain     = var.organization.domain
  prefix                  = ""${var.prefix}-dev-dp""
  project_services        = var.project_services
  project_suffix          = var.project_suffix
  region                  = var.region
  service_encryption_keys = var.service_encryption_keys
}
",module,34,40.0,c5fa5d62e4a87fdc631e6e9426d6395373d81cf1,208902c8da28d7f013234dd508bea1975931cf13,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c5fa5d62e4a87fdc631e6e9426d6395373d81cf1/fast/stages/03-data-platform/dev/main.tf#L34,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/208902c8da28d7f013234dd508bea1975931cf13/fast/stages/3-data-platform/dev/main.tf#L40,2022-02-16 14:12:39+01:00,2024-01-20 08:49:46+01:00,13,0,0,1,0,0,1,0,0,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,294,kube.example.tf,kube.example.tf,0,fix,"#  If you are going to use Rancher addons for instance, it's always a good idea to fix the kube version to latest - 0.01,","### The following values are entirely optional (and can be removed from this if unused)  
 # You can refine a base domain name to be use in this form of nodename.base_domain for setting the reserve dns inside Hetzner 
 # base_domain = ""mycluster.example.com""  
 # Cluster Autoscaler 
 # Providing at least one map for the array enables the cluster autoscaler feature, default is disabled 
 # Please note that the autoscaler should not be used with initial_k3s_channel < ""v1.25"". So ideally lock it to ""v1.25"". 
 # * Example below: 
 # autoscaler_nodepools = [ 
 #   { 
 #     name        = ""autoscaler"" 
 #     server_type = ""cpx21"" # must be same or better than the control_plane server type (regarding disk size)! 
 #     location    = ""fsn1"" 
 #     min_nodes   = 0 
 #     max_nodes   = 5 
 #   } 
 # ]  
 # Enable etcd snapshot backups to S3 storage. 
 # Just provide a map with the needed settings (according to your S3 storage provider) and backups to S3 will 
 # be enabled (with the default settings for etcd snapshots). 
 # Cloudflare's R2 offers 10GB, 10 million reads and 1 million writes per month for free. 
 # For proper context, have a look at https://docs.k3s.io/backup-restore. 
 # etcd_s3_backup = { 
 #   etcd-s3-endpoint        = ""xxxx.r2.cloudflarestorage.com"" 
 #   etcd-s3-access-key      = ""<access-key>"" 
 #   etcd-s3-secret-key      = ""<secret-key>"" 
 #   etcd-s3-bucket          = ""k3s-etcd-snapshots"" 
 # }  
 # To use local storage on the nodes, you can enable Longhorn, default is ""false"". 
 # See a full recap on how to configure agent nodepools for longhorn here https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/discussions/373#discussioncomment-3983159 
 # enable_longhorn = true  
 # By default, longhorn is pulled from https://charts.longhorn.io. 
 # If you need a version of longhorn which assures compatibility with rancher you can set this variable to https://charts.rancher.io. 
 # longhorn_repository = ""https://charts.rancher.io""  
 # The namespace for longhorn deployment, default is ""longhorn-system"". 
 # longhorn_namespace = ""longhorn-system""  
 # The file system type for Longhorn, if enabled (ext4 is the default, otherwise you can choose xfs). 
 # longhorn_fstype = ""xfs""  
 # how many replica volumes should longhorn create (default is 3). 
 # longhorn_replica_count = 1  
 # When you enable Longhorn, you can go with the default settings and just modify the above two variables OR you can add a longhorn_values variable 
 # with all needed helm values, see towards the end of the file in the advanced section. 
 # If that file is present, the system will use it during the deploy, if not it will use the default values with the two variable above that can be customized. 
 # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.  
 # Also, you can choose to use a Hetzner volume with Longhorn. By default, it will use the nodes own storage space, but if you add an attribute of 
 # longhorn_volume_size ( not a variable, just a possible agent nodepool attribute) with a value between 10 and 10000 GB to your agent nodepool definition, it will create and use the volume in question. 
 # See the agent nodepool section for an example of how to do that.  
 # To disable Hetzner CSI storage, you can set the following to ""true"", default is ""false"". 
 # disable_hetzner_csi = true  
 # If you want to use a specific Hetzner CCM and CSI version, set them below; otherwise, leave them as-is for the latest versions. 
 # hetzner_ccm_version = """" 
 # hetzner_csi_version = """"  
 # If you want to specify the Kured version, set it below - otherwise it'll use the latest version available. 
 # kured_version = """"  
 # If you want to specify what time Kured starts restarting nodes, set it below. Default is 3AM. 
 # kured_start_time = """"  
 # If you want to specify what time Kured stops restarting nodes, set it below. Default is 8AM. 
 # kured_end_time = """"  
 # If you want to specify what timezone Kured uses, set it below. Default is Local. 
 # kured_time_zone = """"  
 # If you want to enable the Nginx ingress controller (https://kubernetes.github.io/ingress-nginx/) instead of Traefik, you can set this to ""nginx"". Default is ""traefik"". 
 # By the default we load optimal Traefik and Nginx ingress controller config for Hetzner, however you may need to tweak it to your needs, so to do, 
 # we allow you to add a traefik_values and nginx_values, see towards the end of this file in the advanced section. 
 # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration. 
 # If you want to disable both controllers set this to ""none"" 
 # ingress_controller = ""nginx""  
 # You can change the number of replicas for selected ingress controller here. The default 0 means autoselecting based on number of agent nodes (1 node = 1 replica, 2 nodes = 2 replicas, 3+ nodes = 3 replicas) 
 # ingress_replica_count = 1  
 # Use the klipperLB (similar to metalLB), instead of the default Hetzner one, that has an advantage of dropping the cost of the setup. 
 # Automatically ""true"" in the case of single node cluster (as it does not make sense to use the Hetzner LB in that situation). 
 # It can work with any ingress controller that you choose to deploy. 
 # Please note that because the klipperLB points to all nodes, we automatically allow scheduling on the control plane when it is active. 
 # enable_klipper_metal_lb = ""true""  
 # If you want to configure additional arguments for traefik, enter them here as a list and in the form of traefik CLI arguments; see https://doc.traefik.io/traefik/reference/static-configuration/cli/ 
 # They are the options that go into the additionalArguments section of the Traefik helm values file. 
 # Example: traefik_additional_options = [""--log.level=DEBUG"", ""--tracing=true""] 
 # traefik_additional_options = []  
 # By default traefik is configured to redirect http traffic to https, you can set this to ""false"" to disable the redirection. 
 # traefik_redirect_to_https = false  
 # If you want to disable the metric server set this to ""false"". Default is ""true"". 
 # enable_metrics_server = false  
 # If you want to allow non-control-plane workloads to run on the control-plane nodes, set this to ""true"". The default is ""false"". 
 # True by default for single node clusters, and when enable_klipper_metal_lb is true. In those cases, the value below will be ignored. 
 # allow_scheduling_on_control_plane = true  
 # If you want to disable the automatic upgrade of k3s, you can set below to ""false"". 
 # Ideally, keep it on, to always have the latest Kubernetes version, but lock the initial_k3s_channel to a kube major version, 
 # of your choice, like v1.25 or v1.26. That way you get the best of both worlds without the breaking changes risk. 
 # For production use, always use an HA setup with at least 3 control-plane nodes and 2 agents, and keep this on for maximum security.  
 # The default is ""true"" (in HA setup i.e. at least 3 control plane nodes & 2 agents, just keep it enabled since it works flawlessly). 
 # automatically_upgrade_k3s = false  
 # The default is ""true"" (in HA setup it works wonderfully well, with automatic roll-back to the previous snapshot in case of an issue). 
 # IMPORTANT! For non-HA clusters i.e. when the number of control-plane nodes is < 3, you have to turn it off. 
 # automatically_upgrade_os = false  
 # If you need more control over kured and the reboot behaviour, you can pass additional options to kured. 
 # For example limiting reboots to certain timeframes. For all options see: https://kured.dev/docs/configuration/ 
 # The default options are: `--reboot-command=/usr/bin/systemctl reboot --pre-reboot-node-labels=kured=rebooting --post-reboot-node-labels=kured=done --period=5m` 
 # Defaults can be overridden by using the same key. 
 # kured_options = { 
 #   ""reboot-days"": ""su"" 
 #   ""start-time"": ""3am"" 
 #   ""end-time"": ""8am"" 
 # }  
 # Allows you to specify either stable, latest, testing or supported minor versions. 
 # see https://rancher.com/docs/k3s/latest/en/upgrades/basic/ and https://update.k3s.io/v1-release/channels 
 #  If you are going to use Rancher addons for instance, it's always a good idea to fix the kube version to latest - 0.01, 
 # at the time of writing the latest is v1.26, so setting the value below to ""v1.25"" will insure maximum compatibility with Rancher, Longhorn and so on! 
 # The default is ""v1.25"". 
 # initial_k3s_channel = ""stable""  
 # The cluster name, by default ""k3s"" 
 # cluster_name = """"  
 # Whether to use the cluster name in the node name, in the form of {cluster_name}-{nodepool_name}, the default is ""true"". 
 # use_cluster_name_in_node_name = false  
 # Extra k3s registries. This is useful if you have private registries and you 
 # want to pull images without additional secrets. 
 # registries.yaml file docs: https://docs.k3s.io/installation/private-registry 
 /* k3s_registries = <<-EOT 
 mirrors: 
 hub.my_registry.com: 
 endpoint: 
 - ""hub.my_registry.com"" 
 configs: 
 hub.my_registry.com: 
 auth: 
 username: username 
 password: password 
 EOT */  
 # If you want to allow all outbound traffic you can set this to ""false"". Default is ""true"". 
 # restrict_outbound_traffic = false  
 # Adding extra firewall rules, like opening a port 
 # More info on the format here https://registry.terraform.io/providers/hetznercloud/hcloud/latest/docs/resources/firewall 
 # extra_firewall_rules = [ 
 #   # For Postgres 
 #   { 
 #     direction       = ""in"" 
 #     protocol        = ""tcp"" 
 #     port            = ""5432"" 
 #     source_ips      = [""0.0.0.0/0"", ""::/0""] 
 #     destination_ips = [] # Won't be used for this rule 
 #   }, 
 #   # To Allow ArgoCD access to resources via SSH 
 #   { 
 #     direction       = ""out"" 
 #     protocol        = ""tcp"" 
 #     port            = ""22"" 
 #     source_ips      = [] # Won't be used for this rule 
 #     destination_ips = [""0.0.0.0/0"", ""::/0""] 
 #   } 
 # ]  
 # If you want to configure a different CNI for k3s, use this flag 
 # possible values: flannel (Default), calico, and cilium 
 # As for Cilium, we allow infinite configurations via helm values, please check the CNI section of the readme over at https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/#cni. 
 # Also, see the cilium_values at towards the end of this file, in the advanced section. 
 # cni_plugin = ""cilium""  
 # If you want to disable the k3s default network policy controller, use this flag! 
 # Both Calico and Ciliun cni_plugin values override this value to true automatically, the default is ""false"". 
 # disable_network_policy = true  
 # If you want to disable the automatic use of placement group ""spread"". See https://docs.hetzner.com/cloud/placement-groups/overview/ 
 # That may be useful if you need to deploy more than 500 nodes! The default is ""false"". 
 # placement_group_disable = true  
 # By default, we allow ICMP ping in to the nodes, to check for liveness for instance. If you do not want to allow that, you can. Just set this flag to true (false by default). 
 # block_icmp_ping_in = true  
 # You can enable cert-manager (installed by Helm behind the scenes) with the following flag, the default is ""false"". 
 # enable_cert_manager = true  
 # By default we use a known good mirror to download the needed OpenSUSE, but for instance, it may not be available in US-East location, in which case, 
 # you can find a working mirror for you at https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2.mirrorlist, 
 # Or use the value below to automatically select an optimal mirror (by default we have it fixed to one german mirror that we know works for most locations) 
 # opensuse_microos_mirror_link = ""https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2""  
 # IP Addresses to use for the DNS Servers, set to an empty list to use the ones provided by Hetzner, defaults to [""1.1.1.1"", "" 1.0.0.1"", ""8.8.8.8""]. 
 # For rancher installs, best to leave it as default. 
 # dns_servers = []  
 # When this is enabled, rather than the first node, all external traffic will be routed via a control-plane loadbalancer, allowing for high availability. 
 # The default is false. 
 # use_control_plane_lb = true  
 # Let's say you are not using the control plane LB solution above, and still want to have one hostname point to all your control-plane nodes. 
 # You could create multiple A records of to let's say cp.cluster.my.org pointing to all of your control-plane nodes ips. 
 # In which case, you need to define that hostname in the k3s TLS-SANs config to allow connection through it. It can be hostnames or IP addresses. 
 # additional_tls_sans = [""cp.cluster.my.org""]  
 # Oftentimes, you need to communicate to the cluster from inside the cluster itself, in which case it is important to set this value, as it will configure the hostname 
 # at the load balancer level, and will save you from many slows downs when initiating communications from inside. Later on, you can point your DNS to the IP given 
 # to the LB. And if you have other services pointing to it, you are also free to create CNAMES to point to it, or whatever you see fit. 
 # If set, it will apply to either ingress controllers, Traefik or Ingress-Nginx. 
 # lb_hostname = """"  
 # You can enable Rancher (installed by Helm behind the scenes) with the following flag, the default is ""false"". 
 # When Rancher is enabled, it automatically installs cert-manager too, and it uses rancher's own self-signed certificates. 
 # See for options https://rancher.com/docs/rancher/v2.0-v2.4/en/installation/resources/advanced/helm2/helm-rancher/#choose-your-ssl-configuration 
 # The easiest thing is to leave everything as is (using the default rancher self-signed certificate) and put Cloudflare in front of it. 
 # As for the number of replicas, by default it is set to the numbe of control plane nodes. 
 # You can customized all of the above by adding a rancher_values variable see at the end of this file in the advanced section. 
 # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration. 
 # IMPORTANT: Rancher's install is quite memory intensive, you will require at least 4GB if RAM, meaning cx21 server type (for your control plane). 
 # ALSO, in order for Rancher to successfully deploy, you have to set the ""rancher_hostname"". 
 # enable_rancher = true  
 # If using Rancher you can set the Rancher hostname, it must be unique hostname even if you do not use it. 
 # If not pointing the DNS, you can just port-forward locally via kubectl to get access to the dashboard. 
 # If you already set the lb_hostname above and are using a Hetzner LB, you do not need to set this one, as it will be used by default. 
 # But if you set this one explicitly, it will have preference over the lb_hostname in rancher settings. 
 # rancher_hostname = ""rancher.xyz.dev""  
 # When Rancher is deployed, by default is uses the ""latest"" channel. But this can be customized. 
 # The allowed values are ""stable"" or ""latest"". 
 # rancher_install_channel = ""stable""  
 # Finally, you can specify a bootstrap-password for your rancher instance. Minimum 48 characters long! 
 # If you leave empty, one will be generated for you. 
 # (Can be used by another rancher2 provider to continue setup of rancher outside this module.) 
 # rancher_bootstrap_password = """"  
 # Separate from the above Rancher config (only use one or the other). You can import this cluster directly on an 
 # an already active Rancher install. By clicking ""import cluster"" choosing ""generic"", giving it a name and pasting 
 # the cluster registration url below. However, you can also ignore that and apply the url via kubectl as instructed 
 # by Rancher in the wizard, and that would register your cluster too. 
 # More information about the registration can be found here https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/registered-clusters/ 
 # rancher_registration_manifest_url = ""https://rancher.xyz.dev/v3/import/xxxxxxxxxxxxxxxxxxYYYYYYYYYYYYYYYYYYYzzzzzzzzzzzzzzzzzzzzz.yaml""  
 # Extra values that will be passed to the `extra-manifests/kustomization.yaml.tpl` if its present. 
 # extra_kustomize_parameters={}  
 # It is best practice to turn this off, but for backwards compatibility it is set to ""true"" by default. 
 # See https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/issues/349 
 # When ""false"". The kubeconfig file can instead be created by executing: ""terraform output --raw kubeconfig > cluster_kubeconfig.yaml"" 
 # Always be careful to not commit this file! 
 # create_kubeconfig = false  
 # Don't create the kustomize backup. This can be helpful for automation. 
 # create_kustomization = false  
 ### ADVANCED - Custom helm values for packages above (search _values if you want to located where those are mentioned upper in this file) 
 #  Inside the _values variable below are examples, up to you to find out the best helm values possible, we do not provide support for customized helm values. 
 # Please understand that the indentation is very important, inside the EOTs, as those are proper yaml helm values. 
 # We advise you to use the default values, and only change them if you know what you are doing!  
 # Cilium, all Cilium helm values can be found at https://github.com/cilium/cilium/blob/master/install/kubernetes/cilium/values.yaml 
 # The following is an example, please note that the current indentation inside the EOT is important. 
 /*   cilium_values = <<EOT 
 ipam: 
 mode: kubernetes 
 devices: ""eth1"" 
 k8s: 
 requireIPv4PodCIDR: true 
 kubeProxyReplacement: strict 
 EOT */  
 # Cert manager, all cert-manager helm values can be found at https://github.com/cert-manager/cert-manager/blob/master/deploy/charts/cert-manager/values.yaml 
 # The following is an example, please note that the current indentation inside the EOT is important. 
 /*   cert_manager_values = <<EOT 
 installCRDs: true 
 replicaCount: 3 
 webhook: 
 replicaCount: 3 
 cainjector: 
 replicaCount: 3 
 EOT */  
 # Longhorn, all Longhorn helm values can be found at https://github.com/longhorn/longhorn/blob/master/chart/values.yaml 
 # The following is an example, please note that the current indentation inside the EOT is important. 
 /*   longhorn_values = <<EOT 
 defaultSettings: 
 defaultDataPath: /var/longhorn 
 persistence: 
 defaultFsType: ext4 
 defaultClassReplicaCount: 3 
 defaultClass: true 
 EOT */  
 # Nginx, all Nginx helm values can be found at https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml 
 # You can also have a look at https://kubernetes.github.io/ingress-nginx/, to understand how it works, and all the options at your disposal. 
 # The following is an example, please note that the current indentation inside the EOT is important. 
 /*   nginx_ingress_values = <<EOT 
 controller: 
 watchIngressWithoutClass: ""true"" 
 kind: ""DaemonSet"" 
 config: 
 ""use-forwarded-headers"": ""true"" 
 ""compute-full-forwarded-for"": ""true"" 
 ""use-proxy-protocol"": ""true"" 
 service: 
 annotations: 
 ""load-balancer.hetzner.cloud/name"": ""k3s"" 
 ""load-balancer.hetzner.cloud/use-private-ip"": ""true"" 
 ""load-balancer.hetzner.cloud/disable-private-ingress"": ""true"" 
 ""load-balancer.hetzner.cloud/location"": ""nbg1"" 
 ""load-balancer.hetzner.cloud/type"": ""lb11"" 
 ""load-balancer.hetzner.cloud/uses-proxyprotocol"": ""true"" 
 EOT */  
 # Rancher, all Rancher helm values can be found at https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/chart-options/ 
 # The following is an example, please note that the current indentation inside the EOT is important. 
 /*   rancher_values = <<EOT 
 ingress: 
 tls: 
 source: ""rancher"" 
 hostname: ""rancher.example.com"" 
 replicas: 1 
 bootstrapPassword: ""supermario"" 
 EOT */ ","module ""kube-hetzner"" {
  providers = {
    hcloud = hcloud
  }
  hcloud_token = local.hcloud_token

  # Then fill or edit the below values. Only the first values starting with a * are obligatory; the rest can remain with their default values, or you
  # could adapt them to your needs.

  # * For local dev, path to the git repo
  # source = ""../../kube-hetzner/""
  # If you want to use the latest master branch
  # source = ""github.com/kube-hetzner/terraform-hcloud-kube-hetzner""
  # For normal use, this is the path to the terraform registry
  source = ""kube-hetzner/kube-hetzner/hcloud""

  # You can optionally specify a version number
  # version = ""1.2.0""

  # Note that some values, notably ""location"" and ""public_key"" have no effect after initializing the cluster.
  # This is to keep Terraform from re-provisioning all nodes at once, which would lose data. If you want to update
  # those, you should instead change the value here and manually re-provision each node. Grep for ""lifecycle"".

  # Customize the SSH port (by default 22)
  # ssh_port = 2222

  # * Your ssh public key
  ssh_public_key = file(""~/.ssh/id_rsa.pub"")
  # * Your private key must be ""ssh_private_key = null"" when you want to use ssh-agent for a Yubikey-like device authentification or an SSH key-pair with a passphrase.
  # For more details on SSH see https://github.com/kube-hetzner/kube-hetzner/blob/master/docs/ssh.md
  ssh_private_key = file(""~/.ssh/id_rsa"")
  # You can add additional SSH public Keys to grant other team members root access to your cluster nodes.
  # ssh_additional_public_keys = []

  # You can also add additional SSH public Keys which are saved in the hetzner cloud by a label.
  # See https://docs.hetzner.cloud/#label-selector
  # ssh_hcloud_key_label = ""role=admin""

  # If you want to use an ssh key that is already registered within hetzner cloud, you can pass its id.
  # If no id is passed, a new ssh key will be registered within hetzner cloud.
  # It is important that exactly this key is passed via `ssh_public_key` & `ssh_private_key` vars.
  # hcloud_ssh_key_id = """"

  # These can be customized, or left with the default values
  # * For Hetzner locations see https://docs.hetzner.com/general/others/data-centers-and-connection/
  network_region = ""eu-central"" # change to `us-east` if location is ash

  # For the control planes, at least three nodes are the minimum for HA. Otherwise, you need to turn off the automatic upgrades (see README).
  # **It must always be an ODD number, never even!** Search the internet for ""splitbrain problem with etcd"" or see https://rancher.com/docs/k3s/latest/en/installation/ha-embedded/
  # For instance, one is ok (non-HA), two is not ok, and three is ok (becomes HA). It does not matter if they are in the same nodepool or not! So they can be in different locations and of various types.

  # Of course, you can choose any number of nodepools you want, with the location you want. The only constraint on the location is that you need to stay in the same network region, Europe, or the US.
  # For the server type, the minimum instance supported is cpx11 (just a few cents more than cx11); see https://www.hetzner.com/cloud.

  # IMPORTANT: Before you create your cluster, you can do anything you want with the nodepools, but you need at least one of each, control plane and agent.
  # Once the cluster is up and running, you can change nodepool count and even set it to 0 (in the case of the first control-plane nodepool, the minimum is 1).
  # You can also rename it (if the count is 0), but do not remove a nodepool from the list.

  # The only nodepools that are safe to remove from the list are at the end. That is due to how subnets and IPs get allocated (FILO).
  # You can, however, freely add other nodepools at the end of each list if you want. The maximum number of nodepools you can create combined for both lists is 255.
  # Also, before decreasing the count of any nodepools to 0, it's essential to drain and cordon the nodes in question. Otherwise, it will leave your cluster in a bad state.

  # Before initializing the cluster, you can change all parameters and add or remove any nodepools. You need at least one nodepool of each kind, control plane, and agent.
  # The nodepool names are entirely arbitrary, you can choose whatever you want, but no special characters or underscore, and they must be unique; only alphanumeric characters and dashes are allowed.

  # If you want to have a single node cluster, have one control plane nodepools with a count of 1, and one agent nodepool with a count of 0.

  # Please note that changing labels and taints after the first run will have no effect. If needed, you can do that through Kubernetes directly.

  # * Example below:

  control_plane_nodepools = [
    {
      name        = ""control-plane-fsn1"",
      server_type = ""cpx11"",
      location    = ""fsn1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""control-plane-nbg1"",
      server_type = ""cpx11"",
      location    = ""nbg1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""control-plane-hel1"",
      server_type = ""cpx11"",
      location    = ""hel1"",
      labels      = [],
      taints      = [],
      count       = 1
    }
  ]

  agent_nodepools = [
    {
      name        = ""agent-small"",
      server_type = ""cpx11"",
      location    = ""fsn1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""agent-large"",
      server_type = ""cpx21"",
      location    = ""nbg1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""storage"",
      server_type = ""cpx21"",
      location    = ""fsn1"",
      # Fully optional, just a demo.
      labels      = [
        ""node.kubernetes.io/server-usage=storage""
      ],
      taints      = [],
      count       = 1
      # In the case of using Longhorn, you can use Hetzner volumes instead of using the node's own storage by specifying a value from 10 to 10000 (in GB)
      # It will create one volume per node in the nodepool, and configure Longhorn to use them.
      # Something worth noting is that Volume storage is slower than node storage, which is achieved by not mentioning longhorn_volume_size or setting it to 0.
      # So for something like DBs, you definitely want node storage, for other things like backups, volume storage is fine, and cheaper.
      # longhorn_volume_size = 20
    }
  ]
  # Add custom control plane configuration options here.
  # E.g to enable monitoring for etcd, proxy etc:
  # control_planes_custom_config = {
  #  etcd-expose-metrics = true,
  #  kube-controller-manager-arg = ""bind-address=0.0.0.0"",
  #  kube-proxy-arg =""metrics-bind-address=0.0.0.0"",
  #  kube-scheduler-arg = ""bind-address=0.0.0.0"",
  # }

  # * LB location and type, the latter will depend on how much load you want it to handle, see https://www.hetzner.com/cloud/load-balancer
  load_balancer_type     = ""lb11""
  load_balancer_location = ""fsn1""

  ### The following values are entirely optional (and can be removed from this if unused)

  # You can refine a base domain name to be use in this form of nodename.base_domain for setting the reserve dns inside Hetzner
  # base_domain = ""mycluster.example.com""

  # Cluster Autoscaler
  # Providing at least one map for the array enables the cluster autoscaler feature, default is disabled
  # Please note that the autoscaler should not be used with initial_k3s_channel < ""v1.25"". So ideally lock it to ""v1.25"".
  # * Example below:
  # autoscaler_nodepools = [
  #   {
  #     name        = ""autoscaler""
  #     server_type = ""cpx21"" # must be same or better than the control_plane server type (regarding disk size)!
  #     location    = ""fsn1""
  #     min_nodes   = 0
  #     max_nodes   = 5
  #   }
  # ]

  # Enable etcd snapshot backups to S3 storage.
  # Just provide a map with the needed settings (according to your S3 storage provider) and backups to S3 will
  # be enabled (with the default settings for etcd snapshots).
  # Cloudflare's R2 offers 10GB, 10 million reads and 1 million writes per month for free.
  # For proper context, have a look at https://docs.k3s.io/backup-restore.
  # etcd_s3_backup = {
  #   etcd-s3-endpoint        = ""xxxx.r2.cloudflarestorage.com""
  #   etcd-s3-access-key      = ""<access-key>""
  #   etcd-s3-secret-key      = ""<secret-key>""
  #   etcd-s3-bucket          = ""k3s-etcd-snapshots""
  # }

  # To use local storage on the nodes, you can enable Longhorn, default is ""false"".
  # See a full recap on how to configure agent nodepools for longhorn here https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/discussions/373#discussioncomment-3983159
  # enable_longhorn = true

  # By default, longhorn is pulled from https://charts.longhorn.io.
  # If you need a version of longhorn which assures compatibility with rancher you can set this variable to https://charts.rancher.io. 
  # longhorn_repository = ""https://charts.rancher.io""

  # The namespace for longhorn deployment, default is ""longhorn-system"".
  # longhorn_namespace = ""longhorn-system""

  # The file system type for Longhorn, if enabled (ext4 is the default, otherwise you can choose xfs).
  # longhorn_fstype = ""xfs""

  # how many replica volumes should longhorn create (default is 3).
  # longhorn_replica_count = 1

  # When you enable Longhorn, you can go with the default settings and just modify the above two variables OR you can add a longhorn_values variable
  # with all needed helm values, see towards the end of the file in the advanced section.
  # If that file is present, the system will use it during the deploy, if not it will use the default values with the two variable above that can be customized.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.

  # Also, you can choose to use a Hetzner volume with Longhorn. By default, it will use the nodes own storage space, but if you add an attribute of
  # longhorn_volume_size ( not a variable, just a possible agent nodepool attribute) with a value between 10 and 10000 GB to your agent nodepool definition, it will create and use the volume in question.
  # See the agent nodepool section for an example of how to do that.

  # To disable Hetzner CSI storage, you can set the following to ""true"", default is ""false"".
  # disable_hetzner_csi = true

  # If you want to use a specific Hetzner CCM and CSI version, set them below; otherwise, leave them as-is for the latest versions.
  # hetzner_ccm_version = """"
  # hetzner_csi_version = """"

  # If you want to specify the Kured version, set it below - otherwise it'll use the latest version available.
  # kured_version = """"

  # If you want to specify what time Kured starts restarting nodes, set it below. Default is 3AM.
  # kured_start_time = """"

  # If you want to specify what time Kured stops restarting nodes, set it below. Default is 8AM.
  # kured_end_time = """"

  # If you want to specify what timezone Kured uses, set it below. Default is Local.
  # kured_time_zone = """"

  # If you want to enable the Nginx ingress controller (https://kubernetes.github.io/ingress-nginx/) instead of Traefik, you can set this to ""nginx"". Default is ""traefik"".
  # By the default we load optimal Traefik and Nginx ingress controller config for Hetzner, however you may need to tweak it to your needs, so to do,
  # we allow you to add a traefik_values and nginx_values, see towards the end of this file in the advanced section.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.
  # If you want to disable both controllers set this to ""none""
  # ingress_controller = ""nginx""

  # You can change the number of replicas for selected ingress controller here. The default 0 means autoselecting based on number of agent nodes (1 node = 1 replica, 2 nodes = 2 replicas, 3+ nodes = 3 replicas)
  # ingress_replica_count = 1

  # Use the klipperLB (similar to metalLB), instead of the default Hetzner one, that has an advantage of dropping the cost of the setup.
  # Automatically ""true"" in the case of single node cluster (as it does not make sense to use the Hetzner LB in that situation).
  # It can work with any ingress controller that you choose to deploy.
  # Please note that because the klipperLB points to all nodes, we automatically allow scheduling on the control plane when it is active.
  # enable_klipper_metal_lb = ""true""

  # If you want to configure additional arguments for traefik, enter them here as a list and in the form of traefik CLI arguments; see https://doc.traefik.io/traefik/reference/static-configuration/cli/
  # They are the options that go into the additionalArguments section of the Traefik helm values file.
  # Example: traefik_additional_options = [""--log.level=DEBUG"", ""--tracing=true""]
  # traefik_additional_options = []

  # By default traefik is configured to redirect http traffic to https, you can set this to ""false"" to disable the redirection.
  # traefik_redirect_to_https = false

  # If you want to disable the metric server set this to ""false"". Default is ""true"".
  # enable_metrics_server = false

  # If you want to allow non-control-plane workloads to run on the control-plane nodes, set this to ""true"". The default is ""false"".
  # True by default for single node clusters, and when enable_klipper_metal_lb is true. In those cases, the value below will be ignored.
  # allow_scheduling_on_control_plane = true

  # If you want to disable the automatic upgrade of k3s, you can set below to ""false"".
  # Ideally, keep it on, to always have the latest Kubernetes version, but lock the initial_k3s_channel to a kube major version,
  # of your choice, like v1.25 or v1.26. That way you get the best of both worlds without the breaking changes risk.
  # For production use, always use an HA setup with at least 3 control-plane nodes and 2 agents, and keep this on for maximum security.

  # The default is ""true"" (in HA setup i.e. at least 3 control plane nodes & 2 agents, just keep it enabled since it works flawlessly).
  # automatically_upgrade_k3s = false

  # The default is ""true"" (in HA setup it works wonderfully well, with automatic roll-back to the previous snapshot in case of an issue).
  # IMPORTANT! For non-HA clusters i.e. when the number of control-plane nodes is < 3, you have to turn it off.
  # automatically_upgrade_os = false

  # If you need more control over kured and the reboot behaviour, you can pass additional options to kured.
  # For example limiting reboots to certain timeframes. For all options see: https://kured.dev/docs/configuration/
  # The default options are: `--reboot-command=/usr/bin/systemctl reboot --pre-reboot-node-labels=kured=rebooting --post-reboot-node-labels=kured=done --period=5m`
  # Defaults can be overridden by using the same key.
  # kured_options = {
  #   ""reboot-days"": ""su""
  #   ""start-time"": ""3am""
  #   ""end-time"": ""8am""
  # }

  # Allows you to specify either stable, latest, testing or supported minor versions.
  # see https://rancher.com/docs/k3s/latest/en/upgrades/basic/ and https://update.k3s.io/v1-release/channels
  #  If you are going to use Rancher addons for instance, it's always a good idea to fix the kube version to latest - 0.01,
  # at the time of writing the latest is v1.26, so setting the value below to ""v1.25"" will insure maximum compatibility with Rancher, Longhorn and so on!
  # The default is ""v1.25"".
  # initial_k3s_channel = ""stable""

  # The cluster name, by default ""k3s""
  # cluster_name = """"

  # Whether to use the cluster name in the node name, in the form of {cluster_name}-{nodepool_name}, the default is ""true"".
  # use_cluster_name_in_node_name = false

  # Extra k3s registries. This is useful if you have private registries and you
  # want to pull images without additional secrets.
  # registries.yaml file docs: https://docs.k3s.io/installation/private-registry
  /* k3s_registries = <<-EOT
    mirrors:
      hub.my_registry.com:
        endpoint:
          - ""hub.my_registry.com""
    configs:
      hub.my_registry.com:
        auth:
          username: username
          password: password
  EOT */

  # If you want to allow all outbound traffic you can set this to ""false"". Default is ""true"".
  # restrict_outbound_traffic = false

  # Adding extra firewall rules, like opening a port
  # More info on the format here https://registry.terraform.io/providers/hetznercloud/hcloud/latest/docs/resources/firewall
  # extra_firewall_rules = [
  #   # For Postgres
  #   {
  #     direction       = ""in""
  #     protocol        = ""tcp""
  #     port            = ""5432""
  #     source_ips      = [""0.0.0.0/0"", ""::/0""]
  #     destination_ips = [] # Won't be used for this rule
  #   },
  #   # To Allow ArgoCD access to resources via SSH
  #   {
  #     direction       = ""out""
  #     protocol        = ""tcp""
  #     port            = ""22""
  #     source_ips      = [] # Won't be used for this rule
  #     destination_ips = [""0.0.0.0/0"", ""::/0""]
  #   }
  # ]

  # If you want to configure a different CNI for k3s, use this flag
  # possible values: flannel (Default), calico, and cilium
  # As for Cilium, we allow infinite configurations via helm values, please check the CNI section of the readme over at https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/#cni.
  # Also, see the cilium_values at towards the end of this file, in the advanced section.
  # cni_plugin = ""cilium""

  # If you want to disable the k3s default network policy controller, use this flag!
  # Both Calico and Ciliun cni_plugin values override this value to true automatically, the default is ""false"".
  # disable_network_policy = true

  # If you want to disable the automatic use of placement group ""spread"". See https://docs.hetzner.com/cloud/placement-groups/overview/
  # That may be useful if you need to deploy more than 500 nodes! The default is ""false"".
  # placement_group_disable = true

  # By default, we allow ICMP ping in to the nodes, to check for liveness for instance. If you do not want to allow that, you can. Just set this flag to true (false by default).
  # block_icmp_ping_in = true

  # You can enable cert-manager (installed by Helm behind the scenes) with the following flag, the default is ""false"".
  # enable_cert_manager = true

  # By default we use a known good mirror to download the needed OpenSUSE, but for instance, it may not be available in US-East location, in which case,
  # you can find a working mirror for you at https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2.mirrorlist,
  # Or use the value below to automatically select an optimal mirror (by default we have it fixed to one german mirror that we know works for most locations)
  # opensuse_microos_mirror_link = ""https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2""

  # IP Addresses to use for the DNS Servers, set to an empty list to use the ones provided by Hetzner, defaults to [""1.1.1.1"", "" 1.0.0.1"", ""8.8.8.8""].
  # For rancher installs, best to leave it as default.
  # dns_servers = []

  # When this is enabled, rather than the first node, all external traffic will be routed via a control-plane loadbalancer, allowing for high availability.
  # The default is false.
  # use_control_plane_lb = true

  # Let's say you are not using the control plane LB solution above, and still want to have one hostname point to all your control-plane nodes.
  # You could create multiple A records of to let's say cp.cluster.my.org pointing to all of your control-plane nodes ips.
  # In which case, you need to define that hostname in the k3s TLS-SANs config to allow connection through it. It can be hostnames or IP addresses.
  # additional_tls_sans = [""cp.cluster.my.org""]

  # Oftentimes, you need to communicate to the cluster from inside the cluster itself, in which case it is important to set this value, as it will configure the hostname
  # at the load balancer level, and will save you from many slows downs when initiating communications from inside. Later on, you can point your DNS to the IP given
  # to the LB. And if you have other services pointing to it, you are also free to create CNAMES to point to it, or whatever you see fit.
  # If set, it will apply to either ingress controllers, Traefik or Ingress-Nginx.
  # lb_hostname = """"

  # You can enable Rancher (installed by Helm behind the scenes) with the following flag, the default is ""false"".
  # When Rancher is enabled, it automatically installs cert-manager too, and it uses rancher's own self-signed certificates.
  # See for options https://rancher.com/docs/rancher/v2.0-v2.4/en/installation/resources/advanced/helm2/helm-rancher/#choose-your-ssl-configuration
  # The easiest thing is to leave everything as is (using the default rancher self-signed certificate) and put Cloudflare in front of it.
  # As for the number of replicas, by default it is set to the numbe of control plane nodes.
  # You can customized all of the above by adding a rancher_values variable see at the end of this file in the advanced section.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.
  # IMPORTANT: Rancher's install is quite memory intensive, you will require at least 4GB if RAM, meaning cx21 server type (for your control plane).
  # ALSO, in order for Rancher to successfully deploy, you have to set the ""rancher_hostname"".
  # enable_rancher = true

  # If using Rancher you can set the Rancher hostname, it must be unique hostname even if you do not use it.
  # If not pointing the DNS, you can just port-forward locally via kubectl to get access to the dashboard.
  # If you already set the lb_hostname above and are using a Hetzner LB, you do not need to set this one, as it will be used by default.
  # But if you set this one explicitly, it will have preference over the lb_hostname in rancher settings.
  # rancher_hostname = ""rancher.xyz.dev""

  # When Rancher is deployed, by default is uses the ""latest"" channel. But this can be customized.
  # The allowed values are ""stable"" or ""latest"".
  # rancher_install_channel = ""stable""

  # Finally, you can specify a bootstrap-password for your rancher instance. Minimum 48 characters long!
  # If you leave empty, one will be generated for you.
  # (Can be used by another rancher2 provider to continue setup of rancher outside this module.)
  # rancher_bootstrap_password = """"

  # Separate from the above Rancher config (only use one or the other). You can import this cluster directly on an
  # an already active Rancher install. By clicking ""import cluster"" choosing ""generic"", giving it a name and pasting
  # the cluster registration url below. However, you can also ignore that and apply the url via kubectl as instructed
  # by Rancher in the wizard, and that would register your cluster too.
  # More information about the registration can be found here https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/registered-clusters/
  # rancher_registration_manifest_url = ""https://rancher.xyz.dev/v3/import/xxxxxxxxxxxxxxxxxxYYYYYYYYYYYYYYYYYYYzzzzzzzzzzzzzzzzzzzzz.yaml""

  # Extra values that will be passed to the `extra-manifests/kustomization.yaml.tpl` if its present.
  # extra_kustomize_parameters={}

  # It is best practice to turn this off, but for backwards compatibility it is set to ""true"" by default.
  # See https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/issues/349
  # When ""false"". The kubeconfig file can instead be created by executing: ""terraform output --raw kubeconfig > cluster_kubeconfig.yaml""
  # Always be careful to not commit this file!
  # create_kubeconfig = false

  # Don't create the kustomize backup. This can be helpful for automation.
  # create_kustomization = false

  ### ADVANCED - Custom helm values for packages above (search _values if you want to located where those are mentioned upper in this file)
  #  Inside the _values variable below are examples, up to you to find out the best helm values possible, we do not provide support for customized helm values.
  # Please understand that the indentation is very important, inside the EOTs, as those are proper yaml helm values.
  # We advise you to use the default values, and only change them if you know what you are doing!

  # Cilium, all Cilium helm values can be found at https://github.com/cilium/cilium/blob/master/install/kubernetes/cilium/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   cilium_values = <<EOT
ipam:
  mode: kubernetes
devices: ""eth1""
k8s:
  requireIPv4PodCIDR: true
kubeProxyReplacement: strict
  EOT */

  # Cert manager, all cert-manager helm values can be found at https://github.com/cert-manager/cert-manager/blob/master/deploy/charts/cert-manager/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   cert_manager_values = <<EOT
installCRDs: true
replicaCount: 3
webhook:
  replicaCount: 3
cainjector:
  replicaCount: 3
  EOT */

  # Longhorn, all Longhorn helm values can be found at https://github.com/longhorn/longhorn/blob/master/chart/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   longhorn_values = <<EOT
defaultSettings:
  defaultDataPath: /var/longhorn
persistence:
  defaultFsType: ext4
  defaultClassReplicaCount: 3
  defaultClass: true
  EOT */

  # Nginx, all Nginx helm values can be found at https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml
  # You can also have a look at https://kubernetes.github.io/ingress-nginx/, to understand how it works, and all the options at your disposal.
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   nginx_ingress_values = <<EOT
controller:
  watchIngressWithoutClass: ""true""
  kind: ""DaemonSet""
  config:
    ""use-forwarded-headers"": ""true""
    ""compute-full-forwarded-for"": ""true""
    ""use-proxy-protocol"": ""true""
  service:
    annotations:
      ""load-balancer.hetzner.cloud/name"": ""k3s""
      ""load-balancer.hetzner.cloud/use-private-ip"": ""true""
      ""load-balancer.hetzner.cloud/disable-private-ingress"": ""true""
      ""load-balancer.hetzner.cloud/location"": ""nbg1""
      ""load-balancer.hetzner.cloud/type"": ""lb11""
      ""load-balancer.hetzner.cloud/uses-proxyprotocol"": ""true""
  EOT */

  # Rancher, all Rancher helm values can be found at https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/chart-options/
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   rancher_values = <<EOT
ingress:
  tls:
    source: ""rancher""
hostname: ""rancher.example.com""
replicas: 1
bootstrapPassword: ""supermario""
  EOT */

}
",module,"module ""kube-hetzner"" {
  providers = {
    hcloud = hcloud
  }
  hcloud_token = local.hcloud_token

  # Then fill or edit the below values. Only the first values starting with a * are obligatory; the rest can remain with their default values, or you
  # could adapt them to your needs.

  # * For local dev, path to the git repo
  # source = ""../../kube-hetzner/""
  # If you want to use the latest master branch
  # source = ""github.com/kube-hetzner/terraform-hcloud-kube-hetzner""
  # For normal use, this is the path to the terraform registry
  source = ""kube-hetzner/kube-hetzner/hcloud""

  # You can optionally specify a version number
  # version = ""1.2.0""

  # Note that some values, notably ""location"" and ""public_key"" have no effect after initializing the cluster.
  # This is to keep Terraform from re-provisioning all nodes at once, which would lose data. If you want to update
  # those, you should instead change the value here and manually re-provision each node. Grep for ""lifecycle"".

  # Customize the SSH port (by default 22)
  # ssh_port = 2222

  # * Your ssh public key
  ssh_public_key = file(""~/.ssh/id_rsa.pub"")
  # * Your private key must be ""ssh_private_key = null"" when you want to use ssh-agent for a Yubikey-like device authentification or an SSH key-pair with a passphrase.
  # For more details on SSH see https://github.com/kube-hetzner/kube-hetzner/blob/master/docs/ssh.md
  ssh_private_key = file(""~/.ssh/id_rsa"")
  # You can add additional SSH public Keys to grant other team members root access to your cluster nodes.
  # ssh_additional_public_keys = []

  # You can also add additional SSH public Keys which are saved in the hetzner cloud by a label.
  # See https://docs.hetzner.cloud/#label-selector
  # ssh_hcloud_key_label = ""role=admin""

  # If you want to use an ssh key that is already registered within hetzner cloud, you can pass its id.
  # If no id is passed, a new ssh key will be registered within hetzner cloud.
  # It is important that exactly this key is passed via `ssh_public_key` & `ssh_private_key` vars.
  # hcloud_ssh_key_id = """"

  # These can be customized, or left with the default values
  # * For Hetzner locations see https://docs.hetzner.com/general/others/data-centers-and-connection/
  network_region = ""eu-central"" # change to `us-east` if location is ash

  # For the control planes, at least three nodes are the minimum for HA. Otherwise, you need to turn off the automatic upgrades (see README).
  # **It must always be an ODD number, never even!** Search the internet for ""splitbrain problem with etcd"" or see https://rancher.com/docs/k3s/latest/en/installation/ha-embedded/
  # For instance, one is ok (non-HA), two is not ok, and three is ok (becomes HA). It does not matter if they are in the same nodepool or not! So they can be in different locations and of various types.

  # Of course, you can choose any number of nodepools you want, with the location you want. The only constraint on the location is that you need to stay in the same network region, Europe, or the US.
  # For the server type, the minimum instance supported is cpx11 (just a few cents more than cx11); see https://www.hetzner.com/cloud.

  # IMPORTANT: Before you create your cluster, you can do anything you want with the nodepools, but you need at least one of each, control plane and agent.
  # Once the cluster is up and running, you can change nodepool count and even set it to 0 (in the case of the first control-plane nodepool, the minimum is 1).
  # You can also rename it (if the count is 0), but do not remove a nodepool from the list.

  # The only nodepools that are safe to remove from the list are at the end. That is due to how subnets and IPs get allocated (FILO).
  # You can, however, freely add other nodepools at the end of each list if you want. The maximum number of nodepools you can create combined for both lists is 255.
  # Also, before decreasing the count of any nodepools to 0, it's essential to drain and cordon the nodes in question. Otherwise, it will leave your cluster in a bad state.

  # Before initializing the cluster, you can change all parameters and add or remove any nodepools. You need at least one nodepool of each kind, control plane, and agent.
  # The nodepool names are entirely arbitrary, you can choose whatever you want, but no special characters or underscore, and they must be unique; only alphanumeric characters and dashes are allowed.

  # If you want to have a single node cluster, have one control plane nodepools with a count of 1, and one agent nodepool with a count of 0.

  # Please note that changing labels and taints after the first run will have no effect. If needed, you can do that through Kubernetes directly.

  # * Example below:

  control_plane_nodepools = [
    {
      name        = ""control-plane-fsn1"",
      server_type = ""cpx11"",
      location    = ""fsn1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""control-plane-nbg1"",
      server_type = ""cpx11"",
      location    = ""nbg1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""control-plane-hel1"",
      server_type = ""cpx11"",
      location    = ""hel1"",
      labels      = [],
      taints      = [],
      count       = 1
    }
  ]

  agent_nodepools = [
    {
      name        = ""agent-small"",
      server_type = ""cpx11"",
      location    = ""fsn1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""agent-large"",
      server_type = ""cpx21"",
      location    = ""nbg1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""storage"",
      server_type = ""cpx21"",
      location    = ""fsn1"",
      # Fully optional, just a demo.
      labels      = [
        ""node.kubernetes.io/server-usage=storage""
      ],
      taints      = [],
      count       = 1
      # In the case of using Longhorn, you can use Hetzner volumes instead of using the node's own storage by specifying a value from 10 to 10000 (in GB)
      # It will create one volume per node in the nodepool, and configure Longhorn to use them.
      # Something worth noting is that Volume storage is slower than node storage, which is achieved by not mentioning longhorn_volume_size or setting it to 0.
      # So for something like DBs, you definitely want node storage, for other things like backups, volume storage is fine, and cheaper.
      # longhorn_volume_size = 20
    }
  ]
  # Add custom control plane configuration options here.
  # E.g to enable monitoring for etcd, proxy etc:
  # control_planes_custom_config = {
  #  etcd-expose-metrics = true,
  #  kube-controller-manager-arg = ""bind-address=0.0.0.0"",
  #  kube-proxy-arg =""metrics-bind-address=0.0.0.0"",
  #  kube-scheduler-arg = ""bind-address=0.0.0.0"",
  # }

  # * LB location and type, the latter will depend on how much load you want it to handle, see https://www.hetzner.com/cloud/load-balancer
  load_balancer_type     = ""lb11""
  load_balancer_location = ""fsn1""

  ### The following values are entirely optional (and can be removed from this if unused)

  # You can refine a base domain name to be use in this form of nodename.base_domain for setting the reserve dns inside Hetzner
  # base_domain = ""mycluster.example.com""

  # Cluster Autoscaler
  # Providing at least one map for the array enables the cluster autoscaler feature, default is disabled
  # Please note that the autoscaler should not be used with initial_k3s_channel < ""v1.25"". So ideally lock it to ""v1.25"".
  # * Example below:
  # autoscaler_nodepools = [
  #   {
  #     name        = ""autoscaler""
  #     server_type = ""cpx21"" # must be same or better than the control_plane server type (regarding disk size)!
  #     location    = ""fsn1""
  #     min_nodes   = 0
  #     max_nodes   = 5
  #   }
  # ]

  # Enable etcd snapshot backups to S3 storage.
  # Just provide a map with the needed settings (according to your S3 storage provider) and backups to S3 will
  # be enabled (with the default settings for etcd snapshots).
  # Cloudflare's R2 offers 10GB, 10 million reads and 1 million writes per month for free.
  # For proper context, have a look at https://docs.k3s.io/backup-restore.
  # etcd_s3_backup = {
  #   etcd-s3-endpoint        = ""xxxx.r2.cloudflarestorage.com""
  #   etcd-s3-access-key      = ""<access-key>""
  #   etcd-s3-secret-key      = ""<secret-key>""
  #   etcd-s3-bucket          = ""k3s-etcd-snapshots""
  # }

  # To use local storage on the nodes, you can enable Longhorn, default is ""false"".
  # See a full recap on how to configure agent nodepools for longhorn here https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/discussions/373#discussioncomment-3983159
  # enable_longhorn = true

  # By default, longhorn is pulled from https://charts.longhorn.io.
  # If you need a version of longhorn which assures compatibility with rancher you can set this variable to https://charts.rancher.io. 
  # longhorn_repository = ""https://charts.rancher.io""

  # The namespace for longhorn deployment, default is ""longhorn-system"".
  # longhorn_namespace = ""longhorn-system""

  # The file system type for Longhorn, if enabled (ext4 is the default, otherwise you can choose xfs).
  # longhorn_fstype = ""xfs""

  # how many replica volumes should longhorn create (default is 3).
  # longhorn_replica_count = 1

  # When you enable Longhorn, you can go with the default settings and just modify the above two variables OR you can add a longhorn_values variable
  # with all needed helm values, see towards the end of the file in the advanced section.
  # If that file is present, the system will use it during the deploy, if not it will use the default values with the two variable above that can be customized.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.

  # Also, you can choose to use a Hetzner volume with Longhorn. By default, it will use the nodes own storage space, but if you add an attribute of
  # longhorn_volume_size ( not a variable, just a possible agent nodepool attribute) with a value between 10 and 10000 GB to your agent nodepool definition, it will create and use the volume in question.
  # See the agent nodepool section for an example of how to do that.

  # To disable Hetzner CSI storage, you can set the following to ""true"", default is ""false"".
  # disable_hetzner_csi = true

  # If you want to use a specific Hetzner CCM and CSI version, set them below; otherwise, leave them as-is for the latest versions.
  # hetzner_ccm_version = """"
  # hetzner_csi_version = """"

  # If you want to specify the Kured version, set it below - otherwise it'll use the latest version available.
  # kured_version = """"

  # If you want to specify what time Kured starts restarting nodes, set it below. Default is 3AM.
  # kured_start_time = """"

  # If you want to specify what time Kured stops restarting nodes, set it below. Default is 8AM.
  # kured_end_time = """"

  # If you want to specify what timezone Kured uses, set it below. Default is Local.
  # kured_time_zone = """"

  # If you want to enable the Nginx ingress controller (https://kubernetes.github.io/ingress-nginx/) instead of Traefik, you can set this to ""nginx"". Default is ""traefik"".
  # By the default we load optimal Traefik and Nginx ingress controller config for Hetzner, however you may need to tweak it to your needs, so to do,
  # we allow you to add a traefik_values and nginx_values, see towards the end of this file in the advanced section.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.
  # If you want to disable both controllers set this to ""none""
  # ingress_controller = ""nginx""

  # You can change the number of replicas for selected ingress controller here. The default 0 means autoselecting based on number of agent nodes (1 node = 1 replica, 2 nodes = 2 replicas, 3+ nodes = 3 replicas)
  # ingress_replica_count = 1

  # Use the klipperLB (similar to metalLB), instead of the default Hetzner one, that has an advantage of dropping the cost of the setup.
  # Automatically ""true"" in the case of single node cluster (as it does not make sense to use the Hetzner LB in that situation).
  # It can work with any ingress controller that you choose to deploy.
  # Please note that because the klipperLB points to all nodes, we automatically allow scheduling on the control plane when it is active.
  # enable_klipper_metal_lb = ""true""

  # If you want to configure additional arguments for traefik, enter them here as a list and in the form of traefik CLI arguments; see https://doc.traefik.io/traefik/reference/static-configuration/cli/
  # They are the options that go into the additionalArguments section of the Traefik helm values file.
  # Example: traefik_additional_options = [""--log.level=DEBUG"", ""--tracing=true""]
  # traefik_additional_options = []

  # By default traefik is configured to redirect http traffic to https, you can set this to ""false"" to disable the redirection.
  # traefik_redirect_to_https = false

  # If you want to disable the metric server set this to ""false"". Default is ""true"".
  # enable_metrics_server = false

  # If you want to allow non-control-plane workloads to run on the control-plane nodes, set this to ""true"". The default is ""false"".
  # True by default for single node clusters, and when enable_klipper_metal_lb is true. In those cases, the value below will be ignored.
  # allow_scheduling_on_control_plane = true

  # If you want to disable the automatic upgrade of k3s, you can set below to ""false"".
  # Ideally, keep it on, to always have the latest Kubernetes version, but lock the initial_k3s_channel to a kube major version,
  # of your choice, like v1.25 or v1.26. That way you get the best of both worlds without the breaking changes risk.
  # For production use, always use an HA setup with at least 3 control-plane nodes and 2 agents, and keep this on for maximum security.

  # The default is ""true"" (in HA setup i.e. at least 3 control plane nodes & 2 agents, just keep it enabled since it works flawlessly).
  # automatically_upgrade_k3s = false

  # The default is ""true"" (in HA setup it works wonderfully well, with automatic roll-back to the previous snapshot in case of an issue).
  # IMPORTANT! For non-HA clusters i.e. when the number of control-plane nodes is < 3, you have to turn it off.
  # automatically_upgrade_os = false

  # If you need more control over kured and the reboot behaviour, you can pass additional options to kured.
  # For example limiting reboots to certain timeframes. For all options see: https://kured.dev/docs/configuration/
  # The default options are: `--reboot-command=/usr/bin/systemctl reboot --pre-reboot-node-labels=kured=rebooting --post-reboot-node-labels=kured=done --period=5m`
  # Defaults can be overridden by using the same key.
  # kured_options = {
  #   ""reboot-days"": ""su""
  #   ""start-time"": ""3am""
  #   ""end-time"": ""8am""
  # }

  # Allows you to specify either stable, latest, testing or supported minor versions.
  # see https://rancher.com/docs/k3s/latest/en/upgrades/basic/ and https://update.k3s.io/v1-release/channels
  #  If you are going to use Rancher addons for instance, it's always a good idea to fix the kube version to latest - 0.01,
  # at the time of writing the latest is v1.26, so setting the value below to ""v1.25"" will insure maximum compatibility with Rancher, Longhorn and so on!
  # The default is ""v1.25"".
  # initial_k3s_channel = ""stable""

  # The cluster name, by default ""k3s""
  # cluster_name = """"

  # Whether to use the cluster name in the node name, in the form of {cluster_name}-{nodepool_name}, the default is ""true"".
  # use_cluster_name_in_node_name = false

  # Extra k3s registries. This is useful if you have private registries and you
  # want to pull images without additional secrets.
  # registries.yaml file docs: https://docs.k3s.io/installation/private-registry
  /* k3s_registries = <<-EOT
    mirrors:
      hub.my_registry.com:
        endpoint:
          - ""hub.my_registry.com""
    configs:
      hub.my_registry.com:
        auth:
          username: username
          password: password
  EOT */

  # If you want to allow all outbound traffic you can set this to ""false"". Default is ""true"".
  # restrict_outbound_traffic = false

  # Adding extra firewall rules, like opening a port
  # More info on the format here https://registry.terraform.io/providers/hetznercloud/hcloud/latest/docs/resources/firewall
  # extra_firewall_rules = [
  #   # For Postgres
  #   {
  #     direction       = ""in""
  #     protocol        = ""tcp""
  #     port            = ""5432""
  #     source_ips      = [""0.0.0.0/0"", ""::/0""]
  #     destination_ips = [] # Won't be used for this rule
  #   },
  #   # To Allow ArgoCD access to resources via SSH
  #   {
  #     direction       = ""out""
  #     protocol        = ""tcp""
  #     port            = ""22""
  #     source_ips      = [] # Won't be used for this rule
  #     destination_ips = [""0.0.0.0/0"", ""::/0""]
  #   }
  # ]

  # If you want to configure a different CNI for k3s, use this flag
  # possible values: flannel (Default), calico, and cilium
  # As for Cilium, we allow infinite configurations via helm values, please check the CNI section of the readme over at https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/#cni.
  # Also, see the cilium_values at towards the end of this file, in the advanced section.
  # cni_plugin = ""cilium""

  # If you want to disable the k3s default network policy controller, use this flag!
  # Both Calico and Ciliun cni_plugin values override this value to true automatically, the default is ""false"".
  # disable_network_policy = true

  # If you want to disable the automatic use of placement group ""spread"". See https://docs.hetzner.com/cloud/placement-groups/overview/
  # That may be useful if you need to deploy more than 500 nodes! The default is ""false"".
  # placement_group_disable = true

  # By default, we allow ICMP ping in to the nodes, to check for liveness for instance. If you do not want to allow that, you can. Just set this flag to true (false by default).
  # block_icmp_ping_in = true

  # You can enable cert-manager (installed by Helm behind the scenes) with the following flag, the default is ""false"".
  # enable_cert_manager = true

  # By default we use a known good mirror to download the needed OpenSUSE, but for instance, it may not be available in US-East location, in which case,
  # you can find a working mirror for you at https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2.mirrorlist,
  # Or use the value below to automatically select an optimal mirror (by default we have it fixed to one german mirror that we know works for most locations)
  # opensuse_microos_mirror_link = ""https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2""

  # IP Addresses to use for the DNS Servers, set to an empty list to use the ones provided by Hetzner, defaults to [""1.1.1.1"", "" 1.0.0.1"", ""8.8.8.8""].
  # For rancher installs, best to leave it as default.
  # dns_servers = []

  # When this is enabled, rather than the first node, all external traffic will be routed via a control-plane loadbalancer, allowing for high availability.
  # The default is false.
  # use_control_plane_lb = true

  # Let's say you are not using the control plane LB solution above, and still want to have one hostname point to all your control-plane nodes.
  # You could create multiple A records of to let's say cp.cluster.my.org pointing to all of your control-plane nodes ips.
  # In which case, you need to define that hostname in the k3s TLS-SANs config to allow connection through it. It can be hostnames or IP addresses.
  # additional_tls_sans = [""cp.cluster.my.org""]

  # Oftentimes, you need to communicate to the cluster from inside the cluster itself, in which case it is important to set this value, as it will configure the hostname
  # at the load balancer level, and will save you from many slows downs when initiating communications from inside. Later on, you can point your DNS to the IP given
  # to the LB. And if you have other services pointing to it, you are also free to create CNAMES to point to it, or whatever you see fit.
  # If set, it will apply to either ingress controllers, Traefik or Ingress-Nginx.
  # lb_hostname = """"

  # You can enable Rancher (installed by Helm behind the scenes) with the following flag, the default is ""false"".
  # When Rancher is enabled, it automatically installs cert-manager too, and it uses rancher's own self-signed certificates.
  # See for options https://rancher.com/docs/rancher/v2.0-v2.4/en/installation/resources/advanced/helm2/helm-rancher/#choose-your-ssl-configuration
  # The easiest thing is to leave everything as is (using the default rancher self-signed certificate) and put Cloudflare in front of it.
  # As for the number of replicas, by default it is set to the numbe of control plane nodes.
  # You can customized all of the above by adding a rancher_values variable see at the end of this file in the advanced section.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.
  # IMPORTANT: Rancher's install is quite memory intensive, you will require at least 4GB if RAM, meaning cx21 server type (for your control plane).
  # ALSO, in order for Rancher to successfully deploy, you have to set the ""rancher_hostname"".
  # enable_rancher = true

  # If using Rancher you can set the Rancher hostname, it must be unique hostname even if you do not use it.
  # If not pointing the DNS, you can just port-forward locally via kubectl to get access to the dashboard.
  # If you already set the lb_hostname above and are using a Hetzner LB, you do not need to set this one, as it will be used by default.
  # But if you set this one explicitly, it will have preference over the lb_hostname in rancher settings.
  # rancher_hostname = ""rancher.xyz.dev""

  # When Rancher is deployed, by default is uses the ""latest"" channel. But this can be customized.
  # The allowed values are ""stable"" or ""latest"".
  # rancher_install_channel = ""stable""

  # Finally, you can specify a bootstrap-password for your rancher instance. Minimum 48 characters long!
  # If you leave empty, one will be generated for you.
  # (Can be used by another rancher2 provider to continue setup of rancher outside this module.)
  # rancher_bootstrap_password = """"

  # Separate from the above Rancher config (only use one or the other). You can import this cluster directly on an
  # an already active Rancher install. By clicking ""import cluster"" choosing ""generic"", giving it a name and pasting
  # the cluster registration url below. However, you can also ignore that and apply the url via kubectl as instructed
  # by Rancher in the wizard, and that would register your cluster too.
  # More information about the registration can be found here https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/registered-clusters/
  # rancher_registration_manifest_url = ""https://rancher.xyz.dev/v3/import/xxxxxxxxxxxxxxxxxxYYYYYYYYYYYYYYYYYYYzzzzzzzzzzzzzzzzzzzzz.yaml""

  # Extra values that will be passed to the `extra-manifests/kustomization.yaml.tpl` if its present.
  # extra_kustomize_parameters={}

  # It is best practice to turn this off, but for backwards compatibility it is set to ""true"" by default.
  # See https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/issues/349
  # When ""false"". The kubeconfig file can instead be created by executing: ""terraform output --raw kubeconfig > cluster_kubeconfig.yaml""
  # Always be careful to not commit this file!
  # create_kubeconfig = false

  # Don't create the kustomize backup. This can be helpful for automation.
  # create_kustomization = false

  ### ADVANCED - Custom helm values for packages above (search _values if you want to located where those are mentioned upper in this file)
  #  Inside the _values variable below are examples, up to you to find out the best helm values possible, we do not provide support for customized helm values.
  # Please understand that the indentation is very important, inside the EOTs, as those are proper yaml helm values.
  # We advise you to use the default values, and only change them if you know what you are doing!

  # Cilium, all Cilium helm values can be found at https://github.com/cilium/cilium/blob/master/install/kubernetes/cilium/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   cilium_values = <<EOT
ipam:
  mode: kubernetes
devices: ""eth1""
k8s:
  requireIPv4PodCIDR: true
kubeProxyReplacement: strict
  EOT */

  # Cert manager, all cert-manager helm values can be found at https://github.com/cert-manager/cert-manager/blob/master/deploy/charts/cert-manager/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   cert_manager_values = <<EOT
installCRDs: true
replicaCount: 3
webhook:
  replicaCount: 3
cainjector:
  replicaCount: 3
  EOT */

  # Longhorn, all Longhorn helm values can be found at https://github.com/longhorn/longhorn/blob/master/chart/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   longhorn_values = <<EOT
defaultSettings:
  defaultDataPath: /var/longhorn
persistence:
  defaultFsType: ext4
  defaultClassReplicaCount: 3
  defaultClass: true
  EOT */

  # Nginx, all Nginx helm values can be found at https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml
  # You can also have a look at https://kubernetes.github.io/ingress-nginx/, to understand how it works, and all the options at your disposal.
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   nginx_ingress_values = <<EOT
controller:
  watchIngressWithoutClass: ""true""
  kind: ""DaemonSet""
  config:
    ""use-forwarded-headers"": ""true""
    ""compute-full-forwarded-for"": ""true""
    ""use-proxy-protocol"": ""true""
  service:
    annotations:
      ""load-balancer.hetzner.cloud/name"": ""k3s""
      ""load-balancer.hetzner.cloud/use-private-ip"": ""true""
      ""load-balancer.hetzner.cloud/disable-private-ingress"": ""true""
      ""load-balancer.hetzner.cloud/location"": ""nbg1""
      ""load-balancer.hetzner.cloud/type"": ""lb11""
      ""load-balancer.hetzner.cloud/uses-proxyprotocol"": ""true""
  EOT */

  # Rancher, all Rancher helm values can be found at https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/chart-options/
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   rancher_values = <<EOT
ingress:
  tls:
    source: ""rancher""
hostname: ""rancher.example.com""
replicas: 1
bootstrapPassword: ""supermario""
  EOT */

}
",module,282,282.0,1e07f152504e14250afb8fe82fed2545baec642f,1e07f152504e14250afb8fe82fed2545baec642f,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/1e07f152504e14250afb8fe82fed2545baec642f/kube.example.tf#L282,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/1e07f152504e14250afb8fe82fed2545baec642f/kube.example.tf#L282,2023-01-21 12:53:45+01:00,2023-01-21 12:53:45+01:00,1,0,1,1,0,0,0,0,0,0
https://github.com/CDCgov/prime-simplereport,4,ops/services/alerts/app_service_metrics/exceptions.tf,ops/services/alerts/app_service_metrics/exceptions.tf,0,workaround,"#   than a day, and we only want to alert 1x/week. As a workaround, this hardcodes the alert period to Thursdays","# - Collect all requests that were exceptions in the week preceeding today 
 # - Do the same for today 
 # - leftanti join the two result sets to return only results from today that were not found in the week preceeding today 
 # - Note the first 'where' clause - the azurerm_monitor_scheduled_query_rules_alert resource doesn't allow intervals longer 
 #   than a day, and we only want to alert 1x/week. As a workaround, this hardcodes the alert period to Thursdays 
 #   at 16:00 UTC (12:00 EDT). The alert runs hourly, so this guarantees that it will fire once and only once per week.","resource ""azurerm_monitor_scheduled_query_rules_alert"" ""first_error_in_a_week"" {
  name                = ""${var.env}-first-error-in-a-week""
  description         = ""${local.env_title} alert when an error is seen for the first time in a week""
  location            = data.azurerm_resource_group.app.location
  resource_group_name = var.rg_name

  action {
    action_group = var.action_group_ids
  }

  data_source_id = var.app_insights_id
  enabled        = contains(var.disabled_alerts, ""first_error_in_a_week"") ? false : true

  # - Collect all requests that were exceptions in the week preceeding today
  # - Do the same for today
  # - leftanti join the two result sets to return only results from today that were not found in the week preceeding today
  # - Note the first 'where' clause - the azurerm_monitor_scheduled_query_rules_alert resource doesn't allow intervals longer
  #   than a day, and we only want to alert 1x/week. As a workaround, this hardcodes the alert period to Thursdays
  #   at 16:00 UTC (12:00 EDT). The alert runs hourly, so this guarantees that it will fire once and only once per week.
  query = <<-QUERY
requests
| where dayofweek(now()) == time(4) and hourofday(now()) == 16
| where timestamp <= now() and timestamp > now(-1d) and success == false
| join kind= inner (
    exceptions
    | where timestamp <= now() and timestamp > now(-1d)
    )
    on operation_Id
| project stackTrace = details[0].rawStack, exceptionType = type, failedMethod = method, requestName = name, combinedErrorString = strcat(type, method, name), timestamp
| join kind= leftanti (
    requests
    | where timestamp <= now(-1d) and timestamp > now(-8d) and success == false
    | join kind= inner (
        exceptions
        | where timestamp <= now(-1d) and timestamp > now(-8d)
        )
        on operation_Id
    | project stackTrace = details[0].rawStack, exceptionType = type, failedMethod = method, requestName = name, combinedErrorString = strcat(type, method, name)
    )
    on combinedErrorString
| summarize stackTrace = any(stackTrace) by failedMethod, requestName, exceptionType, timestamp
| sort by timestamp
  QUERY

  severity    = 2
  frequency   = 60 // Run hourly
  time_window = 1440
  trigger {
    operator  = ""GreaterThan""
    threshold = 0
  }
}
",resource,"resource ""azurerm_monitor_scheduled_query_rules_alert"" ""first_error_in_a_week"" {
  name                = ""${var.env}-first-error-in-a-week""
  description         = ""${local.env_title} alert when an error is seen for the first time in a week""
  location            = data.azurerm_resource_group.app.location
  resource_group_name = var.rg_name

  action {
    action_group = var.action_group_ids
  }

  data_source_id = var.app_insights_id
  enabled        = false

  # - Collect all requests that were exceptions in the week preceeding today
  # - Do the same for today
  # - leftanti join the two result sets to return only results from today that were not found in the week preceeding today
  query = <<-QUERY
requests
| where timestamp <= now() and timestamp > now(-1d) and success == false
| join kind= inner (
    exceptions
    | where timestamp <= now() and timestamp > now(-1d)
    )
    on operation_Id
| project stackTrace = details[0].rawStack, exceptionType = type, failedMethod = method, requestName = name, combinedErrorString = strcat(type, method, name), timestamp
| join kind= leftanti (
    requests
    | where timestamp <= now(-1d) and timestamp > now(-8d) and success == false
    | join kind= inner (
        exceptions
        | where timestamp <= now(-1d) and timestamp > now(-8d)
        )
        on operation_Id
    | project stackTrace = details[0].rawStack, exceptionType = type, failedMethod = method, requestName = name, combinedErrorString = strcat(type, method, name)
    )
    on combinedErrorString
| summarize stackTrace = any(stackTrace) by failedMethod, requestName, exceptionType, timestamp
| sort by timestamp
  QUERY

  severity    = 2
  frequency   = 60 // Run hourly
  time_window = 1440
  trigger {
    operator  = ""GreaterThan""
    threshold = 0
  }
}
",resource,76,,d7d55375850dcf95aa027c27e93bd71fd629cd0b,f2feac3dc838381985a2e69d62d79c011a8f0e44,https://github.com/CDCgov/prime-simplereport/blob/d7d55375850dcf95aa027c27e93bd71fd629cd0b/ops/services/alerts/app_service_metrics/exceptions.tf#L76,https://github.com/CDCgov/prime-simplereport/blob/f2feac3dc838381985a2e69d62d79c011a8f0e44/ops/services/alerts/app_service_metrics/exceptions.tf,2021-08-04 12:28:21-04:00,2021-08-05 16:43:08-04:00,2,1,0,1,0,0,0,0,1,0
https://github.com/uyuni-project/sumaform,1389,backend_modules/azure/network/main.tf,backend_modules/azure/network/main.tf,0,/*todo,/*TODO: add tags*/,"/* 
 This module sets up a class B VPC sliced into two subnets, one public and one private. 
 The private network has no Internet access. 
 The public network has an Internet Gateway and accepts SSH connections from a whitelist of trusted IPs. 
 */ 
 /*TODO: add tags*/","locals{

}
",locals,"resource ""azurerm_resource_group"" ""suma-rg"" {
  name     = ""${var.name_prefix}-resources""
  location = ""${var.location}""
}
",resource,6,6.0,1990c31dba05892a132fd5aaa936ff1b1940f4b5,4b92532e86215e984ef5a1d2983372ab470d1c84,https://github.com/uyuni-project/sumaform/blob/1990c31dba05892a132fd5aaa936ff1b1940f4b5/backend_modules/azure/network/main.tf#L6,https://github.com/uyuni-project/sumaform/blob/4b92532e86215e984ef5a1d2983372ab470d1c84/backend_modules/azure/network/main.tf#L6,2021-04-14 11:47:58+01:00,2023-09-05 15:07:37+02:00,13,0,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1514,modules/gke-cluster/main.tf,modules/gke-cluster-standard/main.tf,1,#todo,#TODO add support for configs,#TODO add support for configs,"resource ""google_gke_backup_backup_plan"" ""backup_plan"" {
  for_each =  try(var.backup_configs.enable_backup_agent, false) ? var.backup_configs.backup_plans : null
  name = each.key
  cluster = google_container_cluster.cluster.id
  location = each.value.region
  project = var.project_id
  retention_policy {
    backup_delete_lock_days = try(each.value.retention_policy_delete_lock_days)
    backup_retain_days      = try(each.value.retention_policy_days)
    locked                  = try(each.value.retention_policy_lock)
  }
  backup_schedule {
    cron_schedule = each.value.schedule
  }
  #TODO add support for configs
  backup_config {
    include_volume_data = true
    include_secrets = true
    all_namespaces = true
  }
}
",resource,"resource ""google_gke_backup_backup_plan"" ""backup_plan"" {
  for_each = var.backup_configs.enable_backup_agent ? var.backup_configs.backup_plans : {}
  name     = each.key
  cluster  = google_container_cluster.cluster.id
  location = each.value.region
  project  = var.project_id
  retention_policy {
    backup_delete_lock_days = try(each.value.retention_policy_delete_lock_days)
    backup_retain_days      = try(each.value.retention_policy_days)
    locked                  = try(each.value.retention_policy_lock)
  }
  backup_schedule {
    cron_schedule = each.value.schedule
  }

  backup_config {
    include_volume_data = each.value.include_volume_data
    include_secrets     = each.value.include_secrets

    dynamic ""encryption_key"" {
      for_each = each.value.encryption_key != null ? [""""] : []
      content {
        gcp_kms_encryption_key = each.value.encryption_key
      }
    }

    all_namespaces = lookup(each.value, ""namespaces"", null) != null ? null : true
    dynamic ""selected_namespaces"" {
      for_each = each.value.namespaces != null ? [""""] : []
      content {
        namespaces = each.value.namespaces
      }
    }
  }
}
",resource,403,,64a9952656b49bafb620ec219d6472bb7eb4e590,5763eb53d427a79450d4302863cfa4eaca593154,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/64a9952656b49bafb620ec219d6472bb7eb4e590/modules/gke-cluster/main.tf#L403,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/5763eb53d427a79450d4302863cfa4eaca593154/modules/gke-cluster-standard/main.tf,2023-03-30 12:47:39+02:00,2023-05-02 14:59:12+00:00,6,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1670,modules/secret-manager/main.tf,modules/secret-manager/main.tf,0,# todo,# TODO(jccb): support custom keys inside auto,# TODO(jccb): support custom keys inside auto,"resource ""google_secret_manager_secret"" ""default"" {
  for_each  = var.secrets
  project   = var.project_id
  secret_id = each.key
  labels    = lookup(var.labels, each.key, null)

  dynamic ""replication"" {
    for_each = each.value == null ? [""""] : []
    content {
      # TODO(jccb): support custom keys inside auto
      auto {}
    }
  }

  dynamic ""replication"" {
    for_each = each.value == null ? [] : [each.value]
    iterator = locations
    content {
      user_managed {
        dynamic ""replicas"" {
          for_each = locations.value
          iterator = location
          content {
            location = location.value
            dynamic ""customer_managed_encryption"" {
              for_each = try(var.encryption_key[location.value] != null ? [""""] : [], [])
              content {
                kms_key_name = var.encryption_key[location.value]
              }
            }
          }
        }
      }
    }
  }
}
",resource,"resource ""google_secret_manager_secret"" ""default"" {
  for_each  = var.secrets
  project   = var.project_id
  secret_id = each.key
  labels    = lookup(var.labels, each.key, null)

  dynamic ""replication"" {
    for_each = each.value.locations == null ? [""""] : []
    content {
      auto {
        dynamic ""customer_managed_encryption"" {
          for_each = try(lookup(each.value.keys, ""global"", null) == null ? [] : [""""], [])
          content {
            kms_key_name = each.value.keys[""global""]
          }
        }
      }
    }
  }

  dynamic ""replication"" {
    for_each = each.value.locations == null ? [] : [""""]
    content {
      user_managed {
        dynamic ""replicas"" {
          for_each = each.value.locations
          iterator = location
          content {
            location = location.value
            dynamic ""customer_managed_encryption"" {
              for_each = try(lookup(each.value.keys, location.value, null) == null ? [] : [""""], [])
              content {
                kms_key_name = each.value.keys[location.value]
              }
            }
          }
        }
      }
    }
  }
}
",resource,47,,789328ff5a1f71892a90d2174bc327aeaff08d31,d07f8fd33d0a5fc8eb9118a0258e3158949d95f9,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/789328ff5a1f71892a90d2174bc327aeaff08d31/modules/secret-manager/main.tf#L47,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d07f8fd33d0a5fc8eb9118a0258e3158949d95f9/modules/secret-manager/main.tf,2023-10-03 12:15:36+00:00,2023-11-10 16:45:47+01:00,2,1,0,1,0,1,0,0,0,0
https://github.com/chanzuckerberg/cztack,121,aws-s3-account-public-access-block/variables.tf,aws-s3-account-public-access-block/variables.tf,0,# todo,# # TODO(el): on tf 0.13,"# # TODO(el): on tf 0.13 
 #  validation { 
 #    condition { 
 #       (var.restrict == ""all"" || var.restrict == ""new"" || var.restrict == ""none"") 
 #    } 
 #    error_message = ""restrict must be one of (all, new, none)"" 
 #  }","variable restrict {
  type        = string
  default     = ""all""
  description = <<EOF
  How restrictive should the account-wide access block be. Accepted values are `all`, `new`, `none`.
  `all` blocks public access to all buckets in account.
  `new` prevents you from granting public access to any more buckets, existing public buckets remain public.
  `none` restricts no access.
  EOF

  # # TODO(el): on tf 0.13
  #  validation {
  #    condition {
  #       (var.restrict == ""all"" || var.restrict == ""new"" || var.restrict == ""none"")
  #    }
  #    error_message = ""restrict must be one of (all, new, none)""
  #  }
}
",variable,"variable ""restrict"" {
  type        = string
  default     = ""all""
  description = <<EOF
  How restrictive should the account-wide access block be. Accepted values are `all`, `new`, `none`.
  `all` blocks public access to all buckets in account.
  `new` prevents you from granting public access to any more buckets, existing public buckets remain public.
  `none` restricts no access.
  EOF

  # # TODO(el): on tf 0.13
  #  validation {
  #    condition {
  #       (var.restrict == ""all"" || var.restrict == ""new"" || var.restrict == ""none"")
  #    }
  #    error_message = ""restrict must be one of (all, new, none)""
  #  }
}
",variable,11,11.0,67584526b30ead327b536d614e568998558345ea,9df439500dee7468643ca03a844cf7a5b1e1b313,https://github.com/chanzuckerberg/cztack/blob/67584526b30ead327b536d614e568998558345ea/aws-s3-account-public-access-block/variables.tf#L11,https://github.com/chanzuckerberg/cztack/blob/9df439500dee7468643ca03a844cf7a5b1e1b313/aws-s3-account-public-access-block/variables.tf#L11,2020-10-30 16:45:46-04:00,2021-04-13 14:52:04-04:00,2,0,0,1,1,0,0,0,0,0
https://github.com/chanzuckerberg/cztack,169,databricks-cluster-policy/main.tf,databricks-cluster-policy/main.tf,0,workaround,# Workaround for looping over grantees and setting resource count,# Workaround for looping over grantees and setting resource count,"locals {
  # default policy attributes that can be overridden but are otherwise 
  # included for each policy
  default_policy = {
    ""custom_tags.Cluster_Policy"" : {
      ""type"" : ""fixed"",
      ""value"" : var.policy_name
    },
    ""custom_tags.Databricks_Workspace_Id"" : {
      ""type"" : ""fixed"",
      ""value"" : var.databricks_workspace_id
    },
    ""custom_tags.Databricks_Host"" : {
      ""type"" : ""fixed"",
      ""value"" : var.databricks_host
    },
  }

  # Workaround for looping over grantees and setting resource count
  inherited_cluster_policy_grantees = toset([for grantee in var.grantees : grantee if var.policy_family_id != null])
  custom_cluster_policy_grantees    = toset([for grantee in var.grantees : grantee if var.policy_family_id == null])
}
",locals,"locals {
  # default policy attributes that can be overridden but are otherwise 
  # included for each policy
  default_policy = {
    ""custom_tags.Cluster_Policy"" : {
      ""type"" : ""fixed"",
      ""value"" : var.policy_name
    },
    ""custom_tags.Databricks_Workspace_Id"" : {
      ""type"" : ""fixed"",
      ""value"" : var.databricks_workspace_id
    },
    ""custom_tags.Databricks_Host"" : {
      ""type"" : ""fixed"",
      ""value"" : var.databricks_host
    },
  }

  # Workaround for looping over grantees and setting resource count
  inherited_cluster_policy_grantees = toset([for grantee in var.grantees : grantee if var.policy_family_id != null])
  custom_cluster_policy_grantees    = toset([for grantee in var.grantees : grantee if var.policy_family_id == null])
}
",locals,19,19.0,5f42e9bbb2eafdbde5a3afbc0d0fc1aa6d4093b9,0ab051aab7c11e550fcab20c4eaeef562c4d3e39,https://github.com/chanzuckerberg/cztack/blob/5f42e9bbb2eafdbde5a3afbc0d0fc1aa6d4093b9/databricks-cluster-policy/main.tf#L19,https://github.com/chanzuckerberg/cztack/blob/0ab051aab7c11e550fcab20c4eaeef562c4d3e39/databricks-cluster-policy/main.tf#L19,2023-10-31 13:13:06-07:00,2024-03-06 16:58:57-08:00,3,0,0,1,0,1,0,0,0,0
https://github.com/uyuni-project/sumaform,1584,modules/server/variables.tf,modules/server/variables.tf,0,workaround,"# WORKAROUND: this is causing problems in the testsuite, disable it for now","# WORKAROUND: this is causing problems in the testsuite, disable it for now","variable ""c3p0_connection_timeout"" {
  description = ""c3p0 connections will be closed after this timeout""
  # WORKAROUND: this is causing problems in the testsuite, disable it for now
  default     = false
}
",variable,"variable ""c3p0_connection_timeout"" {
  description = ""c3p0 connections will be closed after this timeout""
  # WORKAROUND: this is causing problems in the testsuite, disable it for now
  default     = false
}
",variable,279,315.0,c7a1ca80f5f3194c87d28ae1506791b150e496dc,9ddd0e0dd8d873e0a44e1b64214d41718f700ff8,https://github.com/uyuni-project/sumaform/blob/c7a1ca80f5f3194c87d28ae1506791b150e496dc/modules/server/variables.tf#L279,https://github.com/uyuni-project/sumaform/blob/9ddd0e0dd8d873e0a44e1b64214d41718f700ff8/modules/server/variables.tf#L315,2023-06-12 11:59:16+02:00,2024-04-08 10:06:08+02:00,17,0,0,1,0,0,0,0,0,1
https://github.com/alphagov/govuk-aws,1089,terraform/projects/infra-database-backups-bucket/main.tf,terraform/projects/infra-database-backups-bucket/main.tf,0,# todo,# TODO: make staging use the same rules as integration. We don't need to,"# Production/Staging lifecycle rules. 
 # 
 # TODO: make staging use the same rules as integration. We don't need to 
 # retain backups of staging for very long. ","resource ""aws_s3_bucket"" ""database_backups"" {
  bucket = ""govuk-${var.aws_environment}-database-backups""
  region = ""${var.aws_region}""

  tags {
    Name            = ""govuk-${var.aws_environment}-database-backups""
    aws_environment = ""${var.aws_environment}""
  }

  logging {
    target_bucket = ""${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}""
    target_prefix = ""s3/govuk-${var.aws_environment}-database-backups/""
  }

  # Production/Staging lifecycle rules.
  #
  # TODO: make staging use the same rules as integration. We don't need to
  # retain backups of staging for very long.

  lifecycle_rule {
    id      = ""mysql_lifecycle_rule""
    prefix  = ""mysql/""
    enabled = ""${var.aws_environment != ""integration""}""

    transition {
      storage_class = ""STANDARD_IA""
      days          = ""${var.standard_s3_storage_time}""
    }

    transition {
      storage_class = ""GLACIER""
      days          = ""${var.glacier_storage_time}""
    }

    expiration {
      days = 120
    }
  }
  lifecycle_rule {
    id      = ""postgres_lifecycle_rule""
    prefix  = ""postgres/""
    enabled = ""${var.aws_environment != ""integration""}""

    transition {
      storage_class = ""STANDARD_IA""
      days          = ""${var.standard_s3_storage_time}""
    }

    transition {
      storage_class = ""GLACIER""
      days          = ""${var.glacier_storage_time}""
    }

    expiration {
      days = 120
    }
  }
  lifecycle_rule {
    id      = ""mongo_daily_lifecycle_rule""
    prefix  = ""mongodb/daily""
    enabled = ""${var.aws_environment != ""integration""}""

    transition {
      storage_class = ""STANDARD_IA""
      days          = ""${var.standard_s3_storage_time}""
    }

    transition {
      storage_class = ""GLACIER""
      days          = ""${var.glacier_storage_time}""
    }

    expiration {
      days = 120
    }
  }
  lifecycle_rule {
    id      = ""mongo_regular_lifecycle_rule""
    prefix  = ""mongodb/regular""
    enabled = true

    expiration {
      days = ""${var.expiration_time_whisper_mongo}""
    }
  }
  lifecycle_rule {
    id      = ""whisper_lifecycle_rule""
    prefix  = ""whisper/""
    enabled = true

    expiration {
      days = ""${var.expiration_time_whisper_mongo}""
    }
  }

  # Integration-specific lifecycle rules. These rules are created in all
  # environments but are only enabled in Integration.
  #
  # TODO: create these only in environments where they're needed, instead of
  # creating them everywhere and leaving them disabled.
  #
  # TODO: these are all set to the same var.expiration_time so just replace
  # them with one rule. Similarly for the prod ones above.

  lifecycle_rule {
    id      = ""mysql_lifecycle_rule_integration""
    prefix  = ""mysql/""
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }
  }
  lifecycle_rule {
    id      = ""postgres_lifecycle_rule_integration""
    prefix  = ""postgres/""
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }
  }
  lifecycle_rule {
    id      = ""mongo_daily_lifecycle_rule_integration""
    prefix  = ""mongodb/daily""
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }
  }
  lifecycle_rule {
    id      = ""whole_bucket_lifecycle_rule_integration""
    prefix  = """"
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }

    noncurrent_version_expiration {
      days = ""1""
    }
  }

  # End of Integration-specific lifecycle rules.


  # Lifecycle rule for coronavirus find support backup

  lifecycle_rule {
    id      = ""coronavirus_find_support_lifecycle_rule""
    prefix  = ""coronavirus-find-support/production.sql.gzip""
    enabled = true

    expiration {
      days = 365
    }
  }

  # Lifecycle rule for coronavirus business volunteer form backup

  lifecycle_rule {
    id      = ""coronavirus_business_volunteer_form_lifecycle_rule""
    prefix  = ""coronavirus-business-volunteer-form/production.sql.gzip""
    enabled = true

    expiration {
      days = 365
    }
  }
  versioning {
    enabled = true
  }
  replication_configuration {
    role = ""${aws_iam_role.backup_replication_role.arn}""

    rules {
      id     = ""main_replication_rule""
      prefix = """"
      status = ""${var.replication_setting}""

      destination {
        bucket        = ""${aws_s3_bucket.database_backups_replica.arn}""
        storage_class = ""STANDARD""
      }
    }
  }
}
",resource,"resource ""aws_s3_bucket"" ""database_backups"" {
  bucket = ""govuk-${var.aws_environment}-database-backups""
  region = ""${var.aws_region}""

  tags {
    Name            = ""govuk-${var.aws_environment}-database-backups""
    aws_environment = ""${var.aws_environment}""
  }

  logging {
    target_bucket = ""${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}""
    target_prefix = ""s3/govuk-${var.aws_environment}-database-backups/""
  }

  versioning {
    # It's not entirely clear if versioning is useful on this bucket  but it was previously configured this way,
    # so we've decided not to change it. Whilst it helps protect against accidental deletion, it doesn't protect
    # against malicious actors, so shouldn't be considered a security feature.
    enabled = true
  }

  lifecycle_rule {
    # Use a long retention period in production
    id      = ""long_retention_period""
    enabled = ""${var.aws_environment == ""production""}""

    # Ideally everything would go in the Standard (Infrequent Access) storage class when created.
    # But newly created objects always go into Standard, and can only move into IA after at least 30 days.
    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html
    transition {
      storage_class = ""STANDARD_IA""
      days          = 30
    }

    # Likewise, we have to wait at least another 30 days before we can move objects into Glacier storage.
    transition {
      storage_class = ""GLACIER""
      days          = 60
    }

    # Versioning is enabled on this bucket, so this rule will 'soft delete' objects.
    # In AWS lingo, this means a 'delete marker' will be set on the current version of the object.
    # More info on how expiration rules apply to versioned buckets here:
    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#intro-lifecycle-rules-actions
    expiration {
      days = 120
    }

    # This rule will 'hard delete' objects 1 day after they were 'soft deleted'.
    # In other words: old database backups will be permanently deleted 1 day after they've expired.
    noncurrent_version_expiration {
      days = ""1""
    }
  }

  lifecycle_rule {
    # Use a short retention period in integration and staging
    id      = ""short_retention_period""
    enabled = ""${var.aws_environment != ""production""}""

    expiration {
      days = ""3""
    }

    noncurrent_version_expiration {
      days = ""1""
    }
  }

  replication_configuration {
    role = ""${aws_iam_role.backup_replication_role.arn}""

    rules {
      id     = ""main_replication_rule""
      status = ""Enabled""

      destination {
        bucket        = ""${aws_s3_bucket.database_backups_replica.arn}""
        storage_class = ""STANDARD_IA""
      }
    }
  }
}
",resource,116,,d1fcb45657475a7de489503eae548845ec8e4296,78cc3f5ce73fb5a8f6d5126ff694ac87e5a0eb79,https://github.com/alphagov/govuk-aws/blob/d1fcb45657475a7de489503eae548845ec8e4296/terraform/projects/infra-database-backups-bucket/main.tf#L116,https://github.com/alphagov/govuk-aws/blob/78cc3f5ce73fb5a8f6d5126ff694ac87e5a0eb79/terraform/projects/infra-database-backups-bucket/main.tf,2020-11-24 17:53:04+00:00,2022-01-31 17:30:52+00:00,4,1,1,1,0,0,0,1,0,0
https://github.com/Worklytics/psoxy,622,infra/modules/worklytics-psoxy-connection-generic/variables.tf,infra/modules/worklytics-psoxy-connection-generic/variables.tf,0,# todo,# TODO: rename to `proxy_instance_id` in future versions avoid coupling to brand name,# TODO: rename to `proxy_instance_id` in future versions avoid coupling to brand name,"variable ""psoxy_instance_id"" {
  type        = string
  description = ""friendly unique-id for Psoxy instance""
  default     = null
}
",variable,"variable ""psoxy_instance_id"" {
  type        = string
  description = ""friendly unique-id for Psoxy instance""
  default     = null
}
",variable,1,1.0,df24acebb5a0d8049e753a7084cbf84c34e773b3,5937e6a45055a94ff2b493f96f21863d91699825,https://github.com/Worklytics/psoxy/blob/df24acebb5a0d8049e753a7084cbf84c34e773b3/infra/modules/worklytics-psoxy-connection-generic/variables.tf#L1,https://github.com/Worklytics/psoxy/blob/5937e6a45055a94ff2b493f96f21863d91699825/infra/modules/worklytics-psoxy-connection-generic/variables.tf#L1,2023-01-16 09:59:11-08:00,2024-03-06 18:11:21+00:00,3,0,0,1,0,0,0,0,0,0
https://github.com/camptocamp/devops-stack,1,modules/aks-azure/main.tf,modules/aks/azure/main.tf,1,# todo,# TODO: I'm not sure this is required,# TODO: I'm not sure this is required,"resource ""azurerm_role_assignment"" ""reader"" {
  scope                = format(""%s/resourcegroups/%s"", data.azurerm_subscription.primary.id, module.cluster.node_resource_group)
  role_definition_name = ""Reader""
  principal_id         = azurerm_user_assigned_identity.cert_manager.principal_id
}
",resource,,,157,0.0,a8d46993274079cba8165c95371bc5b385f9b815,23a76321726eca45b1852f9cbb9a5a46dd17c13e,https://github.com/camptocamp/devops-stack/blob/a8d46993274079cba8165c95371bc5b385f9b815/modules/aks-azure/main.tf#L157,https://github.com/camptocamp/devops-stack/blob/23a76321726eca45b1852f9cbb9a5a46dd17c13e/modules/aks/azure/main.tf#L0,2020-12-03 18:17:13+01:00,2023-04-03 16:40:29+02:00,48,2,0,1,0,1,0,0,0,0
https://github.com/nasa/cumulus,293,tf-modules/archive/process_dead_letter_archive.tf,tf-modules/archive/process_dead_letter_archive.tf,0,todo,"// TODO - need sns topic ARNs for granules, PDRs","// TODO - need sns topic ARNs for granules, PDRs","resource ""aws_lambda_function"" ""process_dead_letter_archive"" {
  filename         = ""${path.module}/../../packages/api/dist/processDeadLetterArchive/lambda.zip""
  source_code_hash = filebase64sha256(""${path.module}/../../packages/api/dist/processDeadLetterArchive/lambda.zip"")
  function_name    = ""${var.prefix}-processDeadLetterArchive""
  role             = aws_iam_role.process_dead_letter_archive_role.arn
  handler          = ""index.handler""
  runtime          = ""nodejs12.x""
  timeout          = 300
  memory_size      = 512

  environment {
    // TODO - need sns topic ARNs for granules, PDRs
    variables = {
      acquireTimeoutMillis      = var.rds_connection_timing_configuration.acquireTimeoutMillis
      createRetryIntervalMillis = var.rds_connection_timing_configuration.createRetryIntervalMillis
      createTimeoutMillis       = var.rds_connection_timing_configuration.createTimeoutMillis
      databaseCredentialSecretArn = var.rds_user_access_secret_arn
      execution_sns_topic_arn   = aws_sns_topic.report_executions_topic.arn
      RDS_DEPLOYMENT_CUMULUS_VERSION = ""9.0.0""
      ExecutionsTable           = var.dynamo_tables.executions.name
      GranulesTable             = var.dynamo_tables.granules.name
      idleTimeoutMillis         = var.rds_connection_timing_configuration.idleTimeoutMillis
      PdrsTable                 = var.dynamo_tables.pdrs.name
      reapIntervalMillis        = var.rds_connection_timing_configuration.reapIntervalMillis
      stackName                 = var.prefix
      system_bucket             = var.system_bucket
      ES_HOST = var.elasticsearch_hostname
    }
  }

  dynamic ""vpc_config"" {
    for_each = length(var.lambda_subnet_ids) == 0 ? [] : [1]
    content {
      subnet_ids = var.lambda_subnet_ids
      security_group_ids = compact([
        aws_security_group.no_ingress_all_egress[0].id,
        var.rds_security_group
      ])
    }
  }

  tags = var.tags
}
",resource,"resource ""aws_lambda_function"" ""process_dead_letter_archive"" {
  filename         = ""${path.module}/../../packages/api/dist/processDeadLetterArchive/lambda.zip""
  source_code_hash = filebase64sha256(""${path.module}/../../packages/api/dist/processDeadLetterArchive/lambda.zip"")
  function_name    = ""${var.prefix}-processDeadLetterArchive""
  role             = aws_iam_role.process_dead_letter_archive_role.arn
  handler          = ""index.handler""
  runtime          = ""nodejs12.x""
  timeout          = 300
  memory_size      = 512

  environment {
    variables = {
      acquireTimeoutMillis           = var.rds_connection_timing_configuration.acquireTimeoutMillis
      createRetryIntervalMillis      = var.rds_connection_timing_configuration.createRetryIntervalMillis
      createTimeoutMillis            = var.rds_connection_timing_configuration.createTimeoutMillis
      databaseCredentialSecretArn    = var.rds_user_access_secret_arn
      ExecutionsTable                = var.dynamo_tables.executions.name
      execution_sns_topic_arn        = aws_sns_topic.report_executions_topic.arn
      GranulesTable                  = var.dynamo_tables.granules.name
      granule_sns_topic_arn          = aws_sns_topic.report_granules_topic.arn
      idleTimeoutMillis              = var.rds_connection_timing_configuration.idleTimeoutMillis
      PdrsTable                      = var.dynamo_tables.pdrs.name
      pdr_sns_topic_arn              = aws_sns_topic.report_pdrs_topic.arn
      reapIntervalMillis             = var.rds_connection_timing_configuration.reapIntervalMillis
      stackName                      = var.prefix
      system_bucket                  = var.system_bucket
      RDS_DEPLOYMENT_CUMULUS_VERSION = ""9.0.0""
      ES_HOST                        = var.elasticsearch_hostname
    }
  }

  dynamic ""vpc_config"" {
    for_each = length(var.lambda_subnet_ids) == 0 ? [] : [1]
    content {
      subnet_ids = var.lambda_subnet_ids
      security_group_ids = compact([
        aws_security_group.no_ingress_all_egress[0].id,
        var.rds_security_group
      ])
    }
  }

  tags = var.tags
}
",resource,93,,a40c205c230cbcfb49db60e837ad202735c8b87f,f5132068ea6e260efe90a13f49d106023930a716,https://github.com/nasa/cumulus/blob/a40c205c230cbcfb49db60e837ad202735c8b87f/tf-modules/archive/process_dead_letter_archive.tf#L93,https://github.com/nasa/cumulus/blob/f5132068ea6e260efe90a13f49d106023930a716/tf-modules/archive/process_dead_letter_archive.tf,2021-09-23 13:03:56-04:00,2021-10-14 09:38:42-07:00,3,1,1,1,0,0,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,224,locals.tf,locals.tf,0,todo,# TODO - move this into `aws-eks-teams` to avoid getting out of sync,# TODO - move this into `aws-eks-teams` to avoid getting out of sync,"locals {

  context = {
    # Data resources
    aws_region_name = data.aws_region.current.name
    # aws_caller_identity
    aws_caller_identity_account_id = data.aws_caller_identity.current.account_id
    aws_caller_identity_arn        = data.aws_caller_identity.current.arn
    # aws_partition
    aws_partition_id         = data.aws_partition.current.id
    aws_partition_dns_suffix = data.aws_partition.current.dns_suffix
    # http details
    http_endpoint               = ""enabled""
    http_tokens                 = ""required""
    http_put_response_hop_limit = 2 # Hop limit should be between 2 and 64 for IMDSv2 instance metadata services
  }

  eks_cluster_id     = module.aws_eks.cluster_id
  cluster_ca_base64  = module.aws_eks.cluster_certificate_authority_data
  cluster_endpoint   = module.aws_eks.cluster_endpoint
  vpc_id             = var.vpc_id
  private_subnet_ids = var.private_subnet_ids
  public_subnet_ids  = var.public_subnet_ids

  enable_workers            = length(var.self_managed_node_groups) > 0 || length(var.managed_node_groups) > 0 ? true : false
  worker_security_group_ids = local.enable_workers ? compact(flatten([[module.aws_eks.node_security_group_id], var.worker_additional_security_group_ids])) : []

  node_group_context = {
    # EKS Cluster Config
    eks_cluster_id    = local.eks_cluster_id
    cluster_ca_base64 = local.cluster_ca_base64
    cluster_endpoint  = local.cluster_endpoint
    cluster_version   = var.cluster_version
    # VPC Config
    vpc_id             = local.vpc_id
    private_subnet_ids = local.private_subnet_ids
    public_subnet_ids  = local.public_subnet_ids

    # Worker Security Group
    worker_security_group_ids = local.worker_security_group_ids

    # Http config
    http_endpoint               = local.context.http_endpoint
    http_tokens                 = local.context.http_tokens
    http_put_response_hop_limit = local.context.http_put_response_hop_limit

    # Data sources
    aws_partition_dns_suffix = local.context.aws_partition_dns_suffix
    aws_partition_id         = local.context.aws_partition_id

    iam_role_path                 = var.iam_role_path
    iam_role_permissions_boundary = var.iam_role_permissions_boundary

    # Service IPv4/IPv6 CIDR range
    service_ipv6_cidr = var.cluster_service_ipv6_cidr
    service_ipv4_cidr = var.cluster_service_ipv4_cidr

    tags = var.tags
  }

  fargate_context = {
    eks_cluster_id                = local.eks_cluster_id
    aws_partition_id              = local.context.aws_partition_id
    iam_role_path                 = var.iam_role_path
    iam_role_permissions_boundary = var.iam_role_permissions_boundary
    tags                          = var.tags
  }

  # Managed node IAM Roles for aws-auth
  managed_node_group_aws_auth_config_map = length(var.managed_node_groups) > 0 == true ? [
    for key, node in var.managed_node_groups : {
      rolearn : try(node.iam_role_arn, ""arn:${local.context.aws_partition_id}:iam::${local.context.aws_caller_identity_account_id}:role/${module.aws_eks.cluster_id}-${node.node_group_name}"")
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes""
      ]
    }
  ] : []

  # Self Managed node IAM Roles for aws-auth
  self_managed_node_group_aws_auth_config_map = length(var.self_managed_node_groups) > 0 ? [
    for key, node in var.self_managed_node_groups : {
      rolearn : try(node.iam_role_arn, ""arn:${local.context.aws_partition_id}:iam::${local.context.aws_caller_identity_account_id}:role/${module.aws_eks.cluster_id}-${node.node_group_name}"")
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes""
      ]
    } if node.launch_template_os != ""windows""
  ] : []

  # Self Managed Windows node IAM Roles for aws-auth
  windows_node_group_aws_auth_config_map = length(var.self_managed_node_groups) > 0 && var.enable_windows_support ? [
    for key, node in var.self_managed_node_groups : {
      rolearn : ""arn:${local.context.aws_partition_id}:iam::${local.context.aws_caller_identity_account_id}:role/${module.aws_eks.cluster_id}-${node.node_group_name}""
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes"",
        ""eks:kube-proxy-windows""
      ]
    } if node.launch_template_os == ""windows""
  ] : []

  # Fargate node IAM Roles for aws-auth
  fargate_profiles_aws_auth_config_map = length(var.fargate_profiles) > 0 ? [
    for key, node in var.fargate_profiles : {
      rolearn : try(node.iam_role_arn, ""arn:${local.context.aws_partition_id}:iam::${local.context.aws_caller_identity_account_id}:role/${module.aws_eks.cluster_id}-${node.fargate_profile_name}"")
      username : ""system:node:{{SessionName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes"",
        ""system:node-proxier""
      ]
    }
  ] : []

  # EMR on EKS IAM Roles for aws-auth
  emr_on_eks_config_map = var.enable_emr_on_eks == true ? [
    {
      rolearn : ""arn:${local.context.aws_partition_id}:iam::${local.context.aws_caller_identity_account_id}:role/AWSServiceRoleForAmazonEMRContainers""
      username : ""emr-containers""
      groups : []
    }
  ] : []

  # Teams
  partition  = local.context.aws_partition_id
  account_id = local.context.aws_caller_identity_account_id

  # TODO - move this into `aws-eks-teams` to avoid getting out of sync
  platform_teams_config_map = length(var.platform_teams) > 0 ? [
    for platform_team_name, platform_team_data in var.platform_teams : {
      rolearn : ""arn:${local.partition}:iam::${local.account_id}:role/${module.aws_eks.cluster_id}-${platform_team_name}-access""
      username : ""${platform_team_name}""
      groups : [
        ""system:masters""
      ]
    }
  ] : []

  # TODO - move this into `aws-eks-teams` to avoid getting out of sync
  application_teams_config_map = length(var.application_teams) > 0 ? [
    for team_name, team_data in var.application_teams : {
      rolearn : ""arn:${local.partition}:iam::${local.account_id}:role/${module.aws_eks.cluster_id}-${team_name}-access""
      username : ""${team_name}""
      groups : [
        ""${team_name}-group""
      ]
    }
  ] : []

  cluster_iam_role_name = var.iam_role_name == null ? ""${var.cluster_name}-cluster-role"" : var.iam_role_name
  cluster_iam_role_arn  = var.create_iam_role ? ""arn:${local.context.aws_partition_id}:iam::${local.context.aws_caller_identity_account_id}:role/${local.cluster_iam_role_name}"" : var.iam_role_arn
}
",locals,,,132,0.0,4757dd4262db9ce2c4743aebb8bfb5486e29b6a4,19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/4757dd4262db9ce2c4743aebb8bfb5486e29b6a4/locals.tf#L132,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1/locals.tf#L0,2022-05-26 19:12:05-04:00,2023-06-05 10:07:47-04:00,6,2,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,613,examples/data-solutions/data-platform-foundations/variables.tf,examples/data-solutions/data-platform-foundations/variables.tf,0,#todo,#TODO Move to network,#TODO Move to network,"variable ""composer_config"" {
  type = object({
    node_count = number
    #TODO Move to network
    ip_range_cloudsql   = string
    ip_range_gke_master = string
    ip_range_web_server = string
    #TODO hardcoded
    project_policy_boolean = map(bool)
    region                 = string
    ip_allocation_policy = object({
      use_ip_aliases                = string
      cluster_secondary_range_name  = string
      services_secondary_range_name = string
    })
    #TODO Add Env variables, Airflow version
  })
  default = {
    node_count             = 3
    ip_range_cloudsql      = ""10.20.10.0/24""
    ip_range_gke_master    = ""10.20.11.0/28""
    ip_range_web_server    = ""10.20.11.16/28""
    project_policy_boolean = null
    region                 = ""europe-west1""
    ip_allocation_policy = {
      use_ip_aliases                = ""true""
      cluster_secondary_range_name  = ""pods""
      services_secondary_range_name = ""services""
    }
  }
}
",variable,"variable ""composer_config"" {
  type = object({
    node_count      = number
    airflow_version = string
    env_variables   = map(string)
  })
  default = {
    node_count      = 3
    airflow_version = ""composer-1.17.5-airflow-2.1.4""
    env_variables   = {}
  }
}
",variable,33,,2e560407c118e7b7abc32f8ac1788a3f48563f21,d8bad5779036aa31639e4611e4935287fc79a4bc,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2e560407c118e7b7abc32f8ac1788a3f48563f21/examples/data-solutions/data-platform-foundations/variables.tf#L33,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d8bad5779036aa31639e4611e4935287fc79a4bc/examples/data-solutions/data-platform-foundations/variables.tf,2022-02-07 17:51:06+01:00,2022-02-07 21:28:54+01:00,2,1,0,1,0,0,1,0,0,0
https://github.com/nebari-dev/nebari,11,qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf,qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf,0,implement,#       A decision has been made to not implement an actual schema at,"# NOTE: While we define a schema, it is a dummy schema that doesn't 
 #       validate anything. We just have it to comply with the schema of 
 #       a CustomResourceDefinition that requires it. 
 # 
 #       A decision has been made to not implement an actual schema at 
 #       this point in time due to the additional maintenance work it 
 #       would require. 
 # 
 #       Reference: https://github.com/dask/dask-gateway/issues/434 
 #","resource ""kubernetes_manifest"" ""main"" {
  manifest = {
    apiVersion = ""apiextensions.k8s.io/v1""
    kind       = ""CustomResourceDefinition""
    metadata = {
      name = ""daskclusters.gateway.dask.org""
    }
    spec = {
      group   = ""gateway.dask.org""
      names = {
        kind     = ""DaskCluster""
        listKind = ""DaskClusterList""
        plural   = ""daskclusters""
        singular = ""daskcluster""
      }
      scope = ""Namespaced""
      versions = [{
        name = ""v1alpha1""
        served = true
        storage = true
        subresources = {
          status = {}
        }

        # NOTE: While we define a schema, it is a dummy schema that doesn't
        #       validate anything. We just have it to comply with the schema of
        #       a CustomResourceDefinition that requires it.
        #
        #       A decision has been made to not implement an actual schema at
        #       this point in time due to the additional maintenance work it
        #       would require.
        #
        #       Reference: https://github.com/dask/dask-gateway/issues/434
        #
        schema = {
          openAPIV3Schema = {
            type = ""object""
            # FIXME: Make this an actual schema instead of this dummy schema that
            #        is a workaround to meet the requirement of having a schema.
            x-kubernetes-preserve-unknown-fields = true
          }
        }
      }]
    }
  }
}
",resource,"resource ""kubernetes_manifest"" ""main"" {
  manifest = {
    apiVersion = ""apiextensions.k8s.io/v1""
    kind       = ""CustomResourceDefinition""
    metadata = {
      name = ""daskclusters.gateway.dask.org""
    }
    spec = {
      group = ""gateway.dask.org""
      names = {
        kind     = ""DaskCluster""
        listKind = ""DaskClusterList""
        plural   = ""daskclusters""
        singular = ""daskcluster""
      }
      scope = ""Namespaced""
      versions = [{
        name    = ""v1alpha1""
        served  = true
        storage = true
        subresources = {
          status = {}
        }

        # NOTE: While we define a schema, it is a dummy schema that doesn't
        #       validate anything. We just have it to comply with the schema of
        #       a CustomResourceDefinition that requires it.
        #
        #       A decision has been made to not implement an actual schema at
        #       this point in time due to the additional maintenance work it
        #       would require.
        #
        #       Reference: https://github.com/dask/dask-gateway/issues/434
        #
        schema = {
          openAPIV3Schema = {
            type = ""object""
            # FIXME: Make this an actual schema instead of this dummy schema that
            #        is a workaround to meet the requirement of having a schema.
            x-kubernetes-preserve-unknown-fields = true
          }
        }
      }]
    }
  }
}
",resource,29,29.0,e65621ed9fc3e374626cc3929742df6ba94fc8d7,d0cc26638fbc9e69aa736105ffd61cfb50d561d6,https://github.com/nebari-dev/nebari/blob/e65621ed9fc3e374626cc3929742df6ba94fc8d7/qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf#L29,https://github.com/nebari-dev/nebari/blob/d0cc26638fbc9e69aa736105ffd61cfb50d561d6/qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf#L29,2022-02-03 11:12:40-05:00,2022-05-26 14:22:33-07:00,2,0,1,1,0,0,0,0,0,0
https://github.com/Azure/sap-automation,19,deploy/terraform/terraform-units/modules/sap_library/storage_accounts.tf,deploy/terraform/terraform-units/modules/sap_library/storage_accounts.tf,0,// todo,// TODO: soft delete for file share,// TODO: soft delete for file share,"resource ""azurerm_storage_account"" ""storage_sapbits"" {
  provider                  = azurerm.main
  count                     = local.sa_sapbits_exists ? 0 : 1
  name                      = local.sa_sapbits_name
  resource_group_name       = local.rg_name
  location                  = local.rg_library_location
  account_replication_type  = local.sa_sapbits_account_replication_type
  account_tier              = local.sa_sapbits_account_tier
  account_kind              = local.sa_sapbits_account_kind
  enable_https_traffic_only = local.sa_sapbits_enable_secure_transfer
  // To support all access levels 'Blob' 'Private' and 'Container'
  allow_blob_public_access = true
  // TODO: soft delete for file share

  network_rules {
    default_action = ""Allow""
    ip_rules = var.use_private_endpoint ? (
      [length(local.deployer_public_ip_address) > 0 ? local.deployer_public_ip_address : null]) : (
      []
    )

    virtual_network_subnet_ids = var.use_private_endpoint ? [local.subnet_mgmt_id] : []
  }
}
",resource,"resource ""azurerm_storage_account"" ""storage_sapbits"" {
  provider                  = azurerm.main
  count                     = local.sa_sapbits_exists ? 0 : 1
  name                      = local.sa_sapbits_name
  resource_group_name       = local.rg_name
  location                  = local.rg_library_location
  account_replication_type  = local.sa_sapbits_account_replication_type
  account_tier              = local.sa_sapbits_account_tier
  account_kind              = local.sa_sapbits_account_kind
  enable_https_traffic_only = local.sa_sapbits_enable_secure_transfer

  network_rules {
    default_action = ""Allow""
    ip_rules = var.use_private_endpoint ? (
      [length(local.deployer_public_ip_address) > 0 ? local.deployer_public_ip_address : null]) : (
      []
    )

    virtual_network_subnet_ids = var.use_private_endpoint ? [local.subnet_management_id] : []
  }
}
",resource,115,,6ff0b891114c36d3aeccb850d830b698cd1fe52a,6343a9003b2bdbfa90c242903fabd92b8a3d0322,https://github.com/Azure/sap-automation/blob/6ff0b891114c36d3aeccb850d830b698cd1fe52a/deploy/terraform/terraform-units/modules/sap_library/storage_accounts.tf#L115,https://github.com/Azure/sap-automation/blob/6343a9003b2bdbfa90c242903fabd92b8a3d0322/deploy/terraform/terraform-units/modules/sap_library/storage_accounts.tf,2021-11-17 19:29:07+02:00,2022-03-25 09:58:42+02:00,6,1,1,1,0,1,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,331,modules/kubernetes-addons/main.tf,modules/kubernetes-addons/main.tf,0,todo,# TODO - remove local source and revert to remote once,"# source  = ""tetratelabs/tetrate-istio-addon/eksblueprints"" 
 # version = ""0.0.7""  
 # TODO - remove local source and revert to remote once 
 # https://github.com/tetratelabs/terraform-eksblueprints-tetrate-istio-addon/pull/12  is merged","module ""tetrate_istio"" {
  # source  = ""tetratelabs/tetrate-istio-addon/eksblueprints""
  # version = ""0.0.7""

  # TODO - remove local source and revert to remote once
  # https://github.com/tetratelabs/terraform-eksblueprints-tetrate-istio-addon/pull/12  is merged
  source = ""./tetrate-istio""

  count = var.enable_tetrate_istio ? 1 : 0

  distribution         = var.tetrate_istio_distribution
  distribution_version = var.tetrate_istio_version
  install_base         = var.tetrate_istio_install_base
  install_cni          = var.tetrate_istio_install_cni
  install_istiod       = var.tetrate_istio_install_istiod
  install_gateway      = var.tetrate_istio_install_gateway
  base_helm_config     = var.tetrate_istio_base_helm_config
  cni_helm_config      = var.tetrate_istio_cni_helm_config
  istiod_helm_config   = var.tetrate_istio_istiod_helm_config
  gateway_helm_config  = var.tetrate_istio_gateway_helm_config
  manage_via_gitops    = var.argocd_manage_add_ons
  addon_context        = local.addon_context
}
",module,,,347,0.0,fd55f69d2f73bd1e975d90826be76e9344bb769b,19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/fd55f69d2f73bd1e975d90826be76e9344bb769b/modules/kubernetes-addons/main.tf#L347,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1/modules/kubernetes-addons/main.tf#L0,2022-10-10 12:47:57-04:00,2023-06-05 10:07:47-04:00,28,2,1,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1327,blueprints/data-solutions/shielded-folder/log-export.tf,blueprints/data-solutions/shielded-folder/log-export.tf,0,#todo,#TODO check if logging bucket support encryption.,#TODO check if logging bucket support encryption.,"module ""log-export-logbucket"" {
  source      = ""../../../modules/logging-bucket""
  for_each    = toset([for k, v in var.log_sinks : k if v.type == ""logging""])
  parent_type = ""project""
  parent      = module.log-export-project.project_id
  id          = ""audit-logs-${each.key}""
  location    = var.log_locations.logging
  #TODO check if logging bucket support encryption.
}
",module,"module ""log-export-logbucket"" {
  source      = ""../../../modules/logging-bucket""
  for_each    = var.enable_features.log_sink ? toset([for k, v in var.log_sinks : k if v.type == ""logging""]) : []
  parent_type = ""project""
  parent      = module.log-export-project[0].project_id
  id          = ""audit-logs-${each.key}""
  location    = var.log_locations.logging
  #TODO check if logging bucket support encryption.
}
",module,85,124.0,4007d42705a930e9e526a8da3616712ad0e646f6,3af7e257d21f889ffaf7b32a3bab974fdbfda6e4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/4007d42705a930e9e526a8da3616712ad0e646f6/blueprints/data-solutions/shielded-folder/log-export.tf#L85,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3af7e257d21f889ffaf7b32a3bab974fdbfda6e4/blueprints/data-solutions/shielded-folder/log-export.tf#L124,2023-01-21 01:08:51+01:00,2024-04-17 10:23:48+02:00,11,0,1,0,0,1,0,0,1,1
https://github.com/Worklytics/psoxy,4393,infra/modules/aws-host/variables.tf,infra/modules/aws-host/variables.tf,0,# todo,"# TODO: generalize to 'secrets', regardless of store (AWS SSM, AWS Secrets Manager, etc)","# TODO: generalize to 'secrets', regardless of store (AWS SSM, AWS Secrets Manager, etc)","variable ""aws_ssm_key_id"" {
  type        = string
  description = ""KMS key id to use for encrypting SSM SecureString parameters created by this module, in any. (by default, will encrypt with AWS-managed keys)""
  default     = null
}
",variable,"variable ""aws_ssm_key_id"" {
  type        = string
  description = ""KMS key id to use for encrypting SSM SecureString parameters created by this module, in any. (by default, will encrypt with AWS-managed keys)""
  default     = null
}
",variable,21,33.0,005e1fed5f46b4310d81d41d29862bb1c4f360b0,078f8d7ab53aae24a1510fabb9f2d2f1482e3461,https://github.com/Worklytics/psoxy/blob/005e1fed5f46b4310d81d41d29862bb1c4f360b0/infra/modules/aws-host/variables.tf#L21,https://github.com/Worklytics/psoxy/blob/078f8d7ab53aae24a1510fabb9f2d2f1482e3461/infra/modules/aws-host/variables.tf#L33,2024-02-06 19:07:07+00:00,2024-04-01 09:04:07-07:00,8,0,0,1,0,1,0,0,0,0
https://github.com/alphagov/govuk-aws,126,terraform/projects/infra-security-groups/backend.tf,terraform/projects/infra-security-groups/backend.tf,0,# todo,# TODO: replace this with ingress from the backend LBs when we build them.,# TODO: replace this with ingress from the backend LBs when we build them.,"resource ""aws_security_group_rule"" ""allow_management_to_backend_elb"" {
  type      = ""ingress""
  from_port = 443
  to_port   = 443
  protocol  = ""tcp""

  security_group_id        = ""${aws_security_group.backend_elb.id}""
  source_security_group_id = ""${aws_security_group.management.id}""
}
",resource,the block associated got renamed or deleted,,47,,2bcd507fd7191532adf6109c13fd3e6d1a0b9899,923e481addb08a4badef9a4ac1d531d4bbfea4b5,https://github.com/alphagov/govuk-aws/blob/2bcd507fd7191532adf6109c13fd3e6d1a0b9899/terraform/projects/infra-security-groups/backend.tf#L47,https://github.com/alphagov/govuk-aws/blob/923e481addb08a4badef9a4ac1d531d4bbfea4b5/terraform/projects/infra-security-groups/backend.tf,2017-07-20 12:27:27+01:00,2017-09-15 17:02:09+01:00,4,1,0,1,0,1,1,0,0,0
https://github.com/Worklytics/psoxy,4405,infra/modules/aws-secretsmanager-secrets/main.tf,infra/modules/aws-secretsmanager-secrets/main.tf,0,implementation,# NOTE: value of this module is a consistent interface across potential Secret store implementations,"# stores secrets as AWS Secret Manager Secrets 
 # NOTE: value of this module is a consistent interface across potential Secret store implementations 
 #   eg, GCP Secret Manager, AWS SSM Parameter Store, Hashicorp Vault, AWS Secrets Manager, etc. 
 #  but is this good Terraform style? clearly in AWS case, this module doesn't do much ... ","locals {
  # AWS SSM param name must be fully qualified if contains `/`;
  # so test for that case, and prefix with `/` if needed
  non_empty_path           = length(var.path) > 0
  non_fully_qualified_path = length(regexall(""/"", var.path)) > 0 && !startswith(var.path, ""/"")
  path_prefix              = local.non_empty_path && local.non_fully_qualified_path ? ""/${var.path}"" : var.path
  PLACEHOLDER_VALUE        = ""fill me""

  # externally_managed_secrets = { for k, spec in var.secrets : k => spec if !(spec.value_managed_by_tf) }
  terraform_managed_secrets = { for k, spec in var.secrets : k => spec if spec.value_managed_by_tf }

  tf_management_description_appendix = ""Value managed by a Terraform configuration; changes outside Terraform may be overwritten by subsequent 'terraform apply' runs""
}
",locals,"locals {
  # AWS SSM param name must be fully qualified if contains `/`;
  # so test for that case, and prefix with `/` if needed
  non_empty_path           = length(var.path) > 0
  non_fully_qualified_path = length(regexall(""/"", var.path)) > 0 && !startswith(var.path, ""/"")
  path_prefix              = local.non_empty_path && local.non_fully_qualified_path ? ""/${var.path}"" : var.path
  PLACEHOLDER_VALUE        = ""fill me""

  # externally_managed_secrets = { for k, spec in var.secrets : k => spec if !(spec.value_managed_by_tf) }
  terraform_managed_secrets = { for k, spec in var.secrets : k => spec if spec.value_managed_by_tf }

  tf_management_description_appendix = ""Value managed by a Terraform configuration; changes outside Terraform may be overwritten by subsequent 'terraform apply' runs""
}
",locals,2,2.0,005e1fed5f46b4310d81d41d29862bb1c4f360b0,005e1fed5f46b4310d81d41d29862bb1c4f360b0,https://github.com/Worklytics/psoxy/blob/005e1fed5f46b4310d81d41d29862bb1c4f360b0/infra/modules/aws-secretsmanager-secrets/main.tf#L2,https://github.com/Worklytics/psoxy/blob/005e1fed5f46b4310d81d41d29862bb1c4f360b0/infra/modules/aws-secretsmanager-secrets/main.tf#L2,2024-02-06 19:07:07+00:00,2024-02-06 19:07:07+00:00,1,0,0,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-project-factory,175,modules/core_project_factory/main.tf,modules/core_project_factory/main.tf,0,todo,# TODO update source once released,"# TODO update source once released 
 #source  = ""terraform-google-modules/gcloud/google"" 
 #version = ""~> 0.1""","module ""gcloud_depriviledge"" {
  # TODO update source once released
  #source  = ""terraform-google-modules/gcloud/google""
  #version = ""~> 0.1""
  source = ""github.com/taylorludwig/terraform-google-gcloud?ref=feature%2Frun-script""

  enabled = var.default_service_account == ""depriviledge""

  create_script           = ""${path.module}/scripts/modify-service-account.sh""
  create_script_arguments = <<-EOT
    --project_id='${google_project.main.project_id}' \
    --sa_id='${data.null_data_source.default_service_account.outputs[""email""]}' \
    --credentials_path='${var.credentials_path}' \
    --impersonate-service-account='${var.impersonate_service_account}' \
    --action='depriviledge'
  EOT

  create_script_triggers = {
    default_service_account = data.null_data_source.default_service_account.outputs[""email""]
    activated_apis          = join("","", local.activate_apis)
    project_services        = module.project_services.project_id
  }
}
",module,the block associated got renamed or deleted,,212,,c0411a25011092d73d3b06dfd6a3e2c7b69acded,af94e3c4d48c039ff77a375fc122a37c0dcf83dc,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/c0411a25011092d73d3b06dfd6a3e2c7b69acded/modules/core_project_factory/main.tf#L212,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/af94e3c4d48c039ff77a375fc122a37c0dcf83dc/modules/core_project_factory/main.tf,2019-12-19 14:13:08-08:00,2019-12-21 22:57:14-08:00,3,1,0,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1305,blueprints/data-solutions/shielded-folder/maint.tf,blueprints/data-solutions/shielded-folder/main.tf,1,#todo,#TODO logsink,#TODO logsink,"module ""folder"" {
  source                 = ""../../../modules/folder""
  folder_create          = var.folder_create != null
  parent                 = try(var.folder_create.parent, null)
  name                   = try(var.folder_create.display_name, null)
  id                     = var.folder_id
  group_iam              = local.group_iam
  org_policies_data_path = ""${var.data_dir}/org-policies""
  firewall_policy_factory = {
    cidr_file   = ""${var.data_dir}/firewall-policies/cidrs.yaml""
    policy_name = ""hierarchical-policy""
    rules_file  = ""${var.data_dir}/firewall-policies/hierarchical-policy-rules.yaml""
  }
  #TODO logsink
}
",module,"module ""folder"" {
  source        = ""../../../modules/folder""
  folder_create = var.folder_create != null
  parent        = try(var.folder_create.parent, null)
  name          = try(var.folder_create.display_name, null)
  id            = var.folder_id
  iam = {
    ""roles/owner""                          = [""serviceAccount:${var.bootstrap_service_account}""]
    ""roles/resourcemanager.projectCreator"" = [""serviceAccount:${var.bootstrap_service_account}""]
  }
  group_iam              = local.group_iam
  org_policies_data_path = ""${var.data_dir}/org-policies""
  firewall_policy_factory = {
    cidr_file   = ""${var.data_dir}/firewall-policies/cidrs.yaml""
    policy_name = ""hierarchical-policy""
    rules_file  = ""${var.data_dir}/firewall-policies/hierarchical-policy-rules.yaml""
  }
  logging_sinks = {
    for name, attrs in var.log_sinks : name => {
      bq_partitioned_table = attrs.type == ""bigquery""
      destination          = local.log_sink_destinations[name].id
      filter               = attrs.filter
      type                 = attrs.type
    }
  }
}
",module,49,,84be665172b21220938ee702c4654e1a0cd0a584,4007d42705a930e9e526a8da3616712ad0e646f6,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/84be665172b21220938ee702c4654e1a0cd0a584/blueprints/data-solutions/shielded-folder/maint.tf#L49,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/4007d42705a930e9e526a8da3616712ad0e646f6/blueprints/data-solutions/shielded-folder/main.tf,2023-01-17 08:49:04+01:00,2023-01-21 01:08:51+01:00,2,1,0,1,0,0,0,0,1,0
https://github.com/alphagov/govuk-aws,1222,terraform/projects/app-locations-api/main.tf,terraform/projects/app-locations-api/main.tf,0,# todo,"# target_group_health_check_path   = ""/api/locations-api"" # TODO","# target_group_health_check_path   = ""/api/locations-api"" # TODO ","module ""internal_lb"" {
  source                           = ""../../modules/aws/lb""
  name                             = ""${var.stackname}-locations-api-internal""
  internal                         = true
  vpc_id                           = ""${data.terraform_remote_state.infra_vpc.vpc_id}""
  access_logs_bucket_name          = ""${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}""
  access_logs_bucket_prefix        = ""elb/locations-api-internal-lb""
  listener_certificate_domain_name = ""${var.elb_internal_certname}""
  # target_group_health_check_path   = ""/api/locations-api"" # TODO

  listener_action = {
    ""HTTPS:443"" = ""HTTP:80""
  }

  subnets         = [""${data.terraform_remote_state.infra_networking.private_subnet_ids}""]
  security_groups = [""${data.terraform_remote_state.infra_security_groups.sg_locations-api_internal_lb_id}""]
  alarm_actions   = [""${data.terraform_remote_state.infra_monitoring.sns_topic_cloudwatch_alarms_arn}""]

  default_tags = {
    Project         = ""${var.stackname}""
    aws_migration   = ""locations_api""
    aws_stackname   = ""${var.stackname}""
    aws_environment = ""${var.aws_environment}""
  }
}
",module,"module ""internal_lb"" {
  source                           = ""../../modules/aws/lb""
  name                             = ""${var.stackname}-locations-api-internal""
  internal                         = true
  vpc_id                           = ""${data.terraform_remote_state.infra_vpc.vpc_id}""
  access_logs_bucket_name          = ""${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}""
  access_logs_bucket_prefix        = ""elb/locations-api-internal-lb""
  listener_certificate_domain_name = ""${var.elb_internal_certname}""

  listener_action = {
    ""HTTPS:443"" = ""HTTP:80""
  }

  subnets         = [""${data.terraform_remote_state.infra_networking.private_subnet_ids}""]
  security_groups = [""${data.terraform_remote_state.infra_security_groups.sg_locations-api_internal_lb_id}""]
  alarm_actions   = [""${data.terraform_remote_state.infra_monitoring.sns_topic_cloudwatch_alarms_arn}""]

  default_tags = {
    Project         = ""${var.stackname}""
    aws_migration   = ""locations_api""
    aws_stackname   = ""${var.stackname}""
    aws_environment = ""${var.aws_environment}""
  }
}
",module,106,,fb9927c2442fd1464aed4736d4625b1b5ff90faf,e94ad50dd6fefb46c096d89f74c946c74ff50322,https://github.com/alphagov/govuk-aws/blob/fb9927c2442fd1464aed4736d4625b1b5ff90faf/terraform/projects/app-locations-api/main.tf#L106,https://github.com/alphagov/govuk-aws/blob/e94ad50dd6fefb46c096d89f74c946c74ff50322/terraform/projects/app-locations-api/main.tf,2022-03-10 07:52:45+00:00,2022-03-10 07:53:37+00:00,2,1,0,1,0,0,1,0,0,0
https://github.com/alphagov/govuk-aws,826,terraform/modules/aws/lb_listener_rules/main.tf,terraform/modules/aws/lb_listener_rules/main.tf,0,implement,"/** * ## Modules: aws/lb_listener_rules * * This module creates Load Balancer listener rules and target groups for * an existing listener resource. * * Limitations: *  - The target group deregistration_delay, health_check_interval and health_check_timeout * values can be configured with variables, but will be the same for all the target groups *  - With Terraform we can't provide a 'count' or list for listener_rule condition blocks, * so at the moment only one condition can be specified per rule *  - At the moment this module only implements Host Header based rules*/","/** 
 * ## Modules: aws/lb_listener_rules 
 * 
 * This module creates Load Balancer listener rules and target groups for 
 * an existing listener resource. 
 * 
 * Limitations: 
 *  - The target group deregistration_delay, health_check_interval and health_check_timeout 
 * values can be configured with variables, but will be the same for all the target groups 
 *  - With Terraform we can't provide a 'count' or list for listener_rule condition blocks, 
 * so at the moment only one condition can be specified per rule 
 *  - At the moment this module only implements Host Header based rules 
 */ ","variable ""default_tags"" {
  type        = ""map""
  description = ""Additional resource tags""
  default     = {}
}
",variable,"variable ""default_tags"" {
  type        = ""map""
  description = ""Additional resource tags""
  default     = {}
}
",variable,1,,5a798362e1d5c877a960847379a7aa278ec185d0,a272a6f68c811aa9bb956809fd9e2caec8fa8089,https://github.com/alphagov/govuk-aws/blob/5a798362e1d5c877a960847379a7aa278ec185d0/terraform/modules/aws/lb_listener_rules/main.tf#L1,https://github.com/alphagov/govuk-aws/blob/a272a6f68c811aa9bb956809fd9e2caec8fa8089/terraform/modules/aws/lb_listener_rules/main.tf,2019-05-24 14:55:00+01:00,2019-06-12 17:18:39+01:00,4,1,0,1,1,0,1,0,0,0
https://github.com/kubernetes/k8s.io,485,infra/aws/terraform/kops-infra-ci/eks.tf,infra/aws/terraform/kops-infra-ci/eks.tf,0,//todo,//TODO(ameukam): Use access entries,//TODO(ameukam): Use access entries,"module ""eks-auth"" {
  source  = ""terraform-aws-modules/eks/aws//modules/aws-auth""
  version = ""~> 20.0""

  manage_aws_auth_configmap = true

  aws_auth_roles = [
    {
      rolearn  = ""arn:aws:iam::468814281478:role/Prow-EKS-Admin""
      username = ""arn:aws:iam::468814281478:role/Prow-EKS-Admin""
      groups   = [""system:masters""]
    },
  ]

  aws_auth_users = [
    {
      userarn  = ""arn:aws:iam::${data.aws_organizations_organization.current.id}:user/ameukam""
      username = ""ameukam""
      groups   = [""system:masters""]
    },
  ]
}
",module,"module ""eks-auth"" {
  source  = ""terraform-aws-modules/eks/aws//modules/aws-auth""
  version = ""~> 20.0""

  manage_aws_auth_configmap = true

  aws_auth_roles = [
    {
      rolearn  = ""arn:aws:iam::468814281478:role/Prow-EKS-Admin""
      username = ""arn:aws:iam::468814281478:role/Prow-EKS-Admin""
      groups   = [""system:masters""]
    },
  ]

  aws_auth_users = [
    {
      userarn  = ""arn:aws:iam::${data.aws_organizations_organization.current.id}:user/ameukam""
      username = ""ameukam""
      groups   = [""system:masters""]
    },
  ]
}
",module,146,146.0,d67626296482f3df01968377c828ffac093efee8,d67626296482f3df01968377c828ffac093efee8,https://github.com/kubernetes/k8s.io/blob/d67626296482f3df01968377c828ffac093efee8/infra/aws/terraform/kops-infra-ci/eks.tf#L146,https://github.com/kubernetes/k8s.io/blob/d67626296482f3df01968377c828ffac093efee8/infra/aws/terraform/kops-infra-ci/eks.tf#L146,2024-03-12 17:14:02+01:00,2024-03-12 17:14:02+01:00,1,0,1,0,0,1,0,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,394,azure/network.tf,azure/network.tf,0,todo,// TODO make the monitoring rule more stricter for some instance later on,"// Todo this security group need to be redifined by cluster roles instead of 1 generic 
 // TODO make the monitoring rule more stricter for some instance later on","resource ""azurerm_network_security_group"" ""mysecgroup"" {
  name                = ""mysecgroup""
  location            = var.az_region
  resource_group_name = azurerm_resource_group.myrg.name
// Todo this security group need to be redifined by cluster roles instead of 1 generic
// TODO make the monitoring rule more stricter for some instance later on
  security_rule {
    name                       = ""OUTALL""
    priority                   = 100
    direction                  = ""Outbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""*""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""LOCAL""
    priority                   = 101
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""*""
    source_address_prefix      = ""10.74.0.0/16""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""SSH""
    priority                   = 1001
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""22""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HTTP""
    priority                   = 1002
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""80""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HTTPS""
    priority                   = 1003
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""443""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HAWK""
    priority                   = 1004
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""7630""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  // monitoring rules
    security_rule {
    name                       = ""nodeExporter""
    priority                   = 1005
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9100""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  security_rule {
    name                       = ""hanadbExporter""
    priority                   = 1006
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""8001""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  security_rule {
    name                       = ""hawkExporter""
    priority                   = 1007
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9001""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""prometheus""
    priority                   = 1008
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9090""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }


  tags = {
    workspace = terraform.workspace
  }
}
",resource,"resource ""azurerm_network_security_group"" ""mysecgroup"" {
  name                = ""mysecgroup""
  location            = var.az_region
  resource_group_name = azurerm_resource_group.myrg.name
  security_rule {
    name                       = ""OUTALL""
    priority                   = 100
    direction                  = ""Outbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""*""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""LOCAL""
    priority                   = 101
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""*""
    source_address_prefix      = ""10.74.0.0/16""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""SSH""
    priority                   = 1001
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""22""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HTTP""
    priority                   = 1002
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""80""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HTTPS""
    priority                   = 1003
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""443""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HAWK""
    priority                   = 1004
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""7630""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  // monitoring rules
  security_rule {
    name                       = ""nodeExporter""
    priority                   = 1005
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9100""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  security_rule {
    name                       = ""hanadbExporter""
    priority                   = 1006
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""8001""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  security_rule {
    name                       = ""hawkExporter""
    priority                   = 1007
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9001""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""prometheus""
    priority                   = 1008
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9090""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }


  tags = {
    workspace = terraform.workspace
  }
}
",resource,305,,517f39e8d665afe6d504a325cda67274995c8e59,9adcf152486838f9f5b600ead5e4ef373918a786,https://github.com/SUSE/ha-sap-terraform-deployments/blob/517f39e8d665afe6d504a325cda67274995c8e59/azure/network.tf#L305,https://github.com/SUSE/ha-sap-terraform-deployments/blob/9adcf152486838f9f5b600ead5e4ef373918a786/azure/network.tf,2019-09-05 00:02:24+02:00,2019-09-05 18:08:13+02:00,3,1,0,0,0,1,1,0,1,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1594,blueprints/factories/project-factory/main.tf,modules/project-factory/main.tf,1,# todo,# TODO: concat lists for each key,# TODO: concat lists for each key,"module ""projects"" {
  source              = ""../../../modules/project""
  for_each            = local.projects
  billing_account     = each.value.billing_account
  name                = each.key
  parent              = try(each.value.parent, null)
  prefix              = each.value.prefix
  auto_create_network = try(each.value.auto_create_network, false)
  compute_metadata    = try(each.value.compute_metadata, {})
  # TODO: concat lists for each key
  contacts = merge(
    each.value.contacts, var.data_merges.contacts
  )
  default_service_account = try(each.value.default_service_account, ""keep"")
  descriptive_name        = try(each.value.descriptive_name, null)
  group_iam               = try(each.value.group_iam, {})
  iam                     = try(each.value.iam, {})
  iam_bindings            = try(each.value.iam_bindings, {})
  iam_bindings_additive   = try(each.value.iam_bindings_additive, {})
  labels                  = each.value.labels
  lien_reason             = try(each.value.lien_reason, null)
  logging_data_access     = try(each.value.logging_data_access, {})
  logging_exclusions      = try(each.value.logging_exclusions, {})
  logging_sinks           = try(each.value.logging_sinks, {})
  metric_scopes = distinct(concat(
    each.value.metric_scopes, var.data_merges.metric_scopes
  ))
  service_encryption_key_ids = merge(
    each.value.service_encryption_key_ids,
    var.data_merges.service_encryption_key_ids
  )
  service_perimeter_bridges = distinct(concat(
    each.value.service_perimeter_bridges,
    var.data_merges.service_perimeter_bridges
  ))
  service_perimeter_standard = each.value.service_perimeter_standard
  services = distinct(concat(
    each.value.services,
    var.data_merges.services
  ))
  shared_vpc_service_config = each.value.shared_vpc_service_config
  tag_bindings = merge(
    each.value.tag_bindings,
    var.data_merges.tag_bindings
  )
}
",module,"module ""projects"" {
  source          = ""../project""
  for_each        = local.projects
  billing_account = each.value.billing_account
  name            = each.key
  parent = try(
    lookup(local.hierarchy, each.value.parent, each.value.parent), null
  )
  prefix              = each.value.prefix
  auto_create_network = try(each.value.auto_create_network, false)
  compute_metadata    = try(each.value.compute_metadata, {})
  # TODO: concat lists for each key
  contacts = merge(
    each.value.contacts, var.data_merges.contacts
  )
  default_service_account = try(each.value.default_service_account, ""keep"")
  descriptive_name        = try(each.value.descriptive_name, null)
  # IAM interpolates automation service accounts
  iam = {
    for k, v in lookup(each.value, ""iam"", {}) : k => [
      for vv in v : try(
        module.automation-service-accounts[""${each.key}/${vv}""].iam_email,
        vv
      )
    ]
  }
  iam_bindings = {
    for k, v in lookup(each.value, ""iam_bindings"", {}) : k => merge(v, {
      members = [
        for vv in v.members : try(
          module.automation-service-accounts[""${each.key}/${vv}""].iam_email,
          vv
        )
      ]
    })
  }
  iam_bindings_additive = {
    for k, v in lookup(each.value, ""iam_bindings_additive"", {}) : k => merge(v, {
      member = try(
        module.automation-service-accounts[""${each.key}/${v.member}""].iam_email,
        v.member
      )
    })
  }
  # IAM principals would trigger dynamic key errors so we don't interpolate
  iam_by_principals = try(each.value.iam_by_principals, {})
  labels = merge(
    each.value.labels, var.data_merges.labels
  )
  lien_reason         = try(each.value.lien_reason, null)
  logging_data_access = try(each.value.logging_data_access, {})
  logging_exclusions  = try(each.value.logging_exclusions, {})
  logging_sinks       = try(each.value.logging_sinks, {})
  metric_scopes = distinct(concat(
    each.value.metric_scopes, var.data_merges.metric_scopes
  ))
  org_policies = each.value.org_policies
  service_encryption_key_ids = merge(
    each.value.service_encryption_key_ids,
    var.data_merges.service_encryption_key_ids
  )
  services = distinct(concat(
    each.value.services,
    var.data_merges.services
  ))
  shared_vpc_host_config    = each.value.shared_vpc_host_config
  shared_vpc_service_config = each.value.shared_vpc_service_config
  tag_bindings = merge(
    each.value.tag_bindings,
    var.data_merges.tag_bindings
  )
  vpc_sc = each.value.vpc_sc
}
",module,26,30.0,819894d2bab4b440f1b52b1ac8035912fb107004,ef5178c92901980294a90dbce384baa6b6479547,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/819894d2bab4b440f1b52b1ac8035912fb107004/blueprints/factories/project-factory/main.tf#L26,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/ef5178c92901980294a90dbce384baa6b6479547/modules/project-factory/main.tf#L30,2023-08-20 09:44:20+02:00,2024-05-22 07:56:34+00:00,13,0,0,1,0,1,0,0,0,0
https://github.com/compiler-explorer/infra,66,terraform/audit.tf,terraform/audit.tf,0,todo,// TODO one day,"  // TODO one day
  //  versioning {
  //    mfa_delete = true
  //  }","resource ""aws_s3_bucket"" ""cloudtrail"" {
  bucket        = ""cloudtrail.godbolt.org""
  force_destroy = true

  // TODO one day
  //  versioning {
  //    mfa_delete = true
  //  }

  lifecycle_rule {
    enabled = true
    expiration {
      days = 200
    }
    noncurrent_version_expiration {
      days = 1
    }
  }
}
",resource,"resource ""aws_s3_bucket"" ""cloudtrail"" {
  bucket        = ""cloudtrail.godbolt.org""
  force_destroy = true

  tags = {
    S3-Bucket-Name = ""cloudtrail.godbolt.org""
  }
}
",resource,47,,b15a4a55a344a8ab58d9b1ca511e08f252034bd2,fc51de832669630cc2f3cad2d8e3f5f838f16c94,https://github.com/compiler-explorer/infra/blob/b15a4a55a344a8ab58d9b1ca511e08f252034bd2/terraform/audit.tf#L47,https://github.com/compiler-explorer/infra/blob/fc51de832669630cc2f3cad2d8e3f5f838f16c94/terraform/audit.tf,2020-06-15 22:57:58-05:00,2024-03-10 19:26:48-05:00,8,1,1,1,0,0,0,0,0,0
https://github.com/Azure/terraform-azurerm-caf-enterprise-scale,2,archetypes/locals.policy_definitions.tf,modules/archetypes/locals.policy_definitions.tf,1,implemented,# Logic implemented to determine whether Policy Definitions,"# Generate the Policy Definition configurations for the specified archetype. 
 # Logic implemented to determine whether Policy Definitions 
 # need to be loaded to save on compute and memory resources 
 # when none defined in archetype definition.","locals {
  archetype_policy_definitions_list      = local.archetype_definition.policy_definitions
  archetype_policy_definitions_specified = try(length(local.archetype_policy_definitions_list) > 0, false)
}
",locals,"locals {
  archetype_policy_definitions_list      = local.archetype_definition.policy_definitions
  archetype_policy_definitions_specified = try(length(local.archetype_policy_definitions_list) > 0, false)
}
",locals,2,2.0,a05be92e463f90e577936a84395ffb88cfd045e7,1b41f7062f286e790be9b2e16a74562936f691df,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/a05be92e463f90e577936a84395ffb88cfd045e7/archetypes/locals.policy_definitions.tf#L2,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/1b41f7062f286e790be9b2e16a74562936f691df/modules/archetypes/locals.policy_definitions.tf#L2,2020-09-25 20:39:19+01:00,2022-01-27 09:57:26+00:00,6,0,0,0,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,102,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,0,# todo,# TODO: rename to subnetwork_self_link,# TODO: rename to subnetwork_self_link,"variable ""nodeset"" {
  description = ""Define nodesets, as a list.""
  type = list(object({
    node_count_static      = optional(number, 0)
    node_count_dynamic_max = optional(number, 1)
    node_conf              = optional(map(string), {})
    nodeset_name           = string
    additional_disks = optional(list(object({
      disk_name    = optional(string)
      device_name  = optional(string)
      disk_size_gb = optional(number)
      disk_type    = optional(string)
      disk_labels  = optional(map(string), {})
      auto_delete  = optional(bool, true)
      boot         = optional(bool, false)
    })), [])
    bandwidth_tier         = optional(string, ""platform_default"")
    can_ip_forward         = optional(bool, false)
    disable_smt            = optional(bool, false)
    disk_auto_delete       = optional(bool, true)
    disk_labels            = optional(map(string), {})
    disk_size_gb           = optional(number)
    disk_type              = optional(string)
    enable_confidential_vm = optional(bool, false)
    enable_placement       = optional(bool, false)
    enable_public_ip       = optional(bool, false)
    enable_oslogin         = optional(bool, true)
    enable_shielded_vm     = optional(bool, false)
    gpu = optional(object({
      count = number
      type  = string
    }))
    instance_template   = optional(string)
    labels              = optional(map(string), {})
    machine_type        = optional(string)
    metadata            = optional(map(string), {})
    min_cpu_platform    = optional(string)
    network_tier        = optional(string, ""STANDARD"")
    on_host_maintenance = optional(string)
    preemptible         = optional(bool, false)
    region              = optional(string)
    service_account = optional(object({
      email  = optional(string)
      scopes = optional(list(string), [""https://www.googleapis.com/auth/cloud-platform""])
    }))
    shielded_instance_config = optional(object({
      enable_integrity_monitoring = optional(bool, true)
      enable_secure_boot          = optional(bool, true)
      enable_vtpm                 = optional(bool, true)
    }))
    source_image_family  = optional(string)
    source_image_project = optional(string)
    source_image         = optional(string)
    subnetwork_project   = optional(string)
    # TODO: rename to subnetwork_self_link 
    subnetwork         = optional(string)
    spot               = optional(bool, false)
    tags               = optional(list(string), [])
    termination_action = optional(string)
    zones              = optional(list(string), [])
    zone_target_shape  = optional(string, ""ANY_SINGLE_ZONE"")
  }))
  default = []

  validation {
    condition     = length(distinct([for x in var.nodeset : x.nodeset_name])) == length(var.nodeset)
    error_message = ""All nodesets must have a unique name.""
  }
}
",variable,"variable ""nodeset"" {
  description = ""Define nodesets, as a list.""
  type = list(object({
    node_count_static      = optional(number, 0)
    node_count_dynamic_max = optional(number, 1)
    node_conf              = optional(map(string), {})
    nodeset_name           = string
    additional_disks = optional(list(object({
      disk_name    = optional(string)
      device_name  = optional(string)
      disk_size_gb = optional(number)
      disk_type    = optional(string)
      disk_labels  = optional(map(string), {})
      auto_delete  = optional(bool, true)
      boot         = optional(bool, false)
    })), [])
    bandwidth_tier         = optional(string, ""platform_default"")
    can_ip_forward         = optional(bool, false)
    disable_smt            = optional(bool, false)
    disk_auto_delete       = optional(bool, true)
    disk_labels            = optional(map(string), {})
    disk_size_gb           = optional(number)
    disk_type              = optional(string)
    enable_confidential_vm = optional(bool, false)
    enable_placement       = optional(bool, false)
    enable_public_ip       = optional(bool, false)
    enable_oslogin         = optional(bool, true)
    enable_shielded_vm     = optional(bool, false)
    gpu = optional(object({
      count = number
      type  = string
    }))
    instance_template   = optional(string)
    labels              = optional(map(string), {})
    machine_type        = optional(string)
    metadata            = optional(map(string), {})
    min_cpu_platform    = optional(string)
    network_tier        = optional(string, ""STANDARD"")
    on_host_maintenance = optional(string)
    preemptible         = optional(bool, false)
    region              = optional(string)
    service_account = optional(object({
      email  = optional(string)
      scopes = optional(list(string), [""https://www.googleapis.com/auth/cloud-platform""])
    }))
    shielded_instance_config = optional(object({
      enable_integrity_monitoring = optional(bool, true)
      enable_secure_boot          = optional(bool, true)
      enable_vtpm                 = optional(bool, true)
    }))
    source_image_family  = optional(string)
    source_image_project = optional(string)
    source_image         = optional(string)
    subnetwork_self_link = string
    spot                 = optional(bool, false)
    tags                 = optional(list(string), [])
    termination_action   = optional(string)
    zones                = optional(list(string), [])
    zone_target_shape    = optional(string, ""ANY_SINGLE_ZONE"")
  }))
  default = []

  validation {
    condition     = length(distinct([for x in var.nodeset : x.nodeset_name])) == length(var.nodeset)
    error_message = ""All nodesets must have a unique name.""
  }
}
",variable,221,,82199854e24ac10e7293605c5c4eae45748e8c99,3d1072da48450aa22b844bb5c288415b270616cc,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/82199854e24ac10e7293605c5c4eae45748e8c99/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf#L221,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/3d1072da48450aa22b844bb5c288415b270616cc/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,2023-11-09 19:50:03+00:00,2024-01-02 14:51:06-08:00,7,1,0,1,0,0,1,0,0,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,369,autoscaler-agents.tf,autoscaler-agents.tf,0,implement,"#! for now we use x86 snapshot, this needs to implement logic to autodetect between x86 and aarch64 based on server type","#! for now we use x86 snapshot, this needs to implement logic to autodetect between x86 and aarch64 based on server type","locals {
  cluster_prefix = var.use_cluster_name_in_node_name ? ""${var.cluster_name}-"" : """"
  autoscaler_yaml = length(var.autoscaler_nodepools) == 0 ? """" : templatefile(
    ""${path.module}/templates/autoscaler.yaml.tpl"",
    {
      cloudinit_config = base64encode(data.cloudinit_config.autoscaler-config[0].rendered)
      ca_image         = var.cluster_autoscaler_image
      ca_version       = var.cluster_autoscaler_version
      ssh_key          = local.hcloud_ssh_key_id
      # for now we use the k3s network, as we cannot reference subnet-ids in autoscaler
      ipv4_subnet_id = hcloud_network.k3s.id
      #! for now we use x86 snapshot, this needs to implement logic to autodetect between x86 and aarch64 based on server type
      snapshot_id    = data.hcloud_image.microos_x86_snapshot.id
      firewall_id    = hcloud_firewall.k3s.id
      cluster_name   = local.cluster_prefix
      node_pools     = var.autoscaler_nodepools
  })
  # A concatenated list of all autoscaled nodes
  autoscaled_nodes = length(var.autoscaler_nodepools) == 0 ? {} : {
    for v in concat([
      for k, v in data.
      hcloud_servers.autoscaled_nodes : [for v in v.servers : v]
    ]...) : v.name => v
  }
}
",locals,"locals {
  cluster_prefix = var.use_cluster_name_in_node_name ? ""${var.cluster_name}-"" : """"
  autoscaler_yaml = length(var.autoscaler_nodepools) == 0 ? """" : templatefile(
    ""${path.module}/templates/autoscaler.yaml.tpl"",
    {
      cloudinit_config = base64encode(data.cloudinit_config.autoscaler-config[0].rendered)
      ca_image         = var.cluster_autoscaler_image
      ca_version       = var.cluster_autoscaler_version
      ssh_key          = local.hcloud_ssh_key_id
      # for now we use the k3s network, as we cannot reference subnet-ids in autoscaler
      ipv4_subnet_id = hcloud_network.k3s.id
      # for now we use x86 snapshot, this needs to implement logic to autodetect between x86 and aarch64 based on server type
      snapshot_id    = data.hcloud_image.microos_x86_snapshot.id
      firewall_id    = hcloud_firewall.k3s.id
      cluster_name   = local.cluster_prefix
      node_pools     = var.autoscaler_nodepools
  })
  # A concatenated list of all autoscaled nodes
  autoscaled_nodes = length(var.autoscaler_nodepools) == 0 ? {} : {
    for v in concat([
      for k, v in data.
      hcloud_servers.autoscaled_nodes : [for v in v.servers : v]
    ]...) : v.name => v
  }
}
",locals,12,,338b42de4849df86985cd5a08fb8029f8fa345c9,297f4a16d496bfaa684bbbcbeb9139501638d3f2,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/338b42de4849df86985cd5a08fb8029f8fa345c9/autoscaler-agents.tf#L12,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/297f4a16d496bfaa684bbbcbeb9139501638d3f2/autoscaler-agents.tf,2023-04-14 16:00:33+02:00,2023-04-14 16:01:31+02:00,2,1,0,1,0,0,0,0,0,0
https://github.com/nasa/cumulus,30,tf-modules/cumulus/archive.tf,tf-modules/cumulus/archive.tf,0,todo,# TODO This should end up coming from the ingest module at some point,# TODO This should end up coming from the ingest module at some point,"data ""aws_lambda_function"" ""message_consumer"" {
  function_name = ""${var.prefix}-messageConsumer""
}
",data,the block associated got renamed or deleted,,6,,ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7,275b9a88869f413f231765ff1784ed9b3c6e042b,https://github.com/nasa/cumulus/blob/ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7/tf-modules/cumulus/archive.tf#L6,https://github.com/nasa/cumulus/blob/275b9a88869f413f231765ff1784ed9b3c6e042b/tf-modules/cumulus/archive.tf,2019-08-14 14:23:38-04:00,2019-08-26 10:31:45-04:00,3,1,1,1,0,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,203,modules/iam/group-workers.tf,modules/iam/group-workers.tf,0,todo,# TODO support keys defined at worker group level,# TODO support keys defined at worker group level,"locals {
  worker_group_name          = ""oke-workers-${var.state_id}""
  worker_compartments        = coalescelist(var.worker_compartments, [var.compartment_id])
  worker_compartment_matches = formatlist(""instance.compartment.id = '%s'"", local.worker_compartments)
  worker_compartment_rule    = format(""ANY {%s}"", join("", "", local.worker_compartment_matches))

  worker_group_rules = var.use_defined_tags ? format(""ALL {%s}"", join("", "", [
    ""tag.${var.tag_namespace}.role.value='worker'"",
    ""tag.${var.tag_namespace}.state_id.value='${var.state_id}'"",
  ])) : local.worker_compartment_rule

  cluster_join_where_clause = format(""ALL {%s}"", join("", "", compact([
    ""target.resource.kind = 'cluster'"",
    var.create_worker_policy ? ""target.cluster.id = '${var.cluster_id}'"" : null,
  ])))

  cluster_join_statements = formatlist(
    ""Allow dynamic-group %s to {CLUSTER_JOIN} in compartment id %s where %s"",
    local.worker_group_name, local.worker_compartments, local.cluster_join_where_clause
  )

  # TODO support keys defined at worker group level
  worker_kms_volume_templates = tolist([
    ""Allow service oke to USE key-delegates in compartment id %s where target.key.id = '%s'"",
    ""Allow service blockstorage to USE keys in compartment id %s where target.key.id = '%s'"",
    ""Allow dynamic-group ${local.worker_group_name} to USE key-delegates in compartment id %s where target.key.id = '%s'""
  ])

  # Block volume encryption using OCI Key Management System (KMS)
  worker_kms_volume_statements = coalesce(var.worker_volume_kms_key_id, ""none"") != ""none"" ? tolist([
    for statement in local.worker_kms_volume_templates :
    formatlist(statement, local.worker_compartments, var.worker_volume_kms_key_id)
  ]) : []

  worker_policy_statements = var.create_worker_policy ? concat(
    local.cluster_join_statements,
    local.worker_kms_volume_statements,
  ) : []
}
",locals,"locals {
  worker_group_name          = format(""oke-workers-%v"", var.state_id)
  worker_compartments        = coalescelist(var.worker_compartments, [var.compartment_id])
  worker_compartment_matches = formatlist(""instance.compartment.id = '%v'"", local.worker_compartments)
  worker_compartment_rule    = format(""ANY {%v}"", join("", "", local.worker_compartment_matches))

  worker_group_rules = var.use_defined_tags ? format(""ALL {%v}"", join("", "", [
    format(""tag.%v.role.value='worker'"", var.tag_namespace),
    format(""tag.%v.state_id.value='%v'"", var.tag_namespace, var.state_id),
  ])) : local.worker_compartment_rule

  cluster_join_where_clause = format(""ALL {%v}"", join("", "", compact([
    var.create_iam_worker_policy && var.cluster_id != null
    ? format(""target.cluster.id = %v"", var.cluster_id) : null
  ])))

  cluster_join_statements = formatlist(
    ""Allow dynamic-group %v to {CLUSTER_JOIN} in compartment id %v where %v"",
    local.worker_group_name, local.worker_compartments, local.cluster_join_where_clause
  )

  # TODO support keys defined at worker group level
  worker_kms_volume_templates = tolist([
    ""Allow service oke to USE key-delegates in compartment id %v where target.key.id = '%v'"",
    ""Allow service blockstorage to USE keys in compartment id %v where target.key.id = '%v'"",
    ""Allow dynamic-group ${local.worker_group_name} to USE key-delegates in compartment id %v where target.key.id = '%v'""
  ])

  # Block volume encryption using OCI Key Management System (KMS)
  worker_kms_volume_statements = coalesce(var.worker_volume_kms_key_id, ""none"") != ""none"" ? flatten(tolist([
    for statement in local.worker_kms_volume_templates :
    formatlist(statement, local.worker_compartments, var.worker_volume_kms_key_id)
  ])) : []

  worker_policy_statements = var.create_iam_worker_policy ? tolist(concat(
    local.cluster_join_statements,
    local.worker_kms_volume_statements,
  )) : []
}
",locals,25,25.0,6c867cd8e9cbf559742f56658989bcded0d1fd89,3ffaf123f92e82daa6d36bb6d9ef890d597ba234,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/modules/iam/group-workers.tf#L25,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/3ffaf123f92e82daa6d36bb6d9ef890d597ba234/modules/iam/group-workers.tf#L25,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,5,0,0,1,0,1,0,0,0,0
https://github.com/Azure/terraform-azurerm-caf-enterprise-scale,3,archetypes/locals.policy_set_definitions.tf,modules/archetypes/locals.policy_set_definitions.tf,1,implemented,# Logic implemented to determine whether Policy Set Definitions,"# Generate the Policy Set Definition configurations for the specified archetype. 
 # Logic implemented to determine whether Policy Set Definitions 
 # need to be loaded to save on compute and memory resources 
 # when none defined in archetype definition.","locals {
  archetype_policy_set_definitions_list      = local.archetype_definition.policy_set_definitions
  archetype_policy_set_definitions_specified = try(length(local.archetype_policy_set_definitions_list) > 0, false)
}
",locals,"locals {
  archetype_policy_set_definitions_list      = local.archetype_definition.policy_set_definitions
  archetype_policy_set_definitions_specified = try(length(local.archetype_policy_set_definitions_list) > 0, false)
}
",locals,2,2.0,a05be92e463f90e577936a84395ffb88cfd045e7,1b41f7062f286e790be9b2e16a74562936f691df,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/a05be92e463f90e577936a84395ffb88cfd045e7/archetypes/locals.policy_set_definitions.tf#L2,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/1b41f7062f286e790be9b2e16a74562936f691df/modules/archetypes/locals.policy_set_definitions.tf#L2,2020-09-25 20:39:19+01:00,2022-01-27 09:57:26+00:00,7,0,0,0,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,1625,infra/modules/gcp-psoxy-bulk/main.tf,infra/modules/gcp-psoxy-bulk/main.tf,0,# todo,# TODO: moved in 0.4.25 remove in 0.5,# TODO: moved in 0.4.25 remove in 0.5,"moved {
  from = google_service_account.service-account
  to   = google_service_account.service_account
}
",moved,the block associated got renamed or deleted,,115,,da62a471ef535349e3645f4a3644ada7cb23ed8e,bb41a6c349584a82517cb604a30196de7d807a75,https://github.com/Worklytics/psoxy/blob/da62a471ef535349e3645f4a3644ada7cb23ed8e/infra/modules/gcp-psoxy-bulk/main.tf#L115,https://github.com/Worklytics/psoxy/blob/bb41a6c349584a82517cb604a30196de7d807a75/infra/modules/gcp-psoxy-bulk/main.tf,2023-06-20 23:36:14+00:00,2023-10-23 15:56:19-07:00,9,1,0,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,4293,infra/modules/aws-psoxy-rest/main.tf,infra/modules/aws-psoxy-rest/main.tf,0,# todo,# TODO: limit by http method here too?,"# The /*/*/ part allows invocation from any stage, method and resource path 
 # within API Gateway REST API. 
 # TODO: limit by http method here too?","resource ""aws_lambda_permission"" ""api_gateway"" {
  count = local.use_api_gateway ? 1 : 0

  statement_id  = ""Allow${module.psoxy_lambda.function_name}Invoke""
  action        = ""lambda:InvokeFunction""
  function_name = module.psoxy_lambda.function_name
  principal     = ""apigateway.amazonaws.com""


  # The /*/*/ part allows invocation from any stage, method and resource path
  # within API Gateway REST API.
  # TODO: limit by http method here too?
  source_arn = ""${var.api_gateway_v2.execution_arn}/*/*/${module.psoxy_lambda.function_name}/{proxy+}""
}
",resource,"resource ""aws_lambda_permission"" ""api_gateway"" {
  count = local.use_api_gateway ? 1 : 0

  statement_id  = ""Allow${module.psoxy_lambda.function_name}Invoke""
  action        = ""lambda:InvokeFunction""
  function_name = module.psoxy_lambda.function_name
  principal     = ""apigateway.amazonaws.com""


  # The /*/*/ part allows invocation from any stage, method and resource path
  # within API Gateway REST API.
  # TODO: limit by http method here too?
  source_arn = ""${var.api_gateway_v2.execution_arn}/*/*/${module.psoxy_lambda.function_name}/{proxy+}""
}
",resource,106,111.0,5ed7bc9ef2e9108a02f4d42e7d0d156eb5c7184d,95b5a184cd35eb5a8eb1a44f3178c5c095260ccc,https://github.com/Worklytics/psoxy/blob/5ed7bc9ef2e9108a02f4d42e7d0d156eb5c7184d/infra/modules/aws-psoxy-rest/main.tf#L106,https://github.com/Worklytics/psoxy/blob/95b5a184cd35eb5a8eb1a44f3178c5c095260ccc/infra/modules/aws-psoxy-rest/main.tf#L111,2024-01-31 10:34:59-08:00,2024-03-21 17:02:36-07:00,7,0,1,1,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,439,infra/aws/terraform/modules/eks-prow-iam/policy_apply.tf,infra/aws/terraform/modules/eks-prow-iam/policy_apply.tf,0,# todo,# TODO(xmudrii-ubuntu): remove after removing ECR repo,# TODO(xmudrii-ubuntu): remove after removing ECR repo,"data ""aws_iam_policy_document"" ""eks_apply"" {
  statement {
    sid       = ""AllowEKSCreateOrUpadate""
    effect    = ""Allow""
    resources = [""*""]

    actions = [
      ""acm:AddTagsToCertificate"",
      ""acm:RequestCertificate"",
      ""autoscaling:CreateOrUpdateTags"",
      ""ec2:AllocateAddress"",
      ""ec2:AssociateRouteTable"",
      ""ec2:AssociateVpcCidrBlock"",
      ""ec2:AttachInternetGateway"",
      ""ec2:AuthorizeSecurityGroupEgress"",
      ""ec2:AuthorizeSecurityGroupIngress"",
      ""ec2:CreateEgressOnlyInternetGateway"",
      ""ec2:CreateInternetGateway"",
      ""ec2:CreateLaunchTemplate"",
      ""ec2:CreateLaunchTemplateVersion"",
      ""ec2:ModifyLaunchTemplate"",
      ""ec2:CreateNatGateway"",
      ""ec2:CreateRoute"",
      ""ec2:CreateRouteTable"",
      ""ec2:CreateSecurityGroup"",
      ""ec2:CreateSubnet"",
      ""ec2:CreateTags"",
      ""ec2:CreateVpc"",
      ""ec2:ModifySubnetAttribute"",
      ""ec2:ModifyVpcAttribute"",
      ""ec2:RevokeSecurityGroupEgress"",
      ""ec2:RunInstances"",
      ""ec2:ModifyInstanceAttribute"",
      ""ec2:TerminateInstances"",
      ""ec2:ImportKeyPair"",
      ""eks:CreateAddon"",
      ""eks:CreateCluster"",
      ""eks:CreateNodegroup"",
      ""eks:TagResource"",
      ""eks:UpdateAddon"",
      ""eks:UpdateClusterConfig"",
      ""eks:UpdateClusterVersion"",
      ""eks:UpdateNodegroupConfig"",
      ""eks:UpdateNodegroupVersion"",
      ""iam:CreateOpenIDConnectProvider"",
      ""iam:CreatePolicy"",
      ""iam:CreatePolicyVersion"",
      ""iam:PassRole"",
      ""iam:TagOpenIDConnectProvider"",
      ""iam:TagPolicy"",
      ""iam:TagRole"",
      ""iam:UpdateOpenIDConnectProviderThumbprint"",
      ""iam:UpdateAssumeRolePolicy"",
      ""kms:CreateAlias"",
      ""kms:CreateGrant"",
      ""kms:CreateKey"",
      ""kms:EnableKeyRotation"",
      ""kms:ListAliases"",
      ""kms:ListResourceTags"",
      ""kms:PutKeyPolicy"",
      ""kms:TagResource"",
      ""logs:CreateLogGroup"",
      ""logs:PutRetentionPolicy"",
      ""logs:TagLogGroup"",
      ""s3:PutObject"",
      # TODO(xmudrii-ubuntu): remove after removing ECR repo
      ""ecr-public:*""
    ]
  }

  // This statement effectively enforces EKSResourcesPermissionBoundary on IAM resources
  // created with this policy.
  statement {
    sid = ""AllowCreateOnlyWithBoundary""

    effect = ""Allow""

    actions = [
      ""iam:CreateRole"",
      ""iam:CreateUser"",
    ]

    resources = [""*""]

    condition {
      test     = ""StringEquals""
      variable = ""iam:PermissionsBoundary""
      values = [
        aws_iam_policy.eks_resources_permission_boundary.arn
      ]
    }
  }

  statement {
    sid       = ""AllowChangeOnlyWithEKSResourceBoundary""
    effect    = ""Allow""
    resources = [""*""]

    actions = [
      ""iam:AttachRolePolicy"",
      ""iam:DeleteRolePolicy"",
      ""iam:PutRolePolicy"",
      ""iam:PutRolePermissionsBoundary"",
      ""iam:AttachUserPolicy"",
      ""iam:DeleteUserPolicy"",
      ""iam:DetachUserPolicy"",
      ""iam:PutUserPolicy"",
      ""iam:PutUserPermissionsBoundary"",
    ]

    condition {
      test     = ""StringEquals""
      variable = ""iam:PermissionsBoundary""
      values = [
        aws_iam_policy.eks_resources_permission_boundary.arn
      ]
    }
  }

  statement {
    sid = ""DenyEditBoundaries""

    effect = ""Deny""

    actions = [
      ""iam:DeletePolicy"",
      ""iam:CreatePolicyVersion"",
      ""iam:DeletePolicyVersion"",
      ""iam:SetDefaultPolicyVersion""
    ]

    resources = [
      ""arn:aws:iam::${local.account_id}:policy/boundary/*""
    ]
  }

  statement {
    sid = ""DenyLeaveOrganisation""

    effect = ""Deny""

    actions = [
      ""organizations:LeaveOrganization""
    ]

    resources = [""*""]
  }
}
",data,"data ""aws_iam_policy_document"" ""eks_apply"" {
  statement {
    sid       = ""AllowEKSCreateOrUpadate""
    effect    = ""Allow""
    resources = [""*""]

    actions = [
      ""acm:AddTagsToCertificate"",
      ""acm:RequestCertificate"",
      ""autoscaling:CreateOrUpdateTags"",
      ""ec2:AllocateAddress"",
      ""ec2:AssociateRouteTable"",
      ""ec2:AssociateVpcCidrBlock"",
      ""ec2:AttachInternetGateway"",
      ""ec2:AuthorizeSecurityGroupEgress"",
      ""ec2:AuthorizeSecurityGroupIngress"",
      ""ec2:CreateEgressOnlyInternetGateway"",
      ""ec2:CreateInternetGateway"",
      ""ec2:CreateLaunchTemplate"",
      ""ec2:CreateLaunchTemplateVersion"",
      ""ec2:ModifyLaunchTemplate"",
      ""ec2:CreateNatGateway"",
      ""ec2:CreateRoute"",
      ""ec2:CreateRouteTable"",
      ""ec2:CreateSecurityGroup"",
      ""ec2:CreateSubnet"",
      ""ec2:CreateTags"",
      ""ec2:CreateVpc"",
      ""ec2:ModifySubnetAttribute"",
      ""ec2:ModifyVpcAttribute"",
      ""ec2:RevokeSecurityGroupEgress"",
      ""ec2:RunInstances"",
      ""ec2:ModifyInstanceAttribute"",
      ""ec2:TerminateInstances"",
      ""ec2:ImportKeyPair"",
      ""eks:CreateAddon"",
      ""eks:CreateCluster"",
      ""eks:CreateNodegroup"",
      ""eks:TagResource"",
      ""eks:UpdateAddon"",
      ""eks:UpdateClusterConfig"",
      ""eks:UpdateClusterVersion"",
      ""eks:UpdateNodegroupConfig"",
      ""eks:UpdateNodegroupVersion"",
      ""iam:CreateOpenIDConnectProvider"",
      ""iam:CreatePolicy"",
      ""iam:CreatePolicyVersion"",
      ""iam:PassRole"",
      ""iam:TagOpenIDConnectProvider"",
      ""iam:TagPolicy"",
      ""iam:TagRole"",
      ""iam:UpdateOpenIDConnectProviderThumbprint"",
      ""iam:UpdateAssumeRolePolicy"",
      ""kms:CreateAlias"",
      ""kms:CreateGrant"",
      ""kms:CreateKey"",
      ""kms:EnableKeyRotation"",
      ""kms:ListAliases"",
      ""kms:ListResourceTags"",
      ""kms:PutKeyPolicy"",
      ""kms:TagResource"",
      ""logs:CreateLogGroup"",
      ""logs:PutRetentionPolicy"",
      ""logs:TagLogGroup"",
      ""s3:PutObject"",
      # TODO(xmudrii-ubuntu): remove after removing ECR repo
      ""ecr-public:*""
    ]
  }

  // This statement effectively enforces EKSResourcesPermissionBoundary on IAM resources
  // created with this policy.
  statement {
    sid = ""AllowCreateOnlyWithBoundary""

    effect = ""Allow""

    actions = [
      ""iam:CreateRole"",
      ""iam:CreateUser"",
    ]

    resources = [""*""]

    condition {
      test     = ""StringEquals""
      variable = ""iam:PermissionsBoundary""
      values = [
        aws_iam_policy.eks_resources_permission_boundary.arn
      ]
    }
  }

  statement {
    sid       = ""AllowChangeOnlyWithEKSResourceBoundary""
    effect    = ""Allow""
    resources = [""*""]

    actions = [
      ""iam:AttachRolePolicy"",
      ""iam:DeleteRolePolicy"",
      ""iam:PutRolePolicy"",
      ""iam:PutRolePermissionsBoundary"",
      ""iam:AttachUserPolicy"",
      ""iam:DeleteUserPolicy"",
      ""iam:DetachUserPolicy"",
      ""iam:PutUserPolicy"",
      ""iam:PutUserPermissionsBoundary"",
    ]

    condition {
      test     = ""StringEquals""
      variable = ""iam:PermissionsBoundary""
      values = [
        aws_iam_policy.eks_resources_permission_boundary.arn
      ]
    }
  }

  statement {
    sid = ""DenyEditBoundaries""

    effect = ""Deny""

    actions = [
      ""iam:DeletePolicy"",
      ""iam:CreatePolicyVersion"",
      ""iam:DeletePolicyVersion"",
      ""iam:SetDefaultPolicyVersion""
    ]

    resources = [
      ""arn:aws:iam::${local.account_id}:policy/boundary/*""
    ]
  }

  statement {
    sid = ""DenyLeaveOrganisation""

    effect = ""Deny""

    actions = [
      ""organizations:LeaveOrganization""
    ]

    resources = [""*""]
  }
}
",data,93,93.0,aac03e99a6c18cf7399a5cfdcf8a7485186b70ae,aac03e99a6c18cf7399a5cfdcf8a7485186b70ae,https://github.com/kubernetes/k8s.io/blob/aac03e99a6c18cf7399a5cfdcf8a7485186b70ae/infra/aws/terraform/modules/eks-prow-iam/policy_apply.tf#L93,https://github.com/kubernetes/k8s.io/blob/aac03e99a6c18cf7399a5cfdcf8a7485186b70ae/infra/aws/terraform/modules/eks-prow-iam/policy_apply.tf#L93,2023-10-16 10:30:36+02:00,2023-10-16 10:30:36+02:00,1,0,0,1,0,1,0,0,0,0
https://github.com/nebari-dev/nebari,2,qhub/template/{{ cookiecutter.repo_directory }}/infrastructure/modules/kubernetes/ingress/main.tf,src/_nebari/stages/kubernetes_ingress/template/modules/kubernetes/ingress/main.tf,1,# todo,# TODO: eventually needs to be tied into traefik middle,"# allow access to the dashboard directly through the port 
 # TODO: eventually needs to be tied into traefik middle 
 # security possibly using jupyterhub auth this is not a 
 # security risk at the moment since this port is not 
 # externally accessible","resource ""kubernetes_deployment"" ""main"" {
  metadata {
    name      = ""${var.name}-traefik-ingress""
    namespace = var.namespace
  }

  spec {
    replicas = 1

    selector {
      match_labels = {
        ""app.kubernetes.io/component"" = ""traefik-ingress""
      }
    }

    template {
      metadata {
        labels = {
          ""app.kubernetes.io/component"" = ""traefik-ingress""
        }
      }

      spec {
        service_account_name             = kubernetes_service_account.main.metadata.0.name
        termination_grace_period_seconds = 60

        affinity {
          node_affinity {
            required_during_scheduling_ignored_during_execution {
              node_selector_term {
                match_expressions {
                  key      = var.node-group.key
                  operator = ""In""
                  values   = [var.node-group.value]
                }
              }
            }
          }
        }

        container {
          image = ""${var.traefik-image.image}:${var.traefik-image.tag}""
          name  = var.name

          security_context {
            capabilities {
              drop = [""ALL""]
              add  = [""NET_BIND_SERVICE""]
            }
          }

          args = concat([
            # Do not send usage stats
            ""--global.checknewversion=false"",
            ""--global.sendanonymoususage=false"",
            # allow access to the dashboard directly through the port
            # TODO: eventually needs to be tied into traefik middle
            # security possibly using jupyterhub auth this is not a
            # security risk at the moment since this port is not
            # externally accessible
            ""--api.insecure=true"",
            ""--api.dashboard=true"",
            ""--ping=true"",
            # Start the Traefik Kubernetes Ingress Controller
            ""--providers.kubernetesingress"",
            ""--providers.kubernetesingress.namespaces=${var.namespace}"",
            ""--providers.kubernetesingress.ingressclass=traefik"",
            # Start the Traefik Kubernetes CRD Controller Provider
            ""--providers.kubernetescrd"",
            ""--providers.kubernetescrd.namespaces=${var.namespace}"",
            ""--providers.kubernetescrd.throttleduration=2s"",
            ""--providers.kubernetescrd.allowcrossnamespace=false"",
            # Define two entrypoint ports, and setup a redirect from HTTP to HTTPS.
            ""--entryPoints.web.address=:80"",
            ""--entryPoints.websecure.address=:443"",
            ""--entrypoints.ssh.address=:8022"",
            ""--entrypoints.sftp.address=:8023"",
            ""--entryPoints.tcp.address=:8786"",
            ""--entryPoints.traefik.address=:9000"",
            ""--entrypoints.web.http.redirections.entryPoint.to=websecure"",
            ""--entrypoints.web.http.redirections.entryPoint.scheme=https"",
            # Enable debug logging. Useful to work out why something might not be
            # working. Fetch logs of the pod.
            ""--log.level=${var.loglevel}"",
            ], var.enable-certificates ? [
            ""--certificatesresolvers.default.acme.tlschallenge"",
            ""--certificatesresolvers.default.acme.email=${var.acme-email}"",
            ""--certificatesresolvers.default.acme.storage=acme.json"",
            ""--certificatesresolvers.default.acme.caserver=${var.acme-server}"",
          ] : [])

          port {
            name           = ""http""
            container_port = 80
          }

          port {
            name           = ""https""
            container_port = 443
          }

          port {
            name           = ""ssh""
            container_port = 8022
          }

          port {
            name           = ""sftp""
            container_port = 8023
          }

          port {
            name           = ""tcp""
            container_port = 8786
          }

          port {
            name           = ""traefik""
            container_port = 9000
          }

          liveness_probe {
            http_get {
              path = ""/ping""
              port = ""traefik""
            }

            initial_delay_seconds = 10
            timeout_seconds       = 2
            period_seconds        = 10
            failure_threshold     = 3
            success_threshold     = 1
          }

          readiness_probe {
            http_get {
              path = ""/ping""
              port = ""traefik""
            }

            initial_delay_seconds = 10
            timeout_seconds       = 2
            period_seconds        = 10
            failure_threshold     = 1
            success_threshold     = 1
          }
        }
      }
    }
  }
}
",resource,"resource ""kubernetes_deployment"" ""main"" {
  metadata {
    name      = ""${var.name}-traefik-ingress""
    namespace = var.namespace
  }

  spec {
    replicas = 1

    selector {
      match_labels = {
        ""app.kubernetes.io/component"" = ""traefik-ingress""
      }
    }

    template {
      metadata {
        labels = {
          ""app.kubernetes.io/component"" = ""traefik-ingress""
        }
      }

      spec {
        service_account_name             = kubernetes_service_account.main.metadata.0.name
        termination_grace_period_seconds = 60

        affinity {
          node_affinity {
            required_during_scheduling_ignored_during_execution {
              node_selector_term {
                match_expressions {
                  key      = var.node-group.key
                  operator = ""In""
                  values   = [var.node-group.value]
                }
              }
            }
          }
        }

        container {
          image = ""${var.traefik-image.image}:${var.traefik-image.tag}""
          name  = var.name

          volume_mount {
            mount_path = ""/mnt/acme-certificates""
            name       = ""acme-certificates""
          }
          security_context {
            capabilities {
              drop = [""ALL""]
              add  = [""NET_BIND_SERVICE""]
            }
          }

          args = concat([
            # Do not send usage stats
            ""--global.checknewversion=false"",
            ""--global.sendanonymoususage=false"",
            # allow access to the dashboard directly through the port
            # TODO: eventually needs to be tied into traefik middle
            # security possibly using jupyterhub auth this is not a
            # security risk at the moment since this port is not
            # externally accessible
            ""--api.insecure=true"",
            ""--api.dashboard=true"",
            ""--ping=true"",
            # Start the Traefik Kubernetes Ingress Controller
            ""--providers.kubernetesingress=true"",
            ""--providers.kubernetesingress.namespaces=${var.namespace}"",
            ""--providers.kubernetesingress.ingressclass=traefik"",
            # Start the Traefik Kubernetes CRD Controller Provider
            ""--providers.kubernetescrd"",
            ""--providers.kubernetescrd.namespaces=${var.namespace}"",
            ""--providers.kubernetescrd.throttleduration=2s"",
            ""--providers.kubernetescrd.allowcrossnamespace=false"",
            # Define two entrypoint ports, and setup a redirect from HTTP to HTTPS.
            ""--entryPoints.web.address=:80"",
            ""--entryPoints.websecure.address=:443"",
            ""--entrypoints.ssh.address=:8022"",
            ""--entrypoints.sftp.address=:8023"",
            ""--entryPoints.tcp.address=:8786"",
            ""--entryPoints.traefik.address=:9000"",
            # Define the entrypoint port for Minio
            ""--entryPoints.minio.address=:9080"",
            # Redirect http -> https
            ""--entrypoints.web.http.redirections.entryPoint.to=websecure"",
            ""--entrypoints.web.http.redirections.entryPoint.scheme=https"",
            # Enable Prometheus Monitoring of Traefik
            ""--metrics.prometheus=true"",
            # Enable debug logging. Useful to work out why something might not be
            # working. Fetch logs of the pod.
            ""--log.level=${var.loglevel}"",
            ],
            local.add-certificate,
            var.additional-arguments,
          )

          port {
            name           = ""http""
            container_port = 80
          }

          port {
            name           = ""https""
            container_port = 443
          }

          port {
            name           = ""ssh""
            container_port = 8022
          }

          port {
            name           = ""sftp""
            container_port = 8023
          }

          port {
            name           = ""tcp""
            container_port = 8786
          }

          port {
            name           = ""traefik""
            container_port = 9000
          }

          port {
            name           = ""minio""
            container_port = 9080
          }

          liveness_probe {
            http_get {
              path = ""/ping""
              port = ""traefik""
            }

            initial_delay_seconds = 10
            timeout_seconds       = 2
            period_seconds        = 10
            failure_threshold     = 3
            success_threshold     = 1
          }

          readiness_probe {
            http_get {
              path = ""/ping""
              port = ""traefik""
            }

            initial_delay_seconds = 10
            timeout_seconds       = 2
            period_seconds        = 10
            failure_threshold     = 1
            success_threshold     = 1
          }
        }
        volume {
          name = ""acme-certificates""
          persistent_volume_claim {
            claim_name = kubernetes_persistent_volume_claim.traefik_certs_pvc.metadata.0.name
          }
        }
      }
    }
  }
}
",resource,167,250.0,43d0a5db3aec048ac325a1db8b00991ad3ea2503,6bb76264e203f87a157aa08ac63094f20dc890b1,https://github.com/nebari-dev/nebari/blob/43d0a5db3aec048ac325a1db8b00991ad3ea2503/qhub/template/{{ cookiecutter.repo_directory }}/infrastructure/modules/kubernetes/ingress/main.tf#L167,https://github.com/nebari-dev/nebari/blob/6bb76264e203f87a157aa08ac63094f20dc890b1/src/_nebari/stages/kubernetes_ingress/template/modules/kubernetes/ingress/main.tf#L250,2021-05-13 15:33:02+01:00,2024-03-27 18:23:09-03:00,13,0,1,1,0,1,1,0,0,0
https://github.com/uyuni-project/sumaform,1666,modules/proxy_containerized/main.tf,modules/proxy_containerized/main.tf,0,// todo,// TODO: Replace Uyuni images with OpenSUSE Leap Micro 15.5 images,"// TODO: Replace Uyuni images with OpenSUSE Leap Micro 15.5 images 
 // (not yet supported in sumaform)","variable ""images"" {
  default = {
    ""head""           = ""slemicro55o""
    // TODO: Replace Uyuni images with OpenSUSE Leap Micro 15.5 images
    // (not yet supported in sumaform)
    ""uyuni-master""   = ""opensuse155o""
    ""uyuni-released"" = ""opensuse155o""
    ""uyuni-pr""       = ""opensuse155o""
  }
}
",variable,"variable ""images"" {
  default = {
    ""head""           = ""slemicro55o""
    ""uyuni-master""   = ""leapmicro55o""
    ""uyuni-released"" = ""leapmicro55o""
    ""uyuni-pr""       = ""leapmicro55o""
  }
}
",variable,4,,2601871d9b76286f60fbfd5ddae395a64d944c0f,31f3f833cfe72cfc8efc420eb4b9507f00e16d4b,https://github.com/uyuni-project/sumaform/blob/2601871d9b76286f60fbfd5ddae395a64d944c0f/modules/proxy_containerized/main.tf#L4,https://github.com/uyuni-project/sumaform/blob/31f3f833cfe72cfc8efc420eb4b9507f00e16d4b/modules/proxy_containerized/main.tf,2024-02-01 15:23:28+01:00,2024-04-10 13:00:59+02:00,6,1,0,1,0,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,34,modules/network/nsgs.tf,modules/network/nsgs.tf,0,# todo,"# TODO: condition for end-to-end SSL/SSL termination
",# TODO: condition for end-to-end SSL/SSL termination,"resource ""oci_core_network_security_group_security_rule"" ""int_lb_egress"" {
  network_security_group_id = oci_core_network_security_group.int_lb[0].id
  description               = local.int_lb_egress[count.index].description
  destination               = local.int_lb_egress[count.index].destination
  destination_type          = local.int_lb_egress[count.index].destination_type
  direction                 = ""EGRESS""
  protocol                  = local.int_lb_egress[count.index].protocol

  stateless = false
  # TODO: condition for end-to-end SSL/SSL termination
  dynamic ""tcp_options"" {
    for_each = local.int_lb_egress[count.index].protocol == local.tcp_protocol && local.int_lb_egress[count.index].port != -1 ? [1] : []
    content {
      destination_port_range {
        min = length(regexall(""-"", local.int_lb_egress[count.index].port)) > 0 ? tonumber(element(split(""-"", local.int_lb_egress[count.index].port), 0)) : local.int_lb_egress[count.index].port
        max = length(regexall(""-"", local.int_lb_egress[count.index].port)) > 0 ? tonumber(element(split(""-"", local.int_lb_egress[count.index].port), 1)) : local.int_lb_egress[count.index].port
      }
    }
  }

  dynamic ""icmp_options"" {
    for_each = local.int_lb_egress[count.index].protocol == local.icmp_protocol ? [1] : []
    content {
      type = 3
      code = 4
    }
  }

  lifecycle {
    ignore_changes = [destination, destination_type, direction, protocol, tcp_options]
  }

  count = var.load_balancers == ""internal"" || var.load_balancers == ""both"" ? length(local.int_lb_egress) : 0
}
",resource,,,291,0.0,2b2991c788c70e2267603919f84a6d916b66baf9,6c867cd8e9cbf559742f56658989bcded0d1fd89,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/2b2991c788c70e2267603919f84a6d916b66baf9/modules/network/nsgs.tf#L291,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/modules/network/nsgs.tf#L0,2021-10-26 10:35:29+11:00,2023-10-25 16:40:02+11:00,16,2,0,1,0,1,1,0,0,0
https://github.com/jenkins-x/terraform-google-jx,1,main.tf,main.tf,0,// todo,// TODO: remove parent_domain & parent_domain_gcp_project when their deprecations are complete,"// ---------------------------------------------------------------------------- 
 // Setup ExternalDNS 
 // TODO: remove parent_domain & parent_domain_gcp_project when their deprecations are complete 
 // ----------------------------------------------------------------------------","module ""dns"" {
  source = ""./modules/dns""

  gcp_project                     = var.gcp_project
  cluster_name                    = local.cluster_name
  apex_domain                     = var.apex_domain != """" ? var.apex_domain : var.parent_domain
  jenkins_x_namespace             = module.cluster.jenkins_x_namespace
  jx2                             = var.jx2
  subdomain                       = var.subdomain
  apex_domain_gcp_project         = var.apex_domain_gcp_project != """" ? var.apex_domain_gcp_project : (var.parent_domain_gcp_project != """" ? var.parent_domain_gcp_project : var.gcp_project)
  apex_domain_integration_enabled = var.apex_domain_integration_enabled

  depends_on = [
    module.cluster
  ]
}
",module,"module ""dns"" {
  source = ""./modules/dns""

  gcp_project                     = var.gcp_project
  cluster_name                    = local.cluster_name
  apex_domain                     = var.apex_domain != """" ? var.apex_domain : var.parent_domain
  jenkins_x_namespace             = module.cluster.jenkins_x_namespace
  jx2                             = var.jx2
  subdomain                       = var.subdomain
  apex_domain_gcp_project         = var.apex_domain_gcp_project != """" ? var.apex_domain_gcp_project : (var.parent_domain_gcp_project != """" ? var.parent_domain_gcp_project : var.gcp_project)
  apex_domain_integration_enabled = var.apex_domain_integration_enabled

  depends_on = [
    module.cluster
  ]
}
",module,229,237.0,ffc0b68673f1e9341ef89e88f6af157631a9086d,6d61937b9d1a81a879246040a6f6e11ed5626a4e,https://github.com/jenkins-x/terraform-google-jx/blob/ffc0b68673f1e9341ef89e88f6af157631a9086d/main.tf#L229,https://github.com/jenkins-x/terraform-google-jx/blob/6d61937b9d1a81a879246040a6f6e11ed5626a4e/main.tf#L237,2020-12-09 11:35:14+10:00,2024-04-25 13:02:59+02:00,30,0,0,1,0,0,1,0,0,0
https://github.com/ministryofjustice/cloud-platform-infrastructure,19,terraform/global-resources/elasticsearch.tf,terraform/global-resources/elasticsearch.tf,0,todo,# returns a single list item then leave it as-is and remove this TODO comment.,"      # TF-UPGRADE-TODO: In Terraform v0.10 and earlier, it was sometimes necessary to
      # force an interpolation expression to be interpreted as a list by wrapping it
      # in an extra set of list brackets. That form was supported for compatibility in
      # v0.11, but is no longer supported in Terraform v0.12.
      #
      # If the expression in the following list itself returns a list, remove the
      # brackets to avoid interpretation as a list of lists. If the expression
      # returns a single list item then leave it as-is and remove this TODO comment.","data ""aws_iam_policy_document"" ""live"" {
  statement {
    actions = [
      ""es:*"",
    ]

    resources = [
      ""arn:aws:es:${data.aws_region.moj-dsd.name}:${data.aws_caller_identity.moj-dsd.account_id}:domain/${local.live_domain}/*"",
    ]

    principals {
      type        = ""AWS""
      identifiers = [""*""]
    }

    condition {
      test     = ""IpAddress""
      variable = ""aws:SourceIp""

      # TF-UPGRADE-TODO: In Terraform v0.10 and earlier, it was sometimes necessary to
      # force an interpolation expression to be interpreted as a list by wrapping it
      # in an extra set of list brackets. That form was supported for compatibility in
      # v0.11, but is no longer supported in Terraform v0.12.
      #
      # If the expression in the following list itself returns a list, remove the
      # brackets to avoid interpretation as a list of lists. If the expression
      # returns a single list item then leave it as-is and remove this TODO comment.
      values = [
        keys(local.allowed_live_ips),
      ]
    }
  }
}
",data,"data ""aws_iam_policy_document"" ""live_1"" {
  statement {
    actions = [
      ""es:*"",
    ]

    resources = [
      ""arn:aws:es:${data.aws_region.moj-cp.name}:${data.aws_caller_identity.moj-cp.account_id}:domain/${local.live_domain}/*"",
    ]

    principals {
      type        = ""AWS""
      identifiers = [""*""]
    }

    condition {
      test     = ""IpAddress""
      variable = ""aws:SourceIp""

      # TF-UPGRADE-TODO: In Terraform v0.10 and earlier, it was sometimes necessary to
      # force an interpolation expression to be interpreted as a list by wrapping it
      # in an extra set of list brackets. That form was supported for compatibility in
      # v0.11, but is no longer supported in Terraform v0.12.
      #
      # If the expression in the following list itself returns a list, remove the
      # brackets to avoid interpretation as a list of lists. If the expression
      # returns a single list item then leave it as-is and remove this TODO comment.
      values = keys(local.allowed_live_1_ips)
    }
  }
}
",data,81,73.0,38754ae14bf7b30f08dd5fc4dfd1424c49b6ead4,c60111476d269d642447f8389a62abd932702104,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/38754ae14bf7b30f08dd5fc4dfd1424c49b6ead4/terraform/global-resources/elasticsearch.tf#L81,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/c60111476d269d642447f8389a62abd932702104/terraform/global-resources/elasticsearch.tf#L73,2019-11-21 10:14:42+00:00,2020-05-28 16:02:51+01:00,4,0,0,1,1,1,0,0,0,0
https://github.com/apache/beam,4,playground/terraform/infrastructure/provider.tf,playground/terraform/infrastructure/provider.tf,0,todo,// TODO may need to run module.setup first independent of this solution and add the terraform service account as a variable,"// TODO may need to run module.setup first independent of this solution and add the terraform service account as a variable 
 // This allows us to use a service account to provision resources without downloading or storing service account keys 
 #  impersonate_service_account = module.setup.terraform_service_account_email","provider ""google-beta"" {
  region = var.region
  // TODO may need to run module.setup first independent of this solution and add the terraform service account as a variable
  // This allows us to use a service account to provision resources without downloading or storing service account keys
  #  impersonate_service_account = module.setup.terraform_service_account_email
}
",provider,,,30,0.0,675c0bc10f813ea593702f5e6a0fd2ce38caf720,ad21d8353c856152346408f1d5029c9af05957c8,https://github.com/apache/beam/blob/675c0bc10f813ea593702f5e6a0fd2ce38caf720/playground/terraform/infrastructure/provider.tf#L30,https://github.com/apache/beam/blob/ad21d8353c856152346408f1d5029c9af05957c8/playground/terraform/infrastructure/provider.tf#L0,2022-02-22 10:04:20-08:00,2022-03-16 14:13:22-07:00,2,2,0,1,0,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,490,examples/istio-mc/locals.tf,examples/istio-mc/locals.tf,0,# todo,# TODO: check when is 15021 required for public,# TODO: check when is 15021 required for public,"locals {

  all_ports = -1

  # Protocols
  # See https://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml
  all_protocols = ""all""
  icmp_protocol = 1
  tcp_protocol  = 6
  udp_protocol  = 17

  anywhere          = ""0.0.0.0/0""
  rule_type_nsg     = ""NETWORK_SECURITY_GROUP""
  rule_type_cidr    = ""CIDR_BLOCK""
  rule_type_service = ""SERVICE_CIDR_BLOCK""

  bastion_ip = one(element([module.c1[*].bastion_public_ip], 0))

  operator_ip = one(element([module.c1[*].operator_private_ip], 0))
  
  # TODO: check when is 15021 required for public
  public_lb_allowed_ports = [80, 443, 15021]

  # ports required to be opened for inter-cluster communication between for Istio
  service_mesh_ports = [15012, 15017, 15021, 15443]

  regions = {
    # Africa
    johannesburg = ""af-johannesburg-1""

    # Asia
    chuncheon = ""ap-chuncheon-1""
    hyderabad = ""ap-hyderabad-1""
    mumbai    = ""ap-mumbai-1""
    osaka     = ""ap-osaka-1""
    seoul     = ""ap-seoul-1""
    singapore = ""ap-singapore-1""
    tokyo     = ""ap-tokyo-1""

    # Europe
    amsterdam = ""eu-amsterdam-1""
    frankfurt = ""eu-frankfurt-1""
    london    = ""uk-london-1""
    madrid    = ""eu-madrid-1""
    marseille = ""eu-marseille-1""
    milan     = ""eu-milan-1""
    newport   = ""uk-cardiff-1""
    paris     = ""eu-paris-1""
    stockholm = ""eu-stockholm-1""
    zurich    = ""eu-zurich-1""

    # Middle East
    abudhabi  = ""me-abudhabi-1""
    dubai     = ""me-dubai-1""
    jeddah    = ""me-jeddah-1""
    jerusalem = ""il-jerusalem-1""

    # Oceania
    melbourne = ""ap-melbourne-1""
    sydney    = ""ap-sydney-1""


    # South America
    bogota     = ""sa-bogota-1""
    santiago   = ""sa-santiago-1""
    saupaulo   = ""sa-saupaulo-1""
    valparaiso = ""sa-valparaiso-1""
    vinhedo    = ""sa-vinhedo-1""

    # North America
    ashburn   = ""us-ashburn-1""
    chicago   = ""us-chicago-1""
    monterrey = ""mx-monterrey-1""
    montreal  = ""ca-montreal-1""
    phoenix   = ""us-phoenix-1""
    queretaro = ""mx-queretaro-1""
    sanjose   = ""us-sanjose-1""
    toronto   = ""ca-toronto-1""

    # US Gov FedRamp
    us-gov-ashburn = ""us-langley-1""
    us-gov-phoenix = ""us-luke-1""

    # US Gov DISA L5
    us-dod-east  = ""us-gov-ashburn-1""
    us-dod-north = ""us-gov-chicago-1""
    us-dod-west  = ""us-gov-phoenix-1""

    # UK Gov
    uk-gov-south = ""uk-gov-london-1""
    uk-gov-west  = ""uk-gov-cardiff-1""

    # Australia Gov
    au-gov-cbr = ""ap-dcc-canberra-1""

  }

  worker_cloud_init = [
    {
      content      = <<-EOT
    runcmd:
    - 'echo ""Kernel module configuration for Istio and worker node initialization""'
    - 'modprobe br_netfilter'
    - 'modprobe nf_nat'
    - 'modprobe xt_REDIRECT'
    - 'modprobe xt_owner'
    - 'modprobe iptable_nat'
    - 'modprobe iptable_mangle'
    - 'modprobe iptable_filter'
    - '/usr/libexec/oci-growfs -y'
    - 'timedatectl set-timezone Australia/Sydney'
    - 'curl --fail -H ""Authorization: Bearer Oracle"" -L0 http://169.254.169.254/opc/v2/instance/metadata/oke_init_script | base64 --decode >/var/run/oke-init.sh'
    - 'bash -x /var/run/oke-init.sh'
    EOT
      content_type = ""text/cloud-config"",
    }
  ]
}
",locals,"locals {

  all_ports = -1

  # Protocols
  # See https://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml
  all_protocols = ""all""
  icmp_protocol = 1
  tcp_protocol  = 6
  udp_protocol  = 17

  anywhere          = ""0.0.0.0/0""
  rule_type_nsg     = ""NETWORK_SECURITY_GROUP""
  rule_type_cidr    = ""CIDR_BLOCK""
  rule_type_service = ""SERVICE_CIDR_BLOCK""

  bastion_ip = one(element([module.c1[*].bastion_public_ip], 0))

  operator_ip = one(element([module.c1[*].operator_private_ip], 0))
  
  # TODO: check when is 15021 required for public
  public_lb_allowed_ports = [80, 443, 15021]

  # ports required to be opened for inter-cluster communication between for Istio
  service_mesh_ports = [15012, 15017, 15021, 15443]

  regions = {
    # Africa
    johannesburg = ""af-johannesburg-1""

    # Asia
    chuncheon = ""ap-chuncheon-1""
    hyderabad = ""ap-hyderabad-1""
    mumbai    = ""ap-mumbai-1""
    osaka     = ""ap-osaka-1""
    seoul     = ""ap-seoul-1""
    singapore = ""ap-singapore-1""
    tokyo     = ""ap-tokyo-1""

    # Europe
    amsterdam = ""eu-amsterdam-1""
    frankfurt = ""eu-frankfurt-1""
    london    = ""uk-london-1""
    madrid    = ""eu-madrid-1""
    marseille = ""eu-marseille-1""
    milan     = ""eu-milan-1""
    newport   = ""uk-cardiff-1""
    paris     = ""eu-paris-1""
    stockholm = ""eu-stockholm-1""
    zurich    = ""eu-zurich-1""

    # Middle East
    abudhabi  = ""me-abudhabi-1""
    dubai     = ""me-dubai-1""
    jeddah    = ""me-jeddah-1""
    jerusalem = ""il-jerusalem-1""

    # Oceania
    melbourne = ""ap-melbourne-1""
    sydney    = ""ap-sydney-1""


    # South America
    bogota     = ""sa-bogota-1""
    santiago   = ""sa-santiago-1""
    saupaulo   = ""sa-saupaulo-1""
    valparaiso = ""sa-valparaiso-1""
    vinhedo    = ""sa-vinhedo-1""

    # North America
    ashburn   = ""us-ashburn-1""
    chicago   = ""us-chicago-1""
    monterrey = ""mx-monterrey-1""
    montreal  = ""ca-montreal-1""
    phoenix   = ""us-phoenix-1""
    queretaro = ""mx-queretaro-1""
    sanjose   = ""us-sanjose-1""
    toronto   = ""ca-toronto-1""

    # US Gov FedRamp
    us-gov-ashburn = ""us-langley-1""
    us-gov-phoenix = ""us-luke-1""

    # US Gov DISA L5
    us-dod-east  = ""us-gov-ashburn-1""
    us-dod-north = ""us-gov-chicago-1""
    us-dod-west  = ""us-gov-phoenix-1""

    # UK Gov
    uk-gov-south = ""uk-gov-london-1""
    uk-gov-west  = ""uk-gov-cardiff-1""

    # Australia Gov
    au-gov-cbr = ""ap-dcc-canberra-1""

  }

  worker_cloud_init = [
    {
      content      = <<-EOT
    runcmd:
    - 'echo ""Kernel module configuration for Istio and worker node initialization""'
    - 'modprobe br_netfilter'
    - 'modprobe nf_nat'
    - 'modprobe xt_REDIRECT'
    - 'modprobe xt_owner'
    - 'modprobe iptable_nat'
    - 'modprobe iptable_mangle'
    - 'modprobe iptable_filter'
    - '/usr/libexec/oci-growfs -y'
    - 'timedatectl set-timezone Australia/Sydney'
    - 'curl --fail -H ""Authorization: Bearer Oracle"" -L0 http://169.254.169.254/opc/v2/instance/metadata/oke_init_script | base64 --decode >/var/run/oke-init.sh'
    - 'bash -x /var/run/oke-init.sh'
    EOT
      content_type = ""text/cloud-config"",
    }
  ]
}
",locals,24,24.0,f4cdb00433193c43fd52f8e0f3093d294e5389ac,f4cdb00433193c43fd52f8e0f3093d294e5389ac,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/f4cdb00433193c43fd52f8e0f3093d294e5389ac/examples/istio-mc/locals.tf#L24,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/f4cdb00433193c43fd52f8e0f3093d294e5389ac/examples/istio-mc/locals.tf#L24,2024-02-23 10:56:04+11:00,2024-02-23 10:56:04+11:00,1,0,0,1,0,0,1,0,0,0
https://github.com/kubernetes/k8s.io,216,infra/gcp/terraform/k8s-infra-prow-build/main.tf,infra/gcp/terraform/k8s-infra-prow-build/main.tf,0,# todo,# TODO(spiffxp): remove this once all jobs/pods have fully migrated over to,"# TODO(spiffxp): remove this once all jobs/pods have fully migrated over to 
 #                pool5","module ""prow_build_nodepool_n1_highmem_8_maxiops"" {
  source        = ""../modules/gke-nodepool""
  project_name  = local.project_id
  cluster_name  = module.prow_build_cluster.cluster.name
  location      = module.prow_build_cluster.cluster.location
  name          = ""pool4""
  initial_count = 1
  min_count     = 1
  max_count     = 80
  # kind-ipv6 jobs need an ipv6 stack; COS doesn't provide one, so we need to
  # use an UBUNTU image instead. Keep parity with the existing google.com
  # k8s-prow-builds/prow cluster by using the CONTAINERD variant
  image_type   = ""UBUNTU_CONTAINERD""
  machine_type = ""n1-highmem-8""
  # Use an ssd volume sized to allow the max IOPS supported by n1 instances w/ 8 vCPU
  disk_size_gb    = 500
  disk_type       = ""pd-ssd""
  service_account = module.prow_build_cluster.cluster_node_sa.email
}
",module,the block associated got renamed or deleted,,137,,f403678c4aa40765125e4738a91fbfd99addd89b,8c45f5a5981f205b35ef6474b8a2270069ef939b,https://github.com/kubernetes/k8s.io/blob/f403678c4aa40765125e4738a91fbfd99addd89b/infra/gcp/terraform/k8s-infra-prow-build/main.tf#L137,https://github.com/kubernetes/k8s.io/blob/8c45f5a5981f205b35ef6474b8a2270069ef939b/infra/gcp/terraform/k8s-infra-prow-build/main.tf,2021-09-28 05:47:06-07:00,2021-09-28 10:19:06-07:00,2,1,1,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,421,infra/examples/msft-365/main.tf,infra/examples/msft-365/main.tf,0,# todo,"# TODO: CLIENT_ID, REFRESH_ENDPOINT better as env variables","# TODO: CLIENT_ID, REFRESH_ENDPOINT better as env variables","resource ""local_file"" ""configure_client_id"" {
  for_each = module.worklytics_connector_specs.enabled_msft_365_connectors


  # TODO: CLIENT_ID, REFRESH_ENDPOINT better as env variables
  filename = ""TODO 1 - setup ${each.key} secrets in AWS (SENSITIVE).md""
  content  = <<EOT

  1. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_CLIENT_ID` as an AWS SSM Parameter to value
     `${module.msft-connection[each.key].connector.application_id}`
  2. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_REFRESH_ENDPOINT` as an AWS SSM Parameter to
     `https://login.microsoftonline.com/${var.msft_tenant_id}/oauth2/v2.0/token`
  3. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_PRIVATE_KEY_ID` as an AWS SSM Parameter to
     `module.msft-connection-auth[each.key].private_key_id`
  4. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_PRIVATE_KEY` as an AWS SSM Parameter to
     `module.msft-connection-auth[each.key].private_key`

EOT
}
",resource,"resource ""local_file"" ""configure_client_id"" {
  for_each = module.worklytics_connector_specs.enabled_msft_365_connectors


  # TODO: CLIENT_ID, REFRESH_ENDPOINT better as env variables
  filename = ""TODO 1 - setup ${each.key} secrets in AWS (SENSITIVE).md""
  content  = <<EOT

  1. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_CLIENT_ID` as an AWS SSM Parameter to value
     `${module.msft-connection[each.key].connector.application_id}`
  2. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_REFRESH_ENDPOINT` as an AWS SSM Parameter to
     `https://login.microsoftonline.com/${var.msft_tenant_id}/oauth2/v2.0/token`
  3. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_PRIVATE_KEY_ID` as an AWS SSM Parameter to
     `module.msft-connection-auth[each.key].private_key_id`
  4. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_PRIVATE_KEY` as an AWS SSM Parameter to
     `module.msft-connection-auth[each.key].private_key`

EOT
}
",resource,78,84.0,a63a56baa7f445fb28653b850d4c6c7c53c97019,8f6786b90a7f0fcb6120cc8f822fd07cab49697f,https://github.com/Worklytics/psoxy/blob/a63a56baa7f445fb28653b850d4c6c7c53c97019/infra/examples/msft-365/main.tf#L78,https://github.com/Worklytics/psoxy/blob/8f6786b90a7f0fcb6120cc8f822fd07cab49697f/infra/examples/msft-365/main.tf#L84,2022-10-07 20:39:52-07:00,2024-04-23 19:32:11-07:00,84,0,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,25,modules/on-prem-in-a-box/main.tf,modules/on-prem-in-a-box/main.tf,0,# todo,# TODO: use a narrower firewall rule and tie it to the service account,# TODO: use a narrower firewall rule and tie it to the service account,"resource ""google_compute_firewall"" ""allow-vpn"" {
  name          = ""onprem-in-a-box-allow-vpn""
  description   = ""Allow VPN traffic to the onprem instance""
  network       = var.network
  project       = var.project_id
  source_ranges = [format(""%s/32"", var.vpn_config.peer_ip)]
  target_tags   = [""onprem""]
  allow {
    protocol = ""tcp""
  }
  allow {
    protocol = ""udp""
  }
  allow {
    protocol = ""icmp""
  }
}
",resource,,,99,0.0,c486bfc66f9814e33b410602cb557a5e4d532912,409407ae7d02e9b7eedf63bd9815c1c789bc45bc,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c486bfc66f9814e33b410602cb557a5e4d532912/modules/on-prem-in-a-box/main.tf#L99,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/409407ae7d02e9b7eedf63bd9815c1c789bc45bc/modules/on-prem-in-a-box/main.tf#L0,2020-04-03 14:06:48+02:00,2020-04-06 16:27:13+02:00,2,2,0,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,22,main.tf,main.tf,0,# todo,# TODO: revisit terraform 0.12 if arrays are available,"# TODO: revisit terraform 0.12 if arrays are available 
 # access { 
 #   role   = ""READER"" 
 #   domain = ""adigangi.com"" 
 # }","resource ""google_bigquery_dataset"" ""main"" {
  dataset_id    = ""${var.dataset_id}""
  friendly_name = ""${var.dataset_name}""
  description   = ""${var.description}""
  location      = ""${local.location}""

  #TODO: terraform 0.12 will enable ""expiration_mode ? a_table_expiration : null"" (https://github.com/hashicorp/terraform/issues/17968)
  default_table_expiration_ms = ""${var.expiration}""
  project                     = ""${var.project_id}""
  labels                      = ""${var.dataset_labels}""

  # TODO: revisit terraform 0.12 if arrays are available
  # access {
  #   role   = ""READER""
  #   domain = ""adigangi.com""
  # }
}
",resource,"resource ""google_bigquery_dataset"" ""main"" {
  dataset_id    = ""${var.dataset_id}""
  friendly_name = ""${var.dataset_name}""
  description   = ""${var.description}""
  location      = ""${local.location}""

  default_table_expiration_ms = ""${var.expiration}""
  project                     = ""${var.project_id}""
  labels                      = ""${var.dataset_labels}""
}
",resource,35,,7f922f7e9df197df38c9b09dfa6e3614d71f19f5,a7966ae4afc7bb73842fb9fd6f0f708b359cf36e,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/7f922f7e9df197df38c9b09dfa6e3614d71f19f5/main.tf#L35,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/a7966ae4afc7bb73842fb9fd6f0f708b359cf36e/main.tf,2019-01-16 18:10:54-05:00,2019-01-28 12:41:47-05:00,2,1,1,1,1,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1562,blueprints/data-solutions/data-platform-minimal/03-curated.tf,blueprints/data-solutions/data-platform-minimal/03-curated.tf,0,fix,# Remove once bug is fixed. https://github.com/apache/airflow/issues/32106,"module.processing-sa-0.iam_email, # Remove once bug is fixed. https://github.com/apache/airflow/issues/32106","locals {
  cur_iam = {
    ""roles/bigquery.dataOwner"" = [module.processing-sa-0.iam_email]
    ""roles/bigquery.dataViewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/bigquery.jobUser"" = [
      module.processing-sa-0.iam_email, # Remove once bug is fixed. https://github.com/apache/airflow/issues/32106
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/datacatalog.tagTemplateViewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/datacatalog.viewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/storage.objectViewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/storage.objectAdmin"" = [module.processing-sa-0.iam_email]
  }
  cur_services = [
    ""bigquery.googleapis.com"",
    ""bigqueryreservation.googleapis.com"",
    ""bigquerystorage.googleapis.com"",
    ""cloudkms.googleapis.com"",
    ""cloudresourcemanager.googleapis.com"",
    ""compute.googleapis.com"",
    ""iam.googleapis.com"",
    ""servicenetworking.googleapis.com"",
    ""serviceusage.googleapis.com"",
    ""stackdriver.googleapis.com"",
    ""storage.googleapis.com"",
    ""storage-component.googleapis.com""
  ]
}
",locals,"locals {
  cur_services = [
    ""bigquery.googleapis.com"",
    ""bigqueryreservation.googleapis.com"",
    ""bigquerystorage.googleapis.com"",
    ""cloudkms.googleapis.com"",
    ""cloudresourcemanager.googleapis.com"",
    ""compute.googleapis.com"",
    ""datalineage.googleapis.com"",
    ""iam.googleapis.com"",
    ""servicenetworking.googleapis.com"",
    ""serviceusage.googleapis.com"",
    ""stackdriver.googleapis.com"",
    ""storage.googleapis.com"",
    ""storage-component.googleapis.com""
  ]
  iam_cur = {
    ""roles/bigquery.dataOwner"" = [
      module.processing-sa-0.iam_email
    ]
    ""roles/bigquery.dataViewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/bigquery.jobUser"" = [
      # Remove once bug is fixed. https://github.com/apache/airflow/issues/32106
      module.processing-sa-0.iam_email,
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/datacatalog.tagTemplateViewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/datacatalog.viewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/storage.objectViewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts
    ]
    ""roles/storage.admin"" = [
      module.processing-sa-0.iam_email,
      local.groups_iam.data-engineers
    ]
  }
  # this only works because the service account module uses a static output
  iam_cur_additive = {
    for k in flatten([
      for role, members in local.iam_cur : [
        for member in members : {
          role   = role
          member = member
        }
      ]
    ]) : ""${k.member}-${k.role}"" => k
  }
}
",locals,26,43.0,099ad03910542c546e50d16676b45fc9ce25ef91,a0ae43fc6f4dfddcd4bd1c99d03a8a7714ace9bb,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/099ad03910542c546e50d16676b45fc9ce25ef91/blueprints/data-solutions/data-platform-minimal/03-curated.tf#L26,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/a0ae43fc6f4dfddcd4bd1c99d03a8a7714ace9bb/blueprints/data-solutions/data-platform-minimal/03-curated.tf#L43,2023-06-28 09:05:48+02:00,2023-11-01 17:53:06+01:00,5,0,0,1,1,0,0,0,0,0
https://github.com/kubernetes/k8s.io,353,infra/gcp/terraform/k8s-infra-oci-proxy/main.tf,infra/gcp/terraform/k8s-infra-oci-proxy/main.tf,0,// todo,// TODO: create a dedicated service account for auto-deployment,"// Currently we only do this for staging, prod is manual deployed by admins 
 // 
 // Ensure gcb-builder can auto-deploy registry-sandbox.k8s.io 
 // 
 // TODO: create a dedicated service account for auto-deployment","data ""google_project"" ""k8s_infra_staging_tools"" {
  project_id = ""k8s-staging-infra-tools""
}
",data,the block associated got renamed or deleted,,45,,585608eaa86847071620860d476b7fb781794e33,f4c3b4d855050bb779bce449c60c07d599e8e2aa,https://github.com/kubernetes/k8s.io/blob/585608eaa86847071620860d476b7fb781794e33/infra/gcp/terraform/k8s-infra-oci-proxy/main.tf#L45,https://github.com/kubernetes/k8s.io/blob/f4c3b4d855050bb779bce449c60c07d599e8e2aa/infra/gcp/terraform/k8s-infra-oci-proxy/main.tf,2023-04-04 15:39:35-07:00,2023-04-05 19:28:57-07:00,2,1,0,1,0,1,0,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,40,gcp/terraform/instances.tf,gcp/terraform/instances.tf,0,xxx,# XXX: The join() is a workaround for https://github.com/hashicorp/terraform/issues/11566,# XXX: The join() is a workaround for https://github.com/hashicorp/terraform/issues/11566,"resource ""google_compute_instance"" ""clusternodes"" {
  machine_type            = ""${var.machine_type}""
  metadata_startup_script = ""${file(""startup.sh"")}""
  count                   = ""2""
  name                    = ""${terraform.workspace}-${var.name}-node-${count.index}""
  zone                    = ""${element(data.google_compute_zones.available.names, count.index)}""

  can_ip_forward = true

  network_interface {
    subnetwork = ""${google_compute_subnetwork.ha_subnet.name}""
    network_ip = ""${cidrhost(var.ip_cidr_range, count.index+2)}""

    access_config {
      nat_ip = """"
    }
  }

  scheduling {
    automatic_restart   = true
    on_host_maintenance = ""MIGRATE""
    preemptible         = false
  }

  boot_disk {
    initialize_params {
      # XXX: The join() is a workaround for https://github.com/hashicorp/terraform/issues/11566
      image = ""${var.use_custom_image == ""true"" ? ""${join("""", google_compute_image.sles4sap_bootable_image.*.self_link)}"" : ""suse-sap-cloud/${var.sles4sap_os_image}""}""
    }

    auto_delete = true
  }

  attached_disk {
    source      = ""${element(google_compute_disk.node_data.*.self_link, count.index)}""
    device_name = ""${element(google_compute_disk.node_data.*.name, count.index)}""
    mode        = ""READ_WRITE""
  }

  attached_disk {
    source      = ""${element(google_compute_disk.backup.*.self_link, count.index)}""
    device_name = ""${element(google_compute_disk.backup.*.name, count.index)}""
    mode        = ""READ_WRITE""
  }

  metadata {
    sshKeys = ""root:${file(var.public_key_location)}""

    # For a description of these:
    # https://storage.googleapis.com/sapdeploy/dm-templates/sap_hana_ha/template.yaml

    post_deployment_script     = ""${var.post_deployment_script}""
    sap_deployment_debug       = ""${var.sap_deployment_debug}""
    sap_hana_backup_bucket     = """"
    sap_hana_deployment_bucket = ""${var.sap_hana_deployment_bucket}""
    sap_hana_instance_number   = ""${var.sap_hana_instance_number}""
    sap_hana_sapsys_gid        = ""${var.sap_hana_sapsys_gid}""
    sap_hana_scaleout_nodes    = ""0""
    sap_hana_sid               = ""${var.sap_hana_sid}""
    sap_hana_sidadm_password   = ""${var.sap_hana_sidadm_password}""
    sap_hana_sidadm_uid        = ""${var.sap_hana_sidadm_uid}""
    sap_hana_standby_nodes     = """"
    sap_hana_system_password   = ""${var.sap_hana_system_password}""
    sap_primary_instance       = ""${terraform.workspace}-${var.name}-node-0""
    sap_primary_zone           = ""${data.google_compute_zones.available.names[0]}""
    sap_secondary_instance     = ""${terraform.workspace}-${var.name}-node-1""
    sap_secondary_zone         = ""${data.google_compute_zones.available.names[1]}""
    sap_vip                    = ""${var.sap_vip}""
    sap_vip_secondary_range    = """"
    suse_regcode               = ""${var.suse_regcode}""
    init_type                  = ""${var.init_type}""
    iscsi_ip                   = ""${var.iscsi_ip}""
    use_gcp_stonith            = ""${var.use_gcp_stonith}""
  }

  service_account {
    scopes = [""compute-rw"", ""storage-rw"", ""logging-write"", ""monitoring-write"", ""service-control"", ""service-management""]
  }

  provisioner ""file"" {
    source      = ""./provision/""
    destination = ""/root/provision/""

    connection {
      type        = ""ssh""
      user        = ""root""
      private_key = ""${file(var.private_key_location)}""
    }
  }
}
",resource,,,74,0.0,35dd6a5cc3e10c7feae7d5f80b69906d73057cae,4873066a74f41c7ff5286f386c32c8400b429675,https://github.com/SUSE/ha-sap-terraform-deployments/blob/35dd6a5cc3e10c7feae7d5f80b69906d73057cae/gcp/terraform/instances.tf#L74,https://github.com/SUSE/ha-sap-terraform-deployments/blob/4873066a74f41c7ff5286f386c32c8400b429675/gcp/terraform/instances.tf#L0,2019-05-06 13:04:28+02:00,2019-08-14 11:31:57+02:00,2,2,1,1,1,0,0,0,0,0
https://github.com/nasa/cumulus,267,lambdas/data-migration/main.tf,lambdas/data-migration1/main.tf,1,# todo,# TODO: add RDS perms,# TODO: add RDS perms,"data ""aws_iam_policy_document"" ""data_migration"" {
  statement {
    actions = [
      ""ec2:CreateNetworkInterface"",
      ""ec2:DeleteNetworkInterface"",
      ""ec2:DescribeNetworkInterfaces"",
      ""logs:CreateLogGroup"",
      ""logs:CreateLogStream"",
      ""logs:DescribeLogStreams"",
      ""logs:PutLogEvents""
    ]
    resources = [""*""]
  }

  statement {
    actions = [
      ""dynamodb:Scan"",
    ]
    resources = [for k, table in var.dynamo_tables : table.arn]
  }

  # TODO: add RDS perms
}
",data,the block associated got renamed or deleted,,44,,8480e4969011afe1186f85d493806bcf48ea9698,42876df16a4c014fb7750bb8f8f9fd1ca55ae5d4,https://github.com/nasa/cumulus/blob/8480e4969011afe1186f85d493806bcf48ea9698/lambdas/data-migration/main.tf#L44,https://github.com/nasa/cumulus/blob/42876df16a4c014fb7750bb8f8f9fd1ca55ae5d4/lambdas/data-migration1/main.tf,2020-08-21 17:47:34-04:00,2020-08-28 17:34:02-04:00,6,1,1,1,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,277,infra/gcp/terraform/k8s-infra-oci-proxy/oci-proxy-sandbox.tf,infra/gcp/terraform/k8s-infra-oci-proxy/oci-proxy-sandbox.tf,0,//todo,//TODO: create a dedicated service account for auto-deployment,"//Ensure gcb-builder can auto-deploy registry-sandbox.k8s.io 
 //TODO: create a dedicated service account for auto-deployment","data ""google_project"" ""k8s_infra_staging_tools"" {
  project_id = ""k8s-staging-infra-tools""
}
",data,,,94,0.0,622302ef9a54357cd041f080e16bc7da4682829f,585608eaa86847071620860d476b7fb781794e33,https://github.com/kubernetes/k8s.io/blob/622302ef9a54357cd041f080e16bc7da4682829f/infra/gcp/terraform/k8s-infra-oci-proxy/oci-proxy-sandbox.tf#L94,https://github.com/kubernetes/k8s.io/blob/585608eaa86847071620860d476b7fb781794e33/infra/gcp/terraform/k8s-infra-oci-proxy/oci-proxy-sandbox.tf#L0,2022-04-21 17:50:25+02:00,2023-04-04 15:39:35-07:00,7,2,0,1,0,1,0,0,0,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,306,init.tf,init.tf,0,implementation,# This method is a stub which could be replaced by a more practical helm implementation,"# Upload the calico patch config, for the kustomization of the calico manifest 
 # This method is a stub which could be replaced by a more practical helm implementation","resource ""null_resource"" ""kustomization"" {
  connection {
    user           = ""root""
    private_key    = var.ssh_private_key
    agent_identity = local.ssh_agent_identity
    host           = module.control_planes[keys(module.control_planes)[0]].ipv4_address
    port           = var.ssh_port
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content = yamlencode({
      apiVersion = ""kustomize.config.k8s.io/v1beta1""
      kind       = ""Kustomization""

      resources = concat(
        [
          ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",
          ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",
          ""https://raw.githubusercontent.com/rancher/system-upgrade-controller/master/manifests/system-upgrade-controller.yaml"",
        ],
        var.disable_hetzner_csi ? [] : [
          ""hcloud-csi.yml""
        ],
        lookup(local.ingress_controller_install_resources, local.ingress_controller, []),
        lookup(local.cni_install_resources, var.cni_plugin, []),
        var.enable_longhorn ? [""longhorn.yaml""] : [],
        var.enable_cert_manager || var.enable_rancher ? [""cert_manager.yaml""] : [],
        var.enable_rancher ? [""rancher.yaml""] : [],
        var.rancher_registration_manifest_url != """" ? [var.rancher_registration_manifest_url] : []
      ),
      patchesStrategicMerge = concat(
        [
          file(""${path.module}/kustomize/system-upgrade-controller.yaml""),
          ""kured.yaml"",
          ""ccm.yaml"",
        ],
        lookup(local.cni_install_resource_patches, var.cni_plugin, [])
      )
    })
    destination = ""/var/post_install/kustomization.yaml""
  }

  # Upload traefik ingress controller config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/traefik_ingress.yaml.tpl"",
      {
        values = indent(4, trimspace(local.traefik_values))
    })
    destination = ""/var/post_install/traefik_ingress.yaml""
  }

  # Upload nginx ingress controller config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/nginx_ingress.yaml.tpl"",
      {
        values = indent(4, trimspace(local.nginx_values))
    })
    destination = ""/var/post_install/nginx_ingress.yaml""
  }

  # Upload the CCM patch config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/ccm.yaml.tpl"",
      {
        cluster_cidr_ipv4   = local.cluster_cidr_ipv4
        default_lb_location = var.load_balancer_location
        using_klipper_lb    = local.using_klipper_lb
    })
    destination = ""/var/post_install/ccm.yaml""
  }

  # Upload the calico patch config, for the kustomization of the calico manifest
  # This method is a stub which could be replaced by a more practical helm implementation
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/calico.yaml.tpl"",
      {
        values = trimspace(local.calico_values)
    })
    destination = ""/var/post_install/calico.yaml""
  }

  # Upload the cilium install file
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cilium.yaml.tpl"",
      {
        values = indent(4, trimspace(local.cilium_values))
    })
    destination = ""/var/post_install/cilium.yaml""
  }

  # Upload the system upgrade controller plans config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/plans.yaml.tpl"",
      {
        channel = var.initial_k3s_channel
    })
    destination = ""/var/post_install/plans.yaml""
  }

  # Upload the Longhorn config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/longhorn.yaml.tpl"",
      {
        longhorn_namespace  = var.longhorn_namespace
        longhorn_repository = var.longhorn_repository
        values              = indent(4, trimspace(local.longhorn_values))
    })
    destination = ""/var/post_install/longhorn.yaml""
  }

  # Upload the cert-manager config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cert_manager.yaml.tpl"",
      {
        values = indent(4, trimspace(local.cert_manager_values))
    })
    destination = ""/var/post_install/cert_manager.yaml""
  }

  # Upload the Rancher config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/rancher.yaml.tpl"",
      {
        rancher_install_channel = var.rancher_install_channel
        values                  = indent(4, trimspace(local.rancher_values))
    })
    destination = ""/var/post_install/rancher.yaml""
  }

  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/kured.yaml.tpl"",
      {
        options = local.kured_options
      }
    )
    destination = ""/var/post_install/kured.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""set -ex"",
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name} --dry-run=client -o yaml | kubectl apply -f -"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token} --dry-run=client -o yaml | kubectl apply -f -"",
      ""curl https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml | sed -e 's|k8s.gcr.io|registry.k8s.io|g' > /var/post_install/hcloud-csi.yml""
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = concat([
      ""set -ex"",

      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /var/post_install/kustomization.yaml"",

      # Wait for k3s to become ready (we check one more time) because in some edge cases,
      # the cluster had become unvailable for a few seconds, at this very instant.
      <<-EOT
      timeout 180 bash <<EOF
        until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do
          echo ""Waiting for the cluster to become ready...""
          sleep 2
        done
      EOF
      EOT
      ]
      ,

      [
        # Ready, set, go for the kustomization
        ""kubectl apply -k /var/post_install"",
        ""echo 'Waiting for the system-upgrade-controller deployment to become available...'"",
        ""kubectl -n system-upgrade wait --for=condition=available --timeout=180s deployment/system-upgrade-controller"",
        ""sleep 5"", # important as the system upgrade controller CRDs sometimes don't get ready right away, especially with Cilium.
        ""kubectl -n system-upgrade apply -f /var/post_install/plans.yaml""
      ],
      local.has_external_load_balancer ? [] : [
        <<-EOT
      timeout 180 bash <<EOF
      until [ -n ""\$(kubectl get -n ${lookup(local.ingress_controller_namespace_names, local.ingress_controller)} service/${lookup(local.ingress_controller_service_names, local.ingress_controller)} --output=jsonpath='{.status.loadBalancer.ingress[0].${var.lb_hostname != """" ? ""hostname"" : ""ip""}}' 2> /dev/null)"" ]; do
          echo ""Waiting for load-balancer to get an IP...""
          sleep 2
      done
      EOF
      EOT
    ])
  }

  depends_on = [
    null_resource.first_control_plane,
    random_password.rancher_bootstrap,
    hcloud_volume.longhorn_volume
  ]
}
",resource,"resource ""null_resource"" ""kustomization"" {
  triggers = {
    # Redeploy helm charts when the underlying values change
    helm_values_yaml = join(""---\n"", [
      local.traefik_values,
      local.nginx_values,
      local.calico_values,
      local.cilium_values,
      local.longhorn_values,
      local.csi_driver_smb_values,
      local.cert_manager_values,
      local.rancher_values
    ])
    # Redeploy when versions of addons need to be updated
    versions = join(""\n"", [
      coalesce(var.initial_k3s_channel, ""N/A""),
      coalesce(var.cluster_autoscaler_version, ""N/A""),
      coalesce(var.hetzner_ccm_version, ""N/A""),
      coalesce(var.hetzner_csi_version, ""N/A""),
      coalesce(var.kured_version, ""N/A""),
      coalesce(var.calico_version, ""N/A""),
      coalesce(var.cilium_version, ""N/A""),
      coalesce(var.traefik_version, ""N/A""),
      coalesce(var.nginx_version, ""N/A""),
    ])
    options = join(""\n"", [
      for option, value in local.kured_options : ""${option}=${value}""
    ])
  }

  connection {
    user           = ""root""
    private_key    = var.ssh_private_key
    agent_identity = local.ssh_agent_identity
    host           = module.control_planes[keys(module.control_planes)[0]].ipv4_address
    port           = var.ssh_port
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content     = local.kustomization_backup_yaml
    destination = ""/var/post_install/kustomization.yaml""
  }

  # Upload traefik ingress controller config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/traefik_ingress.yaml.tpl"",
      {
        version          = var.traefik_version
        values           = indent(4, trimspace(local.traefik_values))
        target_namespace = local.ingress_controller_namespace
    })
    destination = ""/var/post_install/traefik_ingress.yaml""
  }

  # Upload nginx ingress controller config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/nginx_ingress.yaml.tpl"",
      {
        version          = var.nginx_version
        values           = indent(4, trimspace(local.nginx_values))
        target_namespace = local.ingress_controller_namespace
    })
    destination = ""/var/post_install/nginx_ingress.yaml""
  }

  # Upload the CCM patch config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/ccm.yaml.tpl"",
      {
        cluster_cidr_ipv4   = var.cluster_ipv4_cidr
        default_lb_location = var.load_balancer_location
        using_klipper_lb    = local.using_klipper_lb
    })
    destination = ""/var/post_install/ccm.yaml""
  }

  # Upload the calico patch config, for the kustomization of the calico manifest
  # This method is a stub which could be replaced by a more practical helm implementation
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/calico.yaml.tpl"",
      {
        values = trimspace(local.calico_values)
    })
    destination = ""/var/post_install/calico.yaml""
  }

  # Upload the cilium install file
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cilium.yaml.tpl"",
      {
        values  = indent(4, trimspace(local.cilium_values))
        version = var.cilium_version
    })
    destination = ""/var/post_install/cilium.yaml""
  }

  # Upload the system upgrade controller plans config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/plans.yaml.tpl"",
      {
        channel          = var.initial_k3s_channel
        disable_eviction = !var.system_upgrade_enable_eviction
    })
    destination = ""/var/post_install/plans.yaml""
  }

  # Upload the Longhorn config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/longhorn.yaml.tpl"",
      {
        longhorn_namespace  = var.longhorn_namespace
        longhorn_repository = var.longhorn_repository
        values              = indent(4, trimspace(local.longhorn_values))
    })
    destination = ""/var/post_install/longhorn.yaml""
  }

  # Upload the csi-driver-smb config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/csi-driver-smb.yaml.tpl"",
      {
        values = indent(4, trimspace(local.csi_driver_smb_values))
    })
    destination = ""/var/post_install/csi-driver-smb.yaml""
  }

  # Upload the cert-manager config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cert_manager.yaml.tpl"",
      {
        values = indent(4, trimspace(local.cert_manager_values))
    })
    destination = ""/var/post_install/cert_manager.yaml""
  }

  # Upload the Rancher config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/rancher.yaml.tpl"",
      {
        rancher_install_channel = var.rancher_install_channel
        values                  = indent(4, trimspace(local.rancher_values))
    })
    destination = ""/var/post_install/rancher.yaml""
  }

  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/kured.yaml.tpl"",
      {
        options = local.kured_options
      }
    )
    destination = ""/var/post_install/kured.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""set -ex"",
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${data.hcloud_network.k3s.name} --dry-run=client -o yaml | kubectl apply -f -"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token} --dry-run=client -o yaml | kubectl apply -f -"",
      local.csi_version != null ? ""curl https://raw.githubusercontent.com/hetznercloud/csi-driver/${coalesce(local.csi_version, ""v2.4.0"")}/deploy/kubernetes/hcloud-csi.yml -o /var/post_install/hcloud-csi.yml"" : ""echo 'Skipping hetzner csi.'""
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = concat([
      ""set -ex"",

      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /var/post_install/kustomization.yaml"",

      # Wait for k3s to become ready (we check one more time) because in some edge cases,
      # the cluster had become unvailable for a few seconds, at this very instant.
      <<-EOT
      timeout 360 bash <<EOF
        until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do
          echo ""Waiting for the cluster to become ready...""
          sleep 2
        done
      EOF
      EOT
      ]
      ,

      [
        # Ready, set, go for the kustomization
        ""kubectl apply -k /var/post_install"",
        ""echo 'Waiting for the system-upgrade-controller deployment to become available...'"",
        ""kubectl -n system-upgrade wait --for=condition=available --timeout=360s deployment/system-upgrade-controller"",
        ""sleep 7"", # important as the system upgrade controller CRDs sometimes don't get ready right away, especially with Cilium.
        ""kubectl -n system-upgrade apply -f /var/post_install/plans.yaml""
      ],
      local.has_external_load_balancer ? [] : [
        <<-EOT
      timeout 360 bash <<EOF
      until [ -n ""\$(kubectl get -n ${local.ingress_controller_namespace} service/${lookup(local.ingress_controller_service_names, var.ingress_controller)} --output=jsonpath='{.status.loadBalancer.ingress[0].${var.lb_hostname != """" ? ""hostname"" : ""ip""}}' 2> /dev/null)"" ]; do
          echo ""Waiting for load-balancer to get an IP...""
          sleep 2
      done
      EOF
      EOT
    ])
  }

  depends_on = [
    hcloud_load_balancer.cluster,
    null_resource.control_planes,
    random_password.rancher_bootstrap,
    hcloud_volume.longhorn_volume
  ]
}
",resource,163,195.0,c6c30138e6871a8fb690eda917d1f81f8d0e09e0,7b82c726a6d9bc3da643eefbee1b587e0126d889,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/c6c30138e6871a8fb690eda917d1f81f8d0e09e0/init.tf#L163,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/7b82c726a6d9bc3da643eefbee1b587e0126d889/init.tf#L195,2023-01-29 04:20:09+01:00,2024-05-06 13:56:24+02:00,41,0,1,1,0,0,0,1,0,0
https://github.com/uyuni-project/sumaform,318,modules/aws/host/main.tf,backend_modules/aws/host/main.tf,1,hack,# HACK: ephemeral block devices are defined in any case,"# HACK: ephemeral block devices are defined in any case 
 # they will only be used for instance types that provide them","resource ""aws_instance"" ""instance"" {
  ami = ""${var.ami}""
  instance_type = ""${var.instance_type}""
  count = ""${var.count}""
  availability_zone = ""${var.availability_zone}""
  key_name = ""${var.key_name}""
  subnet_id = ""${var.private_subnet_id}""
  vpc_security_group_ids = [""${var.private_security_group_id}""]

  root_block_device {
    volume_size = ""${var.volume_size}""
  }

  # HACK: ephemeral block devices are defined in any case
  # they will only be used for instance types that provide them
  ephemeral_block_device {
    device_name = ""xvda""
    virtual_name = ""ephemeral0""
  }

  ephemeral_block_device {
    device_name = ""xvdb""
    virtual_name = ""ephemeral1""
  }

  tags {
    Name = ""${var.name_prefix}-${var.name}-${count.index}""
  }
}
",resource,,,18,0.0,4c84d7d4843e3689fe1bfe2935361dc81a0118d1,05d52dd67d43ae0332c9d3b41cf9b2d08a0248f8,https://github.com/uyuni-project/sumaform/blob/4c84d7d4843e3689fe1bfe2935361dc81a0118d1/modules/aws/host/main.tf#L18,https://github.com/uyuni-project/sumaform/blob/05d52dd67d43ae0332c9d3b41cf9b2d08a0248f8/backend_modules/aws/host/main.tf#L0,2017-10-18 14:30:01+02:00,2020-05-04 19:34:32+01:00,21,2,1,0,0,0,0,0,0,0
https://github.com/uyuni-project/sumaform,181,modules/libvirt/package_mirror/main.tf,modules/libvirt/package_mirror/main.tf,0,hack,"// HACK: evaluates to ""terraform-network"" if bridge is empty, """" otherwise","// HACK: evaluates to ""terraform-network"" if bridge is empty, """" otherwise","resource ""libvirt_domain"" ""domain"" {
  name = ""package-mirror""
  memory = 512
  vcpu = 1
  running = ""${var.running}""

  disk {
    volume_id = ""${libvirt_volume.main_disk.id}""
  }
  disk {
    volume_id = ""${libvirt_volume.data_disk.id}""
  }

  network_interface {
    wait_for_lease = true
    // HACK: evaluates to ""terraform-network"" if bridge is empty, """" otherwise
    network_name = ""${element(list(""terraform-network"", """"), replace(replace(var.bridge, ""/.+/"", ""1""), ""/^$/"", ""0""))}""
    bridge = ""${var.bridge}""
  }

  connection {
    user = ""root""
    password = ""linux""
  }

  provisioner ""file"" {
    source = ""salt""
    destination = ""/srv""
  }

  provisioner ""file"" {
    content = <<EOF

hostname: package-mirror
domain: ${var.domain}
use-avahi: True
role: package-mirror
cc_username: ${var.cc_username}
cc_password: ${var.cc_password}
data_disk_device: vdb

EOF

    destination = ""/etc/salt/grains""
  }

  provisioner ""remote-exec"" {
    inline = [
      ""salt-call --local state.sls terraform-resource"",
      ""salt-call --local state.highstate""
    ]
  }
}
",resource,"resource ""libvirt_domain"" ""domain"" {
  name = ""package-mirror""
  memory = 512
  vcpu = 1
  running = ""${var.running}""

  disk {
    volume_id = ""${libvirt_volume.main_disk.id}""
  }
  disk {
    volume_id = ""${libvirt_volume.data_disk.id}""
  }

  network_interface {
    wait_for_lease = true
    // HACK: evaluates to ""nat_network"" if bridge is empty, """" otherwise
    network_name = ""${element(list(""${var.name_prefix}nat_network"", """"), replace(replace(var.bridge, ""/.+/"", ""1""), ""/^$/"", ""0""))}""
    bridge = ""${var.bridge}""
    mac = ""${var.mac}""
  }

  connection {
    user = ""root""
    password = ""linux""
  }

  provisioner ""file"" {
    source = ""salt""
    destination = ""/srv""
  }

  provisioner ""file"" {
    content = <<EOF

hostname: package-mirror
domain: ${var.domain}
use-avahi: True
role: package-mirror
cc_username: ${var.cc_username}
cc_password: ${var.cc_password}
data_disk_device: vdb

EOF

    destination = ""/etc/salt/grains""
  }

  provisioner ""remote-exec"" {
    inline = [
      ""salt-call --local state.sls terraform-resource"",
      ""salt-call --local state.highstate""
    ]
  }
}
",resource,31,,6654b3387922337448a757823b9cd540396f8e17,2caf37c92febd157d60a29f7c1151bee01524c62,https://github.com/uyuni-project/sumaform/blob/6654b3387922337448a757823b9cd540396f8e17/modules/libvirt/package_mirror/main.tf#L31,https://github.com/uyuni-project/sumaform/blob/2caf37c92febd157d60a29f7c1151bee01524c62/modules/libvirt/package_mirror/main.tf,2016-11-04 16:33:43+01:00,2016-11-04 16:33:43+01:00,4,1,1,1,0,0,1,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,57,modules/emr-on-eks/main.tf,modules/emr-on-eks/main.tf,0,todo,# TODO Replace this resource once the provider is available for aws emr-containers,"# Update trust relationship for job execution role 
 # Use the below command in shell script to assume a different role 
 #   $(aws sts assume-role --role-arn ${local.pass_local_deployment_role} --role-session-name terraform_run_instance_refresh --query 'Credentials.[`export#AWS_ACCESS_KEY_ID=`,AccessKeyId,`#AWS_SECRET_ACCESS_KEY=`,SecretAccessKey,`#AWS_SESSION_TOKEN=`,SessionToken]' --output text | sed $'s/\t//g' | sed 's/#/ /g') 
 # TODO Replace this resource once the provider is available for aws emr-containers","resource ""null_resource"" ""update_trust_policy"" {
  provisioner ""local-exec"" {
    interpreter = [""/bin/sh"", ""-c""]
    environment = {
      AWS_DEFAULT_REGION = data.aws_region.current.id
    }
    command = <<EOF
set -e

aws emr-containers update-role-trust-policy \
--cluster-name ${var.eks_cluster_id} \
--namespace ${kubernetes_namespace.spark.id} \
--role-name ${aws_iam_role.emr_on_eks_execution.id}

EOF
  }
  //  triggers = {
  //    always_run = timestamp()
  //  }
  depends_on = [kubernetes_namespace.spark, aws_iam_role.emr_on_eks_execution]
}
",resource,,,146,0.0,634c8579408be9f42cae5c089a8845a193e12c33,19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/634c8579408be9f42cae5c089a8845a193e12c33/modules/emr-on-eks/main.tf#L146,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1/modules/emr-on-eks/main.tf#L0,2021-10-15 14:39:05+01:00,2023-06-05 10:07:47-04:00,14,2,1,0,1,0,0,1,0,0
https://github.com/returntocorp/semgrep-rules,4,terraform/aws/security/aws-ssm-document-logging-issues.tf,terraform/aws/security/aws-ssm-document-logging-issues.tf,0,# todo,# todoruleid: aws-ssm-document-logging-issues,# todoruleid: aws-ssm-document-logging-issues,"resource ""aws_ssm_document"" ""cw_enabled_not_encrypted_yaml"" {
  name          = ""SSM-SessionManagerRunShell""
  document_type = ""Session""

  document_format = ""YAML""

  # todoruleid: aws-ssm-document-logging-issues
  content = <<DOC
  schemaVersion: '1.0'
  description: Document to hold regional settings for Session Manager
  sessionType: Standard_Stream
  inputs:
    s3BucketName: ''
    s3KeyPrefix: ''
    s3EncryptionEnabled: false
    cloudWatchLogGroupName: 'example'
    cloudWatchEncryptionEnabled: false
    cloudWatchStreamingEnabled: true
    kmsKeyId: ''
    runAsEnabled: true
    runAsDefaultUser: ''
    idleSessionTimeout: '20'
    shellProfile:
      windows: ''
      linux: ''
DOC
}
",resource,"resource ""aws_ssm_document"" ""cw_enabled_not_encrypted_yaml"" {
  name          = ""SSM-SessionManagerRunShell""
  document_type = ""Session""

  document_format = ""YAML""

  # todoruleid: aws-ssm-document-logging-issues
  content = <<DOC
  schemaVersion: '1.0'
  description: Document to hold regional settings for Session Manager
  sessionType: Standard_Stream
  inputs:
    s3BucketName: ''
    s3KeyPrefix: ''
    s3EncryptionEnabled: false
    cloudWatchLogGroupName: 'example'
    cloudWatchEncryptionEnabled: false
    cloudWatchStreamingEnabled: true
    kmsKeyId: ''
    runAsEnabled: true
    runAsDefaultUser: ''
    idleSessionTimeout: '20'
    shellProfile:
      windows: ''
      linux: ''
DOC
}
",resource,267,267.0,eefcc40d66ea97ca14eb496031ad9a84388214e2,eefcc40d66ea97ca14eb496031ad9a84388214e2,https://github.com/returntocorp/semgrep-rules/blob/eefcc40d66ea97ca14eb496031ad9a84388214e2/terraform/aws/security/aws-ssm-document-logging-issues.tf#L267,https://github.com/returntocorp/semgrep-rules/blob/eefcc40d66ea97ca14eb496031ad9a84388214e2/terraform/aws/security/aws-ssm-document-logging-issues.tf#L267,2022-02-02 16:53:31-06:00,2022-02-02 16:53:31-06:00,1,0,1,1,0,0,0,0,1,0
https://github.com/Worklytics/psoxy,86,infra/examples/aws-dev/main.tf,infra/examples/aws-google-workspace/main.tf,1,# todo,"# TODO: loop over sources to 1) provision client in data source, 2) provision proxy instance in AWS","# TODO: loop over sources to 1) provision client in data source, 2) provision proxy instance in AWS ","locals {
  # Google Workspace Sources; add/remove as you wish
  google_workspace_sources = {
    # GDirectory connections are a PRE-REQ for gmail, gdrive, and gcal connections. remove only
    # if you plan to directly connect Directory to worklytics (without proxy). such a scenario is
    # used for customers who care primarily about pseudonymizing PII of external subjects with whom
    # they collaborate in GMail/GCal/Gdrive. the Directory does not contain PII of subjects external
    # to the Google Workspace, so may be directly connected in such scenarios.
    ""gdirectory"": {
      display_name: ""Google Directory""
      apis_consumed: [
        ""admin.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/admin.directory.user.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.user.alias.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.domain.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.group.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.group.member.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.orgunit.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.rolemanagement.readonly""
      ],
      worklytics_connector_name: ""Google Workspace Directory via Psoxy""
    }
    ""gcal"": {
      display_name: ""Google Calendar""
      apis_consumed: [
        ""calendar-json.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/calendar.readonly""
      ]
    }
    ""gmail"": {
      display_name: ""GMail""
      apis_consumed: [
        ""gmail.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/gmail.metadata""
      ]
    }
    ""google-chat"": {
      display_name: ""Google Chat""
      apis_consumed: [
        ""admin.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/admin.reports.audit.readonly""
      ]
    }
    ""gdrive"": {
      display_name: ""Google Drive""
      apis_consumed: [
        ""drive.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/drive.metadata.readonly""
      ]
    }
    ""google-meet"": {
      display_name: ""Google Meet""
      apis_consumed: [
        ""admin.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/admin.reports.audit.readonly""
      ]
    }
  }
}
",locals,"locals {
  # Google Workspace Sources; add/remove as you wish
  google_workspace_sources = {
    # GDirectory connections are a PRE-REQ for gmail, gdrive, and gcal connections. remove only
    # if you plan to directly connect Directory to worklytics (without proxy). such a scenario is
    # used for customers who care primarily about pseudonymizing PII of external subjects with whom
    # they collaborate in GMail/GCal/Gdrive. the Directory does not contain PII of subjects external
    # to the Google Workspace, so may be directly connected in such scenarios.
    ""gdirectory"": {
      display_name: ""Google Directory""
      apis_consumed: [
        ""admin.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/admin.directory.user.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.user.alias.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.domain.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.group.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.group.member.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.orgunit.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.rolemanagement.readonly""
      ],
      worklytics_connector_name: ""Google Workspace Directory via Psoxy""
    }
    ""gcal"": {
      display_name: ""Google Calendar""
      apis_consumed: [
        ""calendar-json.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/calendar.readonly""
      ]
    }
    ""gmail"": {
      display_name: ""GMail""
      apis_consumed: [
        ""gmail.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/gmail.metadata""
      ]
    }
    ""google-chat"": {
      display_name: ""Google Chat""
      apis_consumed: [
        ""admin.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/admin.reports.audit.readonly""
      ]
    }
    ""gdrive"": {
      display_name: ""Google Drive""
      apis_consumed: [
        ""drive.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/drive.metadata.readonly""
      ]
    }
    ""google-meet"": {
      display_name: ""Google Meet""
      apis_consumed: [
        ""admin.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/admin.reports.audit.readonly""
      ]
    }
  }
}
",locals,55,,2f81e3da70338154c1ff8ee7d1220a5b5602fcab,1af1e4a9aac1a576d4b6e57a4f259c5d2520b19d,https://github.com/Worklytics/psoxy/blob/2f81e3da70338154c1ff8ee7d1220a5b5602fcab/infra/examples/aws-dev/main.tf#L55,https://github.com/Worklytics/psoxy/blob/1af1e4a9aac1a576d4b6e57a4f259c5d2520b19d/infra/examples/aws-google-workspace/main.tf,2022-01-05 16:14:33-08:00,2022-01-17 12:41:53-08:00,16,1,1,1,0,0,0,0,0,0
https://github.com/cookpad/terraform-aws-eks,64,modules/cluster/addons.tf,modules/cluster/addons.tf,0,fix,# The kube-proxy EKS addon introduced regressions of #124 and #209. We will move to the EKS addon when these are fixed.,# The kube-proxy EKS addon introduced regressions of #124 and #209. We will move to the EKS addon when these are fixed.,"module ""kube_proxy"" {
  source = ""./kubectl""
  config = local.config
  manifest = templatefile(
    ""${path.module}/addons/kube-proxy.yaml"",
    { aws_region = data.aws_region.current.name },
  )
}
",module,the block associated got renamed or deleted,,28,,84a41b51db99fb6468a7b371b198925eb286ebda,f2e040bdc100be73d949adda0582df2f2322f183,https://github.com/cookpad/terraform-aws-eks/blob/84a41b51db99fb6468a7b371b198925eb286ebda/modules/cluster/addons.tf#L28,https://github.com/cookpad/terraform-aws-eks/blob/f2e040bdc100be73d949adda0582df2f2322f183/modules/cluster/addons.tf,2021-12-22 17:09:59+00:00,2022-05-23 14:13:41+02:00,3,1,1,1,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,45,infra/gcp/clusters/projects/k8s-infra-prow-build/prow-build/main.tf,infra/gcp/clusters/projects/k8s-infra-prow-build/prow-build/main.tf,0,# todo,# TODO: use data resource to get org role name instead of hardcode,# TODO: use data resource to get org role name instead of hardcode,"resource ""google_project_iam_member"" ""k8s_infra_prow_viewers"" {
  project = local.project_id
  # TODO: use data resource to get org role name instead of hardcode
  role    = ""organizations/758905017065/roles/prow.viewer""
  member  = ""group:k8s-infra-prow-viewers@kubernetes.io""
}
",resource,"resource ""google_project_iam_member"" ""k8s_infra_prow_viewers"" {
  project = local.project_id
  role    = data.google_iam_role.prow_viewer.name
  member  = ""group:k8s-infra-prow-viewers@kubernetes.io""
}
",resource,52,,ad776a80065d3a90e2328b4a8d31fba8feb58bb2,d09eae6759b7f46c3c63dd96593d4a5d8b7c10a1,https://github.com/kubernetes/k8s.io/blob/ad776a80065d3a90e2328b4a8d31fba8feb58bb2/infra/gcp/clusters/projects/k8s-infra-prow-build/prow-build/main.tf#L52,https://github.com/kubernetes/k8s.io/blob/d09eae6759b7f46c3c63dd96593d4a5d8b7c10a1/infra/gcp/clusters/projects/k8s-infra-prow-build/prow-build/main.tf,2020-07-24 13:52:36-07:00,2021-03-03 16:40:35-05:00,17,1,0,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,6,infra/modules/google-workspace-dwd-connection/main.tf,infra/modules/google-workspace-dwd-connection/main.tf,0,# todo,# TODO: specify the following step in in terraform once https://github.com/hashicorp/terraform-provider-google/issues/1959 solved,"# enable domain-wide-delegation via GCP console 
 # NOTE: side effect of enabling domain-wide-delegation is that an ""OAuth 2.0 Client ID"" will be 
 # created for the service account and listed in GCP Console 
 # TODO: specify the following step in in terraform once https://github.com/hashicorp/terraform-provider-google/issues/1959 solved","resource ""local_file"" ""todo"" {
  filename = ""TODO - ${var.display_name} setup.md""
  content  = <<EOT
Complete the following steps via GCP console:
  1. Visit https://console.cloud.google.com/apis/credentials?project=${var.project_id}
  2. Find the `${var.connector_service_account_id}` service account.
  3. Enable 'Domain-wide Delegation' and 'Save'. This provisions an 'Oauth 2.0 client' for the
     service account. Copy the Client ID of that client.

Complete the following steps via the Google Workspace Admin console:
   1. Visit https://admin.google.com/ and navigate to Security --> API Controls, then find ""Manage
      Domain Wide Delegation"". Click ""Add new""
   2. Copy and paste the client ID from the prior section into the ""Client ID"" input in the popup.
   3. Copy and paste the scopes for your data source (see `java/configs/`, and find the `SCOPE`
      variable in the yaml file that matches your source) into the ""Scopes"" input.
   4. Authorize it.

With this, your psoxy instance should be able to authenticate with Google as `${var.connector_service_account_id}`
and request data from Google as authorized by the OAuth scopes you granted.
EOT

}
",resource,the block associated got renamed or deleted,,28,,1259c535e4d315fea708946a07b95f255b249721,58a702bcda40d675a44440f97ae00acbd68d0694,https://github.com/Worklytics/psoxy/blob/1259c535e4d315fea708946a07b95f255b249721/infra/modules/google-workspace-dwd-connection/main.tf#L28,https://github.com/Worklytics/psoxy/blob/58a702bcda40d675a44440f97ae00acbd68d0694/infra/modules/google-workspace-dwd-connection/main.tf,2021-10-06 09:58:36-07:00,2021-11-16 16:56:51-08:00,3,1,0,0,1,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,5,infrastructure/hub-and-spoke-vpns/main.tf,infrastructure/hub-and-spoke-vpns/main.tf,0,todo,# TODO Provide resolver addresses in the output once https://github.com/terraform-providers/terraform-provider-google/issues/3753 resolved.,"############################################################## 
 #                   Inbount DNS Forwarding                   # 
 ##############################################################  
 # TODO Provide resolver addresses in the output once https://github.com/terraform-providers/terraform-provider-google/issues/3753 resolved. 
 # For now please refer to the Documentation on how to get the compute addresses for the DNS Resolver https://cloud.google.com/dns/zones/#creating_a_dns_policy_that_enables_inbound_dns_forwarding","resource ""google_dns_policy"" ""google_dns_policy"" {
  provider = ""google-beta""

  project = var.hub_project_id
  name = ""inbound-dns-forwarding-policy""
  enable_inbound_forwarding = true

  networks {
    network_url = module.vpc-hub.network_self_link
  }
}
",resource,,,332,0.0,cb3260b3979cdc72a3920b385c0f997256bcf34a,c486bfc66f9814e33b410602cb557a5e4d532912,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/cb3260b3979cdc72a3920b385c0f997256bcf34a/infrastructure/hub-and-spoke-vpns/main.tf#L332,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c486bfc66f9814e33b410602cb557a5e4d532912/infrastructure/hub-and-spoke-vpns/main.tf#L0,2019-10-30 16:38:26+01:00,2020-04-03 14:06:48+02:00,3,2,0,1,1,0,1,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,88,community/modules/compute/schedmd-slurm-gcp-v6-nodeset/source_image_logic.tf,community/modules/compute/schedmd-slurm-gcp-v6-nodeset/source_image_logic.tf,0,hack,"# This approach to ""hacking"" the project name allows a chain of Terraform","# This approach to ""hacking"" the project name allows a chain of Terraform 
 # calls to set the instance source_image (boot disk) with a ""relative 
 # resource name"" that passes muster with VPC Service Control rules 
 # 
 # https://github.com/terraform-google-modules/terraform-google-vm/blob/735bd415fc5f034d46aa0de7922e8fada2327c0c/modules/instance_template/main.tf#L28 
 # https://cloud.google.com/apis/design/resource_names#relative_resource_name","locals {
  # Currently supported images and projects
  known_project_families = {
    schedmd-slurm-public = [
      ""slurm-gcp-6-1-debian-11"",
      ""slurm-gcp-6-1-hpc-rocky-linux-8"",
      ""slurm-gcp-6-1-ubuntu-2004-lts"",
      ""slurm-gcp-6-1-ubuntu-2204-lts-arm64"",
      ""slurm-gcp-6-1-hpc-centos-7-k80"",
      ""slurm-gcp-6-1-hpc-centos-7""
    ]
  }

  # This approach to ""hacking"" the project name allows a chain of Terraform
  # calls to set the instance source_image (boot disk) with a ""relative
  # resource name"" that passes muster with VPC Service Control rules
  #
  # https://github.com/terraform-google-modules/terraform-google-vm/blob/735bd415fc5f034d46aa0de7922e8fada2327c0c/modules/instance_template/main.tf#L28
  # https://cloud.google.com/apis/design/resource_names#relative_resource_name
  source_image_project_normalized = (can(var.instance_image.family) ?
    ""projects/${data.google_compute_image.slurm.project}/global/images/family"" :
    ""projects/${data.google_compute_image.slurm.project}/global/images""
  )
  source_image_family = can(var.instance_image.family) ? data.google_compute_image.slurm.family : """"
  source_image        = can(var.instance_image.name) ? data.google_compute_image.slurm.name : """"
}
",locals,"locals {
  # Currently supported images and projects
  known_project_families = {
    schedmd-slurm-public = [
      ""slurm-gcp-6-4-debian-11"",
      ""slurm-gcp-6-4-hpc-rocky-linux-8"",
      ""slurm-gcp-6-4-ubuntu-2004-lts"",
      ""slurm-gcp-6-4-ubuntu-2204-lts-arm64"",
      ""slurm-gcp-6-4-hpc-centos-7-k80"",
      ""slurm-gcp-6-4-hpc-centos-7""
    ]
  }

  # This approach to ""hacking"" the project name allows a chain of Terraform
  # calls to set the instance source_image (boot disk) with a ""relative
  # resource name"" that passes muster with VPC Service Control rules
  #
  # https://github.com/terraform-google-modules/terraform-google-vm/blob/735bd415fc5f034d46aa0de7922e8fada2327c0c/modules/instance_template/main.tf#L28
  # https://cloud.google.com/apis/design/resource_names#relative_resource_name
  source_image_project_normalized = (can(var.instance_image.family) ?
    ""projects/${data.google_compute_image.slurm.project}/global/images/family"" :
    ""projects/${data.google_compute_image.slurm.project}/global/images""
  )
  source_image_family = can(var.instance_image.family) ? data.google_compute_image.slurm.family : """"
  source_image        = can(var.instance_image.name) ? data.google_compute_image.slurm.name : """"
}
",locals,30,30.0,33bf402eaa82607a027754c6048fb0dce6d7668c,e56c284617fd5295bec771512fef0496b4994195,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/33bf402eaa82607a027754c6048fb0dce6d7668c/community/modules/compute/schedmd-slurm-gcp-v6-nodeset/source_image_logic.tf#L30,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/e56c284617fd5295bec771512fef0496b4994195/community/modules/compute/schedmd-slurm-gcp-v6-nodeset/source_image_logic.tf#L30,2023-10-26 09:59:26-07:00,2024-02-21 19:44:32-06:00,5,0,1,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1481,blueprints/data-solutions/bq-ml/vpc.tf,blueprints/data-solutions/bq-ml/vpc.tf,0,fix,#TODO Remove and rely on 'ssh' tag once terraform-provider-google/issues/9273 is fixed,#TODO Remove and rely on 'ssh' tag once terraform-provider-google/issues/9273 is fixed,"module ""vpc-firewall"" {
  source     = ""../../../modules/net-vpc-firewall""
  count      = local.use_shared_vpc ? 0 : 1
  project_id = module.project.project_id
  network    = module.vpc.0.name
  default_rules_config = {
    admin_ranges = [""10.0.0.0/20""]
  }
  ingress_rules = {
    #TODO Remove and rely on 'ssh' tag once terraform-provider-google/issues/9273 is fixed
    (""${var.prefix}-iap"") = {
      description   = ""Enable SSH from IAP on Notebooks.""
      source_ranges = [""35.235.240.0/20""]
      targets       = [""notebook-instance""]
      rules         = [{ protocol = ""tcp"", ports = [22] }]
    }
  }
}
",module,"module ""vpc-firewall"" {
  source     = ""../../../modules/net-vpc-firewall""
  count      = local.use_shared_vpc ? 0 : 1
  project_id = module.project.project_id
  network    = module.vpc[0].name
  default_rules_config = {
    admin_ranges = [""10.0.0.0/20""]
  }
  ingress_rules = {
    #TODO Remove and rely on 'ssh' tag once terraform-provider-google/issues/9273 is fixed
    (""${var.prefix}-iap"") = {
      description   = ""Enable SSH from IAP on Notebooks.""
      source_ranges = [""35.235.240.0/20""]
      targets       = [""notebook-instance""]
      rules         = [{ protocol = ""tcp"", ports = [22] }]
    }
  }
}
",module,40,40.0,9e19f8960861fe61830801eab27111422f1d7a4e,3af7e257d21f889ffaf7b32a3bab974fdbfda6e4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9e19f8960861fe61830801eab27111422f1d7a4e/blueprints/data-solutions/bq-ml/vpc.tf#L40,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3af7e257d21f889ffaf7b32a3bab974fdbfda6e4/blueprints/data-solutions/bq-ml/vpc.tf#L40,2023-03-05 22:02:41+01:00,2024-04-17 10:23:48+02:00,2,0,0,1,1,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1982,modules/organization/variables-logging.tf,modules/organization/variables-logging.tf,0,# todo,# TODO: add support for CMEK,# TODO: add support for CMEK,"variable ""logging_settings"" {
  description = ""Default settings for logging resources.""
  type = object({
    # TODO: add support for CMEK
    disable_default_sink = optional(bool)
    storage_location     = optional(string)
  })
  default = null
}
",variable,"variable ""logging_settings"" {
  description = ""Default settings for logging resources.""
  type = object({
    # TODO: add support for CMEK
    disable_default_sink = optional(bool)
    storage_location     = optional(string)
  })
  default = null
}
",variable,42,42.0,604920dec9ceb39e20b9768f482807f2675092ff,604920dec9ceb39e20b9768f482807f2675092ff,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/604920dec9ceb39e20b9768f482807f2675092ff/modules/organization/variables-logging.tf#L42,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/604920dec9ceb39e20b9768f482807f2675092ff/modules/organization/variables-logging.tf#L42,2024-05-13 09:24:17+02:00,2024-05-13 09:24:17+02:00,1,0,0,1,0,1,0,0,1,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,240,community/modules/scheduler/htcondor-service-accounts/main.tf,community/modules/scheduler/htcondor-service-accounts/main.tf,0,implementation,"# underlying implementation changes, this module should declare explicit","/** 
 * Copyright 2024 Google LLC 
 * 
 * Licensed under the Apache License, Version 2.0 (the ""License""); 
 * you may not use this file except in compliance with the License. 
 * You may obtain a copy of the License at 
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, software 
 * distributed under the License is distributed on an ""AS IS"" BASIS, 
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 * See the License for the specific language governing permissions and 
 * limitations under the License. 
 */  
 # NB: the community/modules/project/service-account module will not output the 
 # service account e-mail address until all IAM bindings have been created; if 
 # underlying implementation changes, this module should declare explicit 
 # depends_on the IAM bindings to prevent race conditions for services that 
 # require them ","module ""access_point_service_account"" {
  source = ""github.com/GoogleCloudPlatform/hpc-toolkit//community/modules/project/service-account?ref=v1.28.1&depth=1""

  project_id      = var.project_id
  display_name    = ""HTCondor Access Point""
  deployment_name = var.deployment_name
  name            = ""access""
  project_roles   = var.access_point_roles
}
",module,"module ""access_point_service_account"" {
  source = ""github.com/GoogleCloudPlatform/hpc-toolkit//community/modules/project/service-account?ref=v1.32.1&depth=1""

  project_id      = var.project_id
  display_name    = ""HTCondor Access Point""
  deployment_name = var.deployment_name
  name            = ""access""
  project_roles   = var.access_point_roles
}
",module,19,19.0,1983ad58ecdb0d56ab724713d6ae8ee4058242b3,0aec7fb77c813bd747de536bc927542877d69de6,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/1983ad58ecdb0d56ab724713d6ae8ee4058242b3/community/modules/scheduler/htcondor-service-accounts/main.tf#L19,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/0aec7fb77c813bd747de536bc927542877d69de6/community/modules/scheduler/htcondor-service-accounts/main.tf#L19,2024-02-21 09:21:11-06:00,2024-04-19 19:04:24+00:00,5,0,0,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,39,modules/data_warehouse/main.tf,modules/data_warehouse/main.tf,0,# todo,# TODO: scope this down,# TODO: scope this down,"resource ""google_project_iam_member"" ""cloud_function_service_account_editor_role"" {
  project = var.project_id
  role    = ""roles/editor""
  member  = ""serviceAccount:${google_service_account.cloud_function_service_account.email}""

  depends_on = [
    google_service_account.cloud_function_service_account
  ]
}
",resource,the block associated got renamed or deleted,,64,,ad3c3472b644fe79c37ae1416b28faf5e0cbe271,ef343096883bf6daaaea016ae561f089fab4539c,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/ad3c3472b644fe79c37ae1416b28faf5e0cbe271/modules/data_warehouse/main.tf#L64,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/ef343096883bf6daaaea016ae561f089fab4539c/modules/data_warehouse/main.tf,2023-02-17 11:54:10-06:00,2023-03-15 18:25:56-05:00,5,1,0,0,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,215,infra/gcp/terraform/k8s-infra-prow-build/main.tf,infra/gcp/terraform/k8s-infra-prow-build/main.tf,0,# todo,# TODO(spiffxp): intent is to replace pool4 with this after scheduling a few select,"# TODO(spiffxp): intent is to replace pool4 with this after scheduling a few select 
 #                jobs to this pool to confirm things work as expected; this will 
 #                involve dropping the taints, raising the max_count","module ""prow_build_nodepool_n1_highmem_8_localssd"" {
  source        = ""../modules/gke-nodepool""
  project_name  = local.project_id
  cluster_name  = module.prow_build_cluster.cluster.name
  location      = module.prow_build_cluster.cluster.location
  name          = ""pool5""
  # NOTE: taints are only applied during creation and ignored after that, see module docs
  taints        = [{ key = ""ephemeral-ssd-experiment"", value = ""true"", effect = ""NO_SCHEDULE"" }]
  initial_count = 1
  min_count     = 1
  max_count     = 5
  # kind-ipv6 jobs need an ipv6 stack; COS doesn't provide one, so we need to
  # use an UBUNTU image instead. Keep parity with the existing google.com
  # k8s-prow-builds/prow cluster by using the CONTAINERD variant
  image_type   = ""UBUNTU_CONTAINERD""
  machine_type = ""n1-highmem-8""
  disk_size_gb    = 100
  disk_type       = ""pd-standard""
  # Use local SSDs for ephemeral storage; each SSD is 375GB
  ephemeral_local_ssd_count =  2
  service_account = module.prow_build_cluster.cluster_node_sa.email
}
",module,"module ""prow_build_nodepool_n1_highmem_8_localssd"" {
  source        = ""../modules/gke-nodepool""
  project_name  = local.project_id
  cluster_name  = module.prow_build_cluster.cluster.name
  location      = module.prow_build_cluster.cluster.location
  name          = ""pool5""
  initial_count = 1
  min_count     = 1
  max_count     = 80
  # kind-ipv6 jobs need an ipv6 stack; COS doesn't provide one, so we need to
  # use an UBUNTU image instead. Keep parity with the existing google.com
  # k8s-prow-builds/prow cluster by using the CONTAINERD variant
  image_type   = ""UBUNTU_CONTAINERD""
  machine_type = ""n1-highmem-8""
  disk_size_gb    = 100
  disk_type       = ""pd-standard""
  # Use local SSDs for ephemeral storage; each SSD is 375GB
  ephemeral_local_ssd_count =  2
  service_account = module.prow_build_cluster.cluster_node_sa.email
}
",module,157,,963ed6bb4beab18030eea9899916522f22f31524,f403678c4aa40765125e4738a91fbfd99addd89b,https://github.com/kubernetes/k8s.io/blob/963ed6bb4beab18030eea9899916522f22f31524/infra/gcp/terraform/k8s-infra-prow-build/main.tf#L157,https://github.com/kubernetes/k8s.io/blob/f403678c4aa40765125e4738a91fbfd99addd89b/infra/gcp/terraform/k8s-infra-prow-build/main.tf,2021-09-27 13:07:57-07:00,2021-09-28 05:47:06-07:00,2,1,1,1,0,0,0,0,0,1
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1480,blueprints/data-solutions/bq-ml/vertex.tf,blueprints/data-solutions/bq-ml/vertex.tf,0,fix,#TODO Uncomment once terraform-provider-google/issues/9273 is fixed,"#TODO Uncomment once terraform-provider-google/issues/9273 is fixed 
 # tags = [""ssh""]","resource ""google_notebooks_instance"" ""playground"" {
  name         = ""${var.prefix}-notebook""
  location     = format(""%s-%s"", var.region, ""b"")
  machine_type = ""e2-medium""
  project      = module.project.project_id

  container_image {
    repository = ""gcr.io/deeplearning-platform-release/base-cpu""
    tag        = ""latest""
  }

  install_gpu_driver = true
  boot_disk_type     = ""PD_SSD""
  boot_disk_size_gb  = 110
  disk_encryption    = try(local.service_encryption_keys.compute != null, false) ? ""CMEK"" : null
  kms_key            = try(local.service_encryption_keys.compute, null)

  no_public_ip    = true
  no_proxy_access = false

  network = local.vpc
  subnet  = local.subnet

  service_account = module.service-account-notebook.email

  # Enable Secure Boot 
  shielded_instance_config {
    enable_secure_boot = true
  }

  # Remove once terraform-provider-google/issues/9164 is fixed
  lifecycle {
    ignore_changes = [disk_encryption, kms_key]
  }

  #TODO Uncomment once terraform-provider-google/issues/9273 is fixed
  # tags = [""ssh""]
  depends_on = [
    google_project_iam_member.shared_vpc,
  ]
}
",resource,"resource ""google_notebooks_instance"" ""playground"" {
  name         = ""${var.prefix}-notebook""
  location     = format(""%s-%s"", var.region, ""b"")
  machine_type = ""e2-medium""
  project      = module.project.project_id

  container_image {
    repository = ""gcr.io/deeplearning-platform-release/base-cpu""
    tag        = ""latest""
  }

  install_gpu_driver = true
  boot_disk_type     = ""PD_SSD""
  boot_disk_size_gb  = 110
  disk_encryption    = try(local.service_encryption_keys.compute != null, false) ? ""CMEK"" : null
  kms_key            = try(local.service_encryption_keys.compute, null)

  no_public_ip    = true
  no_proxy_access = false

  network = local.vpc
  subnet  = local.subnet

  service_account = module.service-account-notebook.email

  # Enable Secure Boot 
  shielded_instance_config {
    enable_secure_boot = true
  }

  # Remove once terraform-provider-google/issues/9164 is fixed
  lifecycle {
    ignore_changes = [disk_encryption, kms_key]
  }

  #TODO Uncomment once terraform-provider-google/issues/9273 is fixed
  # tags = [""ssh""]
  depends_on = [
    google_project_iam_member.shared_vpc,
  ]
}
",resource,99,106.0,9e19f8960861fe61830801eab27111422f1d7a4e,3f9bbc2e5c57ccad4b0532107eef8dd9245d1627,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9e19f8960861fe61830801eab27111422f1d7a4e/blueprints/data-solutions/bq-ml/vertex.tf#L99,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3f9bbc2e5c57ccad4b0532107eef8dd9245d1627/blueprints/data-solutions/bq-ml/vertex.tf#L106,2023-03-05 22:02:41+01:00,2023-03-09 09:13:21+01:00,2,0,1,1,1,0,0,0,0,0
https://github.com/ministryofjustice/cloud-platform-infrastructure,497,terraform/aws-accounts/cloud-platform-aws/account/cloudfront-waf.tf,terraform/aws-accounts/cloud-platform-aws/account/cloudfront-waf.tf,0,implement,# This is a very specific AWS WAF implemention for use with CloudFront only,"# This is a very specific AWS WAF implemention for use with CloudFront only 
 # Users with an ingress must only utilise ModSecurity or the WAF-enabled centralised ingress 
 # This WAF IPSet, rule, and corresponding WAF ACL is very specific to Prisoner Content Hub 
 # who stream videos from an S3 bucket which is limited to access within a prison estate 
 # Using AWS WAF is/was the only way to IP allowlist with CloudFront (as at 2023-10-12). 
 # We should explore alternatives as they become available.  
 # resource ""aws_ssm_parameter"" ""prisoner_content_hub_production"" { 
 #   name  = ""/prisoner-content-hub-production/ip-allow-list"" 
 #   type  = ""String"" 
 #   value = ""bar"" 
 # }  
 # resource ""aws_ssm_parameter"" ""prisoner_content_hub_staging"" { 
 #   name  = ""/prisoner-content-hub-staging/ip-allow-list"" 
 #   type  = ""String"" 
 #   value = ""bar"" 
 # }  
 # resource ""aws_ssm_parameter"" ""prisoner_content_hub_development"" { 
 #   name  = ""/prisoner-content-hub-development/ip-allow-list"" 
 #   type  = ""String"" 
 #   value = ""bar"" 
 # } ","data ""aws_ssm_parameter"" ""test"" {
  name = ""/prisoner-content-hub-test/ip-allow-list""
}
",data,"locals {
  prisoner_content_hub_environments = toset([""production"", ""staging"", ""development""])
}
",locals,1,1.0,1ae9a83fb9a5ccf6da6b53b29bb6ef8ecdc36d0d,33ffa7decc6224b56c3095105529593748aa8707,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/1ae9a83fb9a5ccf6da6b53b29bb6ef8ecdc36d0d/terraform/aws-accounts/cloud-platform-aws/account/cloudfront-waf.tf#L1,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/33ffa7decc6224b56c3095105529593748aa8707/terraform/aws-accounts/cloud-platform-aws/account/cloudfront-waf.tf#L1,2023-10-12 11:54:37+01:00,2023-11-07 10:40:55+00:00,5,0,1,1,0,0,1,0,0,0
