Repo URL,Satd Comment Id,File Path Of First Occurence,File Path Of Last Occurence,renamed,Keyword,SATD Comment,context,bloc of first occurrence,bloc type of first occurrence,bloc of last occurrence,bloc type of last occurrence,SATD Comment Line Of First Occurence,SATD Comment Line Of Last Occurence,first Commit Hash,last Commit Hash,Link To The File Of First Occurence,Link To The File Of Last Occurence/When Adressed,Introduction Time,Last Occurence (even solved or not),number of commits,adressed ?,Computing Management Debt,IaC Code Debt,Dependency Management,Security Debt,Networking Debt,Environment-Based Configuration Debt,Monitoring and Logging Debt,Test Debt
https://github.com/Azure/az-hop,185,tf/variables_local.tf,tf/variables_local.tf,0,implemented,# Lustre - AMLFS not implemented for TF,# Lustre - AMLFS not implemented for TF,"locals {
    # azure environment
    public_cloud_endpoints = {
        KeyVaultSuffix =  ""vault.azure.net""
        BlobStorageSuffix = ""blob.core.windows.net""
        FileStorageSuffix = ""file.core.windows.net""
        MariaDBPrivateLink = ""privatelink.mariadb.database.azure.com""
    }
    usgov_cloud_endpoints = {
        KeyVaultSuffix =  ""vault.usgovcloudapi.net""
        BlobStorageSuffix = ""blob.core.usgovcloudapi.net""
        FileStorageSuffix = ""file.core.usgovcloudapi.net""
        MariaDBPrivateLink = ""privatelink.mariadb.database.usgovcloudapi.net""
    }
    azure_endpoints = {
        AZUREPUBLICCLOUD = local.public_cloud_endpoints
        AZUREUSGOVERNMENTCLOUD = local.usgov_cloud_endpoints
    }
    azure_environment = var.AzureEnvironment
    key_vault_suffix = local.azure_endpoints[local.azure_environment].KeyVaultSuffix #var.KeyVaultSuffix
    blob_storage_suffix = local.azure_endpoints[local.azure_environment].BlobStorageSuffix #var.BlobStorageSuffix

    # azurerm_client_config contains empty values for Managed Identity so use variables instead
    tenant_id = var.tenant_id
    logged_user_objectId = var.logged_user_objectId

    # config files and directories
    packer_root_dir = ""${path.cwd}/packer""
    playbook_root_dir = ""${path.cwd}/playbooks""
    playbooks_template_dir = ""${path.root}/templates""
    configuration_file=""${path.cwd}/config.yml""
    configuration_yml=yamldecode(file(local.configuration_file))
    
    # Load parameters from the configuration file
    location = local.configuration_yml[""location""]
    resource_group = local.configuration_yml[""resource_group""]
    extra_tags = try(local.configuration_yml[""tags""], null)
    common_tags = {
        CreatedBy = var.CreatedBy
        CreatedOn = timestamp()
    }

    # the PUID for telemetry is meant to be unique and identifies azhop, so it should not be changed
    telem_azhop_puid  = ""58d16d1a-5b7c-11ed-8042-00155d5d7a47""

    # local to determine if the user chose to disable telemetry of azhop
    optout_telemetry = try(local.configuration_yml[""optout_telemetry""], false)

    telem_azhop_name = substr(
        format(
            ""pid-%s"",
            local.telem_azhop_puid
        ),
        0,
        64
    )

    # empty arm template to create the telemetry resource
    telem_arm_subscription_template_content = <<TEMPLATE
    {
        ""$schema"": ""https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json#"",
        ""contentVersion"": ""1.0.0.0"",
        ""parameters"": {},
        ""variables"": {},
        ""resources"": [],
        ""outputs"": {
            ""telemetry"": {
                ""type"": ""String"",
                ""value"": ""For more information, see https://azure.github.io/az-hop/deploy/telemetry.html""
            }
        }
    }
    TEMPLATE

    # Log Analytics
    create_log_analytics_workspace = try(local.configuration_yml[""log_analytics""][""create""], false)
    log_analytics_name = try(local.configuration_yml[""log_analytics""][""name""], null)
    log_analytics_resource_group = try(local.configuration_yml[""log_analytics""][""resource_group""], null)
    log_analytics_subscription_id = try(local.configuration_yml[""log_analytics""][""subscription_id""], data.azurerm_subscription.primary.subscription_id)
    log_analytics_workspace_id = try(""/subscriptions/${local.log_analytics_subscription_id}/resourceGroups/${local.log_analytics_resource_group}/providers/Microsoft.OperationalInsights/workspaces/${local.log_analytics_name}"", null)
    use_existing_ws = ( !local.create_log_analytics_workspace && local.log_analytics_workspace_id != null )  ? true : false
     
    monitor = ( local.create_log_analytics_workspace || local.use_existing_ws ) ? true : false
    ama_install = try(local.configuration_yml[""monitoring""][""azure_monitor_agent""], true) && local.monitor ? true : false
    create_grafana = try(local.configuration_yml[""monitoring""][""grafana""], true)

    alert_email = try(local.configuration_yml[""alerting""][""admin_email""], ""admin.mail@contoso.com"")

    #For alerting to be enabled - the analytics workspace needs to be created since log alerts are leveraged. 
    #We also need to ensure that we have an email to send alerts to.  
    create_alerts = local.monitor && local.alert_email != ""admin.mail@contoso.com"" && try(local.configuration_yml[""alerting""][""enabled""], false) ? true : false
    anf_vol_threshold = try(local.configuration_yml[""anf""][""alert_threshold""], 80)  # default to 80% if not specified 

    # will be used with a KQL query that checks the free space percentage of local volumes
    # if the user wants to create an alert when local volumes are 80% full, then the free space percentage should be 20%
    local_vol_threshold = 100 - try(local.configuration_yml[""alerting""][""local_volume_threshold""], 20) 

    mounts = try(local.configuration_yml[""mounts""], {})
    mountpoints =  [ for mount in local.mounts : mount.mountpoint ]
    mountpoints_str = ""[ ${join("","", [for mp in local.mountpoints : format(""%q"", mp)])} ]"" //necessary to build generic KQL query on local volumes

    # Active Directory values
    # Updates the assumptions to the possibility that DNS may not point to Active Directory when using the customer provided AD.
    create_ad             = !try(local.configuration_yml[""domain""].use_existing_dc, false) && (try(local.configuration_yml[""authentication""].user_auth, ""ad"") == ""ad"")
    use_existing_ad       = try(local.configuration_yml[""domain""].use_existing_dc, false)
    create_dns_records    = local.create_ad || local.use_existing_ad
    domain_name           = local.use_existing_ad ? local.configuration_yml[""domain""].name : ""hpc.azure""
    domain_join_user      = local.use_existing_ad ? local.configuration_yml[""domain""].domain_join_user.username : local.admin_username
    domain_join_password  = local.use_existing_ad ? data.azurerm_key_vault_secret.domain_join_password[0].value : random_password.password.result
    domain_join_ou        = local.use_existing_ad ? local.configuration_yml[""domain""].domain_join_ou : ""CN=Computers""
    ad_ha                 = try(local.configuration_yml[""ad""].high_availability, false)
    domain_controlers     = local.use_existing_ad ? zipmap(local.configuration_yml[""domain""].existing_dc_details.domain_controller_names, local.configuration_yml[""domain""].existing_dc_details.domain_controller_names) : (local.ad_ha ? {ad=local.ad_name, ad2=local.ad2_name} : {ad=local.ad_name})
    ldap_server           = local.use_existing_ad ? local.configuration_yml[""domain""].existing_dc_details.domain_controller_names[0]     : local.ad_name
    private_dns_servers   = local.use_existing_ad ? local.configuration_yml[""domain""].existing_dc_details.private_dns_servers            : (local.create_ad ? (local.ad_ha ? [azurerm_network_interface.ad-nic[0].private_ip_address, azurerm_network_interface.ad2-nic[0].private_ip_address] : [azurerm_network_interface.ad-nic[0].private_ip_address]) : [])
    domain_controller_ips = local.use_existing_ad ? local.configuration_yml[""domain""].existing_dc_details.domain_controller_ip_addresses : (local.create_ad ? (local.ad_ha ? [azurerm_network_interface.ad-nic[0].private_ip_address, azurerm_network_interface.ad2-nic[0].private_ip_address] : [azurerm_network_interface.ad-nic[0].private_ip_address]) : [])

    # Use a linux custom image reference if the linux_base_image is defined and contains "":""
    use_linux_image_reference = try(length(split("":"", local.configuration_yml[""linux_base_image""])[1])>0, false)
    # Use a linux custom image reference if the linux_base_image is defined and contains "":""
    use_windows_image_reference = try(length(split("":"", local.configuration_yml[""windows_base_image""])[1])>0, false)
    # Use a linux custom image reference if the linux_base_image is defined and contains "":""
    use_cyclecloud_image_reference = try(length(split("":"", local.configuration_yml[""cyclecloud""][""image""])[1])>0, false)

    linux_base_image_reference = {
        publisher = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[0] : ""OpenLogic""
        offer     = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[1] : ""CentOS""
        sku       = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[2] : ""7_9-gen2""
        version   = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[3] : ""latest""
    }
    windows_base_image_reference = {
        publisher = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[0] : ""MicrosoftWindowsServer""
        offer     = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[1] : ""WindowsServer""
        sku       = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[2] : ""2019-Datacenter-smalldisk""
        version   = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[3] : ""latest""
    }
    cyclecloud_image_reference = {
        publisher = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[0] : ""OpenLogic""
        offer     = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[1] : ""CentOS""
        sku       = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[2] : ""7_9-gen2""
        version   = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[3] : ""latest""
    }

    # Use a linux custom image id if the linux_base_image is defined and contains ""/""
    use_linux_image_id = try(length(split(""/"", local.configuration_yml[""linux_base_image""])[1])>0, false)
    linux_image_id = local.use_linux_image_id ? local.configuration_yml[""linux_base_image""] : null

    # Use a windows custom image id if the windows_base_image is defined and contains ""/""
    use_windows_image_id = try(length(split(""/"", local.configuration_yml[""windows_base_image""])[1])>0, false)
    windows_image_id = local.use_windows_image_id ? local.configuration_yml[""windows_base_image""] : null

    # Use a cyclecloud custom image id if the cyclecloud_base_image is defined and contains ""/""
    use_cyclecloud_image_id = try(length(split(""/"", local.configuration_yml[""cyclecloud""][""image""])[1])>0, false)
    cyclecloud_image_id = local.use_cyclecloud_image_id ? local.configuration_yml[""cyclecloud""][""image""] : null

    _empty_image_plan = {}
    _linux_base_image_plan = {
        publisher = try(split("":"", local.configuration_yml[""linux_base_plan""])[0], """")
        product   = try(split("":"", local.configuration_yml[""linux_base_plan""])[1], """")
        name      = try(split("":"", local.configuration_yml[""linux_base_plan""])[2], """")
    }
    linux_image_plan = try( length(local._linux_base_image_plan.publisher) > 0 ? local._linux_base_image_plan : local._empty_image_plan, local._empty_image_plan)

    _cyclecloud_image_plan = {
        publisher = try(split("":"", local.configuration_yml[""cyclecloud""][""plan""])[0], """")
        product   = try(split("":"", local.configuration_yml[""cyclecloud""][""plan""])[1], """")
        name      = try(split("":"", local.configuration_yml[""cyclecloud""][""plan""])[2], """")
    }
    cyclecloud_image_plan = try( length(local._cyclecloud_image_plan.publisher) > 0 ? local._cyclecloud_image_plan : local._empty_image_plan, local._empty_image_plan)


    # Create the RG if not using an existing RG and (creating a VNET or when reusing a VNET in another resource group)
    use_existing_rg = try(local.configuration_yml[""use_existing_rg""], false)
    create_rg = (!local.use_existing_rg) && (local.create_vnet || try(split(""/"", local.vnet_id)[4], local.resource_group) != local.resource_group)

    # ANF
    create_anf = try(local.configuration_yml[""anf""][""create""], false)
    anf_size=try(local.configuration_yml[""anf""][""homefs_size_tb""], 4)
    anf_service_level = try(local.configuration_yml[""anf""][""homefs_service_level""], ""Standard"")
    anf_dual_protocol = try(local.configuration_yml[""anf""][""dual_protocol""], false)

    #Azure Files
    create_nfsfiles = try(local.configuration_yml[""azurefiles""][""create""], false)
    azure_files_size= try(local.configuration_yml[""azurefiles""][""size_gb""], 1024)

    # Home Directory
    homedir_type = try(local.configuration_yml[""mounts""][""home""][""type""], ""existing"")
    config_nfs_home_ip = local.configuration_yml[""mounts""][""home""][""server""]
    config_nfs_home_path = local.configuration_yml[""mounts""][""home""][""export""]
    config_nfs_home_opts = local.configuration_yml[""mounts""][""home""][""options""]

    homedir_mountpoint = try(local.configuration_yml[""mounts""][""home""][""mountpoint""], ""/anfhome"")

    admin_username = local.configuration_yml[""admin_user""]
    key_vault_readers = try(local.configuration_yml[""key_vault_readers""], null)

    # Resource names
    scheduler_name = try(local.configuration_yml[""scheduler""][""name""], ""scheduler"")
    ccportal_name = try(local.configuration_yml[""cyclecloud""][""name""], ""ccportal"")
    ondemand_name = try(local.configuration_yml[""ondemand""][""name""], ""ondemand"")
    grafana_name = try(local.configuration_yml[""grafana""][""name""], ""grafana"")
    jumpbox_name = try(local.configuration_yml[""jumpbox""][""name""], ""jumpbox"")
    ad_name = try(local.configuration_yml[""ad""][""name""], ""ad"")
    ad2_name = try(local.configuration_yml[""ad""][""ha_name""], ""ad2"")

    key_vault_name = try(local.configuration_yml[""azure_key_vault""][""name""], format(""%s%s"", ""kv"", random_string.resource_postfix.result))
    storage_account_name = try(local.configuration_yml[""azure_storage_account""][""name""], ""azhop${random_string.resource_postfix.result}"")
    mariadb_name = try(local.configuration_yml[""database""][""name""], ""azhop-${random_string.resource_postfix.result}"")

    # Lustre - AMLFS not implemented for TF
    lustre_enabled = false

    # Use a jumpbox when defined
    jumpbox_enabled = try(length(local.configuration_yml[""jumpbox""]) > 0, false)

    # Queue manager
    queue_manager = try(local.configuration_yml[""queue_manager""], ""openpbs"")

    # Create Database
    create_database  = ( try(local.configuration_yml[""slurm""].accounting_enabled, false) ) && (! local.use_existing_database)
    use_existing_database = try(length(local.configuration_yml[""database""].fqdn) > 0 ? true : false, false)
    database_user = local.create_database ? ""sqladmin"" : (local.use_existing_database ? try(local.configuration_yml[""database""].user, """") : """")
    mariadb_private_dns_zone = local.azure_endpoints[local.azure_environment].MariaDBPrivateLink

    create_sig = try(local.configuration_yml[""image_gallery""][""create""], false)
    
    # VNET
    create_vnet = try(length(local.vnet_id) > 0 ? false : true, true)
    vnet_id = try(local.configuration_yml[""network""][""vnet""][""id""], null)

    # VNET Peering
    vnet_peering = try(tolist(local.configuration_yml[""network""][""peering""]), [])

    # Lockdown scenario
    locked_down_network = try(local.configuration_yml[""locked_down_network""][""enforce""], false)
    grant_access_from   = try(local.configuration_yml[""locked_down_network""][""grant_access_from""], [])
    allow_public_ip     = try(local.configuration_yml[""locked_down_network""][""public_ip""], true)
    jumpbox_ssh_port    = try(local.configuration_yml[""jumpbox""][""ssh_port""], ""22"")
    # subnets
    _subnets = {
        frontend = ""frontend"",
        admin = ""admin"",
        netapp = ""netapp"",
        compute = ""compute""
    }

    # Create subnet if required. If not specified create only if vnet is created
    create_frontend_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""frontend""][""create""], local.create_vnet )
    create_admin_subnet    = try(local.configuration_yml[""network""][""vnet""][""subnets""][""admin""][""create""], local.create_vnet )
    create_netapp_subnet   = try(local.configuration_yml[""network""][""vnet""][""subnets""][""netapp""][""create""], local.create_vnet )
    create_compute_subnet  = try(local.configuration_yml[""network""][""vnet""][""subnets""][""compute""][""create""], local.create_vnet )

    ad_subnet        = try(local.configuration_yml[""network""][""vnet""][""subnets""][""ad""], null)
    no_ad_subnet     = try(length(local.ad_subnet) > 0 ? false : true, true)
    create_ad_subnet = try(local.ad_subnet[""create""], (local.create_ad ? local.create_vnet : false))

    bastion_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""bastion""], null)
    no_bastion_subnet = try(length(local.bastion_subnet) > 0 ? false : true, true )
    create_bastion_subnet  = try(local.bastion_subnet[""create""], local.create_vnet )

    gateway_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""gateway""], null)
    no_gateway_subnet = try(length(local.gateway_subnet) > 0 ? false : true, true )
    create_gateway_subnet  = try(local.gateway_subnet[""create""], local.create_vnet )

    outbounddns_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""outbounddns""], null)
    no_outbounddns_subnet = try(length(local.outbounddns_subnet) > 0 ? false : true, true )
    create_outbounddns_subnet  = try(local.outbounddns_subnet[""create""], local.create_vnet ? (local.no_outbounddns_subnet ? false : true) : false )

    dns_forwarders = try(local.configuration_yml[""dns""][""forwarders""], [])
    create_dnsfw_rules = length(local.dns_forwarders) > 0 ? true : false

    subnets = merge(local._subnets, 
                    local.no_bastion_subnet ? {} : {bastion = ""AzureBastionSubnet""},
                    local.no_gateway_subnet ? {} : {gateway = ""GatewaySubnet""},
                    local.no_outbounddns_subnet ? {} : {outbounddns = ""outbounddns""}
                    )

    # Application Security Groups
    create_nsg = try(local.configuration_yml[""network""][""create_nsg""], local.create_vnet )
    # If create NSG then use the local resource group otherwise use the configured one. Default to local resource group
    asg_resource_group = local.create_nsg ? local.resource_group : try(length(local.configuration_yml[""network""][""asg""][""resource_group""]) > 0 ? local.configuration_yml[""network""][""asg""][""resource_group""] : local.resource_group, local.resource_group )

    _default_asgs = {
        asg-ssh = ""asg-ssh""
        asg-rdp = ""asg-rdp""
        asg-jumpbox = ""asg-jumpbox""
        asg-ad = ""asg-ad""
        asg-ad-client = ""asg-ad-client""
        asg-lustre-client = ""asg-lustre-client""
        asg-pbs = ""asg-pbs""
        asg-pbs-client = ""asg-pbs-client""
        asg-cyclecloud = ""asg-cyclecloud""
        asg-cyclecloud-client = ""asg-cyclecloud-client""
        asg-nfs-client = ""asg-nfs-client""
        asg-telegraf = ""asg-telegraf""
        asg-grafana = ""asg-grafana""
        asg-robinhood = ""asg-robinhood""
        asg-ondemand = ""asg-ondemand""
        asg-deployer = ""asg-deployer""
        asg-mariadb-client = ""asg-mariadb-client""
    }
    #asgs = local.create_nsg ? local._default_asgs :  try(local.configuration_yml[""network""][""asg""][""names""], local._default_asgs)
    asgs = try(local.configuration_yml[""network""][""asg""][""names""], local._default_asgs)
    #asgs = { for v in local.default_asgs : v => v }
    empty_array = []
    empty_map = { for v in local.empty_array : v => v }

    # VM name to list of ASGs associations
    # TODO : Add mapping for names
    asg_associations = {
        ad        = [""asg-ad"", ""asg-rdp"", ""asg-ad-client""] # asg-ad-client will allow the secondary DC scenario
        ccportal  = [""asg-ssh"", ""asg-cyclecloud"", ""asg-telegraf"", ""asg-ad-client""]
        grafana   = [""asg-ssh"", ""asg-grafana"", ""asg-ad-client"", ""asg-telegraf"", ""asg-nfs-client""]
        jumpbox   = [""asg-ssh"", ""asg-jumpbox"", ""asg-ad-client"", ""asg-telegraf"", ""asg-nfs-client""]
        ondemand  = [""asg-ssh"", ""asg-ondemand"", ""asg-ad-client"", ""asg-nfs-client"", ""asg-pbs-client"", ""asg-lustre-client"", ""asg-telegraf"", ""asg-cyclecloud-client"", ""asg-mariadb-client""]
        robinhood = [""asg-ssh"", ""asg-robinhood"", ""asg-lustre-client"", ""asg-telegraf""]
        scheduler = [""asg-ssh"", ""asg-pbs"", ""asg-ad-client"", ""asg-cyclecloud-client"", ""asg-nfs-client"", ""asg-telegraf"", ""asg-mariadb-client""]
    }

    # Open ports for NSG TCP rules
    # ANF and SMB https://docs.microsoft.com/en-us/azure/azure-netapp-files/create-active-directory-connections
    nsg_destination_ports = {
        All = [""0-65535""]
        Bastion = [""22"", ""3389""]
        Web = [""443"", ""80""]
        Ssh    = [""22""]
        Public_Ssh = [local.jumpbox_ssh_port]
        # DNS, Kerberos, RpcMapper, Ldap, Smb, KerberosPass, LdapSsl, LdapGc, LdapGcSsl, AD Web Services, RpcSam
        DomainControlerTcp = [""53"", ""88"", ""135"", ""389"", ""445"", ""464"", ""636"", ""3268"", ""3269"", ""9389"", ""49152-65535""]
        # DNS, Kerberos, W32Time, NetBIOS, Ldap, KerberosPass, LdapSsl
        DomainControlerUdp = [""53"", ""88"", ""123"", ""138"", ""389"", ""464"", ""636""]
        # Web, NoVNC, WebSockify
        NoVnc = [""80"", ""443"", ""5900-5910"", ""61001-61010""]
        Dns = [""53""]
        Rdp = [""3389""]
        Pbs = [""6200"", ""15001-15009"", ""17001"", ""32768-61000"", ""6817-6819""]
        Slurmd = [""6818""]
        Lustre = [""635"", ""988""]
        Nfs = [""111"", ""635"", ""2049"", ""4045"", ""4046""]
        SMB = [""445""]
        Telegraf = [""8086""]
        Grafana = [""3000""]
        # HTTPS, AMQP
        CycleCloud = [""9443"", ""5672""],
        # MariaDB
        MariaDB = [""3306"", ""33060""],
        # WinRM
        WinRM = [""5985"", ""5986""]
    }

    #Replace the AD ASG with domain controller IP addresses when customer is bringing their own AD
    #use an indexing concept since we can't substitute a list for a string
    ad_nsg_index = local.use_existing_ad ? ""ips/dc_ips"" : ""asg/asg-ad""
    ips = {
        dc_ips = local.domain_controller_ips
    }

    # Array of NSG rules to be applied on the common NSG
    # NsgRuleName = [priority, direction, access, protocol, destination_port_range, source, destination]
    #   - priority               : integer value from 100 to 4096
    #   - direction              : Inbound, Outbound
    #   - access                 : Allow, Deny
    #   - protocol               : Tcp, Udp, *
    #   - destination_port_range : name of one of the nsg_destination_ports defined above
    #   - source                 : asg/<asg-name>, subnet/<subnet-name>, tag/<tag-name>. tag-name = any Azure tags like Internet, VirtualNetwork, AzureLoadBalancer, ...
    #   - destination            : same as source
    _nsg_rules = {
        # ================================================================================================================================================================
        #                          ###
        #                           #     #    #  #####    ####   #    #  #    #  #####
        #                           #     ##   #  #    #  #    #  #    #  ##   #  #    #
        #                           #     # #  #  #####   #    #  #    #  # #  #  #    #
        #                           #     #  # #  #    #  #    #  #    #  #  # #  #    #
        #                           #     #   ##  #    #  #    #  #    #  #   ##  #    #
        #                          ###    #    #  #####    ####    ####   #    #  #####
        # ================================================================================================================================================================
        # AD communication
        AllowAdServerTcpIn        = [""220"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdServerUdpIn        = [""230"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdClientTcpIn        = [""240"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdClientUdpIn        = [""250"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdServerComputeTcpIn = [""260"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdServerComputeUdpIn = [""270"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdClientComputeTcpIn = [""280"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdClientComputeUdpIn = [""290"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdServerNetappTcpIn  = [""300"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""subnet/netapp"", local.ad_nsg_index],
        AllowAdServerNetappUdpIn  = [""310"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""subnet/netapp"", local.ad_nsg_index],

        # SSH internal rules
        AllowSshFromJumpboxIn       = [""320"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-jumpbox"",   ""asg/asg-ssh""],
        AllowSshFromComputeIn       = [""330"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""subnet/compute"",    ""asg/asg-ssh""],
        AllowSshFromDeployerIn      = [""340"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",  ""asg/asg-ssh""], # Only in a deployer VM scenario
        AllowDeployerToPackerSshIn  = [""350"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",  ""subnet/admin""], # Only in a deployer VM scenario
        AllowSshToComputeIn         = [""360"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-ssh"",       ""subnet/compute""],
        AllowSshComputeComputeIn    = [""365"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""subnet/compute"",    ""subnet/compute""],

        # PBS
        AllowPbsIn                  = [""369"", ""Inbound"", ""Allow"", ""*"",   ""Pbs"",                ""asg/asg-pbs"",        ""asg/asg-pbs-client""],
        AllowPbsClientIn            = [""370"", ""Inbound"", ""Allow"", ""*"",   ""Pbs"",                ""asg/asg-pbs-client"", ""asg/asg-pbs""],
        AllowPbsComputeIn           = [""380"", ""Inbound"", ""Allow"", ""*"",   ""Pbs"",                ""asg/asg-pbs"",        ""subnet/compute""],
        AllowComputePbsClientIn     = [""390"", ""Inbound"", ""Allow"", ""*"",   ""Pbs"",                ""subnet/compute"",     ""asg/asg-pbs-client""],
        AllowComputePbsIn           = [""400"", ""Inbound"", ""Allow"", ""*"",   ""Pbs"",                ""subnet/compute"",     ""asg/asg-pbs""],
        AllowComputeComputePbsIn    = [""401"", ""Inbound"", ""Allow"", ""*"",   ""Pbs"",                ""subnet/compute"",     ""subnet/compute""],

        # SLURM
        AllowComputeSlurmIn         = [""405"", ""Inbound"", ""Allow"", ""*"",   ""Slurmd"",             ""asg/asg-ondemand"",    ""subnet/compute""],

        # Lustre
        AllowLustreClientIn         = [""410"", ""Inbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""asg/asg-lustre-client"", ""subnet/admin""],
        AllowLustreClientComputeIn  = [""420"", ""Inbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""subnet/compute"",        ""subnet/admin""],

        # CycleCloud
        AllowCycleWebIn             = [""440"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                ""asg/asg-ondemand"",          ""asg/asg-cyclecloud""],
        AllowCycleClientIn          = [""450"", ""Inbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud-client"", ""asg/asg-cyclecloud""],
        AllowCycleClientComputeIn   = [""460"", ""Inbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""subnet/compute"",            ""asg/asg-cyclecloud""],
        AllowCycleServerIn          = [""465"", ""Inbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud"",        ""asg/asg-cyclecloud-client""],

        # OnDemand NoVNC
        AllowComputeNoVncIn         = [""470"", ""Inbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""subnet/compute"",            ""asg/asg-ondemand""],
        AllowNoVncComputeIn         = [""480"", ""Inbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""asg/asg-ondemand"",          ""subnet/compute""],

        # Admin and Deployment
        AllowWinRMIn                = [""520"", ""Inbound"", ""Allow"", ""Tcp"", ""WinRM"",              ""asg/asg-jumpbox"",          ""asg/asg-rdp""],
        AllowRdpIn                  = [""550"", ""Inbound"", ""Allow"", ""Tcp"", ""Rdp"",                ""asg/asg-jumpbox"",          ""asg/asg-rdp""],

        # MariaDB
        AllowMariaDBIn              = [""700"", ""Inbound"", ""Allow"", ""Tcp"", ""MariaDB"",             ""asg/asg-mariadb-client"",    ""subnet/admin""],

        # Deny all remaining traffic
        DenyVnetInbound             = [""3100"", ""Inbound"", ""Deny"", ""*"", ""All"",                  ""tag/VirtualNetwork"",       ""tag/VirtualNetwork""],

        # ================================================================================================================================================================
        #                            #######
        #                            #     #  #    #   #####  #####    ####   #    #  #    #  #####
        #                            #     #  #    #     #    #    #  #    #  #    #  ##   #  #    #
        #                            #     #  #    #     #    #####   #    #  #    #  # #  #  #    #
        #                            #     #  #    #     #    #    #  #    #  #    #  #  # #  #    #
        #                            #     #  #    #     #    #    #  #    #  #    #  #   ##  #    #
        #                            #######   ####      #    #####    ####    ####   #    #  #####
        # ================================================================================================================================================================
        # AD communication
        AllowAdClientTcpOut        = [""200"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdClientUdpOut        = [""210"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdClientComputeTcpOut = [""220"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdClientComputeUdpOut = [""230"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdServerTcpOut        = [""240"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdServerUdpOut        = [""250"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdServerComputeTcpOut = [""260"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdServerComputeUdpOut = [""270"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdServerNetappTcpOut  = [""280"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""subnet/netapp""],
        AllowAdServerNetappUdpOut  = [""290"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""subnet/netapp""],

        # CycleCloud
        AllowCycleServerOut         = [""300"", ""Outbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud"",        ""asg/asg-cyclecloud-client""],
        AllowCycleClientOut         = [""310"", ""Outbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud-client"", ""asg/asg-cyclecloud""],
        AllowComputeCycleClientIn   = [""320"", ""Outbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""subnet/compute"",            ""asg/asg-cyclecloud""],
        AllowCycleWebOut            = [""330"", ""Outbound"", ""Allow"", ""Tcp"", ""Web"",                ""asg/asg-ondemand"",          ""asg/asg-cyclecloud""],

        # PBS
        AllowPbsOut                 = [""340"", ""Outbound"", ""Allow"", ""*"",   ""Pbs"",                ""asg/asg-pbs"",        ""asg/asg-pbs-client""],
        AllowPbsClientOut           = [""350"", ""Outbound"", ""Allow"", ""*"",   ""Pbs"",                ""asg/asg-pbs-client"", ""asg/asg-pbs""],
        AllowPbsComputeOut          = [""360"", ""Outbound"", ""Allow"", ""*"",   ""Pbs"",                ""asg/asg-pbs"",        ""subnet/compute""],
        AllowPbsClientComputeOut    = [""370"", ""Outbound"", ""Allow"", ""*"",   ""Pbs"",                ""subnet/compute"",     ""asg/asg-pbs""],
        AllowComputePbsClientOut    = [""380"", ""Outbound"", ""Allow"", ""*"",   ""Pbs"",                ""subnet/compute"",     ""asg/asg-pbs-client""],
        AllowComputeComputePbsOut   = [""381"", ""Outbound"", ""Allow"", ""*"",   ""Pbs"",                ""subnet/compute"",     ""subnet/compute""],

        # SLURM
        AllowSlurmComputeOut        = [""385"", ""Outbound"", ""Allow"", ""*"",   ""Slurmd"",             ""asg/asg-ondemand"",        ""subnet/compute""],

        # Lustre
        AllowLustreClientOut        = [""400"", ""Outbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""asg/asg-lustre-client"",    ""subnet/admin""],
        AllowLustreClientComputeOut = [""420"", ""Outbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""subnet/compute"",           ""subnet/admin""],

        # NFS
        AllowNfsOut                 = [""440"", ""Outbound"", ""Allow"", ""*"",   ""Nfs"",                ""asg/asg-nfs-client"",       ""subnet/netapp""],
        AllowNfsComputeOut          = [""450"", ""Outbound"", ""Allow"", ""*"",   ""Nfs"",                ""subnet/compute"",           ""subnet/netapp""],

        # SMB
        AllowSMBComputeOut          = [""455"", ""Outbound"", ""Allow"", ""*"",   ""SMB"",                ""subnet/compute"",            ""subnet/netapp""],

        # SSH internal rules
        AllowSshFromJumpboxOut      = [""490"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-jumpbox"",          ""asg/asg-ssh""],
        AllowSshComputeOut          = [""500"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-ssh"",              ""subnet/compute""],
        AllowSshDeployerOut         = [""510"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",         ""asg/asg-ssh""],
        AllowSshDeployerPackerOut   = [""520"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",         ""subnet/admin""],
        AllowSshFromComputeOut      = [""530"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""subnet/compute"",           ""asg/asg-ssh""],
        AllowSshComputeComputeOut   = [""540"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""subnet/compute"",           ""subnet/compute""],

        # OnDemand NoVNC
        AllowComputeNoVncOut        = [""550"", ""Outbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""subnet/compute"",            ""asg/asg-ondemand""],
        AllowNoVncComputeOut        = [""560"", ""Outbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""asg/asg-ondemand"",          ""subnet/compute""],

        # Admin and Deployment
        AllowRdpOut                 = [""570"", ""Outbound"", ""Allow"", ""Tcp"", ""Rdp"",                ""asg/asg-jumpbox"",          ""asg/asg-rdp""],
        AllowWinRMOut               = [""580"", ""Outbound"", ""Allow"", ""Tcp"", ""WinRM"",              ""asg/asg-jumpbox"",          ""asg/asg-rdp""],
        AllowDnsOut                 = [""590"", ""Outbound"", ""Allow"", ""*"",   ""Dns"",                ""tag/VirtualNetwork"",       ""tag/VirtualNetwork""],

        # MariaDB
        AllowMariaDBOut             = [""700"", ""Outbound"", ""Allow"", ""Tcp"", ""MariaDB"",             ""asg/asg-mariadb-client"",    ""subnet/admin""],

        # Deny all remaining traffic and allow Internet access
        AllowInternetOutBound       = [""3000"", ""Outbound"", ""Allow"", ""Tcp"", ""All"",               ""tag/VirtualNetwork"",       ""tag/Internet""],
        DenyVnetOutbound            = [""3100"", ""Outbound"", ""Deny"",  ""*"",   ""All"",               ""tag/VirtualNetwork"",       ""tag/VirtualNetwork""],
    }

    internet_nsg_rules = {
        AllowInternetSshIn          = [""200"", ""Inbound"", ""Allow"", ""Tcp"", ""Public_Ssh"",         ""tag/Internet"", ""asg/asg-jumpbox""], # Only when using a PIP
        AllowInternetHttpIn         = [""210"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                ""tag/Internet"", ""asg/asg-ondemand""], # Only when using a PIP
    }

    hub_nsg_rules = {
        AllowHubSshIn          = [""200"", ""Inbound"", ""Allow"", ""Tcp"", ""Public_Ssh"",               ""tag/VirtualNetwork"", ""asg/asg-jumpbox""],
        AllowHubHttpIn         = [""210"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                      ""tag/VirtualNetwork"", ""asg/asg-ondemand""],
        AllowPackerWinRMIn     = [""560"", ""Inbound"", ""Allow"", ""Tcp"", ""WinRM"",                    ""tag/VirtualNetwork"", ""subnet/compute""],
    }

    bastion_nsg_rules = {
        AllowBastionIn              = [""530"", ""Inbound"", ""Allow"", ""Tcp"", ""Bastion"",            ""subnet/bastion"",           ""tag/VirtualNetwork""],
    }

    gateway_nsg_rules = {
        AllowInternalWebUsersIn     = [""540"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                ""subnet/gateway"",           ""asg/asg-ondemand""],
    }

    grafana_nsg_rules = {
        # Telegraf / Grafana
        AllowTelegrafIn             = [""490"", ""Inbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""asg/asg-telegraf"",          ""asg/asg-grafana""],
        AllowComputeTelegrafIn      = [""500"", ""Inbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""subnet/compute"",            ""asg/asg-grafana""],
        AllowGrafanaIn              = [""510"", ""Inbound"", ""Allow"", ""Tcp"", ""Grafana"",            ""asg/asg-ondemand"",          ""asg/asg-grafana""],

        # Telegraf / Grafana
        AllowTelegrafOut            = [""460"", ""Outbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""asg/asg-telegraf"",          ""asg/asg-grafana""],
        AllowComputeTelegrafOut     = [""470"", ""Outbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""subnet/compute"",            ""asg/asg-grafana""],
        AllowGrafanaOut             = [""480"", ""Outbound"", ""Allow"", ""Tcp"", ""Grafana"",            ""asg/asg-ondemand"",          ""asg/asg-grafana""],
    }

    nsg_rules = merge(  local._nsg_rules, 
                        local.no_bastion_subnet ? {} : local.bastion_nsg_rules, 
                        local.no_gateway_subnet ? {} : local.gateway_nsg_rules,
                        local.allow_public_ip ? local.internet_nsg_rules : local.hub_nsg_rules,
                        local.create_grafana ? local.grafana_nsg_rules : {})

}
",locals,"locals {
    # azure environment
    public_cloud_endpoints = {
        KeyVaultSuffix =  ""vault.azure.net""
        BlobStorageSuffix = ""blob.core.windows.net""
        FileStorageSuffix = ""file.core.windows.net""
        MariaDBPrivateLink = ""privatelink.mariadb.database.azure.com""
    }
    usgov_cloud_endpoints = {
        KeyVaultSuffix =  ""vault.usgovcloudapi.net""
        BlobStorageSuffix = ""blob.core.usgovcloudapi.net""
        FileStorageSuffix = ""file.core.usgovcloudapi.net""
        MariaDBPrivateLink = ""privatelink.mariadb.database.usgovcloudapi.net""
    }
    azure_endpoints = {
        AZUREPUBLICCLOUD = local.public_cloud_endpoints
        AZUREUSGOVERNMENTCLOUD = local.usgov_cloud_endpoints
    }
    azure_environment = var.AzureEnvironment
    key_vault_suffix = local.azure_endpoints[local.azure_environment].KeyVaultSuffix #var.KeyVaultSuffix
    blob_storage_suffix = local.azure_endpoints[local.azure_environment].BlobStorageSuffix #var.BlobStorageSuffix

    # azurerm_client_config contains empty values for Managed Identity so use variables instead
    tenant_id = var.tenant_id
    logged_user_objectId = var.logged_user_objectId

    # config files and directories
    packer_root_dir = ""${path.cwd}/packer""
    playbook_root_dir = ""${path.cwd}/playbooks""
    playbooks_template_dir = ""${path.root}/templates""
    configuration_file=""${path.cwd}/config.yml""
    configuration_yml=yamldecode(file(local.configuration_file))
    
    # Load parameters from the configuration file
    location = local.configuration_yml[""location""]
    resource_group = local.configuration_yml[""resource_group""]
    extra_tags = try(local.configuration_yml[""tags""], null)
    common_tags = {
        CreatedBy = var.CreatedBy
        CreatedOn = timestamp()
    }

    # the PUID for telemetry is meant to be unique and identifies azhop, so it should not be changed
    telem_azhop_puid  = ""58d16d1a-5b7c-11ed-8042-00155d5d7a47""

    # local to determine if the user chose to disable telemetry of azhop
    optout_telemetry = try(local.configuration_yml[""optout_telemetry""], false)

    telem_azhop_name = substr(
        format(
            ""pid-%s"",
            local.telem_azhop_puid
        ),
        0,
        64
    )

    # empty arm template to create the telemetry resource
    telem_arm_subscription_template_content = <<TEMPLATE
    {
        ""$schema"": ""https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json#"",
        ""contentVersion"": ""1.0.0.0"",
        ""parameters"": {},
        ""variables"": {},
        ""resources"": [],
        ""outputs"": {
            ""telemetry"": {
                ""type"": ""String"",
                ""value"": ""For more information, see https://azure.github.io/az-hop/deploy/telemetry.html""
            }
        }
    }
    TEMPLATE

    # Log Analytics
    create_log_analytics_workspace = try(local.configuration_yml[""log_analytics""][""create""], false)
    log_analytics_name = try(local.configuration_yml[""log_analytics""][""name""], null)
    log_analytics_resource_group = try(local.configuration_yml[""log_analytics""][""resource_group""], null)
    log_analytics_subscription_id = try(local.configuration_yml[""log_analytics""][""subscription_id""], data.azurerm_subscription.primary.subscription_id)
    log_analytics_workspace_id = try(""/subscriptions/${local.log_analytics_subscription_id}/resourceGroups/${local.log_analytics_resource_group}/providers/Microsoft.OperationalInsights/workspaces/${local.log_analytics_name}"", null)
    use_existing_ws = ( !local.create_log_analytics_workspace && local.log_analytics_workspace_id != null )  ? true : false
     
    monitor = ( local.create_log_analytics_workspace || local.use_existing_ws ) ? true : false
    ama_install = try(local.configuration_yml[""monitoring""][""azure_monitor_agent""], true) && local.monitor ? true : false
    create_grafana = try(local.configuration_yml[""monitoring""][""grafana""], true)
    create_ondemand = try(length(local.configuration_yml[""ondemand""]) > 0, false)

    alert_email = try(local.configuration_yml[""alerting""][""admin_email""], ""admin.mail@contoso.com"")

    #For alerting to be enabled - the analytics workspace needs to be created since log alerts are leveraged. 
    #We also need to ensure that we have an email to send alerts to.  
    create_alerts = local.monitor && local.alert_email != ""admin.mail@contoso.com"" && try(local.configuration_yml[""alerting""][""enabled""], false) ? true : false
    anf_vol_threshold = try(local.configuration_yml[""anf""][""alert_threshold""], 80)  # default to 80% if not specified 

    # will be used with a KQL query that checks the free space percentage of local volumes
    # if the user wants to create an alert when local volumes are 80% full, then the free space percentage should be 20%
    local_vol_threshold = 100 - try(local.configuration_yml[""alerting""][""local_volume_threshold""], 20) 

    mounts = try(local.configuration_yml[""mounts""], {})
    mountpoints =  [ for mount in local.mounts : mount.mountpoint ]
    mountpoints_str = ""[ ${join("","", [for mp in local.mountpoints : format(""%q"", mp)])} ]"" //necessary to build generic KQL query on local volumes

    # Active Directory values
    # Updates the assumptions to the possibility that DNS may not point to Active Directory when using the customer provided AD.
    create_ad             = !try(local.configuration_yml[""domain""].use_existing_dc, false) && (try(local.configuration_yml[""authentication""].user_auth, ""ad"") == ""ad"")
    use_existing_ad       = try(local.configuration_yml[""domain""].use_existing_dc, false)
    create_dns_records    = local.create_ad || local.use_existing_ad
    domain_name           = local.use_existing_ad ? local.configuration_yml[""domain""].name : ""hpc.azure""
    domain_join_user      = local.use_existing_ad ? local.configuration_yml[""domain""].domain_join_user.username : local.admin_username
    domain_join_password  = local.use_existing_ad ? data.azurerm_key_vault_secret.domain_join_password[0].value : random_password.password.result
    domain_join_ou        = local.use_existing_ad ? local.configuration_yml[""domain""].domain_join_ou : ""CN=Computers""
    ad_ha                 = try(local.configuration_yml[""ad""].high_availability, false)
    domain_controlers     = local.use_existing_ad ? zipmap(local.configuration_yml[""domain""].existing_dc_details.domain_controller_names, local.configuration_yml[""domain""].existing_dc_details.domain_controller_names) : (local.ad_ha ? {ad=local.ad_name, ad2=local.ad2_name} : {ad=local.ad_name})
    ldap_server           = local.use_existing_ad ? local.configuration_yml[""domain""].existing_dc_details.domain_controller_names[0]     : local.ad_name
    private_dns_servers   = local.use_existing_ad ? local.configuration_yml[""domain""].existing_dc_details.private_dns_servers            : (local.create_ad ? (local.ad_ha ? [azurerm_network_interface.ad-nic[0].private_ip_address, azurerm_network_interface.ad2-nic[0].private_ip_address] : [azurerm_network_interface.ad-nic[0].private_ip_address]) : [])
    domain_controller_ips = local.use_existing_ad ? local.configuration_yml[""domain""].existing_dc_details.domain_controller_ip_addresses : (local.create_ad ? (local.ad_ha ? [azurerm_network_interface.ad-nic[0].private_ip_address, azurerm_network_interface.ad2-nic[0].private_ip_address] : [azurerm_network_interface.ad-nic[0].private_ip_address]) : [])

    # private DNS 
    create_private_dns = try(local.configuration_yml[""private_dns""].create, false)
    private_dns_zone_name = try(local.configuration_yml[""private_dns""].name, ""hpc.azure"")
    private_dns_registration_enabled = try(local.configuration_yml[""private_dns""].registration_enabled, false)

    # Use a linux custom image reference if the linux_base_image is defined and contains "":""
    use_linux_image_reference = try(length(split("":"", local.configuration_yml[""linux_base_image""])[1])>0, false)
    # Use a linux custom image reference if the linux_base_image is defined and contains "":""
    use_windows_image_reference = try(length(split("":"", local.configuration_yml[""windows_base_image""])[1])>0, false)
    # Use a linux custom image reference if the linux_base_image is defined and contains "":""
    use_cyclecloud_image_reference = try(length(split("":"", local.configuration_yml[""cyclecloud""][""image""])[1])>0, false)

    linux_base_image_reference = {
        publisher = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[0] : ""OpenLogic""
        offer     = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[1] : ""CentOS""
        sku       = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[2] : ""7_9-gen2""
        version   = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[3] : ""latest""
    }
    windows_base_image_reference = {
        publisher = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[0] : ""MicrosoftWindowsServer""
        offer     = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[1] : ""WindowsServer""
        sku       = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[2] : ""2019-Datacenter-smalldisk""
        version   = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[3] : ""latest""
    }
    cyclecloud_image_reference = {
        publisher = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[0] : local.linux_base_image_reference.publisher
        offer     = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[1] : local.linux_base_image_reference.offer
        sku       = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[2] : local.linux_base_image_reference.sku
        version   = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[3] : local.linux_base_image_reference.version
    }

    # Use a linux custom image id if the linux_base_image is defined and contains ""/""
    use_linux_image_id = try(length(split(""/"", local.configuration_yml[""linux_base_image""])[1])>0, false)
    linux_image_id = local.use_linux_image_id ? local.configuration_yml[""linux_base_image""] : null

    # Use a windows custom image id if the windows_base_image is defined and contains ""/""
    use_windows_image_id = try(length(split(""/"", local.configuration_yml[""windows_base_image""])[1])>0, false)
    windows_image_id = local.use_windows_image_id ? local.configuration_yml[""windows_base_image""] : null

    # Use a cyclecloud custom image id if the cyclecloud_base_image is defined and contains ""/""
    use_cyclecloud_image_id = try(length(split(""/"", local.configuration_yml[""cyclecloud""][""image""])[1])>0, false)
    cyclecloud_image_id = local.use_cyclecloud_image_id ? local.configuration_yml[""cyclecloud""][""image""] : null

    _empty_image_plan = {}
    _linux_base_image_plan = {
        publisher = try(split("":"", local.configuration_yml[""linux_base_plan""])[0], """")
        product   = try(split("":"", local.configuration_yml[""linux_base_plan""])[1], """")
        name      = try(split("":"", local.configuration_yml[""linux_base_plan""])[2], """")
    }
    linux_image_plan = try( length(local._linux_base_image_plan.publisher) > 0 ? local._linux_base_image_plan : local._empty_image_plan, local._empty_image_plan)

    _cyclecloud_image_plan = {
        publisher = try(split("":"", local.configuration_yml[""cyclecloud""][""plan""])[0], """")
        product   = try(split("":"", local.configuration_yml[""cyclecloud""][""plan""])[1], """")
        name      = try(split("":"", local.configuration_yml[""cyclecloud""][""plan""])[2], """")
    }
    cyclecloud_image_plan = try( length(local._cyclecloud_image_plan.publisher) > 0 ? local._cyclecloud_image_plan : local._empty_image_plan, local._empty_image_plan)


    # Create the RG if not using an existing RG and (creating a VNET or when reusing a VNET in another resource group)
    use_existing_rg = try(local.configuration_yml[""use_existing_rg""], false)
    create_rg = (!local.use_existing_rg) && (local.create_vnet || try(split(""/"", local.vnet_id)[4], local.resource_group) != local.resource_group)

    # ANF
    create_anf = try(local.configuration_yml[""anf""][""create""], false)
    anf_size=try(local.configuration_yml[""anf""][""homefs_size_tb""], 4)
    anf_service_level = try(local.configuration_yml[""anf""][""homefs_service_level""], ""Standard"")
    anf_dual_protocol = try(local.configuration_yml[""anf""][""dual_protocol""], false)

    #Azure Files
    create_nfsfiles = try(local.configuration_yml[""azurefiles""][""create""], false)
    azure_files_size= try(local.configuration_yml[""azurefiles""][""size_gb""], 1024)

    # Home Directory
    homedir_type = try(local.configuration_yml[""mounts""][""home""][""type""], ""existing"")
    config_nfs_home_ip = local.configuration_yml[""mounts""][""home""][""server""]
    config_nfs_home_path = local.configuration_yml[""mounts""][""home""][""export""]
    config_nfs_home_opts = local.configuration_yml[""mounts""][""home""][""options""]

    homedir_mountpoint = try(local.configuration_yml[""mounts""][""home""][""mountpoint""], ""/anfhome"")

    admin_username = local.configuration_yml[""admin_user""]
    key_vault_readers = try(local.configuration_yml[""key_vault_readers""], null)

    # Resource names
    scheduler_name = try(local.configuration_yml[""scheduler""][""name""], ""scheduler"")
    ccportal_name = try(local.configuration_yml[""cyclecloud""][""name""], ""ccportal"")
    ondemand_name = try(local.configuration_yml[""ondemand""][""name""], ""ondemand"")
    grafana_name = try(local.configuration_yml[""grafana""][""name""], ""grafana"")
    jumpbox_name = try(local.configuration_yml[""jumpbox""][""name""], ""jumpbox"")
    ad_name = try(local.configuration_yml[""ad""][""name""], ""ad"")
    ad2_name = try(local.configuration_yml[""ad""][""ha_name""], ""ad2"")

    key_vault_name = try(local.configuration_yml[""azure_key_vault""][""name""], format(""%s%s"", ""kv"", random_string.resource_postfix.result))
    storage_account_name = try(local.configuration_yml[""azure_storage_account""][""name""], ""azhop${random_string.resource_postfix.result}"")
    db_name = try(local.configuration_yml[""database""][""name""], ""mysql-${random_string.resource_postfix.result}"")

    # Lustre - AMLFS not implemented for TF
    lustre_enabled = false

    # Use a jumpbox when defined
    jumpbox_enabled = try(length(local.configuration_yml[""jumpbox""]) > 0, false)

    # Queue manager
    queue_manager = try(local.configuration_yml[""queue_manager""], ""openpbs"")

    # Create Database
    create_database  = ( try(local.configuration_yml[""slurm""].accounting_enabled, false) ) && (! local.use_existing_database)
    use_existing_database = try(length(local.configuration_yml[""database""].fqdn) > 0 ? true : false, false)
    database_user = local.create_database ? ""sqladmin"" : (local.use_existing_database ? try(local.configuration_yml[""database""].user, ""sqladmin"") : ""sqladmin"")
    #mariadb_private_dns_zone = local.azure_endpoints[local.azure_environment].MariaDBPrivateLink

    create_sig = try(local.configuration_yml[""image_gallery""][""create""], false)
    
    # VNET
    create_vnet = try(length(local.vnet_id) > 0 ? false : true, true)
    vnet_id = try(local.configuration_yml[""network""][""vnet""][""id""], null)

    # VNET Peering
    vnet_peering = try(tolist(local.configuration_yml[""network""][""peering""]), [])

    # Lockdown scenario
    locked_down_network = try(local.configuration_yml[""locked_down_network""][""enforce""], false)
    grant_access_from   = try(local.configuration_yml[""locked_down_network""][""grant_access_from""], [])
    allow_public_ip     = try(local.configuration_yml[""locked_down_network""][""public_ip""], true)
    jumpbox_ssh_port    = try(local.configuration_yml[""jumpbox""][""ssh_port""], ""22"")

    # NAT Gateway
    create_nat_gateway = try(local.configuration_yml[""nat_gateway""][""create""], false)
    nat_gateway_name = try(local.configuration_yml[""nat_gateway""][""name""], ""natgw-${random_string.resource_postfix.result}"")

    # subnets
    _subnets = {
        frontend = ""frontend"",
        admin = ""admin"",
        netapp = ""netapp"",
        compute = ""compute"",
        ad = ""ad"",
        database = ""database""
    }

    # Create subnet if required. If not specified create only if vnet is created
    create_frontend_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""frontend""][""create""], local.create_vnet )
    create_admin_subnet    = try(local.configuration_yml[""network""][""vnet""][""subnets""][""admin""][""create""], local.create_vnet )
    create_netapp_subnet   = try(local.configuration_yml[""network""][""vnet""][""subnets""][""netapp""][""create""], local.create_vnet )
    create_database_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""database""][""create""], local.create_vnet )
    create_compute_subnet  = try(local.configuration_yml[""network""][""vnet""][""subnets""][""compute""][""create""], local.create_vnet )

    ad_subnet        = try(local.configuration_yml[""network""][""vnet""][""subnets""][""ad""], null)
    no_ad_subnet     = try(length(local.ad_subnet) > 0 ? false : true, true)
    create_ad_subnet = try(local.ad_subnet[""create""], (local.create_ad ? local.create_vnet : false))

    bastion_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""bastion""], null)
    no_bastion_subnet = try(length(local.bastion_subnet) > 0 ? false : true, true )
    create_bastion_subnet  = try(local.bastion_subnet[""create""], local.create_vnet )

    gateway_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""gateway""], null)
    no_gateway_subnet = try(length(local.gateway_subnet) > 0 ? false : true, true )
    create_gateway_subnet  = try(local.gateway_subnet[""create""], local.create_vnet )

    outbounddns_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""outbounddns""], null)
    no_outbounddns_subnet = try(length(local.outbounddns_subnet) > 0 ? false : true, true )
    create_outbounddns_subnet  = try(local.outbounddns_subnet[""create""], local.create_vnet ? (local.no_outbounddns_subnet ? false : true) : false )

    dns_forwarders = try(local.configuration_yml[""dns""][""forwarders""], [])
    create_dnsfw_rules = length(local.dns_forwarders) > 0 ? true : false

    subnets = merge(local._subnets, 
                    local.no_bastion_subnet ? {} : {bastion = ""AzureBastionSubnet""},
                    local.no_gateway_subnet ? {} : {gateway = ""GatewaySubnet""},
                    local.no_outbounddns_subnet ? {} : {outbounddns = ""outbounddns""}
                    )

    # Application Security Groups
    create_nsg = try(local.configuration_yml[""network""][""create_nsg""], local.create_vnet )
    # If create NSG then use the local resource group otherwise use the configured one. Default to local resource group
    asg_resource_group = local.create_nsg ? local.resource_group : try(length(local.configuration_yml[""network""][""asg""][""resource_group""]) > 0 ? local.configuration_yml[""network""][""asg""][""resource_group""] : local.resource_group, local.resource_group )

    _asg_ad = {
        asg-ad = ""asg-ad""
        asg-ad-client = ""asg-ad-client""
        asg-rdp = ""asg-rdp""
     }

    _asg_grafana = {
        asg-telegraf = ""asg-telegraf""
        asg-grafana = ""asg-grafana""
    }

    _asg_ondemand = {
        asg-ondemand = ""asg-ondemand""
    }

    _asg_lustre = {
        asg-lustre-client = ""asg-lustre-client""
    }

    _asg_mysql = {
        asg-mysql-client = ""asg-mysql-client""
    }

    _default_asgs = merge ({
            asg-ssh = ""asg-ssh""
            asg-jumpbox = ""asg-jumpbox""
            asg-sched = ""asg-sched""
            asg-cyclecloud = ""asg-cyclecloud""
            asg-cyclecloud-client = ""asg-cyclecloud-client""
            asg-nfs-client = ""asg-nfs-client""
            asg-deployer = ""asg-deployer""
        },
        local.create_ad || local.use_existing_ad ? local._asg_ad : {},
        local.create_grafana ? local._asg_grafana : {},
        local.create_ondemand ? local._asg_ondemand : {},
        local.lustre_enabled ? local._asg_lustre : {},
        local.create_database || local.use_existing_database ? local._asg_mysql : {}
    )

    #asgs = local.create_nsg ? local._default_asgs :  try(local.configuration_yml[""network""][""asg""][""names""], local._default_asgs)
    asgs = try(local.configuration_yml[""network""][""asg""][""names""], local._default_asgs)
    #asgs = { for v in local.default_asgs : v => v }
    empty_array = []
    empty_map = { for v in local.empty_array : v => v }

    # VM name to list of ASGs associations
    asg_asso_ad = [""asg-ad"", ""asg-rdp"", ""asg-ad-client""] # asg-ad-client will allow the secondary DC scenario
    asg_asso_ccportal = concat([""asg-ssh"", ""asg-cyclecloud""], 
                            local.create_grafana ? [""asg-telegraf""] : [], 
                            local.create_ad || local.use_existing_ad ? [""asg-ad-client""] : [])
    asg_asso_grafana = concat([""asg-ssh"", ""asg-grafana"", ""asg-telegraf"", ""asg-nfs-client""], 
                            local.create_ad || local.use_existing_ad ? [""asg-ad-client""] : [])
    asg_asso_jumpbox = concat([""asg-ssh"", ""asg-jumpbox"" ],
                                local.create_grafana ? [""asg-telegraf""] : [])
    asg_asso_ondemand = concat([""asg-ssh"", ""asg-ondemand"", ""asg-nfs-client"", ""asg-sched"", ""asg-cyclecloud-client"" ],
                            local.create_grafana ? [""asg-telegraf""] : [],
                            local.lustre_enabled ? [""asg-lustre-client""] : [],
                            local.create_ad || local.use_existing_ad ? [""asg-ad-client""] : [])
    asg_asso_scheduler = concat([""asg-ssh"", ""asg-sched"", ""asg-cyclecloud-client"", ""asg-nfs-client""],
                            local.create_grafana ? [""asg-telegraf""] : [], 
                            local.create_ad || local.use_existing_ad ? [""asg-ad-client""] : [],
                            local.create_database || local.use_existing_database ? [""asg-mysql-client""] : [])

    asg_associations = {
        ad        = local.asg_asso_ad 
        ccportal  = local.asg_asso_ccportal
        grafana   = local.asg_asso_grafana
        jumpbox   = local.asg_asso_jumpbox
        ondemand  = local.asg_asso_ondemand
        scheduler = local.asg_asso_scheduler
    }

    # Open ports for NSG TCP rules
    # ANF and SMB https://docs.microsoft.com/en-us/azure/azure-netapp-files/create-active-directory-connections
    nsg_destination_ports = {
        All = [""0-65535""]
        Bastion = [""22"", ""3389""]
        Web = [""443"", ""80""]
        Ssh    = [""22""]
        Public_Ssh = [local.jumpbox_ssh_port]
        # DNS, Kerberos, RpcMapper, Ldap, Smb, KerberosPass, LdapSsl, LdapGc, LdapGcSsl, AD Web Services, RpcSam
        DomainControlerTcp = [""53"", ""88"", ""135"", ""389"", ""445"", ""464"", ""636"", ""3268"", ""3269"", ""9389"", ""49152-65535""]
        # DNS, Kerberos, W32Time, NetBIOS, Ldap, KerberosPass, LdapSsl
        DomainControlerUdp = [""53"", ""88"", ""123"", ""138"", ""389"", ""464"", ""636""]
        # Web, NoVNC, WebSockify
        NoVnc = [""80"", ""443"", ""5900-5910"", ""61001-61010""]
        Dns = [""53""]
        Rdp = [""3389""]
        #Pbs = [""6200"", ""15001-15009"", ""17001"", ""32768-61000""]
        #Slurmd = [""6817-6819""]
        Sched = (local.queue_manager == ""slurm"") ? [""6817-6819"", ""59000-61000""] : [""6200"", ""15001-15009"", ""17001"", ""32768-61000""]
        Lustre = [""635"", ""988""]
        Nfs = [""111"", ""635"", ""2049"", ""4045"", ""4046""]
        SMB = [""445""]
        Telegraf = [""8086""]
        Grafana = [""3000""]
        # HTTPS, AMQP
        CycleCloud = [""9443"", ""5672""],
        # MySQL
        MySQL = [""3306"", ""33060""],
        # WinRM
        WinRM = [""5985"", ""5986""]
    }

    #Replace the AD ASG with domain controller IP addresses when customer is bringing their own AD
    #use an indexing concept since we can't substitute a list for a string
    ad_nsg_index = local.use_existing_ad ? ""ips/dc_ips"" : ""asg/asg-ad""
    ips = {
        dc_ips = local.domain_controller_ips
    }

    # Array of NSG rules to be applied on the common NSG
    # NsgRuleName = [priority, direction, access, protocol, destination_port_range, source, destination]
    #   - priority               : integer value from 100 to 4096
    #   - direction              : Inbound, Outbound
    #   - access                 : Allow, Deny
    #   - protocol               : Tcp, Udp, *
    #   - destination_port_range : name of one of the nsg_destination_ports defined above
    #   - source                 : asg/<asg-name>, subnet/<subnet-name>, tag/<tag-name>. tag-name = any Azure tags like Internet, VirtualNetwork, AzureLoadBalancer, ...
    #   - destination            : same as source
    _nsg_rules = {
        # ================================================================================================================================================================
        #                          ###
        #                           #     #    #  #####    ####   #    #  #    #  #####
        #                           #     ##   #  #    #  #    #  #    #  ##   #  #    #
        #                           #     # #  #  #####   #    #  #    #  # #  #  #    #
        #                           #     #  # #  #    #  #    #  #    #  #  # #  #    #
        #                           #     #   ##  #    #  #    #  #    #  #   ##  #    #
        #                          ###    #    #  #####    ####    ####   #    #  #####
        # ================================================================================================================================================================

        # SSH internal rules
        AllowSshFromJumpboxIn       = [""320"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-jumpbox"",   ""asg/asg-ssh""],
        AllowSshFromComputeIn       = [""330"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""subnet/compute"",    ""asg/asg-ssh""],
        AllowSshFromDeployerIn      = [""340"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",  ""asg/asg-ssh""], # Only in a deployer VM scenario
        AllowDeployerToPackerSshIn  = [""350"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",  ""subnet/admin""], # Only in a deployer VM scenario
        AllowSshToComputeIn         = [""360"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-ssh"",       ""subnet/compute""],
        AllowAllComputeComputeIn    = [""365"", ""Inbound"", ""Allow"", ""Tcp"", ""All"",                ""subnet/compute"",    ""subnet/compute""],

        # Scheduler
        AllowSchedIn                = [""369"", ""Inbound"", ""Allow"", ""*"",   ""Sched"",                ""asg/asg-sched"",      ""asg/asg-sched""],
        #AllowPbsClientIn            = [""370"", ""Inbound"", ""Allow"", ""*"",   ""Sched"",                ""asg/asg-pbs-client"", ""asg/asg-pbs""],
        AllowSchedComputeIn         = [""380"", ""Inbound"", ""Allow"", ""*"",   ""Sched"",                ""asg/asg-sched"",      ""subnet/compute""],
        #AllowComputePbsClientIn     = [""390"", ""Inbound"", ""Allow"", ""*"",   ""Sched"",                ""subnet/compute"",     ""asg/asg-pbs-client""],
        AllowComputeSchedIn         = [""400"", ""Inbound"", ""Allow"", ""*"",   ""Sched"",                ""subnet/compute"",     ""asg/asg-sched""],
        #AllowComputeComputeSchedIn  = [""401"", ""Inbound"", ""Allow"", ""*"",   ""Sched"",                ""subnet/compute"",     ""subnet/compute""],

        # CycleCloud
        AllowCycleClientIn          = [""450"", ""Inbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud-client"", ""asg/asg-cyclecloud""],
        AllowCycleClientComputeIn   = [""460"", ""Inbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""subnet/compute"",            ""asg/asg-cyclecloud""],
        AllowCycleServerIn          = [""465"", ""Inbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud"",        ""asg/asg-cyclecloud-client""],

        # Deny all remaining traffic
        DenyVnetInbound             = [""3100"", ""Inbound"", ""Deny"", ""*"", ""All"",                  ""tag/VirtualNetwork"",       ""tag/VirtualNetwork""],

        # ================================================================================================================================================================
        #                            #######
        #                            #     #  #    #   #####  #####    ####   #    #  #    #  #####
        #                            #     #  #    #     #    #    #  #    #  #    #  ##   #  #    #
        #                            #     #  #    #     #    #####   #    #  #    #  # #  #  #    #
        #                            #     #  #    #     #    #    #  #    #  #    #  #  # #  #    #
        #                            #     #  #    #     #    #    #  #    #  #    #  #   ##  #    #
        #                            #######   ####      #    #####    ####    ####   #    #  #####
        # ================================================================================================================================================================

        # CycleCloud
        AllowCycleServerOut         = [""300"", ""Outbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud"",        ""asg/asg-cyclecloud-client""],
        AllowCycleClientOut         = [""310"", ""Outbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud-client"", ""asg/asg-cyclecloud""],
        AllowComputeCycleClientIn   = [""320"", ""Outbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""subnet/compute"",            ""asg/asg-cyclecloud""],

        # Scheduler
        AllowSchedOut               = [""340"", ""Outbound"", ""Allow"", ""*"",   ""Sched"",                ""asg/asg-sched"",      ""asg/asg-sched""],
        #AllowPbsClientOut           = [""350"", ""Outbound"", ""Allow"", ""*"",   ""Sched"",                ""asg/asg-pbs-client"", ""asg/asg-pbs""],
        AllowSchedComputeOut        = [""360"", ""Outbound"", ""Allow"", ""*"",   ""Sched"",                ""asg/asg-sched"",      ""subnet/compute""],
        AllowComputeSchedOut        = [""370"", ""Outbound"", ""Allow"", ""*"",   ""Sched"",                ""subnet/compute"",     ""asg/asg-sched""],
        #AllowComputePbsClientOut    = [""380"", ""Outbound"", ""Allow"", ""*"",   ""Sched"",                ""subnet/compute"",     ""asg/asg-pbs-client""],
        #AllowComputeComputeSchedOut = [""381"", ""Outbound"", ""Allow"", ""*"",   ""Sched"",                ""subnet/compute"",     ""subnet/compute""],

        # SSH internal rules
        AllowSshFromJumpboxOut      = [""490"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-jumpbox"",          ""asg/asg-ssh""],
        AllowSshComputeOut          = [""500"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-ssh"",              ""subnet/compute""],
        AllowSshDeployerOut         = [""510"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",         ""asg/asg-ssh""],
        AllowSshDeployerPackerOut   = [""520"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",         ""subnet/admin""],
        AllowSshFromComputeOut      = [""530"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""subnet/compute"",           ""asg/asg-ssh""],
        AllowAllComputeComputeOut   = [""540"", ""Outbound"", ""Allow"", ""Tcp"", ""All"",                ""subnet/compute"",           ""subnet/compute""],

        # Admin and Deployment
        AllowDnsOut                 = [""590"", ""Outbound"", ""Allow"", ""*"",   ""Dns"",                ""tag/VirtualNetwork"",       ""tag/VirtualNetwork""],

        # Deny all remaining traffic and allow Internet access
        AllowInternetOutBound       = [""3000"", ""Outbound"", ""Allow"", ""Tcp"", ""All"",               ""tag/VirtualNetwork"",       ""tag/Internet""],
        DenyVnetOutbound            = [""3100"", ""Outbound"", ""Deny"",  ""*"",   ""All"",               ""tag/VirtualNetwork"",       ""tag/VirtualNetwork""],
    }

    internet_nsg_rules = {
        AllowInternetSshIn          = [""200"", ""Inbound"", ""Allow"", ""Tcp"", ""Public_Ssh"",         ""tag/Internet"", ""asg/asg-jumpbox""], # Only when using a PIP
        AllowInternetHttpIn         = [""210"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                ""tag/Internet"", ""subnet/frontend""], # Only when using a PIP
    }

    hub_nsg_rules = {
        AllowHubSshIn          = [""200"", ""Inbound"", ""Allow"", ""Tcp"", ""Public_Ssh"",               ""tag/VirtualNetwork"", ""tag/VirtualNetwork""],
        AllowHubHttpIn         = [""210"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                      ""tag/VirtualNetwork"", ""tag/VirtualNetwork""],
    }

    ad_nsg_rules = {
        # Inbound
        AllowAdServerTcpIn        = [""220"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdServerUdpIn        = [""230"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdClientTcpIn        = [""240"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdClientUdpIn        = [""250"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdServerComputeTcpIn = [""260"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdServerComputeUdpIn = [""270"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdClientComputeTcpIn = [""280"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdClientComputeUdpIn = [""290"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdServerNetappTcpIn  = [""300"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""subnet/netapp"", local.ad_nsg_index],
        AllowAdServerNetappUdpIn  = [""310"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""subnet/netapp"", local.ad_nsg_index],
        AllowWinRMIn              = [""520"", ""Inbound"", ""Allow"", ""Tcp"", ""WinRM"",              ""asg/asg-jumpbox"", ""asg/asg-rdp""],
        AllowRdpIn                = [""550"", ""Inbound"", ""Allow"", ""Tcp"", ""Rdp"",                ""asg/asg-jumpbox"", ""asg/asg-rdp""],
        AllowPackerWinRMIn        = [""560"", ""Inbound"", ""Allow"", ""Tcp"", ""WinRM"",              ""tag/VirtualNetwork"", ""subnet/compute""],

        # Outbound
        AllowAdClientTcpOut        = [""200"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdClientUdpOut        = [""210"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdClientComputeTcpOut = [""220"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdClientComputeUdpOut = [""230"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdServerTcpOut        = [""240"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdServerUdpOut        = [""250"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdServerComputeTcpOut = [""260"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdServerComputeUdpOut = [""270"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdServerNetappTcpOut  = [""280"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""subnet/netapp""],
        AllowAdServerNetappUdpOut  = [""290"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""subnet/netapp""],
        AllowRdpOut                = [""570"", ""Outbound"", ""Allow"", ""Tcp"", ""Rdp"", ""asg/asg-jumpbox"", ""asg/asg-rdp""],
        AllowWinRMOut              = [""580"", ""Outbound"", ""Allow"", ""Tcp"", ""WinRM"", ""asg/asg-jumpbox"", ""asg/asg-rdp""],
    }

    bastion_nsg_rules = {
        AllowBastionIn              = [""530"", ""Inbound"" , ""Allow"", ""Tcp"", ""Bastion"",            ""subnet/bastion"",           ""tag/VirtualNetwork""],
        AllowBastionOut             = [""531"", ""Outbound"", ""Allow"", ""Tcp"", ""Bastion"",            ""subnet/bastion"",           ""tag/VirtualNetwork""],
    }

    gateway_nsg_rules = {
        AllowInternalWebUsersIn     = [""540"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                ""subnet/gateway"",           ""asg/asg-ondemand""],
    }

    ondemand_nsg_rules = {
        # Inbound
#        AllowComputeSlurmIn         = [""405"", ""Inbound"", ""Allow"", ""*"",   ""Slurmd"",             ""asg/asg-ondemand"",    ""subnet/compute""],
        AllowCycleWebIn             = [""440"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                ""asg/asg-ondemand"",          ""asg/asg-cyclecloud""],
        AllowComputeNoVncIn         = [""470"", ""Inbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""subnet/compute"",            ""asg/asg-ondemand""],
        AllowNoVncComputeIn         = [""480"", ""Inbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""asg/asg-ondemand"",          ""subnet/compute""],
        # Outbound
        AllowCycleWebOut            = [""330"", ""Outbound"", ""Allow"", ""Tcp"", ""Web"",                ""asg/asg-ondemand"",          ""asg/asg-cyclecloud""],
#        AllowSlurmComputeOut        = [""385"", ""Outbound"", ""Allow"", ""*"",   ""Slurmd"",             ""asg/asg-ondemand"",        ""subnet/compute""],
        AllowComputeNoVncOut        = [""550"", ""Outbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""subnet/compute"",            ""asg/asg-ondemand""],
        AllowNoVncComputeOut        = [""560"", ""Outbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""asg/asg-ondemand"",          ""subnet/compute""],

    }
    grafana_nsg_rules = {
        # Telegraf / Grafana
        AllowTelegrafIn             = [""490"", ""Inbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""asg/asg-telegraf"",          ""asg/asg-grafana""],
        AllowComputeTelegrafIn      = [""500"", ""Inbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""subnet/compute"",            ""asg/asg-grafana""],
        AllowGrafanaIn              = [""510"", ""Inbound"", ""Allow"", ""Tcp"", ""Grafana"",            ""asg/asg-ondemand"",          ""asg/asg-grafana""],

        # Telegraf / Grafana
        AllowTelegrafOut            = [""460"", ""Outbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""asg/asg-telegraf"",          ""asg/asg-grafana""],
        AllowComputeTelegrafOut     = [""470"", ""Outbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""subnet/compute"",            ""asg/asg-grafana""],
        AllowGrafanaOut             = [""480"", ""Outbound"", ""Allow"", ""Tcp"", ""Grafana"",            ""asg/asg-ondemand"",          ""asg/asg-grafana""],
    }

    mysql_nsg_rules = {
        # Inbound
        AllowMySQLIn              = [""700"", ""Inbound"", ""Allow"", ""Tcp"", ""MySQL"",             ""asg/asg-mysql-client"",    ""subnet/database""],
        # Outbound
        AllowMySQLOut             = [""700"", ""Outbound"", ""Allow"", ""Tcp"", ""MySQL"",             ""asg/asg-mysql-client"",    ""subnet/database""],
    }

    anf_nsg_rules = {
        # Inbound
        AllowNfsIn                  = [""430"", ""Inbound"", ""Allow"", ""*"",   ""Nfs"",                ""asg/asg-nfs-client"",       ""subnet/netapp""],
        AllowNfsComputeIn           = [""435"", ""Inbound"", ""Allow"", ""*"",   ""Nfs"",                ""subnet/compute"",           ""subnet/netapp""],
        # Outbound
        AllowNfsOut                 = [""440"", ""Outbound"", ""Allow"", ""*"",   ""Nfs"",                ""asg/asg-nfs-client"",       ""subnet/netapp""],
        AllowNfsComputeOut          = [""450"", ""Outbound"", ""Allow"", ""*"",   ""Nfs"",                ""subnet/compute"",           ""subnet/netapp""],
        AllowSMBComputeOut          = [""455"", ""Outbound"", ""Allow"", ""*"",   ""SMB"",                ""subnet/compute"",            ""subnet/netapp""],
    }

    lustre_nsg_rules = {
        # Inbound
        AllowLustreClientIn         = [""410"", ""Inbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""asg/asg-lustre-client"", ""subnet/admin""],
        AllowLustreClientComputeIn  = [""420"", ""Inbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""subnet/compute"",        ""subnet/admin""],
        # Outbound
        AllowLustreClientOut        = [""400"", ""Outbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""asg/asg-lustre-client"",    ""subnet/admin""],
        AllowLustreClientComputeOut = [""420"", ""Outbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""subnet/compute"",           ""subnet/admin""],

    }
    
    nsg_rules = merge(  local._nsg_rules, 
                        local.create_ad || local.use_existing_ad ? local.ad_nsg_rules : {},
                        local.no_bastion_subnet ? {} : local.bastion_nsg_rules, 
                        local.no_gateway_subnet ? {} : local.gateway_nsg_rules,
                        local.allow_public_ip ? local.internet_nsg_rules : local.hub_nsg_rules,
                        local.create_grafana ? local.grafana_nsg_rules : {},
                        local.create_database || local.use_existing_database ? local.mysql_nsg_rules : {},
                        local.create_ondemand ? local.ondemand_nsg_rules : {},
                        local.lustre_enabled ? local.lustre_nsg_rules : {},
                        local.anf_nsg_rules
                    )

}
",locals,209,215.0,1fa901c8a5ca9d56bac65d1f0bd0bf273fd0a509,de6daf4da6feefb4fcbc0cef1f20dac41d83d28e,https://github.com/Azure/az-hop/blob/1fa901c8a5ca9d56bac65d1f0bd0bf273fd0a509/tf/variables_local.tf#L209,https://github.com/Azure/az-hop/blob/de6daf4da6feefb4fcbc0cef1f20dac41d83d28e/tf/variables_local.tf#L215,2023-10-10 12:12:10+02:00,2024-03-29 16:03:05+01:00,12,0,1,0,1,0,0,0,0,0
https://github.com/alphagov/govuk-aws,322,terraform/projects/infra-security-groups/publishing-api.tf,terraform/projects/infra-security-groups/publishing-api.tf,0,# todo,# TODO: test whether egress rules are needed on elbs,# TODO: test whether egress rules are needed on elbs,"resource ""aws_security_group_rule"" ""allow_publishing-api_elb_external_egress"" {
  type              = ""egress""
  from_port         = 0
  to_port           = 0
  protocol          = ""-1""
  cidr_blocks       = [""0.0.0.0/0""]
  security_group_id = ""${aws_security_group.publishing-api_elb_external.id}""
}
",resource,the block associated got renamed or deleted,,99,,b212a5508ed19a405a50c07afee2d3d66c55a60b,7fd13330f8d6238e108f76ce76a76a28a99caaaf,https://github.com/alphagov/govuk-aws/blob/b212a5508ed19a405a50c07afee2d3d66c55a60b/terraform/projects/infra-security-groups/publishing-api.tf#L99,https://github.com/alphagov/govuk-aws/blob/7fd13330f8d6238e108f76ce76a76a28a99caaaf/terraform/projects/infra-security-groups/publishing-api.tf,2017-09-15 17:07:50+01:00,2018-01-02 17:41:32+00:00,4,1,0,0,0,1,1,0,0,1
https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules,18,modules/vm-series/main.tf,modules/vm-series/main.tf,0,fix,# FIXME move it,"variable ""lb_backend_pool_ids"" { # FIXME move it","variable ""lb_backend_pool_ids"" { # FIXME move it 
}
",variable,the block associated got renamed or deleted,,47,,8d5eba64b06fb6b8420aa918a46de617228b3358,aa456ab5fd8767a2a3e40959d3a2376b24ceb368,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/8d5eba64b06fb6b8420aa918a46de617228b3358/modules/vm-series/main.tf#L47,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/aa456ab5fd8767a2a3e40959d3a2376b24ceb368/modules/vm-series/main.tf,2021-02-17 13:27:46+01:00,2021-02-17 13:27:46+01:00,3,1,0,1,0,0,1,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,154,libvirt/terraform/main.tf,libvirt/terraform/main.tf,0,// todo,// todo: verify this,// todo: verify this,"module ""monitoring"" {
  source             = ""./modules/monitoring""
  base_configuration = ""${module.base.configuration}""

  name                   = ""monitoring""
  count                  = 1
  vcpu                   = 4
  memory                 = 4095
  // todo: verify this
  host_ips               = ""${var.host_ips}""
  
  reg_code               = ""${var.reg_code}""
  reg_email              = ""${var.reg_email}""
  reg_additional_modules = ""${var.reg_additional_modules}""
  additional_repos       = ""${var.additional_repos}""
  ha_sap_deployment_repo = ""${var.ha_sap_deployment_repo}""
  provisioner            = ""${var.provisioner}""
  background             = ""${var.background}""
}
",module,"module ""monitoring"" {
  source             = ""./modules/monitoring""
  base_configuration = ""${module.base.configuration}""

  name                   = ""monitoring""
  count                  = 1
  vcpu                   = 4
  memory                 = 4095
  host_ips               = ""${var.host_ips}""
  reg_code               = ""${var.reg_code}""
  reg_email              = ""${var.reg_email}""
  reg_additional_modules = ""${var.reg_additional_modules}""
  additional_repos       = ""${var.additional_repos}""
  ha_sap_deployment_repo = ""${var.ha_sap_deployment_repo}""
  provisioner            = ""${var.provisioner}""
  background             = ""${var.background}""
}
",module,27,,ded573f03083c78a15ca8c85606954edfc0c5ad5,d32b0c93370b96b65c4de6378081d2af5a1b56bf,https://github.com/SUSE/ha-sap-terraform-deployments/blob/ded573f03083c78a15ca8c85606954edfc0c5ad5/libvirt/terraform/main.tf#L27,https://github.com/SUSE/ha-sap-terraform-deployments/blob/d32b0c93370b96b65c4de6378081d2af5a1b56bf/libvirt/terraform/main.tf,2019-07-22 11:53:34+02:00,2019-07-23 12:37:23+02:00,3,1,0,1,0,0,0,0,1,0
https://github.com/kubernetes/k8s.io,430,infra/aws/terraform/kops-infra-ci/vpc.tf,infra/aws/terraform/kops-infra-ci/vpc.tf,0,// todo,// TODO(ameukam): Remove this after https://github.com/kubernetes/k8s.io/issues/5127 is closed,// TODO(ameukam): Remove this after https://github.com/kubernetes/k8s.io/issues/5127 is closed,"module ""vpc"" {
  providers = {
    aws = aws.kops-infra-ci
  }

  source  = ""terraform-aws-modules/vpc/aws""
  version = ""~> 5.0""

  name = ""${local.prefix}-vpc""
  cidr = aws_vpc_ipam_preview_next_cidr.main.cidr

  ipv4_ipam_pool_id = aws_vpc_ipam_pool.main.id

  azs             = local.azs
  private_subnets = local.private_subnets
  public_subnets  = local.public_subnets

  enable_nat_gateway     = true
  single_nat_gateway     = false
  one_nat_gateway_per_az = true

  // TODO(ameukam): Remove this after https://github.com/kubernetes/k8s.io/issues/5127 is closed
  enable_flow_log                                 = true
  create_flow_log_cloudwatch_iam_role             = true
  create_flow_log_cloudwatch_log_group            = true
  flow_log_cloudwatch_log_group_retention_in_days = 30

  enable_dns_hostnames = true

  public_subnet_tags = {
    ""kubernetes.io/role/elb"" = 1
  }

  private_subnet_tags = {
    ""kubernetes.io/role/internal-elb"" = 1
  }

  tags = merge(var.tags, {
    ""region"" = ""${data.aws_region.current.name}""
  })
}
",module,"module ""vpc"" {
  providers = {
    aws = aws.kops-infra-ci
  }

  source  = ""terraform-aws-modules/vpc/aws""
  version = ""~> 5.0""

  name = ""${local.prefix}-vpc""
  cidr = aws_vpc_ipam_preview_next_cidr.main.cidr

  ipv4_ipam_pool_id = aws_vpc_ipam_pool.main.id

  azs             = local.azs
  private_subnets = local.private_subnets
  public_subnets  = local.public_subnets

  enable_nat_gateway     = true
  single_nat_gateway     = false
  one_nat_gateway_per_az = true

  // TODO(ameukam): Remove this after https://github.com/kubernetes/k8s.io/issues/5127 is closed
  enable_flow_log                                 = true
  create_flow_log_cloudwatch_iam_role             = true
  create_flow_log_cloudwatch_log_group            = true
  flow_log_cloudwatch_log_group_retention_in_days = 30

  enable_dns_hostnames = true
  enable_dns_support   = true

  public_subnet_tags = {
    ""kubernetes.io/role/elb"" = 1
  }

  private_subnet_tags = {
    ""kubernetes.io/role/internal-elb"" = 1
  }

  tags = merge(var.tags, {
    ""region"" = ""${data.aws_region.current.name}""
  })
}
",module,84,84.0,7bb0b42e29e4e4069cc3fbe8e40963c3ec76895b,8110ebffa612964181d3b647db6e884d74f4c0d4,https://github.com/kubernetes/k8s.io/blob/7bb0b42e29e4e4069cc3fbe8e40963c3ec76895b/infra/aws/terraform/kops-infra-ci/vpc.tf#L84,https://github.com/kubernetes/k8s.io/blob/8110ebffa612964181d3b647db6e884d74f4c0d4/infra/aws/terraform/kops-infra-ci/vpc.tf#L84,2023-08-08 19:18:57+02:00,2023-12-08 14:54:27+01:00,4,0,0,1,1,0,1,0,1,0
https://github.com/terraform-google-modules/terraform-google-project-factory,126,examples/shared_vpc/main.tf,examples/shared_vpc/main.tf,0,fix,# is merged and released.  This is here to fix the `Error: Unsupported block,"# TODO: Switch to released version once 
 # https://github.com/terraform-google-modules/terraform-google-network/pull/47 
 # is merged and released.  This is here to fix the `Error: Unsupported block 
 # type` on the `triggers` block in network's main.tf file. 
 # 
 # source  = ""terraform-google-modules/network/google"" 
 # version = ""0.8.0""","module ""vpc"" {
  # TODO: Switch to released version once
  # https://github.com/terraform-google-modules/terraform-google-network/pull/47
  # is merged and released.  This is here to fix the `Error: Unsupported block
  # type` on the `triggers` block in network's main.tf file.
  #
  # source  = ""terraform-google-modules/network/google""
  # version = ""0.8.0""
  source = ""git::https://github.com/terraform-google-modules/terraform-google-network.git?ref=aaron-lane-0.12""

  project_id   = module.host-project.project_id
  network_name = var.network_name

  delete_default_internet_gateway_routes = ""true""
  shared_vpc_host                        = ""true""

  subnets = [
    {
      subnet_name   = local.subnet_01
      subnet_ip     = ""10.10.10.0/24""
      subnet_region = ""us-west1""
    },
    {
      subnet_name           = local.subnet_02
      subnet_ip             = ""10.10.20.0/24""
      subnet_region         = ""us-west1""
      subnet_private_access = ""true""
      subnet_flow_logs      = ""true""
    },
  ]

  secondary_ranges = {
    ""${local.subnet_01}"" = [
      {
        range_name    = ""${local.subnet_01}-01""
        ip_cidr_range = ""192.168.64.0/24""
      },
      {
        range_name    = ""${local.subnet_01}-02""
        ip_cidr_range = ""192.168.65.0/24""
      },
    ]

    ""${local.subnet_02}"" = [
      {
        range_name    = ""${local.subnet_02}-01""
        ip_cidr_range = ""192.168.66.0/24""
      },
    ]
  }
}
",module,"module ""vpc"" {
  # source  = ""terraform-google-modules/network/google""
  # version = ""~> 1.4.0""

  project_id   = module.host-project.project_id
  network_name = var.network_name

  delete_default_internet_gateway_routes = true
  shared_vpc_host                        = true

  subnets = [
    {
      subnet_name   = local.subnet_01
      subnet_ip     = ""10.10.10.0/24""
      subnet_region = ""us-west1""
    },
    {
      subnet_name           = local.subnet_02
      subnet_ip             = ""10.10.20.0/24""
      subnet_region         = ""us-west1""
      subnet_private_access = true
      subnet_flow_logs      = true
    },
  ]

  secondary_ranges = {
    ""${local.subnet_01}"" = [
      {
        range_name    = ""${local.subnet_01}-01""
        ip_cidr_range = ""192.168.64.0/24""
      },
      {
        range_name    = ""${local.subnet_01}-02""
        ip_cidr_range = ""192.168.65.0/24""
      },
    ]

    ""${local.subnet_02}"" = [
      {
        range_name    = ""${local.subnet_02}-01""
        ip_cidr_range = ""192.168.66.0/24""
      },
    ]
  }
}
",module,54,,c40a3d3c37d2cf4d9376d08a31b8ac05af21360e,6557d7a7d4dacc0ddcfcb9c89e61627d4dfe1a90,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/c40a3d3c37d2cf4d9376d08a31b8ac05af21360e/examples/shared_vpc/main.tf#L54,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/6557d7a7d4dacc0ddcfcb9c89e61627d4dfe1a90/examples/shared_vpc/main.tf,2019-07-12 13:49:19-04:00,2019-10-19 11:56:49-07:00,4,1,0,1,1,0,1,0,0,0
https://github.com/kubernetes/k8s.io,414,infra/aws/terraform/modules/eks-prow-iam/eks_infra_admin_boundary.tf,infra/aws/terraform/modules/eks-prow-iam/boundary_eks_infra_admin.tf,1,# todo,# TODO(pkprzekwas): remove after replacing boundary name in EKS cluster roles and applying changes.,"/* 
 Copyright 2023 The Kubernetes Authors. 
  
 Licensed under the Apache License, Version 2.0 (the ""License""); 
 you may not use this file except in compliance with the License. 
 You may obtain a copy of the License at 
  
 http://www.apache.org/licenses/LICENSE-2.0 
  
 Unless required by applicable law or agreed to in writing, software 
 distributed under the License is distributed on an ""AS IS"" BASIS, 
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 See the License for the specific language governing permissions and 
 limitations under the License. 
 */  
 # TODO(pkprzekwas): remove after replacing boundary name in EKS cluster roles and applying changes.","resource ""aws_iam_policy"" ""provisioner_permission_boundary"" {
  name        = ""ProvisionerPermissionBoundary""
  description = ""Permission boundary for terraform operator roles.""
  policy      = data.aws_iam_policy_document.eks_resources_permission_boundary_doc.json
  tags        = var.tags
}
",resource,,,17,0.0,e071c9f3f230a0512fc9b671682d5890bb6cce2d,1751c18e379b789c52b5380be377ca1493c94c97,https://github.com/kubernetes/k8s.io/blob/e071c9f3f230a0512fc9b671682d5890bb6cce2d/infra/aws/terraform/modules/eks-prow-iam/eks_infra_admin_boundary.tf#L17,https://github.com/kubernetes/k8s.io/blob/1751c18e379b789c52b5380be377ca1493c94c97/infra/aws/terraform/modules/eks-prow-iam/boundary_eks_infra_admin.tf#L0,2023-05-23 10:35:27+02:00,2023-05-23 16:27:33+02:00,6,2,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1591,blueprints/data-solutions/data-platform-foundations/locals-03-orchestration.tf,blueprints/data-solutions/data-platform-foundations/locals-03-orchestration.tf,0,# todo,# TODO: use new artifact registry module output,# TODO: use new artifact registry module output,"locals {
  _orch_iam = flatten([
    for principal, roles in local.orch_iam : [
      for role in roles : {
        key       = ""${principal}-${role}""
        principal = principal
        role      = role
      }
    ]
  ])
  orch_iam_additive = {
    for binding in local._orch_iam : binding.key => {
      role   = binding.role
      member = local.iam_principals[binding.principal]
    }
  }
  orch_iam_auth = {
    for binding in local._orch_iam :
    binding.role => local.iam_principals[binding.principal]...
  }
  orch_subnet = (
    local.use_shared_vpc
    ? var.network_config.subnet_self_links.orchestration
    : values(module.orch-vpc.0.subnet_self_links)[0]
  )
  orch_vpc = (
    local.use_shared_vpc
    ? var.network_config.network_self_link
    : module.orch-vpc.0.self_link
  )
  # TODO: use new artifact registry module output
  orch_docker_path = format(""%s-docker.pkg.dev/%s/%s"",
  var.region, module.orch-project.project_id, module.orch-artifact-reg.name)
}
",locals,"locals {
  _orch_iam = flatten([
    for principal, roles in local.orch_iam : [
      for role in roles : {
        key       = ""${principal}-${role}""
        principal = principal
        role      = role
      }
    ]
  ])
  orch_iam_additive = {
    for binding in local._orch_iam : binding.key => {
      role   = binding.role
      member = local.iam_principals[binding.principal]
    }
  }
  orch_iam_auth = {
    for binding in local._orch_iam :
    binding.role => local.iam_principals[binding.principal]...
  }
  orch_subnet = (
    local.use_shared_vpc
    ? var.network_config.subnet_self_links.orchestration
    : values(module.orch-vpc[0].subnet_self_links)[0]
  )
  orch_vpc = (
    local.use_shared_vpc
    ? var.network_config.network_self_link
    : module.orch-vpc[0].self_link
  )
  # TODO: use new artifact registry module output
  orch_docker_path = format(""%s-docker.pkg.dev/%s/%s"",
  var.region, module.orch-project.project_id, module.orch-artifact-reg.name)
}
",locals,47,47.0,819894d2bab4b440f1b52b1ac8035912fb107004,3af7e257d21f889ffaf7b32a3bab974fdbfda6e4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/819894d2bab4b440f1b52b1ac8035912fb107004/blueprints/data-solutions/data-platform-foundations/locals-03-orchestration.tf#L47,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3af7e257d21f889ffaf7b32a3bab974fdbfda6e4/blueprints/data-solutions/data-platform-foundations/locals-03-orchestration.tf#L47,2023-08-20 09:44:20+02:00,2024-04-17 10:23:48+02:00,2,0,0,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-iam,10,modules/helper/main.tf,modules/helper/main.tf,0,workaround,# expression is resolved synchonously. And we have to workaround,"# It is important to provide a set for the `for_each` instead of 
 # the map, since we have to guarantee that the `for_each` 
 # expression is resolved synchonously. And we have to workaround 
 # the potential dependency on dynamic resource values by polyfilling 
 # the `for_each` with `count`-like list of indexes.","locals {
  authoritative = var.mode == ""authoritative""
  additive      = var.mode == ""additive""

  # When there are *_num specified, consider the module configuration
  # dynamic. In this case the `for_each` will basically work as
  # a polyfill for `count`
  #
  # The downside of the dynamic mode is that we can't guarantee the
  # resources being reused whenever the configuration changes.
  # Which leads to unnecessary resource recreations.
  dynamic = var.entities_num > 0 || var.bindings_num > 0

  calculated_entities_num = (
    var.entities_num > 0
    ? var.entities_num
    : length(var.entities)
  )

  bindings_by_role = distinct(flatten([
    for name in var.entities
    : [
      for role, members in var.bindings
      : { name = name, role = role, members = members }
    ]
  ]))

  bindings_by_member = distinct(flatten([
    for binding in local.bindings_by_role
    : [
      for member in binding[""members""]
      : { name = binding[""name""], role = binding[""role""], member = member }
    ]
  ]))

  total_roles = (
    var.bindings_num > 0
    ? var.bindings_num * local.calculated_entities_num
    : length(local.bindings_by_role)
  )

  total_members = (
    var.bindings_num > 0
    ? var.bindings_num * local.calculated_entities_num
    : length(local.bindings_by_member)
  )

  keys_authoritative = (
    local.dynamic
    # [dynamic] fallback for_each to a simple list of indexes
    ? [for i in range(local.total_roles) : tostring(i)]
    # [static] generate unique ids which are resilient to updates
    : [
      for binding in local.bindings_by_role
      : ""${binding[""name""]}--${binding[""role""]}""
    ]
  )

  keys_additive = (
    local.dynamic
    # [dynamic] fallback for_each to a simple list of indexes
    ? [for i in range(local.total_members) : tostring(i)]
    # [static] generate unique ids which are resilient to updates
    : [
      for binding in local.bindings_by_member
      : ""${binding[""name""]}--${binding[""role""]}--${binding[""member""]}""
    ]
  )

  bindings_authoritative = (
    local.authoritative
    ? zipmap(local.keys_authoritative, local.bindings_by_role)
    : {}
  )

  bindings_additive = (
    local.additive
    ? zipmap(local.keys_additive, local.bindings_by_member)
    : {}
  )

  # It is important to provide a set for the `for_each` instead of
  # the map, since we have to guarantee that the `for_each`
  # expression is resolved synchonously. And we have to workaround
  # the potential dependency on dynamic resource values by polyfilling
  # the `for_each` with `count`-like list of indexes.
  set_authoritative = (
    local.authoritative
    ? toset(local.keys_authoritative)
    : []
  )

  set_additive = (
    local.additive
    ? toset(local.keys_additive)
    : []
  )
}
",locals,"locals {
  authoritative = var.mode == ""authoritative""
  additive      = var.mode == ""additive""

  # When there is only one entity, consider that the entity passed
  # might be dynamic. In this case the `for_each` will not use
  # entity name when constructing the unique ID.
  #
  # Other rules regrading the dynamic nature of resources:
  # 1. The roles might never be dynamic.
  # 2. Members might only be dynamic in `authoritative` mode.
  singular = length(var.entities) == 1

  # In singular mode, replace entity name with a constant ""default"". This
  # will prevent the potentially dynamic resource name usage in the `for_each`
  aliased_entities = local.singular ? [""default""] : var.entities

  bindings_by_role = distinct(flatten([
    for name in var.entities
    : [
      for role, members in var.bindings
      : { name = name, role = role, members = members }
    ]
  ]))

  bindings_by_member = distinct(flatten([
    for binding in local.bindings_by_role
    : [
      for member in binding[""members""]
      : { name = binding[""name""], role = binding[""role""], member = member }
    ]
  ]))

  keys_authoritative = distinct(flatten([
    for alias in local.aliased_entities
    : [
      for role in keys(var.bindings)
      : ""${alias}--${role}""
    ]
  ]))

  keys_additive = distinct(flatten([
    for alias in local.aliased_entities
    : [
      for role, members in var.bindings
      : [
        for member in members
        : ""${alias}--${role}--${member}""
      ]
    ]
  ]))

  # TODO: Refactor this to force the order somehow.
  #       If you are to change the algo of generating `keys_authoritative`
  #       or `bindings_by_role`, you have to make sure that the order
  #       of the elements inside them matches.
  bindings_authoritative = (
    local.authoritative
    ? zipmap(local.keys_authoritative, local.bindings_by_role)
    : {}
  )

  # TODO: Refactor this to force the order somehow.
  #       If you are to change the algo of generating `keys_additive`
  #       or `bindings_by_member`, you have to make sure that the order
  #       of the elements inside them matches.
  bindings_additive = (
    local.additive
    ? zipmap(local.keys_additive, local.bindings_by_member)
    : {}
  )

  # It is important to provide a set for the `for_each` instead of
  # the map, since we have to guarantee that the `for_each`
  # expression is resolved synchonously.
  set_authoritative = (
    local.authoritative
    ? toset(local.keys_authoritative)
    : []
  )

  set_additive = (
    local.additive
    ? toset(local.keys_additive)
    : []
  )
}
",locals,100,,5afe9a37ed825d0e9ef6c69920df2a6f6cd4fe2b,665a160dd97a99c80a325250d7fea70fa9f4fac5,https://github.com/terraform-google-modules/terraform-google-iam/blob/5afe9a37ed825d0e9ef6c69920df2a6f6cd4fe2b/modules/helper/main.tf#L100,https://github.com/terraform-google-modules/terraform-google-iam/blob/665a160dd97a99c80a325250d7fea70fa9f4fac5/modules/helper/main.tf,2019-10-11 19:00:07+03:00,2019-10-14 19:29:14+03:00,2,1,0,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,414,infra/modules/aws-psoxy-lambda/main.tf,infra/modules/aws-psoxy-lambda/main.tf,0,# todo,# TODO: param_arn_prefix without relying something that itself is provisioned by terraform ...,"# q: makes policy dynamic, so actual statements don't appear in `terraform plan`? 
 # TODO: param_arn_prefix without relying something that itself is provisioned by terraform ...","data ""aws_arn"" ""lambda"" {
  arn = aws_lambda_function.psoxy-instance.arn
}
",data,the block associated got renamed or deleted,,74,,5d8f4928978c96878368db88bc908047217bcda8,8608c0c31d22d62e86546a8c14c6a97ef0c50f39,https://github.com/Worklytics/psoxy/blob/5d8f4928978c96878368db88bc908047217bcda8/infra/modules/aws-psoxy-lambda/main.tf#L74,https://github.com/Worklytics/psoxy/blob/8608c0c31d22d62e86546a8c14c6a97ef0c50f39/infra/modules/aws-psoxy-lambda/main.tf,2022-10-06 13:05:00-07:00,2022-10-06 13:13:36-07:00,3,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,484,tests/fast/stages/s03_project_factory/fixture/variables.tf,tests/fast/stages/s03_project_factory/fixture/variables.tf,0,#todo,#TODO: tfdoc annotations,"/** 
 * Copyright 2022 Google LLC 
 * 
 * Licensed under the Apache License, Version 2.0 (the ""License""); 
 * you may not use this file except in compliance with the License. 
 * You may obtain a copy of the License at 
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, software 
 * distributed under the License is distributed on an ""AS IS"" BASIS, 
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 * See the License for the specific language governing permissions and 
 * limitations under the License. 
 */  
 #TODO: tfdoc annotations ","variable ""billing_account_id"" {
  # tfdoc:variable:source 00-bootstrap
  description = ""Billing account id.""
  type        = string
}
",variable,,,17,0.0,cee207b4544cfe2bc2eb517fd91c79952e3052b3,dc3a2ad7be032c093ae4d82d3abfeb628c16dfe6,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/cee207b4544cfe2bc2eb517fd91c79952e3052b3/tests/fast/stages/s03_project_factory/fixture/variables.tf#L17,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/dc3a2ad7be032c093ae4d82d3abfeb628c16dfe6/tests/fast/stages/s03_project_factory/fixture/variables.tf#L0,2022-01-17 10:36:38+01:00,2022-02-24 15:05:18+01:00,5,2,0,0,0,1,0,0,0,0
https://github.com/cattle-ops/terraform-aws-gitlab-runner,88,modules/cache/main.tf,modules/cache/main.tf,0,fix,# checkov:skip=CKV_AWS_300:False positive. Can be removed when https://github.com/bridgecrewio/checkov/issues/4733 is fixed.,# checkov:skip=CKV_AWS_300:False positive. Can be removed when https://github.com/bridgecrewio/checkov/issues/4733 is fixed.,"resource ""aws_s3_bucket_lifecycle_configuration"" ""build_cache_versioning"" {
  # checkov:skip=CKV_AWS_300:False positive. Can be removed when https://github.com/bridgecrewio/checkov/issues/4733 is fixed.
  bucket = aws_s3_bucket.build_cache.id

  rule {
    id     = ""AbortIncompleteMultipartUploads""
    status = ""Enabled""

    abort_incomplete_multipart_upload {
      days_after_initiation = 1
    }
  }

  rule {
    id     = ""clear""
    status = var.cache_lifecycle_clear ? ""Enabled"" : ""Disabled""

    filter {
      prefix = var.cache_lifecycle_prefix
    }

    expiration {
      days = var.cache_expiration_days
    }
  }
}
",resource,"resource ""aws_s3_bucket_lifecycle_configuration"" ""build_cache_versioning"" {
  # checkov:skip=CKV_AWS_300:False positive. Can be removed when https://github.com/bridgecrewio/checkov/issues/4733 is fixed.
  bucket = aws_s3_bucket.build_cache.id

  rule {
    id     = ""AbortIncompleteMultipartUploads""
    status = ""Enabled""

    abort_incomplete_multipart_upload {
      days_after_initiation = 1
    }
  }

  rule {
    id     = ""clear""
    status = var.cache_lifecycle_clear ? ""Enabled"" : ""Disabled""

    filter {
      prefix = var.cache_lifecycle_prefix
    }

    expiration {
      days = var.cache_expiration_days
    }
  }
}
",resource,59,53.0,5f8d24e835462b1f08f2b7889c015255961ebbbd,aa93e768a1e2d414197feea1330ff3290f65dbb2,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/5f8d24e835462b1f08f2b7889c015255961ebbbd/modules/cache/main.tf#L59,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/aa93e768a1e2d414197feea1330ff3290f65dbb2/modules/cache/main.tf#L53,2023-03-29 07:27:10+02:00,2024-04-11 09:52:40+02:00,3,0,1,1,0,0,0,1,0,1
https://github.com/oracle-terraform-modules/terraform-oci-oke,254,modules/utilities/drain.tf,modules/utilities/drain.tf,0,todo,# TODO Implement,count = false && var.expected_node_count > 0 ? 1 : 0 # TODO Implement,"resource ""null_resource"" ""drain_workers"" {
  count = false && var.expected_node_count > 0 ? 1 : 0 # TODO Implement
  triggers = {
    drain_count = jsonencode(keys(local.worker_pools_draining))
  }

  connection {
    bastion_host        = var.bastion_host
    bastion_user        = var.bastion_user
    bastion_private_key = var.ssh_private_key
    host                = var.operator_host
    user                = var.operator_user
    private_key         = var.ssh_private_key
    timeout             = ""40m""
    type                = ""ssh""
  }

  provisioner ""remote-exec"" {
    inline = [
      ""echo kubectl get nodes ..."",             # TODO List nodes by label for draining pools
      ""echo kubectl drain --ignore-daemonsets"", # TODO Drain nodes for draining pools
    ]
  }
}
",resource,"resource ""null_resource"" ""drain_workers"" {
  count = local.drain_enabled ? 1 : 0
  triggers = {
    drain_workers = jsonencode(sort(keys(local.worker_pools_draining)))
  }

  connection {
    bastion_host        = var.bastion_host
    bastion_user        = var.bastion_user
    bastion_private_key = var.ssh_private_key
    host                = var.operator_host
    user                = var.operator_user
    private_key         = var.ssh_private_key
    timeout             = ""40m""
    type                = ""ssh""
  }

  provisioner ""remote-exec"" {
    inline = [
      ""echo kubectl get nodes ..."",             # TODO List nodes by label for draining pools
      ""echo kubectl drain --ignore-daemonsets"", # TODO Drain nodes for draining pools
    ]
  }
}
",resource,9,,663103be472bc1a358aa48b1bd6f619aa4de7bb0,e711d80a08eed0078476fecb9e0665a40bada4ed,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/663103be472bc1a358aa48b1bd6f619aa4de7bb0/modules/utilities/drain.tf#L9,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/e711d80a08eed0078476fecb9e0665a40bada4ed/modules/utilities/drain.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,2,1,0,1,0,0,0,1,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,540,modules/project/logging.tf,modules/project/logging.tf,0,# todo,# TODO(jccb): use a condition to limit writer-identity only to this,"# TODO(jccb): use a condition to limit writer-identity only to this 
 # bucket","resource ""google_project_iam_member"" ""bucket-sinks-binding"" {
  for_each = local.sink_bindings[""logging""]
  project  = split(""/"", each.value.destination)[1]
  role     = ""roles/logging.bucketWriter""
  member   = google_logging_project_sink.sink[each.key].writer_identity
  # TODO(jccb): use a condition to limit writer-identity only to this
  # bucket
}
",resource,"resource ""google_project_iam_member"" ""bucket-sinks-binding"" {
  for_each = local.sink_bindings[""logging""]
  project  = split(""/"", each.value.destination.target)[1]
  role     = ""roles/logging.bucketWriter""
  member   = google_logging_project_sink.sink[each.key].writer_identity

  condition {
    title       = ""${each.key} bucket writer""
    description = ""Grants bucketWriter to ${google_logging_project_sink.sink[each.key].writer_identity} used by log sink ${each.key} on ${local.project.project_id}""
    expression  = ""resource.name.endsWith('${each.value.destination.target}')""
  }
}
",resource,88,,9a533180a0cfd87493bce91f9c0211987382eeab,486d398c7d68ce1b0784a540feb7be5fb2c680c1,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9a533180a0cfd87493bce91f9c0211987382eeab/modules/project/logging.tf#L88,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/486d398c7d68ce1b0784a540feb7be5fb2c680c1/modules/project/logging.tf,2022-01-22 11:34:18+01:00,2022-11-11 19:22:05+01:00,4,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1984,modules/net-vpc/subnets.tf,modules/net-vpc/subnets.tf,0,# todo,# TODO(sruffilli): Provider 5.29.1 disabled reserved_internal_range because of a bug.,"# TODO(sruffilli): Provider 5.29.1 disabled reserved_internal_range because of a bug. 
 #                  Revert to the following once fixed. 
 # ip_cidr_range = ( 
 #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"") 
 #   ? null 
 #   : secondary_ip_range.value 
 # ) 
 # reserved_internal_range = ( 
 #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"") 
 #   ? secondary_ip_range.value 
 #   : null 
 # )","resource ""google_compute_subnetwork"" ""subnetwork"" {
  for_each      = local.subnets
  project       = var.project_id
  network       = local.network.name
  name          = each.value.name
  region        = each.value.region
  ip_cidr_range = each.value.ip_cidr_range
  description = (
    each.value.description == null
    ? ""Terraform-managed.""
    : each.value.description
  )
  private_ip_google_access = each.value.enable_private_access
  stack_type = (
    try(each.value.ipv6, null) != null ? ""IPV4_IPV6"" : null
  )
  ipv6_access_type = (
    try(each.value.ipv6, null) != null ? each.value.ipv6.access_type : null
  )
  # private_ipv6_google_access = try(each.value.ipv6.enable_private_access, null)
  dynamic ""secondary_ip_range"" {
    for_each = each.value.secondary_ip_ranges == null ? {} : each.value.secondary_ip_ranges
    content {
      range_name    = secondary_ip_range.key
      ip_cidr_range = secondary_ip_range.value
      # TODO(sruffilli): Provider 5.29.1 disabled reserved_internal_range because of a bug.
      #                  Revert to the following once fixed.
      # ip_cidr_range = (
      #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"")
      #   ? null
      #   : secondary_ip_range.value
      # )    
      # reserved_internal_range = (
      #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"")
      #   ? secondary_ip_range.value
      #   : null
      # )
    }
  }
  dynamic ""log_config"" {
    for_each = each.value.flow_logs_config != null ? [""""] : []
    content {
      aggregation_interval = each.value.flow_logs_config.aggregation_interval
      filter_expr          = each.value.flow_logs_config.filter_expression
      flow_sampling        = each.value.flow_logs_config.flow_sampling
      metadata             = each.value.flow_logs_config.metadata
      metadata_fields = (
        each.value.flow_logs_config.metadata == ""CUSTOM_METADATA""
        ? each.value.flow_logs_config.metadata_fields
        : null
      )
    }
  }
}
",resource,"resource ""google_compute_subnetwork"" ""subnetwork"" {
  for_each      = local.subnets
  project       = var.project_id
  network       = local.network.name
  name          = each.value.name
  region        = each.value.region
  ip_cidr_range = each.value.ip_cidr_range
  description = (
    each.value.description == null
    ? ""Terraform-managed.""
    : each.value.description
  )
  private_ip_google_access = each.value.enable_private_access
  stack_type = (
    try(each.value.ipv6, null) != null ? ""IPV4_IPV6"" : null
  )
  ipv6_access_type = (
    try(each.value.ipv6, null) != null ? each.value.ipv6.access_type : null
  )
  # private_ipv6_google_access = try(each.value.ipv6.enable_private_access, null)
  dynamic ""secondary_ip_range"" {
    for_each = each.value.secondary_ip_ranges == null ? {} : each.value.secondary_ip_ranges
    content {
      range_name    = secondary_ip_range.key
      ip_cidr_range = secondary_ip_range.value
      # TODO(sruffilli): Provider 5.29.1 disabled reserved_internal_range because of a bug.
      #                  Revert to the following once fixed.
      # ip_cidr_range = (
      #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"")
      #   ? null
      #   : secondary_ip_range.value
      # )    
      # reserved_internal_range = (
      #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"")
      #   ? secondary_ip_range.value
      #   : null
      # )
    }
  }
  dynamic ""log_config"" {
    for_each = each.value.flow_logs_config != null ? [""""] : []
    content {
      aggregation_interval = each.value.flow_logs_config.aggregation_interval
      filter_expr          = each.value.flow_logs_config.filter_expression
      flow_sampling        = each.value.flow_logs_config.flow_sampling
      metadata             = each.value.flow_logs_config.metadata
      metadata_fields = (
        each.value.flow_logs_config.metadata == ""CUSTOM_METADATA""
        ? each.value.flow_logs_config.metadata_fields
        : null
      )
    }
  }
}
",resource,162,162.0,d3ffcc2b1cf2f2ffc3fc480b90719116dee59563,d3ffcc2b1cf2f2ffc3fc480b90719116dee59563,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d3ffcc2b1cf2f2ffc3fc480b90719116dee59563/modules/net-vpc/subnets.tf#L162,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d3ffcc2b1cf2f2ffc3fc480b90719116dee59563/modules/net-vpc/subnets.tf#L162,2024-05-15 05:46:18+00:00,2024-05-15 05:46:18+00:00,1,0,0,1,1,0,1,0,0,0
https://github.com/Azure/sap-automation,1117,deploy/terraform/run/sap_system/output.tf,deploy/terraform/run/sap_system/output.tf,0,#todo,#TODO Change to use Admin IP,) #TODO Change to use Admin IP,"output ""db_vm_ips"" {
  description = ""Database Virtual Machine IPs""
  value       =   upper(try(local.database.platform, ""HANA"")) == ""HANA"" ? (
    module.hdb_node.db_ip) : (
    module.anydb_node.anydb_db_ip
  ) #TODO Change to use Admin IP

}
",output,"output ""db_vm_ips""                     {
                                         description = ""Database Virtual Machine IPs""
                                         value = upper(try(local.database.platform, ""HANA"")) == ""HANA"" ? (
                                                   module.hdb_node.database_server_ips) : (
                                                   module.anydb_node.database_server_ips
                                                 ) #TODO Change to use Admin IP
                                       }
",output,148,163.0,ee9ec96d7068dfd9bb390253f31394ff30652eb8,db20ac2a47d9d00329385330cb4af6b3c726c400,https://github.com/Azure/sap-automation/blob/ee9ec96d7068dfd9bb390253f31394ff30652eb8/deploy/terraform/run/sap_system/output.tf#L148,https://github.com/Azure/sap-automation/blob/db20ac2a47d9d00329385330cb4af6b3c726c400/deploy/terraform/run/sap_system/output.tf#L163,2023-02-15 12:47:00+02:00,2024-03-11 23:15:11+05:30,24,0,1,1,0,0,1,0,0,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,11,master.tf,master.tf,0,hack,"/*# Run the first control plane   provisioner ""remote-exec"" {     inline = [       # set the hostname in a persistent fashion       ""hostnamectl set-hostname ${self.name}"",       # first we disable automatic reboot (after transactional updates), and configure the reboot method as kured       ""rebootmgrctl set-strategy off && echo 'REBOOT_METHOD=kured' > /etc/transactional-update.conf"",       # prepare a directory for our post-installation kustomizations       ""mkdir -p /tmp/post_install"",       # then we initiate the cluster       ""systemctl enable k3s-server"",       # wait for k3s to get ready       <<-EOT       timeout 120 bash <<EOF         until systemctl status k3s-server > /dev/null; do           systemctl start k3s-server           echo ""Initiating the cluster...""           sleep 1         done         until [ -e /etc/rancher/k3s/k3s.yaml ]; do           echo ""Waiting for kubectl config...""           sleep 1         done         until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do           echo ""Waiting for the cluster to become ready...""           sleep 1         done       EOF       EOT     ]   }    # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.   provisioner ""file"" {     content = yamlencode({       apiVersion = ""kustomize.config.k8s.io/v1beta1""       kind       = ""Kustomization""       resources = [         ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",         ""https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml"",         ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",         ""./traefik.yaml""       ]       patchesStrategicMerge = [         file(""${path.module}/patches/kured.yaml""),         file(""${path.module}/patches/ccm.yaml"")       ]     })     destination = ""/tmp/post_install/kustomization.yaml""   }    # Upload traefik config   provisioner ""file"" {     content = templatefile(       ""${path.module}/templates/traefik_config.yaml.tpl"",       {         lb_disable_ipv6    = var.lb_disable_ipv6         lb_server_type     = var.lb_server_type         location           = var.location         traefik_acme_tls   = var.traefik_acme_tls         traefik_acme_email = var.traefik_acme_email     })     destination = ""/tmp/post_install/traefik.yaml""   }    # Deploy secrets, logging is automatically disabled due to sensitive variables   provisioner ""remote-exec"" {     inline = [       ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name}"",       ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token}"",     ]   }    # Deploy our post-installation kustomization   provisioner ""remote-exec"" {     inline = [       # This ugly hack is here, because terraform serializes the       # embedded yaml files with ""- |2"", when there is more than       # one yamldocument in the embedded file. Kustomize does not understand       # that syntax and tries to parse the blocks content as a file, resulting       # in weird errors. so gnu sed with funny escaping is used to       # replace lines like ""- |3"" by ""- |"" (yaml block syntax).       # due to indendation this should not changes the embedded       # manifests themselves       ""sed -i 's/^- |[0-9]\\+$/- |/g' /tmp/post_install/kustomization.yaml"",       ""kubectl apply -k /tmp/post_install"",     ]   }*/","/*# Run the first control plane   provisioner ""remote-exec"" {     inline = [       # set the hostname in a persistent fashion       ""hostnamectl set-hostname ${self.name}"",       # first we disable automatic reboot (after transactional updates), and configure the reboot method as kured       ""rebootmgrctl set-strategy off && echo 'REBOOT_METHOD=kured' > /etc/transactional-update.conf"",       # prepare a directory for our post-installation kustomizations       ""mkdir -p /tmp/post_install"",       # then we initiate the cluster       ""systemctl enable k3s-server"",       # wait for k3s to get ready       <<-EOT       timeout 120 bash <<EOF         until systemctl status k3s-server > /dev/null; do           systemctl start k3s-server           echo ""Initiating the cluster...""           sleep 1         done         until [ -e /etc/rancher/k3s/k3s.yaml ]; do           echo ""Waiting for kubectl config...""           sleep 1         done         until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do           echo ""Waiting for the cluster to become ready...""           sleep 1         done       EOF       EOT     ]   }    # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.   provisioner ""file"" {     content = yamlencode({       apiVersion = ""kustomize.config.k8s.io/v1beta1""       kind       = ""Kustomization""       resources = [         ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",         ""https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml"",         ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",         ""./traefik.yaml""       ]       patchesStrategicMerge = [         file(""${path.module}/patches/kured.yaml""),         file(""${path.module}/patches/ccm.yaml"")       ]     })     destination = ""/tmp/post_install/kustomization.yaml""   }    # Upload traefik config   provisioner ""file"" {     content = templatefile(       ""${path.module}/templates/traefik_config.yaml.tpl"",       {         lb_disable_ipv6    = var.lb_disable_ipv6         lb_server_type     = var.lb_server_type         location           = var.location         traefik_acme_tls   = var.traefik_acme_tls         traefik_acme_email = var.traefik_acme_email     })     destination = ""/tmp/post_install/traefik.yaml""   }    # Deploy secrets, logging is automatically disabled due to sensitive variables   provisioner ""remote-exec"" {     inline = [       ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name}"",       ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token}"",     ]   }    # Deploy our post-installation kustomization   provisioner ""remote-exec"" {     inline = [       # This ugly hack is here, because terraform serializes the       # embedded yaml files with ""- |2"", when there is more than       # one yamldocument in the embedded file. Kustomize does not understand       # that syntax and tries to parse the blocks content as a file, resulting       # in weird errors. so gnu sed with funny escaping is used to       # replace lines like ""- |3"" by ""- |"" (yaml block syntax).       # due to indendation this should not changes the embedded       # manifests themselves       ""sed -i 's/^- |[0-9]\\+$/- |/g' /tmp/post_install/kustomization.yaml"",       ""kubectl apply -k /tmp/post_install"",     ]   }*/","resource ""hcloud_server"" ""first_control_plane"" {
  name = ""k3s-control-plane-0""

  image              = data.hcloud_image.linux.name
  rescue             = ""linux64""
  server_type        = var.control_plane_server_type
  location           = var.location
  ssh_keys           = [hcloud_ssh_key.k3s.id]
  firewall_ids       = [hcloud_firewall.k3s.id]
  placement_group_id = hcloud_placement_group.k3s.id

  labels = {
    ""provisioner"" = ""terraform"",
    ""engine""      = ""k3s""
  }

  connection {
    user           = ""root""
    private_key    = local.ssh_private_key
    agent_identity = local.ssh_identity
    host           = self.ipv4_address
  }

  provisioner ""file"" {
    content = templatefile(""${path.module}/templates/config.ign.tpl"", {
      name           = self.name
      ssh_public_key = local.ssh_public_key
    })
    destination = ""/root/config.ign""
  }

  # Install MicroOS
  provisioner ""remote-exec"" {
    inline = local.MicroOS_install_commands
  }

  # Issue a reboot command
  provisioner ""local-exec"" {
    command = ""ssh ${local.ssh_args} root@${self.ipv4_address} '(sleep 2; reboot)&'; sleep 3""
  }

  # Wait for MicroOS to reboot and be ready
  provisioner ""local-exec"" {
    command = <<-EOT
      until ssh ${local.ssh_args} -o ConnectTimeout=2 root@${self.ipv4_address} true 2> /dev/null
      do
        echo ""Waiting for MicroOS to reboot and become available...""
        sleep 2
      done
    EOT
  }

  # Generating k3s master config file
  provisioner ""file"" {
    content = yamlencode({
      node-name                = self.name
      cluster-init             = true
      disable-cloud-controller = true
      disable                  = [""servicelb"", ""local-storage""]
      flannel-iface            = ""eth1""
      kubelet-arg              = ""cloud-provider=external""
      node-ip                  = local.first_control_plane_network_ip
      advertise-address        = local.first_control_plane_network_ip
      token                    = random_password.k3s_token.result
      node-taint               = var.allow_scheduling_on_control_plane ? [] : [""node-role.kubernetes.io/master:NoSchedule""]
    })
    destination = ""/etc/rancher/k3s/config.yaml""
  }

  /*  # Run the first control plane
  provisioner ""remote-exec"" {
    inline = [
      # set the hostname in a persistent fashion
      ""hostnamectl set-hostname ${self.name}"",
      # first we disable automatic reboot (after transactional updates), and configure the reboot method as kured
      ""rebootmgrctl set-strategy off && echo 'REBOOT_METHOD=kured' > /etc/transactional-update.conf"",
      # prepare a directory for our post-installation kustomizations
      ""mkdir -p /tmp/post_install"",
      # then we initiate the cluster
      ""systemctl enable k3s-server"",
      # wait for k3s to get ready
      <<-EOT
      timeout 120 bash <<EOF
        until systemctl status k3s-server > /dev/null; do
          systemctl start k3s-server
          echo ""Initiating the cluster...""
          sleep 1
        done
        until [ -e /etc/rancher/k3s/k3s.yaml ]; do
          echo ""Waiting for kubectl config...""
          sleep 1
        done
        until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do
          echo ""Waiting for the cluster to become ready...""
          sleep 1
        done
      EOF
      EOT
    ]
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content = yamlencode({
      apiVersion = ""kustomize.config.k8s.io/v1beta1""
      kind       = ""Kustomization""
      resources = [
        ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",
        ""https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml"",
        ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",
        ""./traefik.yaml""
      ]
      patchesStrategicMerge = [
        file(""${path.module}/patches/kured.yaml""),
        file(""${path.module}/patches/ccm.yaml"")
      ]
    })
    destination = ""/tmp/post_install/kustomization.yaml""
  }

  # Upload traefik config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/traefik_config.yaml.tpl"",
      {
        lb_disable_ipv6    = var.lb_disable_ipv6
        lb_server_type     = var.lb_server_type
        location           = var.location
        traefik_acme_tls   = var.traefik_acme_tls
        traefik_acme_email = var.traefik_acme_email
    })
    destination = ""/tmp/post_install/traefik.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name}"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token}"",
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = [
      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /tmp/post_install/kustomization.yaml"",
      ""kubectl apply -k /tmp/post_install"",
    ]
  } */

  network {
    network_id = hcloud_network.k3s.id
    ip         = local.first_control_plane_network_ip
  }

  depends_on = [
    hcloud_network_subnet.k3s,
    hcloud_firewall.k3s
  ]
}
",resource,"resource ""hcloud_server"" ""first_control_plane"" {
  name = ""k3s-control-plane-0""

  image              = data.hcloud_image.linux.name
  rescue             = ""linux64""
  server_type        = var.control_plane_server_type
  location           = var.location
  ssh_keys           = [hcloud_ssh_key.k3s.id]
  firewall_ids       = [hcloud_firewall.k3s.id]
  placement_group_id = hcloud_placement_group.k3s.id

  labels = {
    ""provisioner"" = ""terraform"",
    ""engine""      = ""k3s""
  }

  connection {
    user           = ""root""
    private_key    = local.ssh_private_key
    agent_identity = local.ssh_identity
    host           = self.ipv4_address
  }

  provisioner ""file"" {
    content = templatefile(""${path.module}/templates/config.ign.tpl"", {
      name           = self.name
      ssh_public_key = local.ssh_public_key
    })
    destination = ""/root/config.ign""
  }

  # Install MicroOS
  provisioner ""remote-exec"" {
    inline = local.microOS_install_commands
  }

  # Issue a reboot command and wait for the node to reboot
  provisioner ""local-exec"" {
    command = ""ssh ${local.ssh_args} root@${self.ipv4_address} '(sleep 2; reboot)&'; sleep 3""
  }
  provisioner ""local-exec"" {
    command = <<-EOT
      until ssh ${local.ssh_args} -o ConnectTimeout=2 root@${self.ipv4_address} true 2> /dev/null
      do
        echo ""Waiting for MicroOS to reboot and become available...""
        sleep 2
      done
    EOT
  }

  # Generating k3s master config file
  provisioner ""file"" {
    content = yamlencode({
      node-name                = self.name
      cluster-init             = true
      disable-cloud-controller = true
      disable                  = [""servicelb"", ""local-storage""]
      flannel-iface            = ""eth1""
      kubelet-arg              = ""cloud-provider=external""
      node-ip                  = local.first_control_plane_network_ip
      advertise-address        = local.first_control_plane_network_ip
      token                    = random_password.k3s_token.result
      node-taint               = var.allow_scheduling_on_control_plane ? [] : [""node-role.kubernetes.io/master:NoSchedule""]
      node-label               = var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : []
    })
    destination = ""/tmp/config.yaml""
  }



  # Install k3s server
  provisioner ""remote-exec"" {
    inline = local.install_k3s_server
  }

  # Issue a reboot command and wait for the node to reboot
  provisioner ""local-exec"" {
    command = ""ssh ${local.ssh_args} root@${self.ipv4_address} '(sleep 2; reboot)&'; sleep 3""
  }
  provisioner ""local-exec"" {
    command = <<-EOT
      until ssh ${local.ssh_args} -o ConnectTimeout=2 root@${self.ipv4_address} true 2> /dev/null
      do
        echo ""Waiting for MicroOS to reboot and become available...""
        sleep 2
      done
    EOT
  }

  # Upon reboot verify that the k3s server is starts, and wait for k3s to be ready to receive commands
  provisioner ""remote-exec"" {
    inline = [
      # prepare the post_install directory
      ""mkdir -p /tmp/post_install"",
      # wait for k3s to become ready
      <<-EOT
      timeout 120 bash <<EOF
        until systemctl status k3s > /dev/null; do
          echo ""Waiting for the k3s server to start...""
          sleep 1
        done
        until [ -e /etc/rancher/k3s/k3s.yaml ]; do
          echo ""Waiting for kubectl config...""
          sleep 1
        done
        until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do
          echo ""Waiting for the cluster to become ready...""
          sleep 1
        done
      EOF
      EOT
    ]
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content = yamlencode({
      apiVersion = ""kustomize.config.k8s.io/v1beta1""
      kind       = ""Kustomization""
      resources = [
        ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",
        ""https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml"",
        ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",
        ""https://raw.githubusercontent.com/rancher/system-upgrade-controller/master/manifests/system-upgrade-controller.yaml"",
        ""./traefik.yaml"",
      ]
      patchesStrategicMerge = [
        file(""${path.module}/patches/kured.yaml""),
        file(""${path.module}/patches/ccm.yaml"")
      ]
    })
    destination = ""/tmp/post_install/kustomization.yaml""
  }

  # Upload traefik config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/traefik_config.yaml.tpl"",
      {
        lb_disable_ipv6    = var.lb_disable_ipv6
        lb_server_type     = var.lb_server_type
        location           = var.location
        traefik_acme_tls   = var.traefik_acme_tls
        traefik_acme_email = var.traefik_acme_email
    })
    destination = ""/tmp/post_install/traefik.yaml""
  }

  # Upload the system upgrade controller plans config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/plans.yaml.tpl"",
      {
        channel = var.k3s_upgrade_channel
    })
    destination = ""/tmp/post_install/plans.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""set -ex"",
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name}"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token}"",
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = [
      ""set -ex"",
      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /tmp/post_install/kustomization.yaml"",
      ""kubectl apply -k /tmp/post_install"",
      ""echo 'Waiting for the system-upgrade-controller deployment to become available...' && kubectl -n system-upgrade wait --for=condition=available --timeout=300s deployment/system-upgrade-controller"",
      ""kubectl apply -f /tmp/post_install/plans.yaml""
    ]
  }

  network {
    network_id = hcloud_network.k3s.id
    ip         = local.first_control_plane_network_ip
  }

  depends_on = [
    hcloud_network_subnet.k3s,
    hcloud_firewall.k3s
  ]
}
",resource,70,,1f0c825b234b5cf5a8b2a081c1b1cc9b7ebedd3d,fec695086ad9edc4d9bf53d7c1d06b27945f6962,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/1f0c825b234b5cf5a8b2a081c1b1cc9b7ebedd3d/master.tf#L70,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/fec695086ad9edc4d9bf53d7c1d06b27945f6962/master.tf,2022-02-16 00:13:02+01:00,2022-02-16 03:18:40+01:00,2,1,1,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,640,infra/examples-dev/aws-google-workspace/main.tf,infra/examples-dev/aws-google-workspace/main.tf,0,# todo,"## TODO: requires targeted apply to create key first, bc value of key_id determines map content","## TODO: requires targeted apply to create key first, bc value of key_id determines map content 
 ## in example 
 #resource ""aws_kms_key"" ""key"" { 
 #  description             = ""KMS key for Psoxy"" 
 #  enable_key_rotation     = true 
 #  is_enabled              = true 
 #}  
 # if you generated these, you may want them to import back into your data warehouse","output ""lookup_tables"" {
  value = module.psoxy-aws-google-workspace.lookup_tables
}
",output,"output ""lookup_tables"" {
  value = module.psoxy.lookup_tables
}
",output,87,,3c69b9a75c1ba840ddd15b603df9817b1c425e5b,de496ca8bf20117dbb31db7021b07d22cb4579c3,https://github.com/Worklytics/psoxy/blob/3c69b9a75c1ba840ddd15b603df9817b1c425e5b/infra/examples-dev/aws-google-workspace/main.tf#L87,https://github.com/Worklytics/psoxy/blob/de496ca8bf20117dbb31db7021b07d22cb4579c3/infra/examples-dev/aws-google-workspace/main.tf,2023-01-23 09:56:38-08:00,2023-06-28 10:02:05-07:00,31,1,1,1,0,1,0,0,0,0
https://github.com/aws-observability/terraform-aws-observability-accelerator,1,examples/workloads.tf,examples/workloads.tf,0,fix,"#-- true doesn't work for me, needs fix","  enable_opentelemetry_operator = false #-- true doesn't work for me, needs fix","module ""eks_observability_accelerator"" {
  #source = ""aws-ia/terrarom-aws-observability-accelerator""
  source = ""../""

  aws_region     = var.aws_region
  eks_cluster_id = var.eks_cluster_id

  # deploys AWS Distro for OpenTelemetry operator into the cluster
  enable_amazon_eks_adot = false

  # reusing existing certificate manager? defaults to true
  enable_cert_manager = false

  # # -- or enable opentelemetry operator
  enable_opentelemetry_operator = false #-- true doesn't work for me, needs fix
  # open_telemetry_operator_config = map() // custom config

  # creates a new AMP workspace, defaults to true
  create_managed_prometheus_workspace = false

  # reusing existing AMP -- needs data source for alerting rules
  managed_prometheus_id       = var.managed_prometheus_id
  managed_prometheus_endpoint = var.managed_prometheus_endpoint
  managed_prometheus_region   = var.managed_prometheus_region


  enable_java = true

  # enable_haproxy = true
  # haproxy_config = {
  #   amp_endpoint     = module / amp.endpoint
  #   grafana_endpoint = module.grafana.endpoint
  # }


  # java_config = {
  #   amp_endpoint     = """"
  #   grafana_endpoint = """"
  # }




  # # -- or use an existing one
  # # seems like https://github.com/terraform-aws-modules/terraform-aws-managed-service-prometheus
  # # supports importing
  # amp_workspace_alias = var.amp_alias

  # # enable rules and alerts
  # enable_alert_manager = true

  # # -- or provide custom alerts definition
  # prometheus_custom_alert_rule = var.prometheus_custom_alert_rule


  # # create grafana workspace, and customer to deal with authentication later
  # create_managed_grafana_workspace = true
  # grafana_auth_provider            = var.grafana_auth_provider       //SAML or AWS_SSO
  # grafana_account_access_type      = var.grafana_account_access_type // CURRENT_ACCOUNT or ORGANIZATION
  # grafana_permission_type          = var.grafana_permission_type     // SERVICE_MANAGED or CUSTOMER_MANAGED
  # grafana_permission_role_arn      = var.grafana_permission_role_arn // if CUSTOMER_MANAGED

  # # -- or using existing amg workspace. so we can use API for keys
  # managed_grafana_workspace_id = var.managed_grafana_workspace_id

  tags = local.tags

}
",module,"module ""eks_observability_accelerator"" {
  #source = ""aws-ia/terrarom-aws-observability-accelerator""
  source = ""../""

  aws_region     = var.aws_region
  eks_cluster_id = var.eks_cluster_id

  # deploys AWS Distro for OpenTelemetry operator into the cluster
  enable_amazon_eks_adot = true

  # reusing existing certificate manager? defaults to true
  enable_cert_manager = true

  # # -- or enable opentelemetry operator
  enable_opentelemetry_operator = false
  #open_telemetry_operator_config = map() // custom config

  # creates a new AMP workspace, defaults to true
  enable_managed_prometheus = false

  # reusing existing AMP -- needs data source for alerting rules
  managed_prometheus_id     = var.managed_prometheus_workspace_id
  managed_prometheus_region = null # defaults to the current region, useful for cross region scenarios (same account)

  # sets up the AMP alert manager at the workspace level
  enable_alertmanager = true

  # create a new Grafana workspace - TODO review design
  enable_managed_grafana       = false
  managed_grafana_workspace_id = var.managed_grafana_workspace_id
  grafana_api_key              = var.grafana_api_key

  # enable workload-specific collector, metrics, alerts and dashboards
  enable_java                 = false
  enable_java_recording_rules = false

  # enable_haproxy = true
  # haproxy_config = {
  #   amp_endpoint     = module / amp.endpoint
  #   grafana_endpoint = module.grafana.endpoint
  # }

  enable_infra_metrics = true
  #infra_metrics_config = {}

  tags = local.tags
}
",module,17,,b7e909c92ddaff2b4ee2f3e1be30e99c0b1c6de1,a8fc12f7b7d7f4ed814d48061cf3e0eb4a646c9e,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/b7e909c92ddaff2b4ee2f3e1be30e99c0b1c6de1/examples/workloads.tf#L17,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/a8fc12f7b7d7f4ed814d48061cf3e0eb4a646c9e/examples/workloads.tf,2022-08-26 17:30:03+02:00,2022-08-26 17:30:03+02:00,5,1,1,1,0,0,0,0,1,0
https://github.com/compiler-explorer/infra,221,terraform/cloudfront.tf,terraform/cloudfront.tf,0,todo,# todo change,"    metric_name                = ""deny-banned-ips"" # todo change","resource ""aws_wafv2_web_acl"" ""banned-ips"" {
  name  = ""deny-banned-ips""
  scope = ""REGIONAL"" # TODO all these?
  default_action {
    allow {}
  }

  rule {
    name     = ""deny-ipv4""
    priority = 0
    action {
      block {}
    }
    statement {
      ip_set_reference_statement {
        arn = aws_wafv2_ip_set.banned-ipv4.arn
        ip_set_forwarded_ip_config {
          fallback_behavior = ""MATCH""
          header_name       = ""X-Forwarded-For""
          position          = ""ANY""
        }
      }
    }
    visibility_config {
      cloudwatch_metrics_enabled = true
      metric_name                = ""deny-ipv4""
      sampled_requests_enabled   = true
    }
  }
  rule {
    name     = ""deny-ipv6""
    priority = 1
    action {
      block {}
    }
    statement {
      ip_set_reference_statement {
        arn = aws_wafv2_ip_set.banned-ipv6.arn
        ip_set_forwarded_ip_config {
          fallback_behavior = ""MATCH""
          header_name       = ""X-Forwarded-For""
          position          = ""ANY""
        }
      }
    }
    visibility_config {
      cloudwatch_metrics_enabled = true
      metric_name                = ""deny-ipv6""
      sampled_requests_enabled   = true
    }
  }
  visibility_config {
    cloudwatch_metrics_enabled = true
    metric_name                = ""deny-banned-ips"" # todo change
    sampled_requests_enabled   = true
  }
}
",resource,the block associated got renamed or deleted,,617,,bae9867971b4d1a63f896071dd267f6a8bbe9637,5984967d8fe56d0296be70c38a7c15dd8e16615d,https://github.com/compiler-explorer/infra/blob/bae9867971b4d1a63f896071dd267f6a8bbe9637/terraform/cloudfront.tf#L617,https://github.com/compiler-explorer/infra/blob/5984967d8fe56d0296be70c38a7c15dd8e16615d/terraform/cloudfront.tf,2022-10-06 18:18:22-05:00,2022-10-06 19:03:41-05:00,2,1,0,1,0,0,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,175,fargate.tf,fargate.tf,0,implement,# This is a homemade `depends_on` https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2,"# Hack to ensure ordering of resource creation. 
 # This is a homemade `depends_on` https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2 
 # Do not create node_groups before other resources are ready and removes race conditions 
 # Ensure these resources are created before ""unlocking"" the data source. 
 # Will be removed in Terraform 0.13","module ""fargate"" {
  source                            = ""./modules/fargate""
  cluster_name                      = coalescelist(aws_eks_cluster.this[*].name, [""""])[0]
  create_eks                        = var.create_eks
  create_fargate_pod_execution_role = var.create_fargate_pod_execution_role
  fargate_pod_execution_role_name   = var.fargate_pod_execution_role_name
  fargate_profiles                  = var.fargate_profiles
  iam_path                          = var.iam_path
  iam_policy_arn_prefix             = local.policy_arn_prefix
  subnets                           = var.subnets
  tags                              = var.tags

  # Hack to ensure ordering of resource creation.
  # This is a homemade `depends_on` https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2
  # Do not create node_groups before other resources are ready and removes race conditions
  # Ensure these resources are created before ""unlocking"" the data source.
  # Will be removed in Terraform 0.13
  eks_depends_on = [
    aws_eks_cluster.this,
    kubernetes_config_map.aws_auth,
  ]
}
",module,"module ""fargate"" {
  source = ""./modules/fargate""

  create_eks                        = var.create_eks
  create_fargate_pod_execution_role = var.create_fargate_pod_execution_role

  cluster_name                    = local.cluster_name
  fargate_pod_execution_role_name = var.fargate_pod_execution_role_name
  permissions_boundary            = var.permissions_boundary
  iam_path                        = var.iam_path
  subnets                         = coalescelist(var.fargate_subnets, var.subnets, [""""])

  fargate_profiles = var.fargate_profiles

  tags = var.tags
}
",module,14,,0d77e30075e20ff16fa7357987a240e93f5f28ce,2bdf7d7dd6e4705fdfa267bed40e147bd9287a21,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/0d77e30075e20ff16fa7357987a240e93f5f28ce/fargate.tf#L14,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/2bdf7d7dd6e4705fdfa267bed40e147bd9287a21/fargate.tf,2020-11-07 23:03:12+01:00,2021-09-16 11:35:44+02:00,5,1,1,1,1,0,0,0,0,1
https://github.com/kubernetes-sigs/kubespray,11,contrib/terraform/openstack/modules/compute/main.tf,contrib/terraform/openstack/modules/compute/main.tf,0,workaround,"# As a workaround for creating ""dynamic"" lists (when, for example, no bastion host is created)","# The join() hack is described here: https://github.com/hashicorp/terraform/issues/11566 
 # As a workaround for creating ""dynamic"" lists (when, for example, no bastion host is created) ","resource ""openstack_compute_instance_v2"" ""k8s_master"" {
  name              = ""${var.cluster_name}-k8s-master-${count.index+1}""
  count             = ""${var.number_of_k8s_masters}""
  availability_zone = ""${element(var.az_list, count.index)}""
  image_name        = ""${var.image}""
  flavor_id         = ""${var.flavor_k8s_master}""
  key_pair          = ""${openstack_compute_keypair_v2.k8s.name}""

  network {
    name = ""${var.network_name}""
  }

  # The join() hack is described here: https://github.com/hashicorp/terraform/issues/11566
  # As a workaround for creating ""dynamic"" lists (when, for example, no bastion host is created)

  security_groups = [""${compact(list(
    openstack_networking_secgroup_v2.k8s_master.name,
    join("" "", openstack_networking_secgroup_v2.bastion.*.id),
    openstack_networking_secgroup_v2.k8s.name,
    ""default"",
   ))}""]
  metadata = {
    ssh_user         = ""${var.ssh_user}""
    kubespray_groups = ""etcd,kube-master,${var.supplementary_master_groups},k8s-cluster,vault""
    depends_on       = ""${var.network_id}""
  }
  provisioner ""local-exec"" {
    command = ""sed s/USER/${var.ssh_user}/ contrib/terraform/openstack/ansible_bastion_template.txt | sed s/BASTION_ADDRESS/${element( concat(var.bastion_fips, var.k8s_master_fips), 0)}/ > contrib/terraform/group_vars/no-floating.yml""
  }
}
",resource,"resource ""openstack_compute_instance_v2"" ""k8s_master"" {
  name              = ""${var.cluster_name}-k8s-master-${count.index+1}""
  count             = ""${var.number_of_k8s_masters}""
  availability_zone = ""${element(var.az_list, count.index)}""
  image_name        = ""${var.image}""
  flavor_id         = ""${var.flavor_k8s_master}""
  key_pair          = ""${openstack_compute_keypair_v2.k8s.name}""

  network {
    name = ""${var.network_name}""
  }

  security_groups = [""${openstack_networking_secgroup_v2.k8s_master.name}"",
    ""${openstack_networking_secgroup_v2.k8s.name}"",
    ""default"",
  ]

  metadata = {
    ssh_user         = ""${var.ssh_user}""
    kubespray_groups = ""etcd,kube-master,${var.supplementary_master_groups},k8s-cluster,vault""
    depends_on       = ""${var.network_id}""
  }

  provisioner ""local-exec"" {
    command = ""sed s/USER/${var.ssh_user}/ contrib/terraform/openstack/ansible_bastion_template.txt | sed s/BASTION_ADDRESS/${element( concat(var.bastion_fips, var.k8s_master_fips), 0)}/ > contrib/terraform/group_vars/no-floating.yml""
  }
}
",resource,106,,20ebb49568547d9621bfdd13945c725a991d5916,7f1d9ff543247a4a1868eab44e79a7fa4438ab70,https://github.com/kubernetes-sigs/kubespray/blob/20ebb49568547d9621bfdd13945c725a991d5916/contrib/terraform/openstack/modules/compute/main.tf#L106,https://github.com/kubernetes-sigs/kubespray/blob/7f1d9ff543247a4a1868eab44e79a7fa4438ab70/contrib/terraform/openstack/modules/compute/main.tf,2019-04-09 04:01:09-07:00,2019-04-15 07:22:08-07:00,2,1,1,1,1,1,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,11,resources/third-party/compute/SchedMD-slurm-on-gcp-v5-partition/main.tf,resources/third-party/compute/SchedMD-slurm-on-gcp-v5-partition/main.tf,0,# todo,# TODO: this next one does not like '-',"source = ""git::https://gitlab.com/SchedMD/slurm-gcp.git//terraform/modules/slurm_partition?ref=dev-v5""  
 # TODO: this next one does not like '-'","module ""slurm_partition"" {
  source = ""git::https://gitlab.com/SchedMD/slurm-gcp.git//terraform/modules/slurm_partition?ref=dev-v5""

  # TODO: this next one does not like '-'
  slurm_cluster_name      = var.deployment_name
  partition_nodes         = local.partition_nodes
  enable_job_exclusive    = var.exclusive
  enable_placement_groups = var.enable_placement
  network_storage         = var.network_storage
  partition_name          = var.partition_name
  project_id              = var.project_id
  region                  = var.region
  slurm_cluster_id        = ""placeholder""
  subnetwork              = ""default""
}
",module,"module ""slurm_partition"" {
  source = ""git::https://gitlab.com/SchedMD/slurm-gcp.git//terraform/modules/slurm_partition?ref=dev-v5""

  slurm_cluster_name      = var.slurm_cluster_name
  partition_nodes         = local.partition_nodes
  enable_job_exclusive    = var.exclusive
  enable_placement_groups = var.enable_placement
  network_storage         = var.network_storage
  partition_name          = var.partition_name
  project_id              = var.project_id
  region                  = var.region
  slurm_cluster_id        = ""placeholder""
  subnetwork              = var.subnetwork_self_link
  partition_conf = {
    Default = ""YES""
  }
}
",module,57,,306743da2f4c8f924b52d7fe2a84de9666dba59f,ebb8cbdcdc388c89f8e050809fa8d7b2c186ebb2,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/306743da2f4c8f924b52d7fe2a84de9666dba59f/resources/third-party/compute/SchedMD-slurm-on-gcp-v5-partition/main.tf#L57,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/ebb8cbdcdc388c89f8e050809fa8d7b2c186ebb2/resources/third-party/compute/SchedMD-slurm-on-gcp-v5-partition/main.tf,2022-08-29 16:17:21+00:00,2022-08-29 16:17:21+00:00,2,1,1,0,0,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,361,modules/network/subnets.tf,modules/network/subnets.tf,0,todo,# TODO reflect default security_list_id instead of ignore,# TODO reflect default security_list_id instead of ignore,"resource ""oci_core_subnet"" ""oke"" {
  for_each = local.subnets_to_create

  compartment_id             = var.compartment_id
  vcn_id                     = var.vcn_id
  cidr_block                 = lookup(local.subnet_cidrs_all, each.key)
  display_name               = ""${each.key}-${var.state_id}""
  dns_label                  = var.assign_dns ? lookup(var.subnets, ""id"", substr(each.key, 0, 2)) : null
  prohibit_public_ip_on_vnic = !tobool(lookup(each.value, ""is_public"", false))
  route_table_id             = !tobool(lookup(each.value, ""is_public"", false)) ? var.nat_route_table_id : var.ig_route_table_id
  security_list_ids          = compact([lookup(lookup(oci_core_security_list.oke, each.key, {}), ""id"", null)])
  defined_tags               = local.defined_tags
  freeform_tags              = local.freeform_tags

  lifecycle {
    # TODO reflect default security_list_id instead of ignore
    ignore_changes = [security_list_ids, freeform_tags, defined_tags, dns_label, display_name, cidr_block]
  }
}
",resource,"resource ""oci_core_subnet"" ""oke"" {
  for_each = local.subnets_to_create

  compartment_id             = var.compartment_id
  vcn_id                     = var.vcn_id
  cidr_block                 = lookup(local.subnet_cidrs_all, each.key)
  display_name               = format(""%v-%v"", each.key, var.state_id)
  dns_label                  = lookup(local.subnet_dns_labels, each.key)
  prohibit_public_ip_on_vnic = !tobool(lookup(each.value, ""is_public"", false))
  route_table_id             = !tobool(lookup(each.value, ""is_public"", false)) ? var.nat_route_table_id : var.ig_route_table_id
  security_list_ids          = compact([lookup(lookup(oci_core_security_list.oke, each.key, {}), ""id"", null)])
  defined_tags               = var.defined_tags
  freeform_tags              = var.freeform_tags

  lifecycle {
    ignore_changes = [
      freeform_tags, defined_tags, display_name,
      cidr_block, dns_label, security_list_ids,
    ]
  }
}
",resource,127,,f49f1da39d79cf260d80dcb10ee8e399828e6e1c,52588fd5e9123c180f01072f5a2f3c2f4a349a25,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/f49f1da39d79cf260d80dcb10ee8e399828e6e1c/modules/network/subnets.tf#L127,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/52588fd5e9123c180f01072f5a2f3c2f4a349a25/modules/network/subnets.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,6,1,0,1,0,1,1,0,0,0
https://github.com/Worklytics/psoxy,1456,infra/examples-dev/gcp/main.tf,infra/examples-dev/gcp/main.tf,0,# todo,# TODO: this has 5 remote modules combine some?,"# TODO: this has 5 remote modules; combine some? 
 #  eg, worklytics-connectors + gcp-host + worklytics-psoxy-connection-generic into a single 
 #     gcp-host-for-worklytics? poor TF style, but simplifies root module?  
 # in effect, these are for sources for which authentication/authorization cannot (or need not) 
 # be provisioned via Terraform, so doesn't add any dependencies 
 # call this 'generic_source_connectors'?","module ""worklytics_connectors"" {
  source = ""../../modules/worklytics-connectors""

  enabled_connectors    = var.enabled_connectors
  example_jira_issue_id = var.example_jira_issue_id
  jira_cloud_id         = var.jira_cloud_id
  jira_server_url       = var.jira_server_url
  salesforce_domain     = var.salesforce_domain
}
",module,"module ""worklytics_connectors"" {
  source = ""../../modules/worklytics-connectors""
  # source = ""git::https://github.com/worklytics/psoxy//infra/modules/worklytics-connectors?ref=v0.4.53""


  enabled_connectors               = var.enabled_connectors
  jira_cloud_id                    = var.jira_cloud_id
  jira_server_url                  = var.jira_server_url
  jira_example_issue_id            = var.jira_example_issue_id
  salesforce_domain                = var.salesforce_domain
  github_api_host                  = var.github_api_host
  github_enterprise_server_host    = var.github_enterprise_server_host
  github_enterprise_server_version = var.github_enterprise_server_version
  github_installation_id           = var.github_installation_id
  github_organization              = var.github_organization
  github_example_repository        = var.github_example_repository
  salesforce_example_account_id    = var.salesforce_example_account_id
}
",module,34,23.0,f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e,8f6786b90a7f0fcb6120cc8f822fd07cab49697f,https://github.com/Worklytics/psoxy/blob/f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e/infra/examples-dev/gcp/main.tf#L34,https://github.com/Worklytics/psoxy/blob/8f6786b90a7f0fcb6120cc8f822fd07cab49697f/infra/examples-dev/gcp/main.tf#L23,2023-06-16 14:08:45-07:00,2024-04-23 19:32:11-07:00,82,0,0,1,1,1,0,0,0,0
https://github.com/rust-lang/simpleinfra,24,terragrunt/modules/ecs-service/main.tf,terragrunt/modules/ecs-service/main.tf,0,// todo,// TODO: We assign a public IP address so that the service communicate,"// TODO: We assign a public IP address so that the service communicate 
 // to all the services it needs (e.g., SSM and ECR). Eventually, we'd 
 // like to shut down public access to the ecs service, but the work 
 // around is tediuous.","resource ""aws_ecs_service"" ""service"" {
  name             = var.name
  cluster          = var.cluster_config.cluster_id
  task_definition  = var.task_arn
  desired_count    = var.tasks_count
  launch_type      = ""FARGATE""
  platform_version = var.platform_version

  deployment_minimum_healthy_percent = var.deployment_minimum_healty_percent
  deployment_maximum_percent         = var.deployment_maximum_percent

  enable_ecs_managed_tags = true

  load_balancer {
    target_group_arn = aws_lb_target_group.service.arn
    container_name   = var.http_container
    container_port   = var.http_port
  }

  network_configuration {
    subnets = var.cluster_config.subnet_ids
    security_groups = concat(
      [var.cluster_config.service_security_group_id],
      var.additional_security_group_ids,
    )
    // TODO: We assign a public IP address so that the service communicate
    // to all the services it needs (e.g., SSM and ECR). Eventually, we'd
    // like to shut down public access to the ecs service, but the work
    // around is tediuous.
    assign_public_ip = true
  }
}
",resource,"resource ""aws_ecs_service"" ""service"" {
  name             = var.name
  cluster          = var.cluster_config.cluster_id
  task_definition  = var.task_arn
  desired_count    = var.tasks_count
  launch_type      = ""FARGATE""
  platform_version = var.platform_version

  deployment_minimum_healthy_percent = var.deployment_minimum_healty_percent
  deployment_maximum_percent         = var.deployment_maximum_percent

  enable_ecs_managed_tags = true

  load_balancer {
    target_group_arn = aws_lb_target_group.service.arn
    container_name   = var.http_container
    container_port   = var.http_port
  }

  network_configuration {
    subnets = var.cluster_config.subnet_ids
    security_groups = concat(
      [var.cluster_config.service_security_group_id],
      var.additional_security_group_ids,
    )
    // TODO: We assign a public IP address so that the service communicate
    // to all the services it needs (e.g., SSM and ECR). Eventually, we'd
    // like to shut down public access to the ecs service, but the work
    // around is tediuous.
    assign_public_ip = true
  }
}
",resource,26,26.0,4332ccbacf65427cb91e0e9b6f0c3ee53f37ea18,4332ccbacf65427cb91e0e9b6f0c3ee53f37ea18,https://github.com/rust-lang/simpleinfra/blob/4332ccbacf65427cb91e0e9b6f0c3ee53f37ea18/terragrunt/modules/ecs-service/main.tf#L26,https://github.com/rust-lang/simpleinfra/blob/4332ccbacf65427cb91e0e9b6f0c3ee53f37ea18/terragrunt/modules/ecs-service/main.tf#L26,2023-01-11 16:47:08+01:00,2023-01-11 16:47:08+01:00,1,0,1,1,0,1,1,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,343,azure/instances.tf,azure/instances.tf,0,todo,// TODO CHECK THIS group,// TODO CHECK THIS group,"resource ""azurerm_virtual_machine"" ""monitoring"" {
  name                  = ""${terraform.workspace}-monitoring""
  location              = var.az_region
  // TODO CHECK THIS group
  resource_group_name   = azurerm_resource_group.myrg.name
  // 
  network_interface_ids = [azurerm_network_interface.monitoring.id]
  availability_set_id   = azurerm_availability_set.myas.id
  vm_size               = ""Standard_D2s_v3""

  storage_os_disk {
    name              = ""iscsiOsDisk""
    caching           = ""ReadWrite""
    create_option     = ""FromImage""
    managed_disk_type = ""Premium_LRS""
  }

  storage_image_reference {
    id        = var.iscsi_srv_uri != """" ? join("","", azurerm_image.iscsi_srv.*.id) : """"
    publisher = var.iscsi_srv_uri != """" ? """" : ""SUSE""
    offer     = var.iscsi_srv_uri != """" ? """" : ""SLES-SAP-BYOS""
    sku       = var.iscsi_srv_uri != """" ? """" : ""12-sp4""
    version   = var.iscsi_srv_uri != """" ? """" : ""2019.03.06""
  }

  storage_data_disk {
    name              = ""iscsiDevices""
    caching           = ""ReadWrite""
    create_option     = ""Empty""
    disk_size_gb      = ""10""
    lun               = ""0""
    managed_disk_type = ""Standard_LRS""
  }

  os_profile {
    computer_name  = ""monitoring""
    admin_username = var.admin_user
  }

  os_profile_linux_config {
    disable_password_authentication = true

    ssh_keys {
      path     = ""/home/${var.admin_user}/.ssh/authorized_keys""
      key_data = file(var.public_key_location)
    }
  }

  boot_diagnostics {
    enabled     = ""true""
    storage_uri = azurerm_storage_account.mytfstorageacc.primary_blob_endpoint
  }

  tags = {
    workspace = terraform.workspace
  }
}
",resource,"resource ""azurerm_virtual_machine"" ""monitoring"" {
  name     = ""${terraform.workspace}-monitoring""
  location = var.az_region
  resource_group_name = azurerm_resource_group.myrg.name
  network_interface_ids = [azurerm_network_interface.monitoring.id]
  availability_set_id   = azurerm_availability_set.myas.id
  vm_size               = ""Standard_D2s_v3""

  storage_os_disk {
    name              = ""monitoringOsDisk""
    caching           = ""ReadWrite""
    create_option     = ""FromImage""
    managed_disk_type = ""Premium_LRS""
  }
  
  storage_image_reference {
    id        = azurerm_image.monitoring.0.id
    publisher = ""SUSE""
    offer     = ""SLES-SAP-BYOS""
    sku       = ""15""
    version   = ""2019.07.17""
  }

  storage_data_disk {
    name              = ""monitoringDevices""
    caching           = ""ReadWrite""
    create_option     = ""Empty""
    disk_size_gb      = ""10""
    lun               = ""0""
    managed_disk_type = ""Standard_LRS""
  }

  os_profile {
    computer_name  = ""monitoring""
    admin_username = var.admin_user
  }

  os_profile_linux_config {
    disable_password_authentication = true

    ssh_keys {
      path     = ""/home/${var.admin_user}/.ssh/authorized_keys""
      key_data = file(var.public_key_location)
    }
  }

  boot_diagnostics {
    enabled     = ""true""
    storage_uri = azurerm_storage_account.mytfstorageacc.primary_blob_endpoint
  }

  tags = {
    workspace = terraform.workspace
  }
}
",resource,138,,f41baea2a7a45b527e944b62bcab73612c693e02,9adcf152486838f9f5b600ead5e4ef373918a786,https://github.com/SUSE/ha-sap-terraform-deployments/blob/f41baea2a7a45b527e944b62bcab73612c693e02/azure/instances.tf#L138,https://github.com/SUSE/ha-sap-terraform-deployments/blob/9adcf152486838f9f5b600ead5e4ef373918a786/azure/instances.tf,2019-09-05 00:01:31+02:00,2019-09-05 18:08:13+02:00,5,1,1,0,0,1,0,0,0,1
https://github.com/pingcap/tidb-operator,55,deploy/modules/aliyun/tidb-operator/operator.tf,deploy/modules/aliyun/tidb-operator/operator.tf,0,hack,"# Hack, instruct terraform that the kubeconfig_filename is only available until the k8s created","# Hack, instruct terraform that the kubeconfig_filename is only available until the k8s created","data ""template_file"" ""kubeconfig_filename"" {
  template = var.kubeconfig_file
  vars = {
    kubernetes_depedency = alicloud_cs_managed_kubernetes.k8s.client_cert
  }
}
",data,"data ""template_file"" ""kubeconfig_filename"" {
  template = var.kubeconfig_file
  vars = {
    kubernetes_dependency = alicloud_cs_managed_kubernetes.k8s.client_cert
  }
}
",data,1,1.0,042b1a97fbbdf342297002990564828e6644a3f0,32e1f58b34ea891d653e436f263485578df5ce63,https://github.com/pingcap/tidb-operator/blob/042b1a97fbbdf342297002990564828e6644a3f0/deploy/modules/aliyun/tidb-operator/operator.tf#L1,https://github.com/pingcap/tidb-operator/blob/32e1f58b34ea891d653e436f263485578df5ce63/deploy/modules/aliyun/tidb-operator/operator.tf#L1,2019-07-23 19:44:58+08:00,2020-07-23 15:01:18+08:00,6,0,1,1,1,0,0,0,0,0
https://github.com/ministryofjustice/modernisation-platform,197,terraform/environments/data-platform-apps-and-tools/eks-iam-roles.tf,terraform/environments/data-platform-apps-and-tools/eks-iam-roles.tf,0,// todo,// TODO: define SecretsManager path for cluster consumed secrets,"// TODO: define SecretsManager path for cluster consumed secrets 
 // external_secrets_secrets_manager_arns = [] ","module ""external_secrets_role"" {
  #checkov:skip=CKV_TF_1:Module is from Terraform registry

  source  = ""terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks""
  version = ""~> 5.0""

  role_name_prefix               = ""external-secrets""
  attach_external_secrets_policy = true

  // TODO: define SecretsManager path for cluster consumed secrets
  // external_secrets_secrets_manager_arns = []

  oidc_providers = {
    main = {
      provider_arn               = module.eks.oidc_provider_arn
      namespace_service_accounts = [""${kubernetes_namespace.external_secrets.metadata[0].name}:external-secrets""]
    }
  }

  tags = local.tags
}
",module,,,128,0.0,90002b7667baf7c1eb23d0e49dcfa9d9eaee7567,951ffdb805c4255e3bb6d0a5febe5f1bb21407c9,https://github.com/ministryofjustice/modernisation-platform/blob/90002b7667baf7c1eb23d0e49dcfa9d9eaee7567/terraform/environments/data-platform-apps-and-tools/eks-iam-roles.tf#L128,https://github.com/ministryofjustice/modernisation-platform/blob/951ffdb805c4255e3bb6d0a5febe5f1bb21407c9/terraform/environments/data-platform-apps-and-tools/eks-iam-roles.tf#L0,2023-10-18 15:43:32+01:00,2023-12-19 15:49:47+00:00,4,2,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd,124,modules/gke-cloudbuild-private-pool/main.tf,modules/gke-cloudbuild-private-pool/main.tf,0,# todo,# TODO: for_each --> 1 module per destination GKE VPC,# TODO: for_each --> 1 module per destination GKE VPC,"module ""vpn_ha-2"" {
  # TODO: for_each --> 1 module per destination GKE VPC
  source     = ""terraform-google-modules/vpn/google//modules/vpn_ha""
  version    = ""~> 1.3.0""
  project_id = var.project_id
  region     = var.location
  network    = ""https://www.googleapis.com/compute/v1/projects/<PROJECT_ID>/global/networks/local-network"" ## TODO: GKE network self_link
  name       = ""gke-to-cloudbuild""
  router_asn = 64513
  peer_gcp_gateway = module.vpn_ha-1.self_link
  tunnels = {
    remote-0 = {
      bgp_peer = {
        address = ""169.254.1.2""
        asn     = 64514
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.1.1/30""
      ike_version       = 2
      vpn_gateway_interface = 0
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1.random_secret
    }
    remote-1 = {
      bgp_peer = {
        address = ""169.254.2.2""
        asn     = 64514
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.2.1/30""
      ike_version       = 2
      vpn_gateway_interface = 1
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1.random_secret
    }
  }
}",module,"module ""vpn_ha-2"" {
  count = length(local.gke_networks)

  source     = ""terraform-google-modules/vpn/google//modules/vpn_ha""
  version    = ""~> 1.3.0""
  project_id = local.gke_networks[count.index].project_id
  region     = local.gke_networks[count.index].location
  network    = local.gke_networks[count.index].network 
  name       = ""${local.gke_networks[count.index].network}-to-cloudbuild""
  router_asn = 65002+(count.index*2)
  peer_gcp_gateway = module.vpn_ha-1[count.index].self_link
  tunnels = {
    remote-0 = {
      bgp_peer = {
        address = ""169.254.${1+(count.index*2)}.1""
        asn     = 65001+(count.index*2)
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.${1+(count.index*2)}.2/30""
      ike_version       = 2
      vpn_gateway_interface = 0
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1[count.index].random_secret
    }
    remote-1 = {
      bgp_peer = {
        address = ""169.254.${2+(count.index*2)}.1""
        asn     = 65001+(count.index*2)
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.${2+(count.index*2)}.2/30""
      ike_version       = 2
      vpn_gateway_interface = 1
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1[count.index].random_secret
    }
  }
}",module,103,,a384bc29c9bcb80dd1b1f60ece9dee723b4bf378,e29ac91f2eedf8a48e82065434b81010d298a423,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/a384bc29c9bcb80dd1b1f60ece9dee723b4bf378/modules/gke-cloudbuild-private-pool/main.tf#L103,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/e29ac91f2eedf8a48e82065434b81010d298a423/modules/gke-cloudbuild-private-pool/main.tf,2021-11-30 12:07:36-06:00,2021-12-06 17:46:36-06:00,2,1,0,1,0,0,1,0,0,0
https://github.com/chanzuckerberg/cztack,4,aws-params-writer/variables.tf,aws-params-writer/variables.tf,0,// todo,// TODO(el): Remove once tf 0.12 is released,// TODO(el): Remove once tf 0.12 is released,"variable ""parameters_count"" {
  type        = ""string""
  description = ""HACK: The number of keys in var.parameters. To avoid hitting value of count cannot be computed.""
}
",variable,"variable ""parameters_count"" {
  type        = string
  description = ""HACK: The number of keys in var.parameters. To avoid hitting value of count cannot be computed.""
}
",variable,26,26.0,9d5798e3a0ff47602b7db6343dfd114cdbf5c8fa,9df439500dee7468643ca03a844cf7a5b1e1b313,https://github.com/chanzuckerberg/cztack/blob/9d5798e3a0ff47602b7db6343dfd114cdbf5c8fa/aws-params-writer/variables.tf#L26,https://github.com/chanzuckerberg/cztack/blob/9df439500dee7468643ca03a844cf7a5b1e1b313/aws-params-writer/variables.tf#L26,2019-04-22 16:11:24-07:00,2021-04-13 14:52:04-04:00,3,0,0,1,1,0,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,3,eks.tf,eks.tf,0,todo,# TODO Create New SecGroup for each node group,worker_security_group_id  = module.eks.worker_security_group_id # TODO Create New SecGroup for each node group,"module ""managed-node-groups"" {
  for_each = var.managed_node_groups

  source     = ""./modules/aws-eks-managed-node-groups""
  managed_ng = each.value

  eks_cluster_name          = module.eks.cluster_id
  private_subnet_ids        = var.create_vpc == false ? var.private_subnet_ids : module.vpc.private_subnets
  public_subnet_ids         = var.create_vpc == false ? var.public_subnet_ids : module.vpc.public_subnets
  cluster_ca_base64         = module.eks.cluster_certificate_authority_data
  cluster_endpoint          = module.eks.cluster_endpoint
  cluster_autoscaler_enable = var.cluster_autoscaler_enable
  worker_security_group_id  = module.eks.worker_security_group_id # TODO Create New SecGroup for each node group
  tags                      = module.eks-label.tags

  depends_on = [module.eks]

}
",module,the block associated got renamed or deleted,,145,,50f6e2c2dcd3479177d5c5001732c013be2fe6de,354afae1274daf6f39d01eafa51b7c87a3533cb4,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/50f6e2c2dcd3479177d5c5001732c013be2fe6de/eks.tf#L145,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/354afae1274daf6f39d01eafa51b7c87a3533cb4/eks.tf,2021-08-27 00:35:58+01:00,2021-08-27 17:16:49+01:00,2,1,1,1,0,1,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,329,examples/oke-network-only/main.tf,examples/rms/oke-network-only/main.tf,1,todo,# TODO Remaining tags in schema,defined_tags = { # TODO Remaining tags in schema,"module ""oke"" {
  source         = ""github.com/devoncrouse/terraform-oci-oke.git?ref=5.x-stack&depth=1""
  providers      = { oci.home = oci.home }
  tenancy_id     = var.tenancy_ocid
  compartment_id = var.compartment_ocid

  # Identity
  create_iam_resources     = true
  create_iam_tag_namespace = var.create_iam_tag_namespace
  create_iam_defined_tags  = var.create_iam_tag_namespace || var.create_iam_defined_tags
  use_defined_tags         = var.use_defined_tags
  tag_namespace            = var.tag_namespace

  # Network
  create_vcn                  = var.create_vcn
  vcn_id                      = var.vcn_id
  vcn_cidrs                   = split("","", var.vcn_cidrs)
  vcn_create_internet_gateway = var.vcn_create_internet_gateway ? ""always"" : ""never""
  vcn_create_nat_gateway      = var.vcn_create_nat_gateway ? ""always"" : ""never""
  vcn_create_service_gateway  = var.vcn_create_service_gateway ? ""always"" : ""never""
  vcn_name                    = var.vcn_name
  vcn_dns_label               = var.vcn_dns_label
  assign_dns                  = var.assign_dns
  ig_route_table_id           = var.ig_route_table_id
  local_peering_gateways      = var.local_peering_gateways
  lockdown_default_seclist    = var.lockdown_default_seclist
  nat_gateway_public_ip_id    = var.nat_gateway_public_ip_id
  nat_route_table_id          = var.nat_route_table_id
  create_drg                  = var.create_drg
  drg_id                      = var.drg_id
  drg_display_name            = var.drg_display_name

  subnets = {
    bastion  = { create = var.bastion_subnet_create ? ""always"" : ""never"", newbits = var.bastion_subnet_newbits, id = var.bastion_subnet_id }
    operator = { create = var.operator_subnet_create ? ""always"" : ""never"", newbits = var.operator_subnet_newbits, id = var.operator_subnet_id }
    cp       = { create = var.control_plane_subnet_create ? ""always"" : ""never"", newbits = var.control_plane_subnet_newbits, id = var.control_plane_subnet_id }
    int_lb   = { create = var.int_lb_subnet_create ? ""always"" : ""never"", newbits = var.int_lb_subnet_newbits, id = var.int_lb_subnet_id }
    pub_lb   = { create = var.pub_lb_subnet_create ? ""always"" : ""never"", newbits = var.pub_lb_subnet_newbits, id = var.pub_lb_subnet_id }
    workers  = { create = var.worker_subnet_create ? ""always"" : ""never"", newbits = var.worker_subnet_newbits, id = var.worker_subnet_id }
    pods     = { create = var.pod_subnet_create ? ""always"" : ""never"", newbits = var.pod_subnet_newbits, id = var.pod_subnet_id }
    fss      = { create = var.fss_subnet_create ? ""always"" : ""never"", newbits = var.fss_subnet_newbits, id = var.fss_subnet_id }
  }

  # Network Security
  create_nsgs                  = var.create_nsgs
  create_nsgs_always           = true
  allow_node_port_access       = var.allow_node_port_access
  allow_pod_internet_access    = var.allow_pod_internet_access
  allow_rules_internal_lb      = var.allow_rules_internal_lb
  allow_rules_public_lb        = var.allow_rules_public_lb
  allow_worker_internet_access = var.allow_worker_internet_access
  allow_worker_ssh_access      = var.allow_worker_ssh_access
  enable_waf                   = var.enable_waf
  bastion_allowed_cidrs        = compact(split("","", var.bastion_allowed_cidrs))
  bastion_nsg_ids              = compact(split("","", var.bastion_nsg_id))
  control_plane_allowed_cidrs  = compact(split("","", var.control_plane_allowed_cidrs))
  control_plane_is_public      = var.control_plane_is_public
  control_plane_nsg_ids        = compact(split("","", var.control_plane_nsg_id))
  fss_nsg_ids                  = compact(split("","", var.fss_nsg_id))
  load_balancers               = lower(var.load_balancers)
  operator_nsg_ids             = compact(split("","", var.operator_nsg_id))
  pod_nsg_ids                  = compact(split("","", var.pod_nsg_id))
  worker_is_public             = var.worker_is_public
  worker_nsg_ids               = compact(split("","", var.worker_nsg_id))

  # Bastion
  bastion_availability_domain = var.bastion_availability_domain
  bastion_image_id            = var.bastion_image_id
  bastion_image_os            = var.bastion_image_os
  bastion_image_os_version    = var.bastion_image_os_version
  bastion_image_type          = lower(var.bastion_image_type)
  bastion_is_public           = var.bastion_is_public
  bastion_shape               = var.bastion_shape
  bastion_upgrade             = var.bastion_upgrade
  bastion_user                = var.bastion_user
  create_bastion              = var.create_bastion

  # SSH
  ssh_public_key  = local.ssh_public_key
  ssh_private_key = sensitive(local.ssh_key_bundle_content)

  # Cluster
  create_cluster          = false
  preferred_load_balancer = lower(var.preferred_load_balancer)
  create_fss              = false
  create_operator         = false

  freeform_tags = { # TODO Remaining tags in schema
    cluster           = {}
    persistent_volume = {}
    service_lb        = {}
    workers           = {}
    bastion           = lookup(var.bastion_tags, ""freeformTags"", {})
    operator          = {}
    vcn               = {}
  }

  defined_tags = { # TODO Remaining tags in schema
    cluster           = {}
    persistent_volume = {}
    service_lb        = {}
    workers           = {}
    bastion           = lookup(var.bastion_tags, ""definedTags"", {})
    operator          = {}
    vcn               = {}
  }
}
",module,"module ""oke"" {
  source         = ""github.com/oracle-terraform-modules/terraform-oci-oke.git?ref=5.x&depth=1""
  providers      = { oci.home = oci.home }
  tenancy_id     = var.tenancy_ocid
  compartment_id = var.compartment_ocid

  # Identity
  create_iam_resources     = true
  create_iam_tag_namespace = var.create_iam_tag_namespace
  create_iam_defined_tags  = var.create_iam_tag_namespace || var.create_iam_defined_tags
  use_defined_tags         = var.use_defined_tags
  tag_namespace            = var.tag_namespace

  # Network
  create_vcn                  = var.create_vcn
  vcn_id                      = var.vcn_id
  vcn_cidrs                   = split("","", var.vcn_cidrs)
  vcn_create_internet_gateway = var.vcn_create_internet_gateway ? ""always"" : ""never""
  vcn_create_nat_gateway      = var.vcn_create_nat_gateway ? ""always"" : ""never""
  vcn_create_service_gateway  = var.vcn_create_service_gateway ? ""always"" : ""never""
  vcn_name                    = var.vcn_name
  vcn_dns_label               = var.vcn_dns_label
  assign_dns                  = var.assign_dns
  ig_route_table_id           = var.ig_route_table_id
  local_peering_gateways      = var.local_peering_gateways
  lockdown_default_seclist    = var.lockdown_default_seclist
  create_drg                  = var.create_drg
  drg_id                      = var.drg_id
  drg_display_name            = var.drg_display_name

  subnets = {
    bastion = {
      create  = var.bastion_subnet_create ? ""always"" : ""never"",
      newbits = var.bastion_subnet_newbits,
      id      = var.bastion_subnet_id
    }

    operator = {
      create  = var.operator_subnet_create ? ""always"" : ""never"",
      newbits = var.operator_subnet_newbits,
      id      = var.operator_subnet_id
    }

    cp = {
      create  = var.control_plane_subnet_create ? ""always"" : ""never"",
      newbits = var.control_plane_subnet_newbits,
      id      = var.control_plane_subnet_id
    }

    int_lb = {
      create  = var.int_lb_subnet_create ? ""always"" : ""never"",
      newbits = var.int_lb_subnet_newbits,
      id      = var.int_lb_subnet_id
    }

    pub_lb = {
      create  = var.pub_lb_subnet_create ? ""always"" : ""never"",
      newbits = var.pub_lb_subnet_newbits,
      id      = var.pub_lb_subnet_id
    }

    workers = {
      create  = var.worker_subnet_create ? ""always"" : ""never"",
      newbits = var.worker_subnet_newbits,
      id      = var.worker_subnet_id
    }

    pods = {
      create  = var.pod_subnet_create ? ""always"" : ""never"",
      newbits = var.pod_subnet_newbits,
      id      = var.pod_subnet_id
    }
  }

  # Network Security
  nsgs = {
    bastion  = { create = var.create_nsgs ? ""always"" : ""never"" }
    operator = { create = var.create_nsgs ? ""always"" : ""never"" }
    cp       = { create = var.create_nsgs ? ""always"" : ""never"" }
    int_lb   = { create = var.create_nsgs ? ""always"" : ""never"" }
    pub_lb   = { create = var.create_nsgs ? ""always"" : ""never"" }
    workers  = { create = var.create_nsgs ? ""always"" : ""never"" }
    pods     = { create = var.create_nsgs ? ""always"" : ""never"" }
  }

  allow_node_port_access       = var.allow_node_port_access
  allow_pod_internet_access    = var.allow_pod_internet_access
  allow_rules_internal_lb      = var.allow_rules_internal_lb
  allow_rules_public_lb        = var.allow_rules_public_lb
  allow_worker_internet_access = var.allow_worker_internet_access
  allow_worker_ssh_access      = var.allow_worker_ssh_access
  enable_waf                   = var.enable_waf
  bastion_allowed_cidrs        = compact(split("","", var.bastion_allowed_cidrs))
  control_plane_allowed_cidrs  = compact(split("","", var.control_plane_allowed_cidrs))
  control_plane_is_public      = var.control_plane_is_public
  load_balancers               = lower(var.load_balancers)
  worker_is_public             = var.worker_is_public

  # Bastion
  bastion_availability_domain = var.bastion_availability_domain
  bastion_image_id            = var.bastion_image_id
  bastion_image_os            = var.bastion_image_os
  bastion_image_os_version    = var.bastion_image_os_version
  bastion_image_type          = lower(var.bastion_image_type)
  bastion_is_public           = var.bastion_is_public
  bastion_shape               = var.bastion_shape
  bastion_upgrade             = var.bastion_upgrade
  bastion_user                = var.bastion_user
  create_bastion              = var.create_bastion

  # SSH
  ssh_public_key  = local.ssh_public_key
  ssh_private_key = sensitive(local.ssh_key_bundle_content)

  # Cluster
  create_cluster          = false
  preferred_load_balancer = lower(var.preferred_load_balancer)
  create_operator         = false

  freeform_tags = { # TODO Remaining tags in schema
    bastion = lookup(var.bastion_tags, ""freeformTags"", {})
    vcn     = {}
  }

  defined_tags = { # TODO Remaining tags in schema
    bastion = lookup(var.bastion_tags, ""definedTags"", {})
    vcn     = {}
  }
}
",module,119,128.0,ba160d1800dbef893f2535db1e2e00a51df7238c,8c36e1160ec67dbd04cbe6a5e9fccf1fdce9a372,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/ba160d1800dbef893f2535db1e2e00a51df7238c/examples/oke-network-only/main.tf#L119,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/8c36e1160ec67dbd04cbe6a5e9fccf1fdce9a372/examples/rms/oke-network-only/main.tf#L128,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,4,0,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1437,blueprints/data-solutions/bq-ml/main.tf,blueprints/data-solutions/bq-ml/main.tf,0,#todo,#TODO Check/Implement P4SA logic for IAM role,"#TODO Check/Implement P4SA logic for IAM role 
 # encryption_spec { 
 #   kms_key_name = var.service_encryption_keys.ai_metadata_store 
 # }","resource ""google_vertex_ai_metadata_store"" ""store"" {
  provider    = google-beta
  project     = module.project.project_id
  name        = ""${var.prefix}-metadata-store""
  description = ""Vertex Ai Metadata Store""
  region      = var.region
  #TODO Check/Implement P4SA logic for IAM role
  # encryption_spec {
  #   kms_key_name = var.service_encryption_keys.ai_metadata_store
  # }
}
",resource,the block associated got renamed or deleted,,174,,50856e6951763237be2133781acb4a7714bc8c72,9e19f8960861fe61830801eab27111422f1d7a4e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/50856e6951763237be2133781acb4a7714bc8c72/blueprints/data-solutions/bq-ml/main.tf#L174,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9e19f8960861fe61830801eab27111422f1d7a4e/blueprints/data-solutions/bq-ml/main.tf,2023-02-23 18:36:03+01:00,2023-03-05 22:02:41+01:00,4,1,1,1,0,1,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,140,local.tf,locals.tf,1,fix,"# Windows nodes are available from k8s 1.14. If cluster version is less than 1.14, fix ami filter to some constant to not fail on 'terraform plan'.","# Windows nodes are available from k8s 1.14. If cluster version is less than 1.14, fix ami filter to some constant to not fail on 'terraform plan'.","locals {
  asg_tags = [
    for item in keys(var.tags) :
    map(
      ""key"", item,
      ""value"", element(values(var.tags), index(keys(var.tags), item)),
      ""propagate_at_launch"", ""true""
    )
  ]

  cluster_security_group_id = var.cluster_create_security_group ? join("""", aws_security_group.cluster.*.id) : var.cluster_security_group_id
  cluster_iam_role_name     = var.manage_cluster_iam_resources ? join("""", aws_iam_role.cluster.*.name) : var.cluster_iam_role_name
  cluster_iam_role_arn      = var.manage_cluster_iam_resources ? join("""", aws_iam_role.cluster.*.arn) : join("""", data.aws_iam_role.custom_cluster_iam_role.*.arn)
  worker_security_group_id  = var.worker_create_security_group ? join("""", aws_security_group.workers.*.id) : var.worker_security_group_id

  default_iam_role_id    = concat(aws_iam_role.workers.*.id, [""""])[0]
  default_ami_id_linux   = coalesce(local.workers_group_defaults.ami_id, data.aws_ami.eks_worker.id)
  default_ami_id_windows = coalesce(local.workers_group_defaults.ami_id_windows, data.aws_ami.eks_worker_windows.id)

  kubeconfig_name = var.kubeconfig_name == """" ? ""eks_${var.cluster_name}"" : var.kubeconfig_name

  worker_group_count                 = length(var.worker_groups)
  worker_group_launch_template_count = length(var.worker_groups_launch_template)

  worker_ami_name_filter = var.worker_ami_name_filter != """" ? var.worker_ami_name_filter : ""amazon-eks-node-${var.cluster_version}-v*""
  # Windows nodes are available from k8s 1.14. If cluster version is less than 1.14, fix ami filter to some constant to not fail on 'terraform plan'.
  worker_ami_name_filter_windows = (var.worker_ami_name_filter_windows != """" ?
    var.worker_ami_name_filter_windows : ""Windows_Server-2019-English-Core-EKS_Optimized-${tonumber(var.cluster_version) >= 1.14 ? var.cluster_version : 1.14}-*""
  )

  ec2_principal = ""ec2.${data.aws_partition.current.dns_suffix}""

  policy_arn_prefix = ""arn:${data.aws_partition.current.partition}:iam::aws:policy""
  workers_group_defaults_defaults = {
    name                          = ""count.index""               # Name of the worker group. Literal count.index will never be used but if name is not set, the count.index interpolation will be used.
    tags                          = []                          # A list of map defining extra tags to be applied to the worker group autoscaling group.
    ami_id                        = """"                          # AMI ID for the eks linux based workers. If none is provided, Terraform will search for the latest version of their EKS optimized worker AMI based on platform.
    ami_id_windows                = """"                          # AMI ID for the eks windows based workers. If none is provided, Terraform will search for the latest version of their EKS optimized worker AMI based on platform.
    asg_desired_capacity          = ""1""                         # Desired worker capacity in the autoscaling group and changing its value will not affect the autoscaling group's desired capacity because the cluster-autoscaler manages up and down scaling of the nodes. Cluster-autoscaler add nodes when pods are in pending state and remove the nodes when they are not required by modifying the desirec_capacity of the autoscaling group. Although an issue exists in which if the value of the asg_min_size is changed it modifies the value of asg_desired_capacity.
    asg_max_size                  = ""3""                         # Maximum worker capacity in the autoscaling group.
    asg_min_size                  = ""1""                         # Minimum worker capacity in the autoscaling group. NOTE: Change in this paramater will affect the asg_desired_capacity, like changing its value to 2 will change asg_desired_capacity value to 2 but bringing back it to 1 will not affect the asg_desired_capacity.
    asg_force_delete              = false                       # Enable forced deletion for the autoscaling group.
    asg_initial_lifecycle_hooks   = []                          # Initital lifecycle hook for the autoscaling group.
    asg_recreate_on_change        = false                       # Recreate the autoscaling group when the Launch Template or Launch Configuration change.
    default_cooldown              = null                        # The amount of time, in seconds, after a scaling activity completes before another scaling activity can start.
    health_check_grace_period     = null                        # Time in seconds after instance comes into service before checking health.
    instance_type                 = ""m4.large""                  # Size of the workers instances.
    spot_price                    = """"                          # Cost of spot instance.
    placement_tenancy             = """"                          # The tenancy of the instance. Valid values are ""default"" or ""dedicated"".
    root_volume_size              = ""100""                       # root volume size of workers instances.
    root_volume_type              = ""gp2""                       # root volume type of workers instances, can be 'standard', 'gp2', or 'io1'
    root_iops                     = ""0""                         # The amount of provisioned IOPS. This must be set with a volume_type of ""io1"".
    key_name                      = """"                          # The key name that should be used for the instances in the autoscaling group
    pre_userdata                  = """"                          # userdata to pre-append to the default userdata.
    userdata_template_file        = """"                          # alternate template to use for userdata
    userdata_template_extra_args  = {}                          # Additional arguments to use when expanding the userdata template file
    bootstrap_extra_args          = """"                          # Extra arguments passed to the bootstrap.sh script from the EKS AMI (Amazon Machine Image).
    additional_userdata           = """"                          # userdata to append to the default userdata.
    ebs_optimized                 = true                        # sets whether to use ebs optimization on supported types.
    enable_monitoring             = true                        # Enables/disables detailed monitoring.
    public_ip                     = false                       # Associate a public ip address with a worker
    kubelet_extra_args            = """"                          # This string is passed directly to kubelet if set. Useful for adding labels or taints.
    subnets                       = var.subnets                 # A list of subnets to place the worker nodes in. i.e. [""subnet-123"", ""subnet-456"", ""subnet-789""]
    additional_security_group_ids = []                          # A list of additional security group ids to include in worker launch config
    protect_from_scale_in         = false                       # Prevent AWS from scaling in, so that cluster-autoscaler is solely responsible.
    iam_instance_profile_name     = """"                          # A custom IAM instance profile name. Used when manage_worker_iam_resources is set to false. Incompatible with iam_role_id.
    iam_role_id                   = ""local.default_iam_role_id"" # A custom IAM role id. Incompatible with iam_instance_profile_name.  Literal local.default_iam_role_id will never be used but if iam_role_id is not set, the local.default_iam_role_id interpolation will be used.
    suspended_processes           = [""AZRebalance""]             # A list of processes to suspend. i.e. [""AZRebalance"", ""HealthCheck"", ""ReplaceUnhealthy""]
    target_group_arns             = null                        # A list of Application LoadBalancer (ALB) target group ARNs to be associated to the autoscaling group
    enabled_metrics               = []                          # A list of metrics to be collected i.e. [""GroupMinSize"", ""GroupMaxSize"", ""GroupDesiredCapacity""]
    placement_group               = """"                          # The name of the placement group into which to launch the instances, if any.
    service_linked_role_arn       = """"                          # Arn of custom service linked role that Auto Scaling group will use. Useful when you have encrypted EBS
    termination_policies          = []                          # A list of policies to decide how the instances in the auto scale group should be terminated.
    platform                      = ""linux""                     # Platform of workers. either ""linux"" or ""windows""
    additional_ebs_volumes        = []                          # A list of additional volumes to be attached to the instances on this Auto Scaling group. Each volume should be an object with the following: block_device_name (required), volume_size, volume_type, iops, encrypted, kms_key_id (only on launch-template), delete_on_termination. Optional values are grabbed from root volume or from defaults
    # Settings for launch templates
    root_block_device_name            = data.aws_ami.eks_worker.root_device_name # Root device name for workers. If non is provided, will assume default AMI was used.
    root_kms_key_id                   = """"                                       # The KMS key to use when encrypting the root storage device
    launch_template_version           = ""$Latest""                                # The lastest version of the launch template to use in the autoscaling group
    launch_template_placement_tenancy = ""default""                                # The placement tenancy for instances
    launch_template_placement_group   = """"                                       # The name of the placement group into which to launch the instances, if any.
    root_encrypted                    = false                                    # Whether the volume should be encrypted or not
    eni_delete                        = true                                     # Delete the Elastic Network Interface (ENI) on termination (if set to false you will have to manually delete before destroying)
    cpu_credits                       = ""standard""                               # T2/T3 unlimited mode, can be 'standard' or 'unlimited'. Used 'standard' mode as default to avoid paying higher costs
    market_type                       = null
    # Settings for launch templates with mixed instances policy
    override_instance_types                  = [""m5.large"", ""m5a.large"", ""m5d.large"", ""m5ad.large""] # A list of override instance types for mixed instances policy
    on_demand_allocation_strategy            = null                                                 # Strategy to use when launching on-demand instances. Valid values: prioritized.
    on_demand_base_capacity                  = ""0""                                                  # Absolute minimum amount of desired capacity that must be fulfilled by on-demand instances
    on_demand_percentage_above_base_capacity = ""0""                                                  # Percentage split between on-demand and Spot instances above the base on-demand capacity
    spot_allocation_strategy                 = ""lowest-price""                                       # Valid options are 'lowest-price' and 'capacity-optimized'. If 'lowest-price', the Auto Scaling group launches instances using the Spot pools with the lowest price, and evenly allocates your instances across the number of Spot pools. If 'capacity-optimized', the Auto Scaling group launches instances using Spot pools that are optimally chosen based on the available Spot capacity.
    spot_instance_pools                      = 10                                                   # ""Number of Spot pools per availability zone to allocate capacity. EC2 Auto Scaling selects the cheapest Spot pools and evenly allocates Spot capacity across the number of Spot pools that you specify.""
    spot_max_price                           = """"                                                   # Maximum price per unit hour that the user is willing to pay for the Spot instances. Default is the on-demand price
    max_instance_lifetime                    = 0                                                    # Maximum number of seconds instances can run in the ASG. 0 is unlimited.
  }

  workers_group_defaults = merge(
    local.workers_group_defaults_defaults,
    var.workers_group_defaults,
  )

  ebs_optimized_not_supported = [
    ""c1.medium"",
    ""c3.8xlarge"",
    ""c3.large"",
    ""c5d.12xlarge"",
    ""c5d.24xlarge"",
    ""c5d.metal"",
    ""cc2.8xlarge"",
    ""cr1.8xlarge"",
    ""g2.8xlarge"",
    ""g4dn.metal"",
    ""hs1.8xlarge"",
    ""i2.8xlarge"",
    ""m1.medium"",
    ""m1.small"",
    ""m2.xlarge"",
    ""m3.large"",
    ""m3.medium"",
    ""m5ad.16xlarge"",
    ""m5ad.8xlarge"",
    ""m5dn.metal"",
    ""m5n.metal"",
    ""r3.8xlarge"",
    ""r3.large"",
    ""r5ad.16xlarge"",
    ""r5ad.8xlarge"",
    ""r5dn.metal"",
    ""r5n.metal"",
    ""t1.micro"",
    ""t2.2xlarge"",
    ""t2.large"",
    ""t2.medium"",
    ""t2.micro"",
    ""t2.nano"",
    ""t2.small"",
    ""t2.xlarge""
  ]

  kubeconfig = var.create_eks ? templatefile(""${path.module}/templates/kubeconfig.tpl"", {
    kubeconfig_name                   = local.kubeconfig_name
    endpoint                          = aws_eks_cluster.this[0].endpoint
    cluster_auth_base64               = aws_eks_cluster.this[0].certificate_authority[0].data
    aws_authenticator_command         = var.kubeconfig_aws_authenticator_command
    aws_authenticator_command_args    = length(var.kubeconfig_aws_authenticator_command_args) > 0 ? var.kubeconfig_aws_authenticator_command_args : [""token"", ""-i"", aws_eks_cluster.this[0].name]
    aws_authenticator_additional_args = var.kubeconfig_aws_authenticator_additional_args
    aws_authenticator_env_variables   = var.kubeconfig_aws_authenticator_env_variables
  }) : """"

  userdata = [for worker in var.worker_groups : templatefile(
    lookup(
      worker,
      ""userdata_template_file"",
      lookup(worker, ""platform"", local.workers_group_defaults[""platform""]) == ""windows""
      ? ""${path.module}/templates/userdata_windows.tpl""
      : ""${path.module}/templates/userdata.sh.tpl""
    ),
    merge(
      {
        platform            = lookup(worker, ""platform"", local.workers_group_defaults[""platform""])
        cluster_name        = aws_eks_cluster.this[0].name
        endpoint            = aws_eks_cluster.this[0].endpoint
        cluster_auth_base64 = aws_eks_cluster.this[0].certificate_authority[0].data
        pre_userdata = lookup(
          worker,
          ""pre_userdata"",
          local.workers_group_defaults[""pre_userdata""],
        )
        additional_userdata = lookup(
          worker,
          ""additional_userdata"",
          local.workers_group_defaults[""additional_userdata""],
        )
        bootstrap_extra_args = lookup(
          worker,
          ""bootstrap_extra_args"",
          local.workers_group_defaults[""bootstrap_extra_args""],
        )
        kubelet_extra_args = lookup(
          worker,
          ""kubelet_extra_args"",
          local.workers_group_defaults[""kubelet_extra_args""],
        )
      },
      lookup(
        worker,
        ""userdata_template_extra_args"",
        local.workers_group_defaults[""userdata_template_extra_args""]
      )
    )
    ) if var.create_eks
  ]

  launch_template_userdata = [for worker in var.worker_groups_launch_template : templatefile(
    lookup(
      worker,
      ""userdata_template_file"",
      lookup(worker, ""platform"", local.workers_group_defaults[""platform""]) == ""windows""
      ? ""${path.module}/templates/userdata_windows.tpl""
      : ""${path.module}/templates/userdata.sh.tpl""
    ),
    merge(
      {
        platform            = lookup(worker, ""platform"", local.workers_group_defaults[""platform""])
        cluster_name        = aws_eks_cluster.this[0].name
        endpoint            = aws_eks_cluster.this[0].endpoint
        cluster_auth_base64 = aws_eks_cluster.this[0].certificate_authority[0].data
        pre_userdata = lookup(
          worker,
          ""pre_userdata"",
          local.workers_group_defaults[""pre_userdata""],
        )
        additional_userdata = lookup(
          worker,
          ""additional_userdata"",
          local.workers_group_defaults[""additional_userdata""],
        )
        bootstrap_extra_args = lookup(
          worker,
          ""bootstrap_extra_args"",
          local.workers_group_defaults[""bootstrap_extra_args""],
        )
        kubelet_extra_args = lookup(
          worker,
          ""kubelet_extra_args"",
          local.workers_group_defaults[""kubelet_extra_args""],
        )
      },
      lookup(
        worker,
        ""userdata_template_extra_args"",
        local.workers_group_defaults[""userdata_template_extra_args""]
      )
    )
    ) if var.create_eks
  ]
}
",locals,"locals {

  # EKS Cluster
  cluster_id                        = coalescelist(aws_eks_cluster.this[*].id, [""""])[0]
  cluster_arn                       = coalescelist(aws_eks_cluster.this[*].arn, [""""])[0]
  cluster_name                      = coalescelist(aws_eks_cluster.this[*].name, [""""])[0]
  cluster_endpoint                  = coalescelist(aws_eks_cluster.this[*].endpoint, [""""])[0]
  cluster_auth_base64               = coalescelist(aws_eks_cluster.this[*].certificate_authority[0].data, [""""])[0]
  cluster_oidc_issuer_url           = flatten(concat(aws_eks_cluster.this[*].identity[*].oidc[0].issuer, [""""]))[0]
  cluster_primary_security_group_id = coalescelist(aws_eks_cluster.this[*].vpc_config[0].cluster_security_group_id, [""""])[0]

  cluster_security_group_id = var.cluster_create_security_group ? join("""", aws_security_group.cluster.*.id) : var.cluster_security_group_id
  cluster_iam_role_name     = var.manage_cluster_iam_resources ? join("""", aws_iam_role.cluster.*.name) : var.cluster_iam_role_name
  cluster_iam_role_arn      = var.manage_cluster_iam_resources ? join("""", aws_iam_role.cluster.*.arn) : join("""", data.aws_iam_role.custom_cluster_iam_role.*.arn)

  # Worker groups
  worker_security_group_id = var.worker_create_security_group ? join("""", aws_security_group.workers.*.id) : var.worker_security_group_id

  default_iam_role_id    = concat(aws_iam_role.workers.*.id, [""""])[0]
  default_ami_id_linux   = local.workers_group_defaults.ami_id != """" ? local.workers_group_defaults.ami_id : concat(data.aws_ami.eks_worker.*.id, [""""])[0]
  default_ami_id_windows = local.workers_group_defaults.ami_id_windows != """" ? local.workers_group_defaults.ami_id_windows : concat(data.aws_ami.eks_worker_windows.*.id, [""""])[0]

  worker_group_launch_configuration_count = length(var.worker_groups)
  worker_group_launch_template_count      = length(var.worker_groups_launch_template)

  worker_groups_platforms = [for x in concat(var.worker_groups, var.worker_groups_launch_template) : try(x.platform, var.workers_group_defaults[""platform""], var.default_platform)]

  worker_ami_name_filter         = coalesce(var.worker_ami_name_filter, ""amazon-eks-node-${coalesce(var.cluster_version, ""cluster_version"")}-v*"")
  worker_ami_name_filter_windows = coalesce(var.worker_ami_name_filter_windows, ""Windows_Server-2019-English-Core-EKS_Optimized-${coalesce(var.cluster_version, ""cluster_version"")}-*"")

  ec2_principal     = ""ec2.${data.aws_partition.current.dns_suffix}""
  sts_principal     = ""sts.${data.aws_partition.current.dns_suffix}""
  client_id_list    = distinct(compact(concat([local.sts_principal], var.openid_connect_audiences)))
  policy_arn_prefix = ""arn:${data.aws_partition.current.partition}:iam::aws:policy""

  workers_group_defaults_defaults = {
    name                              = ""count.index""               # Name of the worker group. Literal count.index will never be used but if name is not set, the count.index interpolation will be used.
    tags                              = []                          # A list of maps defining extra tags to be applied to the worker group autoscaling group and volumes.
    ami_id                            = """"                          # AMI ID for the eks linux based workers. If none is provided, Terraform will search for the latest version of their EKS optimized worker AMI based on platform.
    ami_id_windows                    = """"                          # AMI ID for the eks windows based workers. If none is provided, Terraform will search for the latest version of their EKS optimized worker AMI based on platform.
    asg_desired_capacity              = ""1""                         # Desired worker capacity in the autoscaling group and changing its value will not affect the autoscaling group's desired capacity because the cluster-autoscaler manages up and down scaling of the nodes. Cluster-autoscaler add nodes when pods are in pending state and remove the nodes when they are not required by modifying the desired_capacity of the autoscaling group. Although an issue exists in which if the value of the asg_min_size is changed it modifies the value of asg_desired_capacity.
    asg_max_size                      = ""3""                         # Maximum worker capacity in the autoscaling group.
    asg_min_size                      = ""1""                         # Minimum worker capacity in the autoscaling group. NOTE: Change in this paramater will affect the asg_desired_capacity, like changing its value to 2 will change asg_desired_capacity value to 2 but bringing back it to 1 will not affect the asg_desired_capacity.
    asg_force_delete                  = false                       # Enable forced deletion for the autoscaling group.
    asg_initial_lifecycle_hooks       = []                          # Initital lifecycle hook for the autoscaling group.
    default_cooldown                  = null                        # The amount of time, in seconds, after a scaling activity completes before another scaling activity can start.
    health_check_type                 = null                        # Controls how health checking is done. Valid values are ""EC2"" or ""ELB"".
    health_check_grace_period         = null                        # Time in seconds after instance comes into service before checking health.
    instance_type                     = ""m4.large""                  # Size of the workers instances.
    instance_store_virtual_name       = ""ephemeral0""                # ""virtual_name"" of the instance store volume.
    spot_price                        = """"                          # Cost of spot instance.
    placement_tenancy                 = """"                          # The tenancy of the instance. Valid values are ""default"" or ""dedicated"".
    root_volume_size                  = ""100""                       # root volume size of workers instances.
    root_volume_type                  = ""gp2""                       # root volume type of workers instances, can be ""standard"", ""gp3"", ""gp2"", or ""io1""
    root_iops                         = ""0""                         # The amount of provisioned IOPS. This must be set with a volume_type of ""io1"".
    root_volume_throughput            = null                        # The amount of throughput to provision for a gp3 volume.
    key_name                          = """"                          # The key pair name that should be used for the instances in the autoscaling group
    pre_userdata                      = """"                          # userdata to pre-append to the default userdata.
    userdata_template_file            = """"                          # alternate template to use for userdata
    userdata_template_extra_args      = {}                          # Additional arguments to use when expanding the userdata template file
    bootstrap_extra_args              = """"                          # Extra arguments passed to the bootstrap.sh script from the EKS AMI (Amazon Machine Image).
    additional_userdata               = """"                          # userdata to append to the default userdata.
    ebs_optimized                     = true                        # sets whether to use ebs optimization on supported types.
    enable_monitoring                 = true                        # Enables/disables detailed monitoring.
    enclave_support                   = false                       # Enables/disables enclave support
    public_ip                         = false                       # Associate a public ip address with a worker
    kubelet_extra_args                = """"                          # This string is passed directly to kubelet if set. Useful for adding labels or taints.
    subnets                           = var.subnets                 # A list of subnets to place the worker nodes in. i.e. [""subnet-123"", ""subnet-456"", ""subnet-789""]
    additional_security_group_ids     = []                          # A list of additional security group ids to include in worker launch config
    protect_from_scale_in             = false                       # Prevent AWS from scaling in, so that cluster-autoscaler is solely responsible.
    iam_instance_profile_name         = """"                          # A custom IAM instance profile name. Used when manage_worker_iam_resources is set to false. Incompatible with iam_role_id.
    iam_role_id                       = ""local.default_iam_role_id"" # A custom IAM role id. Incompatible with iam_instance_profile_name.  Literal local.default_iam_role_id will never be used but if iam_role_id is not set, the local.default_iam_role_id interpolation will be used.
    suspended_processes               = [""AZRebalance""]             # A list of processes to suspend. i.e. [""AZRebalance"", ""HealthCheck"", ""ReplaceUnhealthy""]
    target_group_arns                 = null                        # A list of Application LoadBalancer (ALB) target group ARNs to be associated to the autoscaling group
    load_balancers                    = null                        # A list of Classic LoadBalancer (CLB)'s name to be associated to the autoscaling group
    enabled_metrics                   = []                          # A list of metrics to be collected i.e. [""GroupMinSize"", ""GroupMaxSize"", ""GroupDesiredCapacity""]
    placement_group                   = null                        # The name of the placement group into which to launch the instances, if any.
    service_linked_role_arn           = """"                          # Arn of custom service linked role that Auto Scaling group will use. Useful when you have encrypted EBS
    termination_policies              = []                          # A list of policies to decide how the instances in the auto scale group should be terminated.
    platform                          = var.default_platform        # Platform of workers. Either ""linux"" or ""windows"".
    additional_ebs_volumes            = []                          # A list of additional volumes to be attached to the instances on this Auto Scaling group. Each volume should be an object with the following: block_device_name (required), volume_size, volume_type, iops, throughput, encrypted, kms_key_id (only on launch-template), delete_on_termination. Optional values are grabbed from root volume or from defaults
    additional_instance_store_volumes = []                          # A list of additional instance store (local disk) volumes to be attached to the instances on this Auto Scaling group. Each volume should be an object with the following: block_device_name (required), virtual_name.
    warm_pool                         = null                        # If this block is configured, add a Warm Pool to the specified Auto Scaling group.
    timeouts                          = {}                          # A map of timeouts for create/update/delete operations

    # Settings for launch templates
    root_block_device_name               = concat(data.aws_ami.eks_worker.*.root_device_name, [""""])[0]         # Root device name for Linux workers. If not provided, will assume default Linux AMI was used.
    root_block_device_name_windows       = concat(data.aws_ami.eks_worker_windows.*.root_device_name, [""""])[0] # Root device name for Windows workers. If not provided, will assume default Windows AMI was used.
    root_kms_key_id                      = """"                                                                  # The KMS key to use when encrypting the root storage device
    launch_template_id                   = null                                                                # The id of the launch template used for managed node_groups
    launch_template_version              = ""$Latest""                                                           # The latest version of the launch template to use in the autoscaling group
    update_default_version               = false                                                               # Update the autoscaling group launch template's default version upon each update
    launch_template_placement_tenancy    = ""default""                                                           # The placement tenancy for instances
    launch_template_placement_group      = null                                                                # The name of the placement group into which to launch the instances, if any.
    root_encrypted                       = false                                                               # Whether the volume should be encrypted or not
    eni_delete                           = true                                                                # Delete the Elastic Network Interface (ENI) on termination (if set to false you will have to manually delete before destroying)
    interface_type                       = null                                                                # The type of network interface. To create an Elastic Fabric Adapter (EFA), specify 'efa'.
    cpu_credits                          = ""standard""                                                          # T2/T3 unlimited mode, can be 'standard' or 'unlimited'. Used 'standard' mode as default to avoid paying higher costs
    market_type                          = null
    metadata_http_endpoint               = ""enabled""  # The state of the metadata service: enabled, disabled.
    metadata_http_tokens                 = ""optional"" # If session tokens are required: optional, required.
    metadata_http_put_response_hop_limit = null       # The desired HTTP PUT response hop limit for instance metadata requests.
    # Settings for launch templates with mixed instances policy
    override_instance_types                  = [""m5.large"", ""m5a.large"", ""m5d.large"", ""m5ad.large""] # A list of override instance types for mixed instances policy
    on_demand_allocation_strategy            = null                                                 # Strategy to use when launching on-demand instances. Valid values: prioritized.
    on_demand_base_capacity                  = ""0""                                                  # Absolute minimum amount of desired capacity that must be fulfilled by on-demand instances
    on_demand_percentage_above_base_capacity = ""0""                                                  # Percentage split between on-demand and Spot instances above the base on-demand capacity
    spot_allocation_strategy                 = ""lowest-price""                                       # Valid options are 'lowest-price' and 'capacity-optimized'. If 'lowest-price', the Auto Scaling group launches instances using the Spot pools with the lowest price, and evenly allocates your instances across the number of Spot pools. If 'capacity-optimized', the Auto Scaling group launches instances using Spot pools that are optimally chosen based on the available Spot capacity.
    spot_instance_pools                      = 10                                                   # ""Number of Spot pools per availability zone to allocate capacity. EC2 Auto Scaling selects the cheapest Spot pools and evenly allocates Spot capacity across the number of Spot pools that you specify.""
    spot_max_price                           = """"                                                   # Maximum price per unit hour that the user is willing to pay for the Spot instances. Default is the on-demand price
    max_instance_lifetime                    = 0                                                    # Maximum number of seconds instances can run in the ASG. 0 is unlimited.
    elastic_inference_accelerator            = null                                                 # Type of elastic inference accelerator to be attached. Example values are eia1.medium, eia2.large, etc.
    instance_refresh_enabled                 = false                                                # Enable instance refresh for the worker autoscaling group.
    instance_refresh_strategy                = ""Rolling""                                            # Strategy to use for instance refresh. Default is 'Rolling' which the only valid value.
    instance_refresh_min_healthy_percentage  = 90                                                   # The amount of capacity in the ASG that must remain healthy during an instance refresh, as a percentage of the ASG's desired capacity.
    instance_refresh_instance_warmup         = null                                                 # The number of seconds until a newly launched instance is configured and ready to use. Defaults to the ASG's health check grace period.
    instance_refresh_triggers                = []                                                   # Set of additional property names that will trigger an Instance Refresh. A refresh will always be triggered by a change in any of launch_configuration, launch_template, or mixed_instances_policy.
    capacity_rebalance                       = false                                                # Enable capacity rebalance
  }

  workers_group_defaults = merge(
    local.workers_group_defaults_defaults,
    var.workers_group_defaults,
  )

  ebs_optimized_not_supported = [
    ""c1.medium"",
    ""c3.8xlarge"",
    ""c3.large"",
    ""c5d.12xlarge"",
    ""c5d.24xlarge"",
    ""c5d.metal"",
    ""cc2.8xlarge"",
    ""cr1.8xlarge"",
    ""g2.8xlarge"",
    ""g4dn.metal"",
    ""hs1.8xlarge"",
    ""i2.8xlarge"",
    ""m1.medium"",
    ""m1.small"",
    ""m2.xlarge"",
    ""m3.large"",
    ""m3.medium"",
    ""m5ad.16xlarge"",
    ""m5ad.8xlarge"",
    ""m5dn.metal"",
    ""m5n.metal"",
    ""r3.8xlarge"",
    ""r3.large"",
    ""r5ad.16xlarge"",
    ""r5ad.8xlarge"",
    ""r5dn.metal"",
    ""r5n.metal"",
    ""t1.micro"",
    ""t2.2xlarge"",
    ""t2.large"",
    ""t2.medium"",
    ""t2.micro"",
    ""t2.nano"",
    ""t2.small"",
    ""t2.xlarge""
  ]

  kubeconfig = var.create_eks ? templatefile(""${path.module}/templates/kubeconfig.tpl"", {
    kubeconfig_name                   = coalesce(var.kubeconfig_name, ""eks_${var.cluster_name}"")
    endpoint                          = local.cluster_endpoint
    cluster_auth_base64               = local.cluster_auth_base64
    aws_authenticator_command         = var.kubeconfig_aws_authenticator_command
    aws_authenticator_command_args    = coalescelist(var.kubeconfig_aws_authenticator_command_args, [""token"", ""-i"", local.cluster_name])
    aws_authenticator_additional_args = var.kubeconfig_aws_authenticator_additional_args
    aws_authenticator_env_variables   = var.kubeconfig_aws_authenticator_env_variables
  }) : """"

  launch_configuration_userdata_rendered = [
    for index in range(var.create_eks ? local.worker_group_launch_configuration_count : 0) : templatefile(
      lookup(
        var.worker_groups[index],
        ""userdata_template_file"",
        lookup(var.worker_groups[index], ""platform"", local.workers_group_defaults[""platform""]) == ""windows""
        ? ""${path.module}/templates/userdata_windows.tpl""
        : ""${path.module}/templates/userdata.sh.tpl""
      ),
      merge({
        platform            = lookup(var.worker_groups[index], ""platform"", local.workers_group_defaults[""platform""])
        cluster_name        = local.cluster_name
        endpoint            = local.cluster_endpoint
        cluster_auth_base64 = local.cluster_auth_base64
        pre_userdata = lookup(
          var.worker_groups[index],
          ""pre_userdata"",
          local.workers_group_defaults[""pre_userdata""],
        )
        additional_userdata = lookup(
          var.worker_groups[index],
          ""additional_userdata"",
          local.workers_group_defaults[""additional_userdata""],
        )
        bootstrap_extra_args = lookup(
          var.worker_groups[index],
          ""bootstrap_extra_args"",
          local.workers_group_defaults[""bootstrap_extra_args""],
        )
        kubelet_extra_args = lookup(
          var.worker_groups[index],
          ""kubelet_extra_args"",
          local.workers_group_defaults[""kubelet_extra_args""],
        )
        },
        lookup(
          var.worker_groups[index],
          ""userdata_template_extra_args"",
          local.workers_group_defaults[""userdata_template_extra_args""]
        )
      )
    )
  ]

  launch_template_userdata_rendered = [
    for index in range(var.create_eks ? local.worker_group_launch_template_count : 0) : templatefile(
      lookup(
        var.worker_groups_launch_template[index],
        ""userdata_template_file"",
        lookup(var.worker_groups_launch_template[index], ""platform"", local.workers_group_defaults[""platform""]) == ""windows""
        ? ""${path.module}/templates/userdata_windows.tpl""
        : ""${path.module}/templates/userdata.sh.tpl""
      ),
      merge({
        platform            = lookup(var.worker_groups_launch_template[index], ""platform"", local.workers_group_defaults[""platform""])
        cluster_name        = local.cluster_name
        endpoint            = local.cluster_endpoint
        cluster_auth_base64 = local.cluster_auth_base64
        pre_userdata = lookup(
          var.worker_groups_launch_template[index],
          ""pre_userdata"",
          local.workers_group_defaults[""pre_userdata""],
        )
        additional_userdata = lookup(
          var.worker_groups_launch_template[index],
          ""additional_userdata"",
          local.workers_group_defaults[""additional_userdata""],
        )
        bootstrap_extra_args = lookup(
          var.worker_groups_launch_template[index],
          ""bootstrap_extra_args"",
          local.workers_group_defaults[""bootstrap_extra_args""],
        )
        kubelet_extra_args = lookup(
          var.worker_groups_launch_template[index],
          ""kubelet_extra_args"",
          local.workers_group_defaults[""kubelet_extra_args""],
        )
        },
        lookup(
          var.worker_groups_launch_template[index],
          ""userdata_template_extra_args"",
          local.workers_group_defaults[""userdata_template_extra_args""]
        )
      )
    )
  ]
}
",locals,26,,9bfdba9fb86290b562c096cb404df4cd4e13223e,f37e5af88adfb86214d8c09424913067bac58a4f,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/9bfdba9fb86290b562c096cb404df4cd4e13223e/local.tf#L26,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/f37e5af88adfb86214d8c09424913067bac58a4f/locals.tf,2020-05-06 14:00:20+02:00,2021-10-07 14:21:13+02:00,38,1,1,1,1,0,0,0,0,0
https://github.com/rust-lang/simpleinfra,1,terraform/shared/modules/ecs-app/deployment.tf,terraform/shared/modules/ecs-app/deployment.tf,0,crap,// Security group to allow the monitoring instance to scrape metrics,// Security group to allow the monitoring instance to scrape metrics,"data ""aws_security_group"" ""monitoring"" {
  name = ""monitoring-instance-on-legacy-vpc""
}
",data,"data ""aws_security_group"" ""monitoring"" {
  name = ""monitoring-instance-on-legacy-vpc""
}
",data,79,79.0,b08f657adcb9ae34c6ad6a92a67c0b98d8519657,75291a2cdf0dd5ba60860701274f9500703102da,https://github.com/rust-lang/simpleinfra/blob/b08f657adcb9ae34c6ad6a92a67c0b98d8519657/terraform/shared/modules/ecs-app/deployment.tf#L79,https://github.com/rust-lang/simpleinfra/blob/75291a2cdf0dd5ba60860701274f9500703102da/terraform/shared/modules/ecs-app/deployment.tf#L79,2022-07-09 19:31:38+02:00,2023-04-15 16:55:17+02:00,3,0,0,0,0,1,0,0,1,0
https://github.com/Worklytics/psoxy,218,infra/modules/azuread-local-cert/variables.tf,infra/modules/azuread-local-cert/variables.tf,0,# todo,# TODO: should be your organization!!,"default     = ""/C=US/ST=New York/L=New York/CN=www.worklytics.co"" # TODO: should be your organization!!","variable ""certificate_subject"" {
  type        = string
  description = ""value for 'subject' passed to openssl when generation certificate""
  default     = ""/C=US/ST=New York/L=New York/CN=www.worklytics.co"" # TODO: should be your organization!!
}
",variable,"variable ""certificate_subject"" {
  type        = string
  description = ""value for 'subject' passed to openssl when generation certificate (eg '/C=US/ST=New York/L=New York/CN=www.worklytics.co')""
}
",variable,21,,747196b6d081d59b6abb86a8f1d0f47b590c0cb7,a87c975913efccae7f4f6db646c8b57e9ab45106,https://github.com/Worklytics/psoxy/blob/747196b6d081d59b6abb86a8f1d0f47b590c0cb7/infra/modules/azuread-local-cert/variables.tf#L21,https://github.com/Worklytics/psoxy/blob/a87c975913efccae7f4f6db646c8b57e9ab45106/infra/modules/azuread-local-cert/variables.tf,2022-01-24 15:32:14-08:00,2022-01-24 22:10:59-08:00,2,1,0,1,0,0,0,0,0,0
https://github.com/uyuni-project/sumaform,1383,backend_modules/aws/host/main.tf,backend_modules/aws/host/main.tf,0,workaround,# WORKAROUND: ephemeral block devices are defined in any case,"# WORKAROUND: ephemeral block devices are defined in any case 
 # they will only be used for instance types that provide them","resource ""aws_instance"" ""instance"" {
  ami                    = local.ami
  instance_type          = local.provider_settings[""instance_type""]
  count                  = var.quantity
  availability_zone      = local.availability_zone
  key_name               = local.provider_settings[""key_name""]
  subnet_id              = var.connect_to_base_network ? (local.provider_settings[""public_instance""] ? local.public_subnet_id : local.private_subnet_id) : var.connect_to_additional_network ? local.private_additional_subnet_id : local.private_subnet_id
  vpc_security_group_ids = [var.connect_to_base_network ? (local.provider_settings[""public_instance""] ? local.public_security_group_id : local.private_security_group_id) : var.connect_to_additional_network ? local.private_additional_security_group_id : local.private_security_group_id]

  root_block_device {
    volume_size = local.provider_settings[""volume_size""]
  }

  user_data = data.template_file.user_data[count.index].rendered

  # WORKAROUND: ephemeral block devices are defined in any case
  # they will only be used for instance types that provide them
  ephemeral_block_device {
    device_name  = ""xvdb""
    virtual_name = ""ephemeral0""
  }

  ephemeral_block_device {
    device_name  = ""xvdc""
    virtual_name = ""ephemeral1""
  }

  tags = {
    Name = ""${local.resource_name_prefix}${var.quantity > 1 ? ""-${count.index + 1}"" : """"}""
  }

  # WORKAROUND
  # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner"").
  # After the first `apply`, terraform removes those tags. The following block avoids this behavior.
  # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider
  # See github:terraform-providers/terraform-provider-aws#10689
  lifecycle {
    ignore_changes = [tags]
  }
}
",resource,"resource ""aws_instance"" ""instance"" {
  ami                    = local.ami
  instance_type          = local.provider_settings[""instance_type""]
  count                  = var.quantity
  availability_zone      = local.availability_zone
  key_name               = local.provider_settings[""key_name""]
  subnet_id              = var.connect_to_base_network ? (local.provider_settings[""public_instance""] ? local.public_subnet_id : local.private_subnet_id) : var.connect_to_additional_network ? local.private_additional_subnet_id : local.private_subnet_id
  vpc_security_group_ids = [var.connect_to_base_network ? (local.provider_settings[""public_instance""] ? local.public_security_group_id : local.private_security_group_id) : var.connect_to_additional_network ? local.private_additional_security_group_id : local.private_security_group_id]
  private_ip             = local.private_ip
  iam_instance_profile   = contains(var.roles, ""server"") ? var.base_configuration[""iam_instance_profile""] : null

  root_block_device {
    volume_size = local.provider_settings[""volume_size""]
  }

  user_data = data.template_file.user_data[count.index].rendered

  # WORKAROUND: ephemeral block devices are defined in any case
  # they will only be used for instance types that provide them
  ephemeral_block_device {
    device_name  = ""xvdb""
    virtual_name = ""ephemeral0""
  }

  ephemeral_block_device {
    device_name  = ""xvdc""
    virtual_name = ""ephemeral1""
  }

  tags = {
    Name = ""${local.resource_name_prefix}${var.quantity > 1 ? ""-${count.index + 1}"" : """"}""
  }

  connection {
    private_ip = self.private_ip
  }

  # WORKAROUND
  # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner"").
  # After the first `apply`, terraform removes those tags. The following block avoids this behavior.
  # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider
  # See github:terraform-providers/terraform-provider-aws#10689
  lifecycle {
    ignore_changes = [tags]
  }
}
",resource,62,92.0,12fc857978857ebd94f9b0906480004ca9b88c22,3a05fda9be0240b065bb2031b5812c595b77f973,https://github.com/uyuni-project/sumaform/blob/12fc857978857ebd94f9b0906480004ca9b88c22/backend_modules/aws/host/main.tf#L62,https://github.com/uyuni-project/sumaform/blob/3a05fda9be0240b065bb2031b5812c595b77f973/backend_modules/aws/host/main.tf#L92,2021-01-26 15:58:29+01:00,2024-04-12 13:39:31+12:00,35,0,1,0,0,0,0,0,0,0
https://github.com/compiler-explorer/infra,47,terraform/security.tf,terraform/security.tf,0,fix,# completely (with access only to admin node and the ALB) but this would make diagnosing and fixing,"# It's convenient for Compiler explorer nodes to be able to do things like `git pull` and `docker pull`, 
 # so need to be able to talk to the outside world. Ideally they'd be locked down 
 # completely (with access only to admin node and the ALB); but this would make diagnosing and fixing 
 # issues quickly on-box very difficult.","resource ""aws_security_group_rule"" ""CE_EgressToAll"" {
  security_group_id = aws_security_group.CompilerExplorer.id
  type              = ""egress""
  from_port         = 0
  to_port           = 65535
  cidr_blocks       = [""0.0.0.0/0""]
  ipv6_cidr_blocks  = [""::/0""]
  protocol          = ""-1""
  description       = ""Unfettered outbound access""
}
",resource,"resource ""aws_security_group_rule"" ""CE_EgressToAll"" {
  security_group_id = aws_security_group.CompilerExplorer.id
  type              = ""egress""
  from_port         = 0
  to_port           = 65535
  cidr_blocks       = [""0.0.0.0/0""]
  ipv6_cidr_blocks  = [""::/0""]
  protocol          = ""-1""
  description       = ""Unfettered outbound access""
}
",resource,13,12.0,bb786e00dceef7c73aca83bb026899d8bb90632c,e1b1416e3a1af2b54c905c5b90153208afdd87e7,https://github.com/compiler-explorer/infra/blob/bb786e00dceef7c73aca83bb026899d8bb90632c/terraform/security.tf#L13,https://github.com/compiler-explorer/infra/blob/e1b1416e3a1af2b54c905c5b90153208afdd87e7/terraform/security.tf#L12,2019-09-09 12:16:04-05:00,2024-04-22 22:51:52+02:00,36,0,0,0,0,1,0,0,0,0
https://github.com/camptocamp/devops-stack,99,examples/kind/main.tf,examples/kind/main.tf,0,todo,"# TODO remove useless base_domain and cluster_name variables from ""self-signed"" module.","source = ""git::https://github.com/camptocamp/devops-stack-module-cert-manager.git//self-signed?ref=v2.0.0""  
 # TODO remove useless base_domain and cluster_name variables from ""self-signed"" module.","module ""cert-manager"" {
  source = ""git::https://github.com/camptocamp/devops-stack-module-cert-manager.git//self-signed?ref=v2.0.0""

  # TODO remove useless base_domain and cluster_name variables from ""self-signed"" module.
  cluster_name     = local.cluster_name
  base_domain      = local.base_domain
  argocd_namespace = module.argocd_bootstrap.argocd_namespace

  dependency_ids = {
    argocd = module.argocd_bootstrap.id
  }
}
",module,"module ""cert-manager"" {
  source = ""git::https://github.com/camptocamp/devops-stack-module-cert-manager.git//self-signed?ref=v3.1.0""

  argocd_namespace = module.argocd_bootstrap.argocd_namespace

  enable_service_monitor = local.enable_service_monitor

  dependency_ids = {
    argocd = module.argocd_bootstrap.id
  }
}
",module,86,,e3e1a35b6a90a2990878d6c06775a6dba94637af,a620f2d89d7072298b6a5d5c8c8a9d4b7e8ec11c,https://github.com/camptocamp/devops-stack/blob/e3e1a35b6a90a2990878d6c06775a6dba94637af/examples/kind/main.tf#L86,https://github.com/camptocamp/devops-stack/blob/a620f2d89d7072298b6a5d5c8c8a9d4b7e8ec11c/examples/kind/main.tf,2023-05-16 13:05:31+02:00,2023-05-22 09:53:27+02:00,2,1,1,1,0,0,1,0,0,0
https://github.com/rust-lang/simpleinfra,6,terragrunt/modules/crates-io/fastly-static.tf,terragrunt/modules/crates-io/fastly-static.tf,0,hack,# The below snippet is a hack to get the CNAME for the static domain from the,"# It is currently not possible to get the CNAME for TLS-enabled hostnames as a 
 # Terraform resource. But the ACME HTTP challenge redirects production traffic 
 # to Fastly, for which it uses the CNAME that we are looking for. 
 # 
 # The below snippet is a hack to get the CNAME for the static domain from the 
 # HTTP challenge, until Fastly exposes it in the Terraform provider.","locals {
  # It is currently not possible to get the CNAME for TLS-enabled hostnames as a
  # Terraform resource. But the ACME HTTP challenge redirects production traffic
  # to Fastly, for which it uses the CNAME that we are looking for.
  #
  # The below snippet is a hack to get the CNAME for the static domain from the
  # HTTP challenge, until Fastly exposes it in the Terraform provider.
  fastly_static_destinations = flatten([
    for record in fastly_tls_subscription.static.managed_http_challenges :
    record.record_values if record.record_type == ""CNAME""
  ])
}
",locals,"locals {
  fastly_domain_name = ""fastly-${var.static_domain_name}""

  primary_host_name  = aws_s3_bucket.static.region
  fallback_host_name = aws_s3_bucket.fallback.region

  dictionary_name = ""compute_static""

  request_logs_endpoint = ""s3-request-logs""
  service_logs_endpoint = ""s3-service-logs""
}
",locals,103,,da7cb3473f2c2ad29615132a9eff76ed1933dc60,6b807140a33708afac26894642fe13e46ab7a851,https://github.com/rust-lang/simpleinfra/blob/da7cb3473f2c2ad29615132a9eff76ed1933dc60/terragrunt/modules/crates-io/fastly-static.tf#L103,https://github.com/rust-lang/simpleinfra/blob/6b807140a33708afac26894642fe13e46ab7a851/terragrunt/modules/crates-io/fastly-static.tf,2022-12-21 12:24:08+01:00,2023-03-23 12:37:19+01:00,14,1,0,0,1,1,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,19,modules/dns/variables.tf,modules/dns/variables.tf,0,# todo,# TODO(ludoo): add link to DNSSEC Documentation in README,"# TODO(ludoo): add link to DNSSEC Documentation in README 
 # https://www.terraform.io/docs/providers/google/r/dns_managed_zone.html#dnssec_config ","variable ""default_key_specs_key"" {
  description = ""DNSSEC default key signing specifications: algorithm, key_length, key_type, kind.""
  type        = any
  default     = {}
}
",variable,"variable ""default_key_specs_key"" {
  description = ""DNSSEC default key signing specifications: algorithm, key_length, key_type, kind.""
  type        = any
  default     = {}
}
",variable,33,,c486bfc66f9814e33b410602cb557a5e4d532912,da97405e31c48e470c5f30633f78cc6e052924c8,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c486bfc66f9814e33b410602cb557a5e4d532912/modules/dns/variables.tf#L33,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/da97405e31c48e470c5f30633f78cc6e052924c8/modules/dns/variables.tf,2020-04-03 14:06:48+02:00,2020-05-12 13:35:13+02:00,2,1,0,1,0,0,1,0,0,0
https://github.com/alphagov/govuk-aws,60,terraform/projects/govuk-security-groups/logs-elasticsearch.tf,terraform/projects/infra-security-groups/logs-elasticsearch.tf,1,# todo,# TODO: does anything other than icinga and logging need this?,# TODO: does anything other than icinga and logging need this?,"resource ""aws_security_group_rule"" ""allow_management_to_logs-elasticsearch_elb"" {
  type      = ""ingress""
  from_port = 9200
  to_port   = 9200
  protocol  = ""tcp""

  security_group_id = ""${aws_security_group.logs-elasticsearch_elb.id}""

  # TODO: does anything other than icinga and logging need this?
  source_security_group_id = ""${aws_security_group.management.id}""
}
",resource,,,71,0.0,00346bc2ba10d116f3e41951070ef0ebe0d9085d,0527d5b230dc008ee687667c45d1280de00b40bc,https://github.com/alphagov/govuk-aws/blob/00346bc2ba10d116f3e41951070ef0ebe0d9085d/terraform/projects/govuk-security-groups/logs-elasticsearch.tf#L71,https://github.com/alphagov/govuk-aws/blob/0527d5b230dc008ee687667c45d1280de00b40bc/terraform/projects/infra-security-groups/logs-elasticsearch.tf#L0,2017-07-13 14:53:36+01:00,2017-09-19 20:14:08+01:00,3,2,0,1,0,1,0,0,0,0
https://github.com/compiler-explorer/infra,106,terraform/lambda.tf,terraform/lambda.tf,0,fix,// Comment this in if you want to force a deploy every time... TODO ideally fix this,"  // Comment this in if you want to force a deploy every time... TODO ideally fix this
  //  source_code_hash  = data.aws_s3_bucket_object.lambda_zip.etag","resource ""aws_lambda_function"" ""stats"" {
  description       = ""Respond to various CE-specific stats records""
  s3_bucket         = data.aws_s3_bucket_object.lambda_zip.bucket
  s3_key            = data.aws_s3_bucket_object.lambda_zip.key
  s3_object_version = data.aws_s3_bucket_object.lambda_zip.version_id
  // Comment this in if you want to force a deploy every time... TODO ideally fix this
  //  source_code_hash  = data.aws_s3_bucket_object.lambda_zip.etag
  function_name     = ""stats""
  role              = aws_iam_role.iam_for_lambda.arn
  handler           = ""stats.lambda_handler""
  timeout           = 10

  runtime = ""python3.8""

  environment {
    variables = {
      S3_BUCKET_NAME  = aws_s3_bucket.compiler-explorer-logs.bucket
      SQS_STATS_QUEUE = aws_sqs_queue.stats_queue.id
    }
  }
}
",resource,"resource ""aws_lambda_function"" ""stats"" {
  description       = ""Respond to various CE-specific stats records""
  s3_bucket         = data.aws_s3_bucket_object.lambda_zip.bucket
  s3_key            = data.aws_s3_bucket_object.lambda_zip.key
  s3_object_version = data.aws_s3_bucket_object.lambda_zip.version_id
  source_code_hash  = chomp(data.aws_s3_bucket_object.lambda_zip_sha.body)
  function_name     = ""stats""
  role              = aws_iam_role.iam_for_lambda.arn
  handler           = ""stats.lambda_handler""
  timeout           = 10

  runtime = ""python3.8""

  environment {
    variables = {
      S3_BUCKET_NAME       = aws_s3_bucket.compiler-explorer-logs.bucket
      SQS_STATS_QUEUE      = aws_sqs_queue.stats_queue.id
      COMPILER_BUILD_TABLE = aws_dynamodb_table.compiler-builds.name
    }
  }
}
",resource,99,,e36de219309c76b81e7360525687bfb3e4f845a2,e5d5aff37a3bc047e6d6b093d4184359d92af12c,https://github.com/compiler-explorer/infra/blob/e36de219309c76b81e7360525687bfb3e4f845a2/terraform/lambda.tf#L99,https://github.com/compiler-explorer/infra/blob/e5d5aff37a3bc047e6d6b093d4184359d92af12c/terraform/lambda.tf,2021-07-11 17:06:05-05:00,2021-11-15 17:28:33-06:00,2,1,1,1,0,0,0,0,0,0
https://github.com/ministryofjustice/modernisation-platform,202,terraform/environments/data-platform-apps-and-tools/rds.tf,terraform/environments/data-platform-apps-and-tools/rds.tf,0,todo,"// TODO - Review this rule: ""Instance does not have IAM Authentication enabled"" IAM Auth not understood by our applications ","// TODO - Review this rule: ""Instance does not have IAM Authentication enabled"" IAM Auth not understood by our applications","module ""openmetadata_rds"" {
  #checkov:skip=CKV_TF_1:Module registry does not support commit hashes for versions
  source  = ""terraform-aws-modules/rds/aws""
  version = ""~> 6.0""

  identifier = ""openmetadata""

  engine               = ""postgres""
  engine_version       = ""15""
  family               = ""postgres15""
  major_engine_version = ""15""
  instance_class       = ""db.r6g.xlarge""

  ca_cert_identifier = ""rds-ca-rsa2048-g1""

  allocated_storage     = 128
  max_allocated_storage = 512

  multi_az               = true
  db_subnet_group_name   = module.vpc.database_subnet_group
  vpc_security_group_ids = [module.rds_security_group.security_group_id]

  username                    = ""openmetadata""
  db_name                     = ""openmetadata""
  manage_master_user_password = false
  password                    = random_password.openmetadata.result
  kms_key_id                  = module.openmetadata_rds_kms.key_arn

  parameters = [
    {
      name  = ""rds.force_ssl""
      value = 1
    },
    {
      name  = ""log_statement""
      value = ""all""
    },
    {
      name  = ""log_hostname""
      value = 1
    },
    {
      name  = ""log_connections""
      value = 1
    },
    {
      // Required as per Open Metadata's Documentation https://docs.open-metadata.org/v1.1.x/deployment/upgrade#update-sortbuffersize-mysql-or-workmem-postgres
      name  = ""work_mem""
      value = 10000
    }
  ]

  maintenance_window      = ""Mon:00:00-Mon:03:00""
  backup_window           = ""03:00-06:00""
  backup_retention_period = 7
  deletion_protection     = true

  apply_immediately = true

  performance_insights_enabled = true

  create_monitoring_role          = true
  monitoring_role_use_name_prefix = true
  monitoring_role_name            = ""openmetadata-rds-monitoring""
  monitoring_role_description     = ""Enhanced Monitoring for Open Metadata RDS""
  monitoring_interval             = 30
  enabled_cloudwatch_logs_exports = [""postgresql"", ""upgrade""]

  skip_final_snapshot = true

  tags = local.tags
}
",module,"module ""openmetadata_rds"" {
  #checkov:skip=CKV_TF_1:Module registry does not support commit hashes for versions
  source  = ""terraform-aws-modules/rds/aws""
  version = ""~> 6.0""

  identifier = ""openmetadata""

  engine               = ""postgres""
  engine_version       = ""15""
  family               = ""postgres15""
  major_engine_version = ""15""
  instance_class       = ""db.r6g.xlarge""

  ca_cert_identifier = ""rds-ca-rsa2048-g1""

  allocated_storage     = 128
  max_allocated_storage = 512

  multi_az               = true
  db_subnet_group_name   = module.vpc.database_subnet_group
  vpc_security_group_ids = [module.rds_security_group.security_group_id]

  username                    = ""openmetadata""
  db_name                     = ""openmetadata""
  manage_master_user_password = false
  password                    = random_password.openmetadata.result
  kms_key_id                  = module.openmetadata_rds_kms.key_arn

  parameters = [
    {
      name  = ""rds.force_ssl""
      value = 1
    },
    {
      name  = ""log_statement""
      value = ""all""
    },
    {
      name  = ""log_hostname""
      value = 1
    },
    {
      name  = ""log_connections""
      value = 1
    },
    {
      // Required as per Open Metadata's Documentation https://docs.open-metadata.org/v1.1.x/deployment/upgrade#update-sortbuffersize-mysql-or-workmem-postgres
      name  = ""work_mem""
      value = 10000
    }
  ]

  maintenance_window      = ""Mon:00:00-Mon:03:00""
  backup_window           = ""03:00-06:00""
  backup_retention_period = 7
  deletion_protection     = true

  apply_immediately = true

  performance_insights_enabled = true

  create_monitoring_role          = true
  monitoring_role_use_name_prefix = true
  monitoring_role_name            = ""openmetadata-rds-monitoring""
  monitoring_role_description     = ""Enhanced Monitoring for Open Metadata RDS""
  monitoring_interval             = 30
  enabled_cloudwatch_logs_exports = [""postgresql"", ""upgrade""]

  skip_final_snapshot = true

  tags = local.tags
}
",module,68,,90002b7667baf7c1eb23d0e49dcfa9d9eaee7567,672e8441b8463a463020857479268444f179be0c,https://github.com/ministryofjustice/modernisation-platform/blob/90002b7667baf7c1eb23d0e49dcfa9d9eaee7567/terraform/environments/data-platform-apps-and-tools/rds.tf#L68,https://github.com/ministryofjustice/modernisation-platform/blob/672e8441b8463a463020857479268444f179be0c/terraform/environments/data-platform-apps-and-tools/rds.tf,2023-10-18 15:43:32+01:00,2023-10-19 14:03:41+01:00,2,1,0,1,0,1,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,201,modules/extensions/variables.tf,modules/extensions/variables.tf,0,todo,# TODO move to workers,"# Worker draining 
 # TODO move to workers","variable ""node_pools_to_drain"" { type = list(string) }
",variable,the block associated got renamed or deleted,,58,,6c867cd8e9cbf559742f56658989bcded0d1fd89,c76c39b0b3a6dcb10122f8ab5408f1b327640972,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/modules/extensions/variables.tf#L58,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/c76c39b0b3a6dcb10122f8ab5408f1b327640972/modules/extensions/variables.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,3,1,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1,organization-bootstrap/environments/main.tf,foundations/environments/main.tf,1,# todo,# TODO(ludomagno): move XPN admin role here after checking it now works on folders,"############################################################################### 
 #                              Top-level folders                              # 
 ###############################################################################  
 # TODO(ludomagno): move XPN admin role here after checking it now works on folders ","module ""folders-top-level"" {
  source            = ""terraform-google-modules/folders/google""
  version           = ""2.0.0""
  parent            = var.root_node
  names             = var.environments
  set_roles         = true
  per_folder_admins = module.service-accounts-tf-environments.iam_emails_list
  folder_admin_roles = [
    ""roles/resourcemanager.folderViewer"",
    ""roles/resourcemanager.projectCreator"",
    ""roles/owner"",
    ""roles/compute.networkAdmin"",
  ]
}
",module,"module ""folders-top-level"" {
  source            = ""terraform-google-modules/folders/google""
  version           = ""2.0.0""
  parent            = var.root_node
  names             = var.environments
  set_roles         = true
  per_folder_admins = module.service-accounts-tf-environments.iam_emails_list
  folder_admin_roles = compact(
    [
      ""roles/compute.networkAdmin"",
      ""roles/owner"",
      ""roles/resourcemanager.folderViewer"",
      ""roles/resourcemanager.projectCreator"",
      var.grant_xpn_folder_roles ? ""roles/compute.xpnAdmin"" : """"
    ]
  )
}
",module,79,,e4fa25f22d67f88aa2b46089d0ad4cba53a3dc99,f7d950b39f877a9bab60fa916d95906553ccb3cf,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/e4fa25f22d67f88aa2b46089d0ad4cba53a3dc99/organization-bootstrap/environments/main.tf#L79,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/f7d950b39f877a9bab60fa916d95906553ccb3cf/foundations/environments/main.tf,2019-09-07 05:44:24+02:00,2019-09-19 12:16:45+02:00,2,1,0,1,0,1,0,0,0,1
https://github.com/oracle-terraform-modules/terraform-oci-oke,163,modules/workergroup/cloudinit.tf,modules/workerpools/cloudinit.tf,1,todo,# TODO Collapse w/ variable content_type for cloud-init versions,"# Copyright (c) 2022, 2023 Oracle Corporation and/or its affiliates. 
 # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl  
 # TODO Collapse w/ variable content_type for cloud-init versions","data ""cloudinit_config"" ""worker_once"" {
  gzip          = false
  base64_encode = true

  part {
    filename     = ""worker.template.sh""
    content_type = ""text/x-shellscript""
    content = templatefile(""${path.module}/cloudinit/worker.template.sh"", {
      cluster_ca_cert = local.cluster_ca_cert
      apiserver_host  = var.apiserver_private_host
    })
  }
}
",data,,,4,0.0,4d2b3f3d672a8f41655da3a7c58fded42c6858f3,6c867cd8e9cbf559742f56658989bcded0d1fd89,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/4d2b3f3d672a8f41655da3a7c58fded42c6858f3/modules/workergroup/cloudinit.tf#L4,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/modules/workerpools/cloudinit.tf#L0,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,2,2,0,1,0,0,0,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,303,libvirt/main.tf,libvirt/main.tf,0,// todo,// TODO: check this better,// TODO: check this better,"resource ""libvirt_volume"" ""base_image"" {
  name   = ""${terraform.workspace}-baseimage""
  source = var.base_image
  // TODO: check this better
  pool   = ""terraform""
}
",resource,"resource ""libvirt_volume"" ""base_image"" {
  // the base image will be ""cloned"" and used by other domains, 
  // it is the central  image.
  name   = ""${terraform.workspace}-baseimage""
  source = var.base_image
  // TODO: this can moved to a tfvars
  pool   = var.storage_pool
}
",resource,16,,31fa6cffe7c41a22121d877d5a4244227a875687,b11930acacfa1ed7479d9490adc7b066a3973790,https://github.com/SUSE/ha-sap-terraform-deployments/blob/31fa6cffe7c41a22121d877d5a4244227a875687/libvirt/main.tf#L16,https://github.com/SUSE/ha-sap-terraform-deployments/blob/b11930acacfa1ed7479d9490adc7b066a3973790/libvirt/main.tf,2019-08-28 13:25:20+02:00,2019-08-28 13:42:24+02:00,2,1,1,1,0,0,0,0,0,0
https://github.com/cloudfoundry/bosh-bootloader,1,plan-patches/cfcr-azure/terraform/cfcr_lb_override.tf,plan-patches/cfcr-azure/terraform/cfcr_lb_override.tf,0,todo,# TODO move this to cfcr resource group.,# TODO move this to cfcr resource group.,"resource ""azurerm_lb"" ""cfcr-balancer"" {
  name                = ""${var.env_id}-cfcr-lb""
  location            = ""${var.region}""
  sku                 = ""Standard""
  # TODO move this to cfcr resource group.
  resource_group_name = ""${azurerm_resource_group.bosh.name}""

  frontend_ip_configuration {
    name                 = ""${azurerm_public_ip.cfcr-balancer-ip.name}""
    public_ip_address_id = ""${azurerm_public_ip.cfcr-balancer-ip.id}""
  }
}
",resource,"resource ""azurerm_lb"" ""cfcr-balancer"" {
  name                = ""${var.env_id}-cfcr-lb""
  location            = ""${var.region}""
  sku                 = ""Standard""
  resource_group_name = ""${azurerm_resource_group.cfcr.name}""

  frontend_ip_configuration {
    name                 = ""${azurerm_public_ip.cfcr-balancer-ip.name}""
    public_ip_address_id = ""${azurerm_public_ip.cfcr-balancer-ip.id}""
  }
}
",resource,13,,5169911b36b56c3d7da1815cfc1b4b5e2c9e46eb,f098a6c6ca76caf96d25a1469be65f7cb7d5c514,https://github.com/cloudfoundry/bosh-bootloader/blob/5169911b36b56c3d7da1815cfc1b4b5e2c9e46eb/plan-patches/cfcr-azure/terraform/cfcr_lb_override.tf#L13,https://github.com/cloudfoundry/bosh-bootloader/blob/f098a6c6ca76caf96d25a1469be65f7cb7d5c514/plan-patches/cfcr-azure/terraform/cfcr_lb_override.tf,2018-10-09 15:43:19-07:00,2018-10-29 09:34:36-07:00,2,1,0,1,0,0,1,0,0,0
https://github.com/ministryofjustice/modernisation-platform,230,terraform/environments/core-network-services/cidr-ranges.tf,terraform/environments/core-network-services/cidr-ranges.tf,0,implemented,"# csr-prod = ""10.27.10.0/22"" not yet implemented","# csr-prod = ""10.27.10.0/22"" not yet implemented ","locals {
  mp_core_cidr_ranges = {
    mp-core                     = ""10.20.0.0/16""
    mp-development-test         = ""10.26.0.0/16""
    mp-preproduction-production = ""10.27.0.0/16""
  }

  # This will take the vpc CIDR ranges for each business/env general subnet set, from the environments-networks files
  # e.g. `hmpps-development = ""10.26.24.0/21""
  platform_general_set_cidr_ranges = {
    for key, value in local.core-vpcs : key => value.cidr.subnet_sets.general.cidr
  }

  other_cidr_ranges = {
    alpha-vpn                        = ""100.64.0.0/16""
    analytical-platform-airflow-dev  = ""10.200.0.0/16""
    analytical-platform-airflow-prod = ""10.201.0.0/16""
    atos_arkc_ras                    = ""10.175.0.0/16"" # for DOM1 devices connected to Cisco RAS VPN
    atos_arkf_ras                    = ""10.176.0.0/16"" # for DOM1 devices connected to Cisco RAS VPN
    cloud-platform                   = ""172.20.0.0/16""
    global-protect                   = ""10.184.0.0/16""
    i2n                              = ""10.110.0.0/16""
    moj-core-azure-1                 = ""10.50.25.0/27""
    moj-core-azure-2                 = ""10.50.26.0/24""
    parole-board                     = ""10.50.0.0/16""
    psn                              = ""51.0.0.0/8""
    psn-ppud                         = ""51.247.2.115/32""
    vodafone_wan_nicts_aggregate     = ""10.80.0.0/12"" # for devices connected to Prison Networks

    # hmpps azure cidr ranges
    noms-live-vnet                 = ""10.40.0.0/18""
    noms-live-dr-vnet              = ""10.40.64.0/18""
    noms-mgmt-live-vnet            = ""10.40.128.0/20""
    noms-mgmt-live-dr-vnet         = ""10.40.144.0/20""
    noms-transit-live-vnet         = ""10.40.160.0/20""
    noms-transit-live-dr-vnet      = ""10.40.176.0/20""
    noms-test-vnet                 = ""10.101.0.0/16""
    noms-mgmt-vnet                 = ""10.102.0.0/16""
    noms-test-dr-vnet              = ""10.111.0.0/16""
    noms-mgmt-dr-vnet              = ""10.112.0.0/16""
    aks-studio-hosting-live-1-vnet = ""10.244.0.0/20""
    aks-studio-hosting-dev-1-vnet  = ""10.247.0.0/20""
    aks-studio-hosting-ops-1-vnet  = ""10.247.32.0/20""
    nomisapi-t2-root-vnet          = ""10.47.0.192/26""
    nomisapi-t3-root-vnet          = ""10.47.0.0/26""
    nomisapi-preprod-root-vnet     = ""10.47.0.64/26""
    nomisapi-prod-root-vnet        = ""10.47.0.128/26""

    # hmpps aws cidr ranges
    delius-eng-dev  = ""10.161.98.0/25""
    delius-eng-prod = ""10.160.98.0/25""
    delius-core-dev = ""10.161.20.0/22""
    delius-mis-dev  = ""10.162.32.0/20""
    delius-test     = ""10.162.0.0/20""
    delius-stage    = ""10.160.32.0/20""
    delius-pre-prod = ""10.160.0.0/20""
    delius-training = ""10.162.96.0/20""
    delius-prod     = ""10.160.16.0/20""

    # laa landing zone cidr ranges
    laa-lz-development             = ""10.202.0.0/20""
    laa-lz-test                    = ""10.203.0.0/20""
    laa-lz-uat                     = ""10.206.0.0/20""
    laa-lz-staging                 = ""10.204.0.0/20""
    laa-lz-production              = ""10.205.0.0/20""
    laa-lz-shared-services-nonprod = ""10.200.0.0/20""
    laa-lz-shared-services-prod    = ""10.200.16.0/20""
    laa-appstream-vpc              = ""10.200.32.0/19""
    laa-appstream-vpc_additional   = ""10.200.68.0/22""

    # csr app cidr ranges
    csr-preprod = ""10.27.0.0/22""
    # csr-prod = ""10.27.10.0/22"" not yet implemented

  }

  all_cidr_ranges = merge(
    local.mp_core_cidr_ranges,
    local.platform_general_set_cidr_ranges,
    local.other_cidr_ranges
  )
}
",locals,"locals {
  mp_core_cidr_ranges = {
    mp-core                     = ""10.20.0.0/16""
    mp-development-test         = ""10.26.0.0/16""
    mp-preproduction-production = ""10.27.0.0/16""
  }

  # This will take the vpc CIDR ranges for each business/env general subnet set, from the environments-networks files
  # e.g. `hmpps-development = ""10.26.24.0/21""
  platform_general_set_cidr_ranges = {
    for key, value in local.core-vpcs : key => value.cidr.subnet_sets.general.cidr
  }

  other_cidr_ranges = {
    alpha-vpn                        = ""100.64.0.0/16""
    analytical-platform-airflow-dev  = ""10.200.0.0/16""
    analytical-platform-airflow-prod = ""10.201.0.0/16""
    atos_arkc_ras                    = ""10.175.0.0/16"" # for DOM1 devices connected to Cisco RAS VPN
    atos_arkf_ras                    = ""10.176.0.0/16"" # for DOM1 devices connected to Cisco RAS VPN
    cloud-platform                   = ""172.20.0.0/16""
    global-protect                   = ""10.184.0.0/16""
    i2n                              = ""10.110.0.0/16""
    moj-core-azure-1                 = ""10.50.25.0/27""
    moj-core-azure-2                 = ""10.50.26.0/24""
    parole-board                     = ""10.50.0.0/16""
    psn                              = ""51.0.0.0/8""
    psn-ppud                         = ""51.247.2.115/32""
    vodafone_wan_nicts_aggregate     = ""10.80.0.0/12"" # for devices connected to Prison Networks

    # hmpps azure cidr ranges
    noms-live-vnet                 = ""10.40.0.0/18""
    noms-live-dr-vnet              = ""10.40.64.0/18""
    noms-mgmt-live-vnet            = ""10.40.128.0/20""
    noms-mgmt-live-dr-vnet         = ""10.40.144.0/20""
    noms-transit-live-vnet         = ""10.40.160.0/20""
    noms-transit-live-dr-vnet      = ""10.40.176.0/20""
    noms-test-vnet                 = ""10.101.0.0/16""
    noms-mgmt-vnet                 = ""10.102.0.0/16""
    noms-test-dr-vnet              = ""10.111.0.0/16""
    noms-mgmt-dr-vnet              = ""10.112.0.0/16""
    aks-studio-hosting-live-1-vnet = ""10.244.0.0/20""
    aks-studio-hosting-dev-1-vnet  = ""10.247.0.0/20""
    aks-studio-hosting-ops-1-vnet  = ""10.247.32.0/20""
    nomisapi-t2-root-vnet          = ""10.47.0.192/26""
    nomisapi-t3-root-vnet          = ""10.47.0.0/26""
    nomisapi-preprod-root-vnet     = ""10.47.0.64/26""
    nomisapi-prod-root-vnet        = ""10.47.0.128/26""

    # hmpps aws cidr ranges
    delius-eng-dev  = ""10.161.98.0/25""
    delius-eng-prod = ""10.160.98.0/25""
    delius-core-dev = ""10.161.20.0/22""
    delius-mis-dev  = ""10.162.32.0/20""
    delius-test     = ""10.162.0.0/20""
    delius-stage    = ""10.160.32.0/20""
    delius-pre-prod = ""10.160.0.0/20""
    delius-training = ""10.162.96.0/20""
    delius-prod     = ""10.160.16.0/20""

    # laa landing zone cidr ranges
    laa-lz-development             = ""10.202.0.0/20""
    laa-lz-test                    = ""10.203.0.0/20""
    laa-lz-uat                     = ""10.206.0.0/20""
    laa-lz-staging                 = ""10.204.0.0/20""
    laa-lz-production              = ""10.205.0.0/20""
    laa-lz-shared-services-nonprod = ""10.200.0.0/20""
    laa-lz-shared-services-prod    = ""10.200.16.0/20""
    laa-appstream-vpc              = ""10.200.32.0/19""
    laa-appstream-vpc_additional   = ""10.200.68.0/22""

    hmpps-preproduction-general-private-subnets = ""10.27.0.0/22""
    # hmpps-production-general-private-subnets = ""10.27.10.0/22"" not yet implemented
  }

  all_cidr_ranges = merge(
    local.mp_core_cidr_ranges,
    local.platform_general_set_cidr_ranges,
    local.other_cidr_ranges
  )
}
",locals,73,,1066bcea639188a84ef4f579d78dc1bf91cdcda4,c40f0f81d52046129fb0dd1c39b5f6a628b53717,https://github.com/ministryofjustice/modernisation-platform/blob/1066bcea639188a84ef4f579d78dc1bf91cdcda4/terraform/environments/core-network-services/cidr-ranges.tf#L73,https://github.com/ministryofjustice/modernisation-platform/blob/c40f0f81d52046129fb0dd1c39b5f6a628b53717/terraform/environments/core-network-services/cidr-ranges.tf,2023-11-07 16:51:39+00:00,2023-11-07 17:40:47+00:00,2,1,0,1,0,0,1,1,0,0
https://github.com/alphagov/govuk-aws,440,terraform/projects/app-mysql/main.tf,terraform/projects/app-mysql/main.tf,0,# todo,# TODO: this should be set to 0 when we have migrated to Production,"# TODO: this should be set to 0 when we have migrated to Production 
 # as it not recommended to set this option","resource ""aws_db_parameter_group"" ""mysql-primary"" {
  name_prefix = ""mysql-primary""
  family      = ""mysql5.6""

  parameter {
    name  = ""max_allowed_packet""
    value = 1073741824
  }

  # TODO: this should be set to 0 when we have migrated to Production
  # as it not recommended to set this option
  parameter {
    name  = ""log_bin_trust_function_creators""
    value = 1
  }

  tags {
    aws_stackname = ""${var.stackname}""
  }
}
",resource,,,74,0.0,9a70d18e581efc1a56f74139df69864237d9b017,060aa26bb8ae887ea52c2a00949d13a6faf07fa9,https://github.com/alphagov/govuk-aws/blob/9a70d18e581efc1a56f74139df69864237d9b017/terraform/projects/app-mysql/main.tf#L74,https://github.com/alphagov/govuk-aws/blob/060aa26bb8ae887ea52c2a00949d13a6faf07fa9/terraform/projects/app-mysql/main.tf#L0,2017-11-21 12:34:06+00:00,2022-01-28 14:26:02+00:00,31,2,1,1,0,0,0,1,0,0
https://github.com/zenml-io/mlstacks,74,src/mlstacks/terraform/k3d-modular/k3d.tf,src/mlstacks/terraform/k3d-modular/k3d.tf,0,hack,# This is a hack to attempt get the local stores path from the zenml config,"# This is a hack to attempt get the local stores path from the zenml config 
 # and pass it to the k3d cluster resource, if not set in the local config.","data ""external"" ""zenml_local_stores_path"" {
  program = [
    ""python"",
    ""-u"",
    ""-c"",
    <<-ZENML
%{if local.k3d.local_stores_path != """"}
path = ""${local.k3d.local_stores_path}""
%{else}
try:
  from zenml.config.global_config import GlobalConfiguration
  path = GlobalConfiguration().local_stores_path
except Exception:
  path = """"
%{endif}
print('{""path"": ""' + path + '""}')
    ZENML
  ]
}
",data,"data ""external"" ""zenml_local_stores_path"" {
  program = [
    ""python"",
    ""-u"",
    ""-c"",
    <<-ZENML
%{if local.k3d.local_stores_path != """"}
path = ""${local.k3d.local_stores_path}""
%{else}
try:
  from zenml.config.global_config import GlobalConfiguration
  path = GlobalConfiguration().local_stores_path
except Exception:
  path = """"
%{endif}
print('{""path"": ""' + path + '""}')
    ZENML
  ]
}
",data,28,28.0,513352b0abbf05afdd65e6b869f5619acd1d8c66,14973148b3d05063d333fd9d9ad523be79198c71,https://github.com/zenml-io/mlstacks/blob/513352b0abbf05afdd65e6b869f5619acd1d8c66/src/mlstacks/terraform/k3d-modular/k3d.tf#L28,https://github.com/zenml-io/mlstacks/blob/14973148b3d05063d333fd9d9ad523be79198c71/src/mlstacks/terraform/k3d-modular/k3d.tf#L28,2023-08-23 11:34:20+02:00,2023-12-22 09:15:10+01:00,2,0,1,1,0,0,0,0,0,0
https://github.com/kubernetes/k8s.io,385,infra/aws/terraform/prow-build-cluster/variables.tf,infra/aws/terraform/prow-build-cluster/variables.tf,0,# todo,# TODO: remove once applied on prow-build-cluster,"# TODO: remove once applied on prow-build-cluster 
 # This variable is required in the installation process as we cannot 
 # assume a role that is yet to be created.","variable ""assume_role"" {
  type        = bool
  description = ""Assumes role to get access to EKS cluster after provisioning.""
  default     = true
}
",variable,the block associated got renamed or deleted,,29,,ea61b21c1c9188ea5df32e9b08d1a51fe706715a,d670c1931be3c8304aaf368d71c6b3a537de4aac,https://github.com/kubernetes/k8s.io/blob/ea61b21c1c9188ea5df32e9b08d1a51fe706715a/infra/aws/terraform/prow-build-cluster/variables.tf#L29,https://github.com/kubernetes/k8s.io/blob/d670c1931be3c8304aaf368d71c6b3a537de4aac/infra/aws/terraform/prow-build-cluster/variables.tf,2023-04-26 15:07:51+02:00,2023-04-28 10:05:00+02:00,3,1,0,1,0,1,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,164,modules/workergroup/instanceconfig.tf,modules/workers/instanceconfig.tf,1,todo,# TODO Instance Configuration replacement without delete when supported:,"# TODO Instance Configuration replacement without delete when supported: 
 # https://github.com/hashicorp/terraform/issues/15485","resource ""oci_core_instance_configuration"" ""instance_configuration"" {
  # Create an OCI Instance Configuration resource for each enabled entry of the worker_groups map with a mode that uses one.
  for_each       = local.enabled_instance_configs
  compartment_id = each.value.compartment_id
  display_name   = ""${each.value.label_prefix}-${each.key}""

  instance_details {
    instance_type = ""compute""

    launch_details {
      # Define each configured availability domain for placement, with bounds on # available
      # Configured AD numbers e.g. [1,2,3] are converted into tenancy/compartment-specific names
      availability_domain = lookup(local.ad_number_to_name, (
        contains(keys(each.value), ""placement_ads"")
        ? element(tolist(setintersection(each.value.placement_ads, local.ad_numbers)), 1)
        : element(local.ad_numbers, 1)
      ), """")
      compartment_id = each.value.compartment_id
      defined_tags = merge(
        local.defined_tags,
        lookup(each.value, ""defined_tags"", {}),
      )
      freeform_tags = merge(local.freeform_tags, contains(keys(each.value), ""freeform_tags"") ? each.value.freeform_tags : { worker_group = each.key })

      instance_options {
        are_legacy_imds_endpoints_disabled = false
      }

      create_vnic_details {
        assign_private_dns_record = var.assign_dns
        assign_public_ip          = each.value.assign_public_ip
        nsg_ids                   = each.value.worker_nsgs
        subnet_id                 = each.value.subnet_id
      }

      metadata = {
        apiserver_host           = var.apiserver_private_host
        cluster_ca_cert          = local.cluster_ca_cert
        kubedns_svc_ip           = var.cluster_dns
        oke-k8version            = var.kubernetes_version
        oke-kubeproxy-proxy-mode = var.kubeproxy_mode
        oke-tenancy-id           = local.tenancy_id
        oke-initial-node-labels = join("","", [
          for k, v in merge(var.node_labels, each.value.node_labels) : join(""="", [k, v])
        ])
        ssh_authorized_keys = local.ssh_public_key
        user_data           = data.cloudinit_config.worker_per_boot.rendered
      }

      shape = each.value.shape

      dynamic ""shape_config"" {
        for_each = length(regexall(""Flex"", each.value.shape)) > 0 ? [1] : []
        content {
          ocpus = each.value.ocpus
          memory_in_gbs = ( # If > 64GB memory/core, correct input to exactly 64GB memory/core
            (each.value.memory / each.value.ocpus) > 64 ? each.value.ocpus * 64 : each.value.memory
          )
        }
      }

      source_details {
        boot_volume_size_in_gbs = each.value.boot_volume_size
        image_id                = each.value.image_id
        source_type             = ""image""
      }

      is_pv_encryption_in_transit_enabled = var.enable_pv_encryption_in_transit
    }

    block_volumes {
      attach_details {
        type                                = var.block_volume_type
        is_pv_encryption_in_transit_enabled = var.block_volume_type == ""paravirtualized"" && var.enable_pv_encryption_in_transit
      }

      create_details {
        display_name   = ""${each.value.label_prefix}-${each.key}""
        kms_key_id     = var.volume_kms_key_id
        compartment_id = each.value.compartment_id
      }
    }
  }

  lifecycle {

    # TODO Instance Configuration replacement without delete when supported:
    # https://github.com/hashicorp/terraform/issues/15485
    create_before_destroy = true
    ignore_changes = [
      defined_tags, freeform_tags, display_name,
      instance_details[0].launch_details[0].metadata,
      instance_details[0].launch_details[0].defined_tags,
      instance_details[0].launch_details[0].freeform_tags,
    ]
  }
}",resource,"resource ""oci_core_instance_configuration"" ""workers"" {
  # Create an OCI Instance Configuration resource for each enabled entry of the worker_pools map with a mode that uses one.
  for_each       = local.enabled_instance_configs
  compartment_id = each.value.compartment_id
  display_name   = each.key
  defined_tags   = each.value.defined_tags
  freeform_tags  = each.value.freeform_tags

  instance_details {
    instance_type = ""compute""

    launch_details {
      agent_config {
        are_all_plugins_disabled = each.value.agent_config.are_all_plugins_disabled
        is_management_disabled   = each.value.agent_config.is_management_disabled
        is_monitoring_disabled   = each.value.agent_config.is_monitoring_disabled
        dynamic ""plugins_config"" {
          for_each = each.value.agent_config.plugins_config
          content {
            name          = plugins_config.key
            desired_state = plugins_config.value
          }
        }
      }

      availability_domain = element(each.value.availability_domains, 1)

      # First value specified on pool, or null to select automatically
      fault_domain = try(each.value.placement_fds[0], null)

      compartment_id          = each.value.compartment_id
      defined_tags            = each.value.defined_tags
      freeform_tags           = each.value.freeform_tags
      extended_metadata       = each.value.extended_metadata
      capacity_reservation_id = each.value.capacity_reservation_id

      instance_options {
        are_legacy_imds_endpoints_disabled = false
      }

      create_vnic_details {
        assign_private_dns_record = var.assign_dns
        assign_public_ip          = each.value.assign_public_ip
        nsg_ids                   = each.value.nsg_ids
        subnet_id                 = each.value.subnet_id
        defined_tags              = each.value.defined_tags
        freeform_tags             = each.value.freeform_tags
      }

      metadata = merge(
        {
          apiserver_host           = var.apiserver_private_host
          cluster_ca_cert          = var.cluster_ca_cert
          oke-k8version            = var.kubernetes_version
          oke-kubeproxy-proxy-mode = var.kubeproxy_mode
          oke-tenancy-id           = var.tenancy_id
          oke-initial-node-labels  = join("","", [for k, v in each.value.node_labels : format(""%v=%v"", k, v)])
          secondary_vnics          = jsonencode(lookup(each.value, ""secondary_vnics"", {}))
          ssh_authorized_keys      = var.ssh_public_key
          user_data                = lookup(lookup(data.cloudinit_config.workers, each.key, {}), ""rendered"", """")
        },

        # Only provide cluster DNS service address if set explicitly; determined automatically in practice.
        coalesce(var.cluster_dns, ""none"") == ""none"" ? {} : { kubedns_svc_ip = var.cluster_dns },

        # Extra user-defined fields merged last
        var.node_metadata,                       # global
        lookup(each.value, ""node_metadata"", {}), # pool-specific
      )

      shape = each.value.shape

      dynamic ""shape_config"" {
        for_each = length(regexall(""Flex"", each.value.shape)) > 0 ? [1] : []
        content {
          ocpus = each.value.ocpus
          memory_in_gbs = ( # If > 64GB memory/core, correct input to exactly 64GB memory/core
            (each.value.memory / each.value.ocpus) > 64 ? each.value.ocpus * 64 : each.value.memory
          )
        }
      }

      dynamic ""platform_config"" {
        for_each = each.value.platform_config != null ? [1] : []
        content {
          type = lookup(
            # Attempt lookup against data source for the associated 'type' of configured worker shape
            lookup(local.platform_config_by_shape, each.value.shape, {}), ""type"",
            # Fall back to 'type' on pool with custom platform_config, or INTEL_VM default
            lookup(each.value.platform_config, ""type"", ""INTEL_VM"")
          )
          # Remaining parameters as configured, validated by instance/instance config resource
          are_virtual_instructions_enabled               = lookup(each.value.platform_config, ""are_virtual_instructions_enabled"", null)
          is_access_control_service_enabled              = lookup(each.value.platform_config, ""is_access_control_service_enabled"", null)
          is_input_output_memory_management_unit_enabled = lookup(each.value.platform_config, ""is_input_output_memory_management_unit_enabled"", null)
          is_measured_boot_enabled                       = lookup(each.value.platform_config, ""is_measured_boot_enabled"", null)
          is_memory_encryption_enabled                   = lookup(each.value.platform_config, ""is_memory_encryption_enabled"", null)
          is_secure_boot_enabled                         = lookup(each.value.platform_config, ""is_secure_boot_enabled"", null)
          is_symmetric_multi_threading_enabled           = lookup(each.value.platform_config, ""is_symmetric_multi_threading_enabled"", null)
          is_trusted_platform_module_enabled             = lookup(each.value.platform_config, ""is_trusted_platform_module_enabled"", null)
          numa_nodes_per_socket                          = lookup(each.value.platform_config, ""numa_nodes_per_socket"", null)
          percentage_of_cores_enabled                    = lookup(each.value.platform_config, ""percentage_of_cores_enabled"", null)
        }
      }

      source_details {
        boot_volume_size_in_gbs = each.value.boot_volume_size
        boot_volume_vpus_per_gb = each.value.boot_volume_vpus_per_gb
        image_id                = each.value.image_id
        source_type             = ""image""
      }

      is_pv_encryption_in_transit_enabled = each.value.pv_transit_encryption
    }

    block_volumes {
      attach_details {
        type                                = each.value.block_volume_type
        is_pv_encryption_in_transit_enabled = each.value.pv_transit_encryption
      }

      create_details {
        // Limit to first candidate placement AD for cluster-network; undefined for all otherwise
        availability_domain = each.value.mode == ""cluster-network"" ? element(each.value.availability_domains, 1) : null
        compartment_id      = each.value.compartment_id
        display_name        = each.key
        kms_key_id          = each.value.volume_kms_key_id
      }
    }

    dynamic ""secondary_vnics"" {
      for_each = lookup(each.value, ""secondary_vnics"", {})
      iterator = vnic

      content {
        display_name = vnic.key
        nic_index    = lookup(vnic.value, ""nic_index"", null)

        create_vnic_details {
          assign_private_dns_record = lookup(vnic.value, ""assign_private_dns_record"", null)
          assign_public_ip          = lookup(vnic.value, ""assign_public_ip"", null)
          display_name              = vnic.key
          defined_tags              = lookup(vnic.value, ""defined_tags"", null)
          freeform_tags             = lookup(vnic.value, ""freeform_tags"", null)
          hostname_label            = lookup(vnic.value, ""hostname_label"", null)
          nsg_ids                   = lookup(vnic.value, ""nsg_ids"", null)
          private_ip                = lookup(vnic.value, ""private_ip"", null)
          skip_source_dest_check    = lookup(vnic.value, ""skip_source_dest_check"", null)
          subnet_id                 = lookup(vnic.value, ""subnet_id"", each.value.subnet_id)
        }
      }
    }
  }

  lifecycle {
    # TODO Instance Configuration replacement without delete when supported:
    # https://github.com/hashicorp/terraform/issues/15485
    create_before_destroy = true
    ignore_changes = [
      defined_tags, freeform_tags, display_name,
      instance_details[0].launch_details[0].metadata,
      instance_details[0].launch_details[0].defined_tags,
      instance_details[0].launch_details[0].freeform_tags,
      instance_details[0].launch_details[0].create_vnic_details[0].defined_tags,
      instance_details[0].launch_details[0].create_vnic_details[0].freeform_tags,
      instance_details[0].secondary_vnics,
    ]
  }
}
",resource,90,159.0,4d2b3f3d672a8f41655da3a7c58fded42c6858f3,a1fdfcb7de5e777f0191a43940ef276175a20ba9,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/4d2b3f3d672a8f41655da3a7c58fded42c6858f3/modules/workergroup/instanceconfig.tf#L90,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/a1fdfcb7de5e777f0191a43940ef276175a20ba9/modules/workers/instanceconfig.tf#L159,2023-10-25 16:40:02+11:00,2024-02-12 09:53:40+11:00,22,0,1,0,1,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-vpc-service-controls,1,examples/simple_example/main.tf,examples/simple_example/main.tf,0,todo,## TODO make sure take out interpolation and test again.,## TODO make sure take out interpolation and test again.,"module ""regular-service-perimeter-1"" {
  source         = ""../../modules/regular_service_perimeter""
  policy         = ""${module.org-policy.policy_id}""
  perimeter_name = ""regular_perimeter_1""

  ## TODO make sure take out interpolation and test again.
  description    = ""Perimeter shielding bigquery project ${module.bigquery.dataset_project}""
  resources      = [""743286545054""]

  access_levels = [""${module.access-level-members.name}""]
  restricted_services = [""bigquery.googleapis.com"", ""storage.googleapis.com""]

  shared_resources = {
    all = [""743286545054""]
  }
}
",module,"module ""regular-service-perimeter-1"" {
  source         = ""../../modules/regular_service_perimeter""
  policy         = ""${module.org-policy.policy_id}""
  perimeter_name = ""regular_perimeter_1""

  description = ""Perimeter shielding bigquery project""
  resources   = [""${var.protected_project_ids[""number""]}""]

  access_levels       = [""${module.access-level-members.name}""]
  restricted_services = [""bigquery.googleapis.com"", ""storage.googleapis.com""]

  shared_resources = {
    all = [""${var.protected_project_ids[""number""]}""]
  }
}
",module,43,,07ef41e99d7389c0b4a31c4a67a12a7ab02627bd,62b68f8bcd72cd9bd8ea3d9b2d7cdc38de03fa98,https://github.com/terraform-google-modules/terraform-google-vpc-service-controls/blob/07ef41e99d7389c0b4a31c4a67a12a7ab02627bd/examples/simple_example/main.tf#L43,https://github.com/terraform-google-modules/terraform-google-vpc-service-controls/blob/62b68f8bcd72cd9bd8ea3d9b2d7cdc38de03fa98/examples/simple_example/main.tf,2019-04-26 00:29:28+00:00,2019-05-07 22:41:50+00:00,8,1,0,1,0,0,0,0,0,1
https://github.com/ministryofjustice/aws-root-account,35,organisation-security/terraform/license-manager.tf,organisation-security/terraform/license-manager.tf,0,todo,# TODO get some values below from stack above or make dependant on stack,# TODO get some values below from stack above or make dependant on stack,"resource ""aws_ssm_association"" ""license_manager"" {
  name             = ""OracleDbLTS-Orchestrate""
  association_name = ""OracleDbLicenseTrackingSolutionAssociation""

  # schedule_expression = ""0 1 * * *""
  max_concurrency = 4
  max_errors      = 4
  parameters = {
    AutomationAssumeRole = ""arn:aws:iam::${data.aws_caller_identity.current.id}:role/OracleDbLTS-SystemsManagerAutomationAdministrationRole""
    DeploymentTargets    = local.ou_example
    TargetRegions        = ""eu-west-2""
  }
}
",resource,"resource ""aws_ssm_association"" ""license_manager"" {
  name             = ""OracleDbLTS-Orchestrate""
  association_name = ""OracleDbLicenseTrackingSolutionAssociation""

  schedule_expression = ""0 1 * * *""
  max_concurrency = 4
  max_errors      = 4
  parameters = {
    AutomationAssumeRole = ""arn:aws:iam::${data.aws_caller_identity.current.id}:role/OracleDbLTS-SystemsManagerAutomationAdministrationRole""
    DeploymentTargets    = local.ou_example
    # DeploymentTargets    = local.ou_modernisation_platform_member_id
    TargetRegions        = ""eu-west-2""
  }

  depends_on = [
    aws_cloudformation_stack.oracleblts
  ]
}
",resource,108,,a00bc6abc4748ab2df805939dae7b5e7638befe8,f78db4476f9f701d6b0d34a43ef3b7471e5d7e77,https://github.com/ministryofjustice/aws-root-account/blob/a00bc6abc4748ab2df805939dae7b5e7638befe8/organisation-security/terraform/license-manager.tf#L108,https://github.com/ministryofjustice/aws-root-account/blob/f78db4476f9f701d6b0d34a43ef3b7471e5d7e77/organisation-security/terraform/license-manager.tf,2023-09-13 15:13:48+01:00,2023-09-13 15:13:48+01:00,2,1,1,1,0,0,0,0,0,0
https://github.com/alphagov/govuk-aws,942,terraform/projects/app-licensify-backend/main.tf,terraform/projects/app-licensify-backend/main.tf,0,xxx,"# XXX not sure if passing a map literal works; if not, try making it a local","# XXX not sure if passing a map literal works; if not, try making it a local 
 # and assigning it here using string interpolation.","module ""licensify-backend"" {
  source = ""../../modules/aws/node_group""
  name   = ""${var.stackname}-licensify-backend""

  # XXX not sure if passing a map literal works; if not, try making it a local
  # and assigning it here using string interpolation.
  default_tags = {
    Project         = ""${var.stackname}""
    aws_stackname   = ""${var.stackname}""
    aws_environment = ""${var.aws_environment}""
    aws_migration   = ""licensing_backend""
    aws_hostname    = ""licensify-backend-1""
  }

  instance_subnet_ids               = ""${data.terraform_remote_state.infra_networking.private_subnet_ids}""
  instance_security_group_ids       = [""${data.terraform_remote_state.infra_security_groups.sg_licensify-backend_id}"", ""${data.terraform_remote_state.infra_security_groups.sg_management_id}""]
  instance_type                     = ""${var.instance_type}""
  instance_additional_user_data     = ""${join(""\n"", null_resource.user_data.*.triggers.snippet)}""
  instance_target_group_arns_length = ""1""
  instance_target_group_arns        = [""${module.internal_lb.target_group_arns[0]}""]
  instance_ami_filter_name          = ""${var.instance_ami_filter_name}""
  asg_max_size                      = ""${var.asg_size}""
  asg_min_size                      = ""${var.asg_size}""
  asg_desired_capacity              = ""${var.asg_size}""
  asg_notification_topic_arn        = ""${data.terraform_remote_state.infra_monitoring.sns_topic_autoscaling_group_events_arn}""
}
",module,"module ""licensify-backend"" {
  source = ""../../modules/aws/node_group""
  name   = ""licensify-backend""

  default_tags = {
    Project         = ""${var.stackname}""
    aws_stackname   = ""${var.stackname}""
    aws_environment = ""${var.aws_environment}""
    aws_migration   = ""licensing_backend""
    aws_hostname    = ""licensify-backend-1""
  }

  instance_subnet_ids               = ""${data.terraform_remote_state.infra_networking.private_subnet_ids}""
  instance_security_group_ids       = [""${data.terraform_remote_state.infra_security_groups.sg_licensify-backend_id}"", ""${data.terraform_remote_state.infra_security_groups.sg_management_id}""]
  instance_type                     = ""${var.instance_type}""
  instance_additional_user_data     = ""${join(""\n"", null_resource.user_data.*.triggers.snippet)}""
  instance_target_group_arns_length = ""1""
  instance_target_group_arns        = [""${module.internal_lb.target_group_arns[0]}""]
  instance_ami_filter_name          = ""${var.instance_ami_filter_name}""
  asg_max_size                      = ""${var.asg_size}""
  asg_min_size                      = ""${var.asg_size}""
  asg_desired_capacity              = ""${var.asg_size}""
  asg_notification_topic_arn        = ""${data.terraform_remote_state.infra_monitoring.sns_topic_autoscaling_group_events_arn}""
}
",module,117,,5a72a7fddd6d14951653f13bbb93b6096b2a3a73,4337784ffc75431260bcc9f5f58563f8a04e66db,https://github.com/alphagov/govuk-aws/blob/5a72a7fddd6d14951653f13bbb93b6096b2a3a73/terraform/projects/app-licensify-backend/main.tf#L117,https://github.com/alphagov/govuk-aws/blob/4337784ffc75431260bcc9f5f58563f8a04e66db/terraform/projects/app-licensify-backend/main.tf,2019-08-08 17:17:58+01:00,2019-08-09 10:44:09+01:00,2,1,0,1,0,0,0,0,0,0
https://github.com/camptocamp/devops-stack,96,examples/eks-aws/main.tf,examples/eks/main.tf,1,todo,"/*Available only in provider hashicorp/aws >= v4.0.0 resource ""random_string"" ""admin_password"" {   length  = 25   special = false } # TODO create an output for this password  resource ""aws_cognito_user"" ""admin"" {   user_pool_id = aws_cognito_user_pool.admin.id   username = admin   password = random_string.admin_password.result    message_action = SUPRESS # Do not send welcome message since password is hardcoded and email is non-existant    attributes = {     email = ""admin@example.org""     email_verified = true     terraform = true   } }  resource ""aws_cognito_user_in_group"" ""add_admin_argocd_admin"" {   user_pool_id = aws_cognito_user_pool.admin.id   group_name   = aws_cognito_user_group.argocd_admin_group.name   username     = aws_cognito_user.admin.username }*/","/* Available only in provider hashicorp/aws >= v4.0.0 
 resource ""random_string"" ""admin_password"" { 
 length  = 25 
 special = false 
 } # TODO create an output for this password 
  
 resource ""aws_cognito_user"" ""admin"" { 
 user_pool_id = aws_cognito_user_pool.admin.id 
 username = admin 
 password = random_string.admin_password.result 
  
 message_action = SUPRESS # Do not send welcome message since password is hardcoded and email is non-existant 
  
 attributes = { 
 email = ""admin@example.org"" 
 email_verified = true 
 terraform = true 
 } 
 } 
  
 resource ""aws_cognito_user_in_group"" ""add_admin_argocd_admin"" { 
 user_pool_id = aws_cognito_user_pool.admin.id 
 group_name   = aws_cognito_user_group.argocd_admin_group.name 
 username     = aws_cognito_user.admin.username 
 } 
 */ ","module ""eks"" {
  source = ""git::https://github.com/camptocamp/devops-stack.git//modules/eks/aws?ref=v1""

  cluster_name = ""gh-v1-cluster""
  base_domain  = ""is-sandbox.camptocamp.com""
  # cluster_version = ""1.22""

  vpc_id         = module.vpc.vpc_id
  vpc_cidr_block = module.vpc.vpc_cidr_block

  private_subnet_ids = module.vpc.private_subnets
  public_subnet_ids  = module.vpc.public_subnets

  cluster_endpoint_public_access_cidrs = [""0.0.0.0/0""]

  node_groups = {
    ""${module.eks.cluster_name}-main"" = {
      instance_type     = ""m5a.large""
      min_size          = 2
      max_size          = 3
      desired_size      = 2
      target_group_arns = module.eks.nlb_target_groups
    },
  }

  create_public_nlb = true
}
",module,"module ""eks"" {
  source = ""git::https://github.com/camptocamp/devops-stack-module-cluster-eks?ref=v2.0.2""

  cluster_name       = local.cluster_name
  kubernetes_version = local.kubernetes_version
  base_domain        = local.base_domain

  vpc_id             = module.vpc.vpc_id
  private_subnet_ids = module.vpc.private_subnets
  public_subnet_ids  = module.vpc.public_subnets

  cluster_endpoint_public_access_cidrs = [""0.0.0.0/0""]

  node_groups = {
    ""${module.eks.cluster_name}-main"" = {
      instance_type     = ""m5a.large""
      min_size          = 2
      max_size          = 3
      desired_size      = 2
      target_group_arns = module.eks.nlb_target_groups
    },
  }

  create_public_nlb = true
}
",module,41,,23a76321726eca45b1852f9cbb9a5a46dd17c13e,9f09347de36de8f813051eae5c981dc8cae5c393,https://github.com/camptocamp/devops-stack/blob/23a76321726eca45b1852f9cbb9a5a46dd17c13e/examples/eks-aws/main.tf#L41,https://github.com/camptocamp/devops-stack/blob/9f09347de36de8f813051eae5c981dc8cae5c393/examples/eks/main.tf,2023-04-03 16:40:29+02:00,2023-08-18 14:48:32+02:00,4,1,1,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,24,main.tf,main.tf,0,#todo,"#TODO: terraform 0.12 will enable ""time_partitioning ? time_partitioning_is_required : null"" (https://github.com/hashicorp/terraform/issues/17968)","#TODO: terraform 0.12 will enable ""time_partitioning ? time_partitioning_is_required : null"" (https://github.com/hashicorp/terraform/issues/17968)","resource ""google_bigquery_table"" ""main"" {
  dataset_id = ""${google_bigquery_dataset.main.dataset_id}""
  table_id   = ""${var.table_id}""
  project    = ""${var.project_id}""

  #TODO: terraform 0.12 will enable ""time_partitioning ? time_partitioning_is_required : null"" (https://github.com/hashicorp/terraform/issues/17968)
  time_partitioning {
    type = ""${var.time_partitioning}""
  }

  labels = ""${var.table_labels}""

  schema = ""${file(""${var.schema_file}"")}""
}
",resource,"resource ""google_bigquery_table"" ""main"" {
  dataset_id = ""${google_bigquery_dataset.main.dataset_id}""
  table_id   = ""${var.table_id}""
  project    = ""${var.project_id}""

  time_partitioning {
    type = ""${var.time_partitioning}""
  }

  labels = ""${var.table_labels}""

  schema = ""${file(""${var.schema_file}"")}""
}
",resource,48,,7f922f7e9df197df38c9b09dfa6e3614d71f19f5,a7966ae4afc7bb73842fb9fd6f0f708b359cf36e,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/7f922f7e9df197df38c9b09dfa6e3614d71f19f5/main.tf#L48,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/a7966ae4afc7bb73842fb9fd6f0f708b359cf36e/main.tf,2019-01-16 18:10:54-05:00,2019-01-28 12:41:47-05:00,2,1,0,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,614,examples/data-solutions/data-platform-foundations/variables.tf,examples/data-solutions/data-platform-foundations/variables.tf,0,#todo,#TODO hardcoded,#TODO hardcoded,"variable ""composer_config"" {
  type = object({
    node_count = number
    #TODO Move to network
    ip_range_cloudsql   = string
    ip_range_gke_master = string
    ip_range_web_server = string
    #TODO hardcoded
    project_policy_boolean = map(bool)
    region                 = string
    ip_allocation_policy = object({
      use_ip_aliases                = string
      cluster_secondary_range_name  = string
      services_secondary_range_name = string
    })
    #TODO Add Env variables, Airflow version
  })
  default = {
    node_count             = 3
    ip_range_cloudsql      = ""10.20.10.0/24""
    ip_range_gke_master    = ""10.20.11.0/28""
    ip_range_web_server    = ""10.20.11.16/28""
    project_policy_boolean = null
    region                 = ""europe-west1""
    ip_allocation_policy = {
      use_ip_aliases                = ""true""
      cluster_secondary_range_name  = ""pods""
      services_secondary_range_name = ""services""
    }
  }
}
",variable,"variable ""composer_config"" {
  type = object({
    node_count      = number
    airflow_version = string
    env_variables   = map(string)
  })
  default = {
    node_count      = 3
    airflow_version = ""composer-1.17.5-airflow-2.1.4""
    env_variables   = {}
  }
}
",variable,37,,2e560407c118e7b7abc32f8ac1788a3f48563f21,d8bad5779036aa31639e4611e4935287fc79a4bc,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2e560407c118e7b7abc32f8ac1788a3f48563f21/examples/data-solutions/data-platform-foundations/variables.tf#L37,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d8bad5779036aa31639e4611e4935287fc79a4bc/examples/data-solutions/data-platform-foundations/variables.tf,2022-02-07 17:51:06+01:00,2022-02-07 21:28:54+01:00,2,1,0,1,0,1,0,0,0,0
https://github.com/alphagov/govuk-aws,1064,terraform/projects/app-backend/main.tf,terraform/projects/app-backend/main.tf,0,#todo,#TODO: create new security group with alb name,#TODO: create new security group with alb name,"module ""backend_internal_alb"" {
  source                           = ""../../modules/aws/lb""
  name                             = ""${var.stackname}-backend-internal""
  internal                         = true
  vpc_id                           = ""${data.terraform_remote_state.infra_vpc.vpc_id}""
  access_logs_bucket_name          = ""${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}""
  access_logs_bucket_prefix        = ""elb/${var.stackname}-backend-internal-alb""
  listener_certificate_domain_name = ""${var.elb_internal_certname}""
  listener_action                  = ""${map(""HTTPS:443"", ""HTTP:80"")}""
  subnets                          = [""${data.terraform_remote_state.infra_networking.private_subnet_ids}""]

  #TODO: create new security group with alb name
  security_groups = [""${data.terraform_remote_state.infra_security_groups.sg_backend_elb_internal_id}""]
  alarm_actions   = [""${data.terraform_remote_state.infra_monitoring.sns_topic_cloudwatch_alarms_arn}""]
  default_tags    = ""${map(""Project"", var.stackname, ""aws_migration"", ""backend"", ""aws_environment"", var.aws_environment)}""
}
",module,,,131,0.0,75a0ad6f9093136bed47e62609c049e1bc944505,241558af7d6786415e64eea48e193f23518625c2,https://github.com/alphagov/govuk-aws/blob/75a0ad6f9093136bed47e62609c049e1bc944505/terraform/projects/app-backend/main.tf#L131,https://github.com/alphagov/govuk-aws/blob/241558af7d6786415e64eea48e193f23518625c2/terraform/projects/app-backend/main.tf#L0,2020-05-06 17:19:09+01:00,2023-05-10 15:30:22+01:00,9,2,0,1,0,1,1,0,0,0
https://github.com/kubernetes/k8s.io,2,clusters/k8s-infra-dev-cluster-turnup/test-us/10-cluster-configuration.tf,infra/gcp/clusters/_example/test-us/10-cluster-configuration.tf,1,// todo,// TODO: we should turn this on,enabled = false // TODO: we should turn this on,"resource ""google_container_cluster"" ""cluster"" {
  name     = local.cluster_name
  location = local.cluster_location

  provider = google-beta
  project  = data.google_project.project.id

  // GKE clusters are critical objects and should not be destroyed
  // IMPORTANT: should be true on production cluster
  lifecycle {
    prevent_destroy = false
  }

  // Network config
  network = ""default""
  ip_allocation_policy {
    use_ip_aliases    = true
    create_subnetwork = true
  }

  // Start with a single node, because we're going to delete the default pool
  initial_node_count = 1

  // Removes the default node pool, so we can custom create them as separate
  // objects
  remove_default_node_pool = true

  // Disable local and certificate auth
  master_auth {
    username = """"
    password = """"

    client_certificate_config {
      issue_client_certificate = false
    }
  }

  // Enable google-groups for RBAC
  authenticator_groups_config {
    security_group = ""gke-security-groups@kubernetes.io""
  }

  // Enable workload identity for GCP IAM
  workload_identity_config {
    identity_namespace = ""${data.google_project.project.id}.svc.id.goog""
  }

  // Enable Stackdriver Kubernetes Monitoring
  logging_service    = ""logging.googleapis.com/kubernetes""
  monitoring_service = ""monitoring.googleapis.com/kubernetes""

  // Set maintenance time
  maintenance_policy {
    daily_maintenance_window {
      start_time = ""11:00"" // (in UTC), 03:00 PST
    }
  }

  // Restrict master to Google IP space; use Cloud Shell to access
  master_authorized_networks_config {
  }

  // Enable GKE Usage Metering
  resource_usage_export_config {
    enable_network_egress_metering = true
    bigquery_destination {
      dataset_id = google_bigquery_dataset.usage_metering.dataset_id
    }
  }

  // Enable GKE Network Policy
  network_policy {
    enabled  = true
    provider = ""CALICO""
  }

  // Configure cluster addons
  addons_config {
    horizontal_pod_autoscaling {
      disabled = false
    }
    http_load_balancing {
      disabled = false
    }
    network_policy_config {
      disabled = false
    }
  }

  // Enable PodSecurityPolicy enforcement
  pod_security_policy_config {
    enabled = false // TODO: we should turn this on
  }

  // Enable VPA
  vertical_pod_autoscaling {
    enabled = true
  }
}
",resource,,,153,0.0,7ccb2fdde41974545511928d0eefc85a4403628c,883efd41e26032afd09a8adcd87769ffb5f06e21,https://github.com/kubernetes/k8s.io/blob/7ccb2fdde41974545511928d0eefc85a4403628c/clusters/k8s-infra-dev-cluster-turnup/test-us/10-cluster-configuration.tf#L153,https://github.com/kubernetes/k8s.io/blob/883efd41e26032afd09a8adcd87769ffb5f06e21/infra/gcp/clusters/_example/test-us/10-cluster-configuration.tf#L0,2019-09-11 10:40:20-07:00,2021-05-07 13:12:04-04:00,2,2,1,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-bootstrap,1,modules/tf_cloudbuild_builder/cb.tf,modules/tf_cloudbuild_builder/cb.tf,0,# todo,# todo(bharathkkb): switch to yaml after https://github.com/hashicorp/terraform-provider-google/issues/9818,# todo(bharathkkb): switch to yaml after https://github.com/hashicorp/terraform-provider-google/issues/9818,"resource ""google_cloudbuild_trigger"" ""build_trigger"" {
  project     = var.project_id
  name        = var.trigger_name
  description = ""Builds a Terraform runner image. Managed by Terraform.""
  source_to_build {
    uri       = var.dockerfile_repo_uri
    ref       = var.dockerfile_repo_ref
    repo_type = var.dockerfile_repo_type
  }

  # todo(bharathkkb): switch to yaml after https://github.com/hashicorp/terraform-provider-google/issues/9818
  build {
    step {
      name = ""gcr.io/cloud-builders/docker""
      args = concat(
        [""build""],
        [for img_tag in local.img_tags_subst : ""--tag=${img_tag}""],
        [""--build-arg=TERRAFORM_VERSION=$${_TERRAFORM_FULL_VERSION}"", "".""]
      )
      dir = var.dockerfile_repo_dir != """" ? var.dockerfile_repo_dir : null
    }
    step {
      name = ""${local.gar_uri}:v$${_TERRAFORM_FULL_VERSION}""
      args = [""version""]
    }
    images      = local.img_tags_subst
    logs_bucket = module.bucket.bucket.url
  }

  substitutions   = local.tags_subst
  service_account = local.cloudbuild_sa

  depends_on = [
    google_artifact_registry_repository_iam_member.push_images,
    google_project_iam_member.logs_writer
  ]
}
",resource,"resource ""google_cloudbuild_trigger"" ""build_trigger"" {
  project     = var.project_id
  location    = var.trigger_location
  name        = var.trigger_name
  description = ""Builds a Terraform runner image. Managed by Terraform.""
  source_to_build {
    uri       = var.dockerfile_repo_uri
    ref       = var.dockerfile_repo_ref
    repo_type = var.dockerfile_repo_type
  }

  # todo(bharathkkb): switch to yaml after https://github.com/hashicorp/terraform-provider-google/issues/9818
  build {
    timeout = var.build_timeout
    step {
      name = ""gcr.io/cloud-builders/docker""
      args = concat(
        [""build""],
        [for img_tag in local.img_tags_subst : ""--tag=${img_tag}""],
        [""--build-arg=TERRAFORM_VERSION=$${_TERRAFORM_FULL_VERSION}"", "".""]
      )
      dir = var.dockerfile_repo_dir != """" ? var.dockerfile_repo_dir : null
    }
    step {
      name = ""${local.gar_uri}:v$${_TERRAFORM_FULL_VERSION}""
      args = [""version""]
    }
    images      = local.img_tags_subst
    logs_bucket = module.bucket.bucket.url

    dynamic ""options"" {
      for_each = var.enable_worker_pool ? [""worker_pool""] : []
      content {
        worker_pool = var.worker_pool_id
      }
    }
  }

  substitutions   = local.tags_subst
  service_account = local.cloudbuild_sa

  depends_on = [
    google_artifact_registry_repository_iam_member.push_images,
    google_project_iam_member.logs_writer
  ]
}
",resource,47,54.0,34120e579528dfb72dddace0485d38efaf9202bd,7c8477bd6137745176d27a4e092c997b0da64149,https://github.com/terraform-google-modules/terraform-google-bootstrap/blob/34120e579528dfb72dddace0485d38efaf9202bd/modules/tf_cloudbuild_builder/cb.tf#L47,https://github.com/terraform-google-modules/terraform-google-bootstrap/blob/7c8477bd6137745176d27a4e092c997b0da64149/modules/tf_cloudbuild_builder/cb.tf#L54,2022-05-27 17:27:37-05:00,2024-05-20 17:22:47-05:00,10,0,0,1,1,0,0,1,0,0
https://github.com/Azure/terraform-azurerm-caf-enterprise-scale,64,resources.role_assignments.tf,resources.role_assignments.tf,0,fix,# This was implemented to fix issue:,"# The following module is used to generate the Role 
 # Assignments for Policy Assignments as needed. 
 # This was implemented to fix issue: 
 # https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/issues/266","module ""role_assignments_for_policy"" {
  for_each = local.es_role_assignments_by_policy_assignment
  source   = ""./modules/role_assignments_for_policy""

  # Mandatory resource attributes
  policy_assignment_id = each.key
  scope_id             = azurerm_management_group_policy_assignment.enterprise_scale[each.key].management_group_id
  principal_id         = azurerm_management_group_policy_assignment.enterprise_scale[each.key].identity[0].principal_id
  role_definition_ids  = each.value

  # Optional resource attributes
  additional_scope_ids = local.empty_list

  # Set explicit dependency on Management Group, Policy Definition, Policy Set Definition, and Policy Assignment deployments
  depends_on = [
    time_sleep.after_azurerm_management_group,
    time_sleep.after_azurerm_policy_definition,
    time_sleep.after_azurerm_policy_set_definition,
    time_sleep.after_azurerm_policy_assignment,
    azurerm_role_assignment.policy_assignment,
  ]

}
",module,"module ""role_assignments_for_policy"" {
  for_each = local.es_role_assignments_by_policy_assignment
  source   = ""./modules/role_assignments_for_policy""

  # Mandatory resource attributes
  policy_assignment_id = each.key
  scope_id             = azurerm_management_group_policy_assignment.enterprise_scale[each.key].management_group_id
  principal_id = (
    lookup(azurerm_management_group_policy_assignment.enterprise_scale[each.key].identity[0], ""type"") == ""UserAssigned""
    ? jsondecode(data.azapi_resource.user_msi[each.key].output).properties.principalId # workarround as azurerm_management_group_policy_assignment does not export the principal_id when using UserAssigned identity
    : azurerm_management_group_policy_assignment.enterprise_scale[each.key].identity[0].principal_id
  )
  role_definition_ids = each.value

  # Optional resource attributes
  additional_scope_ids = local.empty_list

  # Set explicit dependency on Management Group, Policy Definition, Policy Set Definition, and Policy Assignment deployments
  depends_on = [
    time_sleep.after_azurerm_management_group,
    time_sleep.after_azurerm_policy_definition,
    time_sleep.after_azurerm_policy_set_definition,
    time_sleep.after_azurerm_policy_assignment,
    azurerm_role_assignment.policy_assignment,
  ]

}
",module,26,26.0,19c124a47efdbed7c92c8e4f1235885520dfe474,1ac8cb891565c233eb4cc7edb8cf75c8b0d82cf2,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/19c124a47efdbed7c92c8e4f1235885520dfe474/resources.role_assignments.tf#L26,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/1ac8cb891565c233eb4cc7edb8cf75c8b0d82cf2/resources.role_assignments.tf#L26,2022-03-31 16:23:03+01:00,2024-04-11 10:48:07+01:00,5,0,0,0,1,1,0,0,0,0
https://github.com/ministryofjustice/cloud-platform-infrastructure,168,terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf,terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf,0,workaround,"# Out of the box you can't specify groups to map, just users. Some people did some workarounds","# Out of the box you can't specify groups to map, just users. Some people did some workarounds 
 # we can explore later: https://ygrene.tech/mapping-iam-groups-to-eks-user-access-66fd745a6b77","module ""eks"" {
  source  = ""terraform-aws-modules/eks/aws""
  version = ""v12.2.0""

  cluster_name     = local.cluster_name
  subnets          = concat(tolist(data.aws_subnet_ids.private.ids), tolist(data.aws_subnet_ids.public.ids))
  vpc_id           = data.aws_vpc.selected.id
  write_kubeconfig = false
  cluster_version  = ""1.17""
  enable_irsa      = true

  node_groups = {
    default_ng = {
      desired_capacity = var.cluster_node_count
      max_capacity     = 30
      min_capacity     = 1
      subnets          = data.aws_subnet_ids.private.ids

      instance_type = var.worker_node_machine_type
      k8s_labels = {
        Terraform = ""true""
        Cluster   = local.cluster_name
        Domain    = local.cluster_base_domain_name
      }
      additional_tags = {
        default_ng = ""true""
      }
    }
  }

  # Out of the box you can't specify groups to map, just users. Some people did some workarounds
  # we can explore later: https://ygrene.tech/mapping-iam-groups-to-eks-user-access-66fd745a6b77
  map_users = [
    {
      userarn  = ""arn:aws:iam::754256621582:user/AlejandroGarrido""
      username = ""AlejandroGarrido""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/PoornimaKrishnasamy""
      username = ""PoornimaKrishnasamy""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/paulWyborn""
      username = ""paulWyborn""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/SabluMiah""
      username = ""SabluMiah""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/jasonBirchall""
      username = ""jasonBirchall""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/RazvanCosma""
      username = ""RazvanCosma""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/cloud-platform/manager-concourse""
      username = ""manager-concourse""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/SteveMarshall""
      username = ""SteveMarshall""
      groups   = [""system:masters""]
    }


  ]

  tags = {
    Terraform = ""true""
    Cluster   = local.cluster_name
    Domain    = local.cluster_base_domain_name
  }
}
",module,"module ""eks"" {
  source  = ""terraform-aws-modules/eks/aws""
  version = ""18.31.2""


  cluster_name              = terraform.workspace
  subnet_ids                = concat(tolist(data.aws_subnets.private.ids), tolist(data.aws_subnets.public.ids))
  vpc_id                    = data.aws_vpc.selected.id
  cluster_version           = lookup(local.cluster_version, terraform.workspace, local.cluster_version[""default""])
  enable_irsa               = true
  cluster_enabled_log_types = var.cluster_enabled_log_types

  cloudwatch_log_group_retention_in_days = var.cluster_log_retention_in_days
  cluster_security_group_description     = ""EKS cluster security group.""
  cluster_security_group_name            = terraform.workspace

  create_node_security_group = false
  node_security_group_id     = aws_security_group.node.id

  iam_role_name    = terraform.workspace
  prefix_separator = """"

  eks_managed_node_groups = {
    default_ng_12_12_23    = local.default_ng_12_12_23
    monitoring_ng_12_12_23 = local.monitoring_ng_12_12_23
  }

  iam_role_additional_policies = [""arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore""]
  # Out of the box you can't specify groups to map, just users. Some people did some workarounds
  # we can explore later: https://ygrene.tech/mapping-iam-groups-to-eks-user-access-66fd745a6b77
  manage_aws_auth_configmap = true
  aws_auth_users = [
    {
      userarn  = ""arn:aws:iam::754256621582:user/PoornimaKrishnasamy""
      username = ""PoornimaKrishnasamy""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/SabluMiah""
      username = ""SabluMiah""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/SteveMarshall""
      username = ""SteveMarshall""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/JackStockley""
      username = ""JackStockley""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/SteveWilliams""
      username = ""SteveWilliams""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/JaskaranSarkaria""
      username = ""JaskaranSarkaria""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/TomSmith""
      username = ""TomSmith""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/cloud-platform/manager-concourse""
      username = ""manager-concourse""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/KyTruong""
      username = ""KyTruong""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/MikeBell""
      username = ""MikeBell""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/DavidElliott""
      username = ""DavidElliott""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/TariqMahmood""
      username = ""TariqMahmood""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/TimCheung""
      username = ""TimCheung""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/FolarinOyenuga""
      username = ""FolarinOyenuga""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/AafreenAnsari""
      username = ""AafreenAnsari""
      groups   = [""system:masters""]
    }
  ]

  tags = local.tags
}
",module,51,192.0,71503ea4248810176ad4883eca86b74a213ac9c9,41c026574305e60538a0740ad17a4f884e51a89b,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/71503ea4248810176ad4883eca86b74a213ac9c9/terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf#L51,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/41c026574305e60538a0740ad17a4f884e51a89b/terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf#L192,2021-03-30 09:50:19+01:00,2024-05-09 14:36:48+01:00,141,0,1,1,0,1,0,0,0,0
https://github.com/awslabs/data-on-eks,40,streaming/spark-streaming/terraform/variables.tf,streaming/spark-streaming/terraform/variables.tf,0,implementation,# There is no need for Yunikorn in this implementation,"description = ""Enable Apache YuniKorn Scheduler"" # There is no need for Yunikorn in this implementation","variable ""enable_yunikorn"" {
  default     = false
  description = ""Enable Apache YuniKorn Scheduler"" # There is no need for Yunikorn in this implementation
  type        = bool
}
",variable,"variable ""enable_yunikorn"" {
  default     = false
  description = ""Enable Apache YuniKorn Scheduler"" # There is no need for Yunikorn in this implementation
  type        = bool
}
",variable,71,71.0,6fa0ca6fb449ccd3842dda6405b7cb0d06388665,6fa0ca6fb449ccd3842dda6405b7cb0d06388665,https://github.com/awslabs/data-on-eks/blob/6fa0ca6fb449ccd3842dda6405b7cb0d06388665/streaming/spark-streaming/terraform/variables.tf#L71,https://github.com/awslabs/data-on-eks/blob/6fa0ca6fb449ccd3842dda6405b7cb0d06388665/streaming/spark-streaming/terraform/variables.tf#L71,2024-05-16 10:57:54-07:00,2024-05-16 10:57:54-07:00,1,0,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,850,fast/stages/03-gke-multitenant/prod/gke-clusters.tf,blueprints/gke/multitenant-fleet/gke-clusters.tf,1,# todo,"# TODO: the attributes below are ""primed"" from project-level defaults","# resource_usage_export_config = { 
 #   enabled = true 
 #   dataset = module.gke-dataset-resource-usage.id 
 # } 
 # TODO: the attributes below are ""primed"" from project-level defaults 
 #       in locals, merge defaults with cluster-level stuff 
 # TODO(jccb): change fabric module","module ""gke-cluster"" {
  source                   = ""../../../../modules/gke-cluster""
  for_each                 = local.clusters
  project_id               = module.gke-project-0.project_id
  name                     = each.key
  description              = each.value.description
  location                 = each.value.location
  network                  = each.value.net.vpc
  subnetwork               = each.value.net.subnet
  secondary_range_pods     = each.value.net.pods
  secondary_range_services = each.value.net.services
  labels                   = each.value.labels
  addons = {
    cloudrun_config                       = each.value.overrides.cloudrun_config
    dns_cache_config                      = true
    http_load_balancing                   = true
    gce_persistent_disk_csi_driver_config = true
    horizontal_pod_autoscaling            = true
    config_connector_config               = true
    kalm_config                           = false
    # enable only if enable_dataplane_v2 is changed to false below
    network_policy_config = false
    istio_config = {
      enabled = false
      tls     = false
    }
  }
  # change these here for all clusters if absolutely needed
  # authenticator_security_group = var.authenticator_security_group
  enable_dataplane_v2         = true
  enable_l4_ilb_subsetting    = false
  enable_intranode_visibility = true
  enable_shielded_nodes       = true
  workload_identity           = true
  private_cluster_config = {
    enable_private_nodes    = true
    enable_private_endpoint = true
    master_ipv4_cidr_block  = each.value.net.master_range
    master_global_access    = true
  }
  dns_config = each.value.dns_domain == null ? null : {
    cluster_dns        = ""CLOUD_DNS""
    cluster_dns_scope  = ""VPC_SCOPE""
    cluster_dns_domain = ""${each.key}.${var.dns_domain}""
  }
  logging_config    = [""SYSTEM_COMPONENTS"", ""WORKLOADS""]
  monitoring_config = [""SYSTEM_COMPONENTS"", ""WORKLOADS""]

  # if you don't have compute.networks.updatePeering in the host
  # project, comment out the next line and ask your network admin to
  # create the peering for you
  peering_config = {
    export_routes = true
    import_routes = false
    project_id    = var.vpc_host_project
  }
  # resource_usage_export_config = {
  #   enabled = true
  #   dataset = module.gke-dataset-resource-usage.id
  # }
  # TODO: the attributes below are ""primed"" from project-level defaults
  #       in locals, merge defaults with cluster-level stuff
  # TODO(jccb): change fabric module
  database_encryption = (
    each.value.overrides.database_encryption_key == null ? {
      enabled  = false
      state    = null
      key_name = null
      } : {
      enabled  = true
      state    = ""ENCRYPTED""
      key_name = each.value.overrides.database_encryption_key
    }
  )
  default_max_pods_per_node   = each.value.overrides.max_pods_per_node
  enable_binary_authorization = each.value.overrides.enable_binary_authorization
  master_authorized_ranges    = each.value.overrides.master_authorized_ranges
  pod_security_policy         = each.value.overrides.pod_security_policy
  release_channel             = each.value.overrides.release_channel
  vertical_pod_autoscaling    = each.value.overrides.vertical_pod_autoscaling
  # dynamic ""cluster_autoscaling"" {
  #   for_each = each.value.cluster_autoscaling == null ? {} : { 1 = 1 }
  #   content {
  #     enabled    = true
  #     cpu_min    = each.value.cluster_autoscaling.cpu_min
  #     cpu_max    = each.value.cluster_autoscaling.cpu_max
  #     memory_min = each.value.cluster_autoscaling.memory_min
  #     memory_max = each.value.cluster_autoscaling.memory_max
  #   }
  # }

  depends_on = [
    google_project_iam_member.host_project_bindings
  ]
}
",module,"module ""gke-cluster"" {
  source      = ""../../../modules/gke-cluster""
  for_each    = local.clusters
  name        = each.key
  project_id  = module.gke-project-0.project_id
  description = each.value.description
  location    = each.value.location
  vpc_config = {
    network    = var.vpc_config.vpc_self_link
    subnetwork = each.value.net.subnet
    secondary_range_names = {
      pods     = each.value.net.pods
      services = each.value.net.services
    }
    master_authorized_ranges = each.value.overrides.master_authorized_ranges
  }
  labels = each.value.labels
  enable_addons = {
    cloudrun                       = each.value.overrides.cloudrun_config
    config_connector               = true
    dns_cache                      = true
    gce_persistent_disk_csi_driver = true
    gcp_filestore_csi_driver       = each.value.overrides.gcp_filestore_csi_driver_config
    gke_backup_agent               = false
    horizontal_pod_autoscaling     = true
    http_load_balancing            = true
  }
  enable_features = {
    cloud_dns = var.dns_domain == null ? null : {
      cluster_dns        = ""CLOUD_DNS""
      cluster_dns_scope  = ""VPC_SCOPE""
      cluster_dns_domain = ""${each.key}.${var.dns_domain}""
    }
    database_encryption = (
      each.value.overrides.database_encryption_key == null
      ? null
      : {
        state    = ""ENCRYPTED""
        key_name = each.value.overrides.database_encryption_key
      }
    )
    dataplane_v2         = true
    groups_for_rbac      = var.authenticator_security_group
    intranode_visibility = true
    pod_security_policy  = each.value.overrides.pod_security_policy
    resource_usage_export = {
      dataset = module.gke-dataset-resource-usage.dataset_id
    }
    shielded_nodes           = true
    vertical_pod_autoscaling = each.value.overrides.vertical_pod_autoscaling
    workload_identity        = true
  }
  private_cluster_config = {
    enable_private_endpoint = true
    master_ipv4_cidr_block  = each.value.net.master_range
    master_global_access    = true
    peering_config = var.peering_config == null ? null : {
      export_routes = var.peering_config.export_routes
      import_routes = var.peering_config.import_routes
      project_id    = var.vpc_config.host_project_id
    }
  }
  logging_config    = [""SYSTEM_COMPONENTS"", ""WORKLOADS""]
  monitoring_config = [""SYSTEM_COMPONENTS"", ""WORKLOADS""]
  max_pods_per_node = each.value.overrides.max_pods_per_node
  release_channel   = each.value.overrides.release_channel
}
",module,86,,f3f9a4a88cedd64f6fc91b64666046aa6726a2f3,16822e94ab70d75099214b9db786affcb231fbf6,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/f3f9a4a88cedd64f6fc91b64666046aa6726a2f3/fast/stages/03-gke-multitenant/prod/gke-clusters.tf#L86,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/16822e94ab70d75099214b9db786affcb231fbf6/blueprints/gke/multitenant-fleet/gke-clusters.tf,2022-06-08 11:41:50+02:00,2022-10-10 09:38:21+02:00,15,1,1,1,0,0,0,0,0,0
https://github.com/kubernetes/k8s.io,325,infra/aws/terraform/prow-build-cluster/eks.tf,infra/aws/terraform/prow-build-cluster/eks.tf,0,todo,# TODO - remove this policy once AWS releases a managed version similar to AmazonEKS_CNI_Policy (IPv4),"# We are using the IRSA created below for permissions 
 # However, we have to deploy with the policy attached FIRST (when creating a fresh cluster) 
 # and then turn this off after the cluster/node group is created. Without this initial policy, 
 # the VPC CNI fails to assign IPs and nodes cannot join the cluster 
 # See https://github.com/aws/containers-roadmap/issues/1666 for more context 
 # TODO - remove this policy once AWS releases a managed version similar to AmazonEKS_CNI_Policy (IPv4)","module ""eks"" {
  source  = ""terraform-aws-modules/eks/aws""
  version = ""19.10.0""

  # General cluster properties.
  cluster_name                   = var.cluster_name
  cluster_version                = var.cluster_version
  cluster_endpoint_public_access = true

  # Manage aws-auth ConfigMap.
  manage_aws_auth_configmap = true

  # We use IPv6 because we require a large number of nodes and pods.
  # With IPv4, we can use only /16 VPC, which is a blocker for running
  # 100+ nodes.
  cluster_ip_family = ""ipv6""

  # We are using the IRSA created below for permissions
  # However, we have to deploy with the policy attached FIRST (when creating a fresh cluster)
  # and then turn this off after the cluster/node group is created. Without this initial policy,
  # the VPC CNI fails to assign IPs and nodes cannot join the cluster
  # See https://github.com/aws/containers-roadmap/issues/1666 for more context
  # TODO - remove this policy once AWS releases a managed version similar to AmazonEKS_CNI_Policy (IPv4)
  create_cni_ipv6_iam_policy = true

  vpc_id                   = module.vpc.vpc_id
  subnet_ids               = module.vpc.private_subnets
  control_plane_subnet_ids = module.vpc.intra_subnets

  cluster_addons = {
    coredns = {
      most_recent = true
    }
    kube-proxy = {
      most_recent = true
    }
    vpc-cni = {
      most_recent              = true
      service_account_role_arn = module.vpc_cni_irsa.iam_role_arn
    }
    aws-ebs-csi-driver = {
      most_recent              = true
      service_account_role_arn = module.ebs_csi_irsa.iam_role_arn
    }
  }

  eks_managed_node_group_defaults = {
    ami_id                     = var.node_ami
    enable_bootstrap_user_data = true
    instance_types             = var.node_instance_types

    # We are using the IRSA created below for permissions
    # However, we have to deploy with the policy attached FIRST (when creating a fresh cluster)
    # and then turn this off after the cluster/node group is created. Without this initial policy,
    # the VPC CNI fails to assign IPs and nodes cannot join the cluster
    # See https://github.com/aws/containers-roadmap/issues/1666 for more context
    iam_role_attach_cni_policy = false
  }

  eks_managed_node_groups = {
    # Build cluster node group.
    build = {
      name            = ""build-managed""
      description     = ""EKS managed node group used for build nodes""
      use_name_prefix = true

      subnet_ids = module.vpc.private_subnets

      min_size     = var.node_min_size
      max_size     = var.node_max_size
      desired_size = var.node_desired_size

      ami_id                     = var.node_ami
      enable_bootstrap_user_data = true

      # Force version update if existing pods are unable to be drained due to a PodDisruptionBudget issue.
      force_update_version = true
      update_config = {
        max_unavailable_percentage = 33
      }

      # Required to ensure Prow works well.
      pre_bootstrap_user_data = <<-EOT
        sysctl -w fs.inotify.max_user_watches=524288
      EOT

      capacity_type  = ""ON_DEMAND""
      instance_types = var.node_instance_types

      ebs_optimized     = true
      enable_monitoring = true

      block_device_mappings = {
        xvda = {
          device_name = ""/dev/xvda""
          ebs = {
            volume_size           = var.node_volume_size
            volume_type           = ""gp3""
            iops                  = 16000 # Maximum for gp3 volume.
            throughput            = 1000  # Maximum for gp3 volume.
            encrypted             = false
            delete_on_termination = true
          }
        }
      }

      enclave_options = {
        enabled = true
      }

      tags = local.node_group_tags
    }
  }
}
",module,"module ""eks"" {
  source  = ""terraform-aws-modules/eks/aws""
  version = ""19.10.0""

  # General cluster properties.
  cluster_name                   = var.cluster_name
  cluster_version                = var.cluster_version
  cluster_endpoint_public_access = true

  # Manage aws-auth ConfigMap.
  manage_aws_auth_configmap = true

  # We use IPv4 for the best compatibility with the existing setup.
  # Additionally, Ubuntu EKS optimized AMI doesn't support IPv6 well.
  cluster_ip_family = ""ipv4""

  vpc_id                   = module.vpc.vpc_id
  subnet_ids               = module.vpc.private_subnets
  control_plane_subnet_ids = module.vpc.intra_subnets

  cluster_addons = {
    coredns = {
      most_recent = true
    }
    kube-proxy = {
      most_recent = true
    }
    vpc-cni = {
      most_recent              = true
      service_account_role_arn = module.vpc_cni_irsa.iam_role_arn
    }
    aws-ebs-csi-driver = {
      most_recent              = true
      service_account_role_arn = module.ebs_csi_irsa.iam_role_arn
    }
  }

  eks_managed_node_group_defaults = {
    ami_id                     = var.node_ami
    enable_bootstrap_user_data = true
    instance_types             = var.node_instance_types

    # We are using the IRSA created below for permissions
    # However, we have to deploy with the policy attached FIRST (when creating a fresh cluster)
    # and then turn this off after the cluster/node group is created. Without this initial policy,
    # the VPC CNI fails to assign IPs and nodes cannot join the cluster
    # See https://github.com/aws/containers-roadmap/issues/1666 for more context
    iam_role_attach_cni_policy = true
  }

  eks_managed_node_groups = {
    # Build cluster node group.
    build = {
      name            = ""build-managed""
      description     = ""EKS managed node group used for build nodes""
      use_name_prefix = true

      subnet_ids = module.vpc.private_subnets

      min_size     = var.node_min_size
      max_size     = var.node_max_size
      desired_size = var.node_desired_size

      ami_id                     = var.node_ami
      enable_bootstrap_user_data = true

      # Force version update if existing pods are unable to be drained due to a PodDisruptionBudget issue.
      force_update_version = true
      update_config = {
        max_unavailable_percentage = var.node_max_unavailable_percentage
      }

      # Required to ensure Prow works well.
      pre_bootstrap_user_data = <<-EOT
        sysctl -w fs.inotify.max_user_watches=524288
      EOT

      capacity_type  = ""ON_DEMAND""
      instance_types = var.node_instance_types

      ebs_optimized     = true
      enable_monitoring = true

      block_device_mappings = {
        xvda = {
          device_name = ""/dev/xvda""
          ebs = {
            volume_size           = var.node_volume_size
            volume_type           = ""gp3""
            iops                  = 16000 # Maximum for gp3 volume.
            throughput            = 1000  # Maximum for gp3 volume.
            encrypted             = false
            delete_on_termination = true
          }
        }
      }

      enclave_options = {
        enabled = true
      }

      tags = local.node_group_tags
    }
  }
}
",module,43,,fae0a7eda405a007e05cf1366e820cb8603816d9,d02931c774caf517e6ff91fe4193fdfc4dcb724a,https://github.com/kubernetes/k8s.io/blob/fae0a7eda405a007e05cf1366e820cb8603816d9/infra/aws/terraform/prow-build-cluster/eks.tf#L43,https://github.com/kubernetes/k8s.io/blob/d02931c774caf517e6ff91fe4193fdfc4dcb724a/infra/aws/terraform/prow-build-cluster/eks.tf,2023-03-01 10:38:47+01:00,2023-03-02 17:03:17+01:00,2,1,1,1,1,1,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,245,modules/network/subnets.tf,modules/network/subnets.tf,0,todo,"# TODO enumerate worker pools for public/private overrides, conditional subnets for both
","# Map of subnets for standard components with additional configuration derived 
 # TODO enumerate worker pools for public/private overrides, conditional subnets for both","locals {
  # VCN subnet configuration
  # See https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengnetworkconfig.htm#vcnconfig
  vcn_cidr = element(var.vcn_cidrs, 1)

  # Filter configured subnets eligible for resource creation
  new_subnet_cidrs = {
    for k, v in var.subnets : k => v
    if lookup(v, ""id"", null) == null && lookup(v, ""create"", ""auto"") != ""never""
  }

  # Generate CIDR ranges for subnets to be created
  subnet_cidrs = {
    for k, v in local.new_subnet_cidrs :
    k => cidrsubnet(local.vcn_cidr, lookup(v, ""newbits""), lookup(v, ""netnum""))
  }

  # Map of subnets for standard components with additional configuration derived
  # TODO enumerate worker pools for public/private overrides, conditional subnets for both
  subnet_info = {
    bastion  = { create = var.create_bastion, is_public = var.bastion_is_public }
    cp       = { is_public = var.control_plane_is_public }
    workers  = { is_public = var.worker_is_public }
    pods     = { create = var.cni_type == ""npn"" }
    operator = { create = var.create_operator }
    fss      = { create = var.create_fss }
    int_lb = {
      create         = var.load_balancers == ""internal"" || var.load_balancers == ""both"",
      create_seclist = true, dns_label = ""ilb"",
    }
    pub_lb = {
      create         = var.load_balancers == ""public"" || var.load_balancers == ""both"",
      create_seclist = true, is_public = true, dns_label = ""plb"",
    }
  }
}
",locals,"locals {
  # VCN subnet configuration
  # See https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengnetworkconfig.htm#vcnconfig
  # May be undefined when VCN is neither created nor required, e.g. when creating only workers for
  # an existing cluster. Fallback value is unused.
  vcn_cidr = length(var.vcn_cidrs) > 0 ? element(var.vcn_cidrs, 0) : ""0.0.0.0/16""

  # Filter configured subnets eligible for resource creation
  subnet_cidrs_new = {
    for k, v in var.subnets : k => merge(v, {
      ""type"" = (lookup(v, ""netnum"", null) == null && lookup(v, ""newbits"", null) != null ? ""newbits""
        : (lookup(v, ""netnum"", null) != null && lookup(v, ""newbits"", null) != null ? ""netnum""
          : (lookup(v, ""cidr"", null) != null ? ""cidr""
            : (lookup(v, ""id"", null) != null ? ""id""
      : ""invalid""))))
    })
  }

  # Handle subnets configured with provided CIDRs
  subnet_cidrs_cidr_input = {
    for k, v in local.subnet_cidrs_new : k => lookup(v, ""cidr"") if v.type == ""cidr""
  }

  # Handle subnets configured with only newbits for sizing
  subnet_cidrs_newbits_input = {
    for k, v in local.subnet_cidrs_new : k => lookup(v, ""newbits"") if v.type == ""newbits""
  }

  # Generate CIDR ranges for subnets to be created
  subnet_cidrs_newbits_ranges = cidrsubnets(local.vcn_cidr, values(local.subnet_cidrs_newbits_input)...)
  subnet_cidrs_newbits_resolved = length(local.vcn_cidr) > 0 ? {
    for k, v in local.subnet_cidrs_newbits_input : k => element(local.subnet_cidrs_newbits_ranges, index(keys(local.subnet_cidrs_newbits_input), k))
  } : {}

  # Handle subnets configured with netnum + newbits for sizing
  subnet_cidrs_netnum_newbits_ranges = {
    for k, v in local.subnet_cidrs_new : k => cidrsubnet(local.vcn_cidr, lookup(v, ""newbits""), lookup(v, ""netnum""))
    if v.type == ""netnum""
  }

  // Combine provided and calculated subnet CIDRs
  subnet_cidrs_all = merge(local.subnet_cidrs_cidr_input, local.subnet_cidrs_newbits_resolved, local.subnet_cidrs_netnum_newbits_ranges)

  # Map of subnets for standard components with additional configuration derived
  # TODO enumerate worker pools for public/private overrides, conditional subnets for both
  subnet_info = {
    bastion  = { create = var.create_bastion, is_public = var.bastion_is_public }
    cp       = { create = var.create_cluster, is_public = var.control_plane_is_public }
    workers  = { create = var.create_cluster, is_public = var.worker_is_public }
    pods     = { create = var.create_cluster && var.cni_type == ""npn"" }
    operator = { create = var.create_operator }
    fss      = { create = var.create_fss }
    int_lb = {
      create         = var.create_cluster && contains([""both"", ""internal""], var.load_balancers),
      create_seclist = true, dns_label = ""ilb"",
    }
    pub_lb = {
      create         = var.create_cluster && contains([""both"", ""public""], var.load_balancers),
      create_seclist = true, is_public = true, dns_label = ""plb"",
    }
  }

  # Create subnets if when all are true:
  # - Associated component is enabled OR configured with create == 'always'
  # - Subnet is configured with newbits and/or netnum/cidr
  # - Not configured with create == 'never'
  # - Not configured with an existing 'id'
  subnets_to_create = length(var.vcn_cidrs) > 0 ? merge(
    { for k, v in local.subnet_info : k =>
      # Override `create = true` if configured with ""always""
      merge(v, lookup(lookup(var.subnets, k, {}), ""create"", ""auto"") == ""always"" ? { ""create"" = true } : {})
      if alltrue([                                                       # Filter disabled subnets from output
        contains(keys(local.subnet_cidrs_all), k),                       # has a calculated CIDR range (not id input)
        lookup(lookup(var.subnets, k, {}), ""create"", ""auto"") != ""never"", # not disabled
        anytrue([
          tobool(lookup(v, ""create"", true)),                               # automatically enabled
          lookup(lookup(var.subnets, k, {}), ""create"", ""auto"") == ""always"" # force enabled
        ]),
      ])
    }
  ) : {}

  subnet_output = { for k, v in var.subnets :
    k => lookup(v, ""id"", null) != null ? v.id : lookup(lookup(oci_core_subnet.oke, k, {}), ""id"", null)
  }
}
",locals,22,,c3f1db06105d2c72b3e28ba10475c6c9f019204c,f49f1da39d79cf260d80dcb10ee8e399828e6e1c,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/c3f1db06105d2c72b3e28ba10475c6c9f019204c/modules/network/subnets.tf#L22,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/f49f1da39d79cf260d80dcb10ee8e399828e6e1c/modules/network/subnets.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,9,1,0,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1571,modules/artifact-registry/main.tf,modules/artifact-registry/main.tf,0,# todo,# TODO: open a bug on the provider for this permadiff,# TODO: open a bug on the provider for this permadiff,"resource ""google_artifact_registry_repository"" ""registry"" {
  project       = var.project_id
  location      = var.location
  description   = var.description
  format        = upper(local.format_string)
  labels        = var.labels
  repository_id = var.name
  mode          = ""${upper(local.mode_string)}_REPOSITORY""
  kms_key_name  = var.encryption_key

  dynamic ""docker_config"" {
    # TODO: open a bug on the provider for this permadiff
    for_each = (
      local.format_string == ""docker"" && var.format.docker.immutable_tags == true
      ? [""""]
      : []
    )
    content {
      immutable_tags = var.format.docker.immutable_tags
    }
  }

  dynamic ""maven_config"" {
    for_each = local.format_string == ""maven"" ? [""""] : []
    content {
      allow_snapshot_overwrites = var.format.maven.allow_snapshot_overwrites
      version_policy            = var.format.maven.version_policy
    }
  }

  dynamic ""remote_repository_config"" {
    for_each = local.mode_string == ""remote"" ? [""""] : []
    content {
      dynamic ""docker_repository"" {
        for_each = local.format_string == ""docker"" ? [""""] : []
        content {
          public_repository = ""DOCKER_HUB""
        }
      }
      dynamic ""maven_repository"" {
        for_each = local.format_string == ""maven"" ? [""""] : []
        content {
          public_repository = ""MAVEN_CENTRAL""
        }
      }
      dynamic ""npm_repository"" {
        for_each = local.format_string == ""npm"" ? [""""] : []
        content {
          public_repository = ""NPMJS""
        }
      }
      dynamic ""python_repository"" {
        for_each = local.format_string == ""python"" ? [""""] : []
        content {
          public_repository = ""PYPI""
        }
      }
    }
  }

  dynamic ""virtual_repository_config"" {
    for_each = local.mode_string == ""virtual"" ? [""""] : []
    content {
      dynamic ""upstream_policies"" {
        for_each = var.mode.virtual
        content {
          id         = upstream_policies.key
          repository = upstream_policies.value.repository
          priority   = upstream_policies.value.priority
        }
      }
    }
  }

  lifecycle {
    precondition {
      condition = local.mode_string != ""remote"" || contains(
        [""docker"", ""maven"", ""npm"", ""python""], local.format_string
      )
      error_message = ""Invalid format for remote repository.""
    }
  }

}
",resource,"resource ""google_artifact_registry_repository"" ""registry"" {
  provider      = google-beta
  project       = var.project_id
  location      = var.location
  description   = var.description
  format        = upper(local.format_string)
  labels        = var.labels
  repository_id = var.name
  mode          = ""${upper(local.mode_string)}_REPOSITORY""
  kms_key_name  = var.encryption_key

  cleanup_policy_dry_run = var.cleanup_policy_dry_run
  dynamic ""cleanup_policies"" {
    for_each = var.cleanup_policies == null ? {} : var.cleanup_policies
    content {
      id     = cleanup_policies.key
      action = cleanup_policies.value.action

      dynamic ""condition"" {
        for_each = (cleanup_policies.value.condition != null) ? [""""] : []
        content {
          tag_state             = cleanup_policies.value.condition.tag_state
          tag_prefixes          = cleanup_policies.value.condition.tag_prefixes
          version_name_prefixes = cleanup_policies.value.condition.version_name_prefixes
          package_name_prefixes = cleanup_policies.value.condition.package_name_prefixes
          newer_than            = cleanup_policies.value.condition.newer_than
          older_than            = cleanup_policies.value.condition.older_than
        }
      }

      dynamic ""most_recent_versions"" {
        for_each = (cleanup_policies.value.most_recent_versions != null) ? [""""] : []
        content {
          package_name_prefixes = cleanup_policies.value.most_recent_versions.package_name_prefixes
          keep_count            = cleanup_policies.value.most_recent_versions.keep_count
        }
      }
    }
  }

  dynamic ""docker_config"" {
    # TODO: open a bug on the provider for this permadiff
    for_each = (
      local.format_string == ""docker"" && try(var.format.docker.immutable_tags, null) == true
      ? [""""]
      : []
    )
    content {
      immutable_tags = var.format.docker.immutable_tags
    }
  }

  dynamic ""maven_config"" {
    for_each = local.format_string == ""maven"" ? [""""] : []
    content {
      allow_snapshot_overwrites = var.format.maven.allow_snapshot_overwrites
      version_policy            = var.format.maven.version_policy
    }
  }

  dynamic ""remote_repository_config"" {
    for_each = local.mode_string == ""remote"" ? [""""] : []
    content {
      dynamic ""docker_repository"" {
        for_each = local.format_string == ""docker"" ? [""""] : []
        content {
          public_repository = ""DOCKER_HUB""
        }
      }
      dynamic ""maven_repository"" {
        for_each = local.format_string == ""maven"" ? [""""] : []
        content {
          public_repository = ""MAVEN_CENTRAL""
        }
      }
      dynamic ""npm_repository"" {
        for_each = local.format_string == ""npm"" ? [""""] : []
        content {
          public_repository = ""NPMJS""
        }
      }
      dynamic ""python_repository"" {
        for_each = local.format_string == ""python"" ? [""""] : []
        content {
          public_repository = ""PYPI""
        }
      }
    }
  }

  dynamic ""virtual_repository_config"" {
    for_each = local.mode_string == ""virtual"" ? [""""] : []
    content {
      dynamic ""upstream_policies"" {
        for_each = var.mode.virtual
        content {
          id         = upstream_policies.key
          repository = upstream_policies.value.repository
          priority   = upstream_policies.value.priority
        }
      }
    }
  }

  lifecycle {
    precondition {
      condition = local.mode_string != ""remote"" || contains(
        [""docker"", ""maven"", ""npm"", ""python""], local.format_string
      )
      error_message = ""Invalid format for remote repository.""
    }
  }

}
",resource,33,63.0,3df98c8feba71fc46212b6096c23738041d5b103,3a2484843c6825f133ba8e05cfdb10de34d6ee07,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3df98c8feba71fc46212b6096c23738041d5b103/modules/artifact-registry/main.tf#L33,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3a2484843c6825f133ba8e05cfdb10de34d6ee07/modules/artifact-registry/main.tf#L63,2023-07-31 18:04:07+02:00,2023-12-01 10:33:02+00:00,3,0,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1665,fast/stages/0-bootstrap/organization-iam.tf,fast/stages/0-bootstrap/organization-iam.tf,0,# todo,# TODO: align additive roles with the README,# TODO: align additive roles with the README,"locals {
  # IAM roles in the org to reset (remove principals)
  iam_delete_roles = [
    ""roles/billing.creator""
  ]
  # domain IAM bindings
  iam_domain_bindings = {
    ""domain:${var.organization.domain}"" = {
      authoritative = [""roles/browser""]
      additive      = []
    }
  }
  # human (groups) IAM bindings
  iam_group_bindings = {
    (local.groups.gcp-billing-admins) = {
      authoritative = []
      additive = (
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin"",
          ""roles/billing.costsManager""
        ]
      )
    }
    (local.groups.gcp-network-admins) = {
      authoritative = [
        ""roles/cloudasset.owner"",
        ""roles/cloudsupport.techSupportEditor"",
      ]
      additive = [
        ""roles/compute.orgFirewallPolicyAdmin"",
        ""roles/compute.xpnAdmin""
      ]
    }
    (local.groups.gcp-organization-admins) = {
      authoritative = [
        ""roles/cloudasset.owner"",
        ""roles/cloudsupport.admin"",
        ""roles/compute.osAdminLogin"",
        ""roles/compute.osLoginExternalUser"",
        ""roles/owner"",
        ""roles/resourcemanager.folderAdmin"",
        ""roles/resourcemanager.organizationAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.tagAdmin""
      ]
      additive = concat(
        [
          ""roles/orgpolicy.policyAdmin""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin"",
          ""roles/billing.costsManager""
        ]
      )
    }
    (local.groups.gcp-security-admins) = {
      authoritative = [
        ""roles/cloudasset.owner"",
        ""roles/cloudsupport.techSupportEditor"",
        ""roles/iam.securityReviewer"",
        ""roles/logging.admin"",
        ""roles/securitycenter.admin"",
      ]
      additive = [
        ""roles/accesscontextmanager.policyAdmin"",
        ""roles/iam.organizationRoleAdmin"",
        ""roles/orgpolicy.policyAdmin""
      ]
    }
    (local.groups.gcp-support) = {
      authoritative = [
        ""roles/cloudsupport.techSupportEditor"",
        ""roles/logging.viewer"",
        ""roles/monitoring.viewer"",
      ]
      additive = []
    }
  }
  # machine (service accounts) IAM bindings, in logical format
  # the service account module's ""magic"" outputs allow us to use dynamic values
  iam_sa_bindings = {
    (module.automation-tf-bootstrap-sa.iam_email) = {
      authoritative = [
        ""roles/logging.admin"",
        ""roles/resourcemanager.organizationAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.projectMover"",
        ""roles/resourcemanager.tagAdmin""
      ]
      additive = concat(
        [
          ""roles/iam.organizationRoleAdmin"",
          ""roles/orgpolicy.policyAdmin""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin"",
          ""roles/billing.costsManager""
        ]
      )
    }
    (module.automation-tf-resman-sa.iam_email) = {
      authoritative = [
        ""roles/logging.admin"",
        ""roles/resourcemanager.folderAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.tagAdmin"",
        ""roles/resourcemanager.tagUser""
      ]
      additive = concat(
        [
          ""roles/orgpolicy.policyAdmin""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin"",
          ""roles/billing.costsManager""
        ]
      )
    }
  }
  # bootstrap user bindings
  iam_user_bootstrap_bindings = var.bootstrap_user == null ? {} : {
    ""user:${var.bootstrap_user}"" = {
      authoritative = [
        ""roles/logging.admin"",
        ""roles/owner"",
        ""roles/resourcemanager.organizationAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.tagAdmin""
      ]
      # TODO: align additive roles with the README
      additive = (
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin"",
          ""roles/billing.costsManager""
        ]
      )
    }
  }
}
",locals,"locals {
  # IAM roles in the org to reset (remove principals)
  iam_delete_roles = [
    ""roles/billing.creator""
  ]
  # domain IAM bindings
  iam_domain_bindings = var.organization.domain == null ? {} : {
    ""domain:${var.organization.domain}"" = {
      authoritative = [""roles/browser""]
      additive      = []
    }
  }
  # human (groups) IAM bindings
  iam_principal_bindings = {
    (local.principals.gcp-billing-admins) = {
      authoritative = []
      additive = (
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin""
        ]
      )
    }
    (local.principals.gcp-network-admins) = {
      authoritative = [
        ""roles/cloudasset.owner"",
        ""roles/cloudsupport.techSupportEditor"",
      ]
      additive = [
        ""roles/compute.orgFirewallPolicyAdmin"",
        ""roles/compute.xpnAdmin""
      ]
    }
    (local.principals.gcp-organization-admins) = {
      authoritative = [
        ""roles/cloudasset.owner"",
        ""roles/cloudsupport.admin"",
        ""roles/compute.osAdminLogin"",
        ""roles/compute.osLoginExternalUser"",
        ""roles/owner"",
        ""roles/resourcemanager.folderAdmin"",
        ""roles/resourcemanager.organizationAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.tagAdmin"",
        ""roles/iam.workforcePoolAdmin""
      ]
      additive = concat(
        [
          ""roles/orgpolicy.policyAdmin""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin""
        ]
      )
    }
    (local.principals.gcp-security-admins) = {
      authoritative = [
        ""roles/cloudasset.owner"",
        ""roles/cloudsupport.techSupportEditor"",
        ""roles/iam.securityReviewer"",
        ""roles/logging.admin"",
        ""roles/securitycenter.admin"",
      ]
      additive = [
        ""roles/accesscontextmanager.policyAdmin"",
        ""roles/iam.organizationRoleAdmin"",
        ""roles/orgpolicy.policyAdmin""
      ]
    }
    (local.principals.gcp-support) = {
      authoritative = [
        ""roles/cloudsupport.techSupportEditor"",
        ""roles/logging.viewer"",
        ""roles/monitoring.viewer"",
      ]
      additive = []
    }
  }
  # machine (service accounts) IAM bindings, in logical format
  # the service account module's ""magic"" outputs allow us to use dynamic values
  iam_sa_bindings = {
    (module.automation-tf-bootstrap-sa.iam_email) = {
      authoritative = [
        ""roles/essentialcontacts.admin"",
        ""roles/iam.workforcePoolAdmin"",
        ""roles/logging.admin"",
        ""roles/resourcemanager.organizationAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.projectMover"",
        ""roles/resourcemanager.tagAdmin""
      ]
      additive = concat(
        [
          ""roles/iam.organizationRoleAdmin"",
          ""roles/orgpolicy.policyAdmin""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin""
        ]
      )
    }
    (module.automation-tf-bootstrap-r-sa.iam_email) = {
      authoritative = [
        ""roles/essentialcontacts.viewer"",
        ""roles/logging.viewer"",
        ""roles/resourcemanager.folderViewer"",
        ""roles/resourcemanager.tagViewer""
      ]
      additive = concat(
        [
          # the organizationAdminViewer custom role is granted via the SA module
          ""roles/iam.organizationRoleViewer"",
          ""roles/iam.workforcePoolViewer"",
          ""roles/orgpolicy.policyViewer""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.viewer""
        ]
      )
    }
    (module.automation-tf-resman-sa.iam_email) = {
      authoritative = [
        ""roles/essentialcontacts.admin"",
        ""roles/logging.admin"",
        ""roles/resourcemanager.folderAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.tagAdmin"",
        ""roles/resourcemanager.tagUser""
      ]
      additive = concat(
        [
          ""roles/accesscontextmanager.policyAdmin"",
          ""roles/orgpolicy.policyAdmin""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin""
        ]
      )
    }
    (module.automation-tf-resman-r-sa.iam_email) = {
      authoritative = [
        ""roles/accesscontextmanager.policyReader"",
        ""roles/essentialcontacts.viewer"",
        ""roles/logging.viewer"",
        ""roles/resourcemanager.folderViewer"",
        ""roles/resourcemanager.tagViewer"",
        ""roles/serviceusage.serviceUsageViewer""
      ]
      additive = concat(
        [
          # the organizationAdminViewer custom role is granted via the SA module
          ""roles/orgpolicy.policyViewer""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.viewer""
        ]
      )
    }
  }
  # bootstrap user bindings
  iam_user_bootstrap_bindings = var.bootstrap_user == null ? {} : {
    ""user:${var.bootstrap_user}"" = {
      authoritative = [
        ""roles/logging.admin"",
        ""roles/owner"",
        ""roles/resourcemanager.organizationAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.tagAdmin""
      ]
      # TODO: align additive roles with the README
      additive = (
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin""
        ]
      )
    }
  }
}
",locals,148,187.0,b2d27b5f12a55e696f955c39530e29f27478bea4,be9214f99a0718eb8698e9d539e2ad93cb442ac7,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b2d27b5f12a55e696f955c39530e29f27478bea4/fast/stages/0-bootstrap/organization-iam.tf#L148,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/be9214f99a0718eb8698e9d539e2ad93cb442ac7/fast/stages/0-bootstrap/organization-iam.tf#L187,2023-09-28 11:41:56+02:00,2024-05-21 10:39:47+02:00,10,0,0,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-sql-db,6,modules/backup/variables.tf,modules/backup/variables.tf,0,#todo,#TODO: gleichda change default to true on next major release (#336),#TODO: gleichda change default to true on next major release (#336),"variable ""compress_export"" {
  description = ""Whether or not to compress the export when storing in the bucket; Only valid for MySQL and PostgreSQL""
  type        = bool
  default     = false
}
",variable,"variable ""compress_export"" {
  description = ""Whether or not to compress the export when storing in the bucket; Only valid for MySQL and PostgreSQL""
  type        = bool
  default     = true
}
",variable,91,,b1ef34d0ee1a84ef2c0be4141cb83448052264da,ff3724429a04b54d25bfea6eb3db68d78d1128bb,https://github.com/terraform-google-modules/terraform-google-sql-db/blob/b1ef34d0ee1a84ef2c0be4141cb83448052264da/modules/backup/variables.tf#L91,https://github.com/terraform-google-modules/terraform-google-sql-db/blob/ff3724429a04b54d25bfea6eb3db68d78d1128bb/modules/backup/variables.tf,2022-08-09 09:38:28-05:00,2022-10-24 10:03:17-07:00,2,1,0,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,1533,infra/modules/worklytics-connectors/main.tf,infra/modules/worklytics-connectors/main.tf,0,# todo,# TODO: deal w/ adding the OAUTH_REFRESH_TOKEN_STUFF from above,# TODO: deal w/ adding the OAUTH_REFRESH_TOKEN_STUFF from above,"locals {
  # TODO: deal w/ adding the OAUTH_REFRESH_TOKEN_STUFF from above
  enabled_api_connectors  = module.worklytics_connector_specs.enabled_oauth_long_access_connectors
  enabled_bulk_connectors = module.worklytics_connector_specs.enabled_bulk_connectors
}
",locals,"locals {
  enabled_api_connectors  = module.worklytics_connector_specs.enabled_oauth_long_access_connectors
  enabled_bulk_connectors = module.worklytics_connector_specs.enabled_bulk_connectors
}
",locals,21,,f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e,7a2ae9aca88a5f018b9671f325f71db8e9285358,https://github.com/Worklytics/psoxy/blob/f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e/infra/modules/worklytics-connectors/main.tf#L21,https://github.com/Worklytics/psoxy/blob/7a2ae9aca88a5f018b9671f325f71db8e9285358/infra/modules/worklytics-connectors/main.tf,2023-06-16 14:08:45-07:00,2023-07-11 15:51:07-07:00,2,1,0,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,2058,infra/modules/aws/main.tf,infra/modules/aws/main.tf,0,# todo,# TODO: take version from somewhere else here this isn't *necessary* the version if local build or remote artifact,# TODO: take version from somewhere else here this isn't *necessary* the version if local build or remote artifact,"module ""test_tool"" {
  count = var.install_test_tool ? 1 : 0

  source = ""../psoxy-test-tool""

  path_to_tools = ""${var.psoxy_base_dir}tools""
  # TODO: take version from somewhere else here; this isn't *necessary* the version if local build or remote artifact
  psoxy_version = module.psoxy_package.version
}
",module,"module ""test_tool"" {
  count = var.install_test_tool ? 1 : 0

  source = ""../psoxy-test-tool""

  path_to_tools = ""${var.psoxy_base_dir}tools""
  # TODO: take version from somewhere else here; this isn't *necessary* the version if local build or remote artifact
  psoxy_version = module.psoxy_package.version
}
",module,160,224.0,43055bd38ca21063c5d534c45a35c5f3c82f048c,e07a69ceca80240af2462aa09465535cc795d0b6,https://github.com/Worklytics/psoxy/blob/43055bd38ca21063c5d534c45a35c5f3c82f048c/infra/modules/aws/main.tf#L160,https://github.com/Worklytics/psoxy/blob/e07a69ceca80240af2462aa09465535cc795d0b6/infra/modules/aws/main.tf#L224,2023-07-07 15:05:16+00:00,2024-03-05 12:38:07-08:00,5,0,0,1,0,0,0,0,0,0
https://github.com/chanzuckerberg/cztack,21,aws-ecs-job/main.tf,aws-ecs-job/main.tf,0,implementation,# Defaults to a minimal hello-world implementation should be updated separately from,"# Default container definition if var.manage_task_definition == false 
 # Defaults to a minimal hello-world implementation; should be updated separately from 
 # Terraform, e.g. using ecs deploy or czecs","locals {
  template = <<TEMPLATE
[
  {
    ""name"": ""${local.container_name}"",
    ""image"": ""library/busybox:1.29"",
    ""command"": [""sh"", ""-c"", ""while true; do { echo -e 'HTTP/1.1 200 OK\r\n\nRunning stub server'; date; } | nc -l -p 8080; done""],
    ""memoryReservation"": 4
  }
]
TEMPLATE
}
",locals,"locals {
  template = <<TEMPLATE
[
  {
    ""name"": ""${local.container_name}"",
    ""image"": ""library/busybox:1.29"",
    ""command"": [""sh"", ""-c"", ""while true; do { echo -e 'HTTP/1.1 200 OK\r\n\nRunning stub server'; date; } | nc -l -p 8080; done""],
    ""memoryReservation"": 4
  }
]
TEMPLATE
}
",locals,52,68.0,6918848f1dab99c67e49d21bdc839d907ff8b647,093abc491ab7fdbc1693d8360da3a386dd81f7fe,https://github.com/chanzuckerberg/cztack/blob/6918848f1dab99c67e49d21bdc839d907ff8b647/aws-ecs-job/main.tf#L52,https://github.com/chanzuckerberg/cztack/blob/093abc491ab7fdbc1693d8360da3a386dd81f7fe/aws-ecs-job/main.tf#L68,2019-09-25 09:47:44-07:00,2019-10-08 07:57:34-07:00,3,0,1,1,1,0,0,0,0,0
https://github.com/ManagedKube/kubernetes-ops,7,terraform-modules/aws/msk_1.0.9/module/context.tf,terraform-modules/aws/msk_1.0.9/module/context.tf,0,fix,"# which was not fixed until Terraform 1.0.0,","# Note: we have to use [] instead of null for unset lists due to 
 # https://github.com/hashicorp/terraform/issues/28137 
 # which was not fixed until Terraform 1.0.0, 
 # but we want the default to be all the labels in `label_order` 
 # and we want users to be able to prevent all tag generation 
 # by setting `labels_as_tags` to `[]`, so we need 
 # a different sentinel to indicate ""default""","variable ""context"" {
  type = any
  default = {
    enabled             = true
    namespace           = null
    tenant              = null
    environment         = null
    stage               = null
    name                = null
    delimiter           = null
    attributes          = []
    tags                = {}
    additional_tag_map  = {}
    regex_replace_chars = null
    label_order         = []
    id_length_limit     = null
    label_key_case      = null
    label_value_case    = null
    descriptor_formats  = {}
    # Note: we have to use [] instead of null for unset lists due to
    # https://github.com/hashicorp/terraform/issues/28137
    # which was not fixed until Terraform 1.0.0,
    # but we want the default to be all the labels in `label_order`
    # and we want users to be able to prevent all tag generation
    # by setting `labels_as_tags` to `[]`, so we need
    # a different sentinel to indicate ""default""
    labels_as_tags = [""unset""]
  }
  description = <<-EOT
    Single object for setting entire context at once.
    See description of individual variables for details.
    Leave string and numeric variables as `null` to use default value.
    Individual variable settings (non-null) override settings in context object,
    except for attributes, tags, and additional_tag_map, which are merged.
  EOT

  validation {
    condition     = lookup(var.context, ""label_key_case"", null) == null ? true : contains([""lower"", ""title"", ""upper""], var.context[""label_key_case""])
    error_message = ""Allowed values: `lower`, `title`, `upper`.""
  }

  validation {
    condition     = lookup(var.context, ""label_value_case"", null) == null ? true : contains([""lower"", ""title"", ""upper"", ""none""], var.context[""label_value_case""])
    error_message = ""Allowed values: `lower`, `title`, `upper`, `none`.""
  }
}
",variable,"variable ""context"" {
  type = any
  default = {
    enabled             = true
    namespace           = null
    tenant              = null
    environment         = null
    stage               = null
    name                = null
    delimiter           = null
    attributes          = []
    tags                = {}
    additional_tag_map  = {}
    regex_replace_chars = null
    label_order         = []
    id_length_limit     = null
    label_key_case      = null
    label_value_case    = null
    descriptor_formats  = {}
    # Note: we have to use [] instead of null for unset lists due to
    # https://github.com/hashicorp/terraform/issues/28137
    # which was not fixed until Terraform 1.0.0,
    # but we want the default to be all the labels in `label_order`
    # and we want users to be able to prevent all tag generation
    # by setting `labels_as_tags` to `[]`, so we need
    # a different sentinel to indicate ""default""
    labels_as_tags = [""unset""]
  }
  description = <<-EOT
    Single object for setting entire context at once.
    See description of individual variables for details.
    Leave string and numeric variables as `null` to use default value.
    Individual variable settings (non-null) override settings in context object,
    except for attributes, tags, and additional_tag_map, which are merged.
  EOT

  validation {
    condition     = lookup(var.context, ""label_key_case"", null) == null ? true : contains([""lower"", ""title"", ""upper""], var.context[""label_key_case""])
    error_message = ""Allowed values: `lower`, `title`, `upper`.""
  }

  validation {
    condition     = lookup(var.context, ""label_value_case"", null) == null ? true : contains([""lower"", ""title"", ""upper"", ""none""], var.context[""label_value_case""])
    error_message = ""Allowed values: `lower`, `title`, `upper`, `none`.""
  }
}
",variable,71,71.0,c8193c7d74e2f7c624f0867337294cb66a2b9469,c8193c7d74e2f7c624f0867337294cb66a2b9469,https://github.com/ManagedKube/kubernetes-ops/blob/c8193c7d74e2f7c624f0867337294cb66a2b9469/terraform-modules/aws/msk_1.0.9/module/context.tf#L71,https://github.com/ManagedKube/kubernetes-ops/blob/c8193c7d74e2f7c624f0867337294cb66a2b9469/terraform-modules/aws/msk_1.0.9/module/context.tf#L71,2023-12-14 10:29:30-08:00,2023-12-14 10:29:30-08:00,1,0,0,1,1,0,0,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,346,azure/salt_provisioner.tf,azure/salt_provisioner.tf,0,// todo,// TODO: add or don't add this (from libvirt),"// TODO: add or don't add this (from libvirt) 
 // network_domain: ${var.network_domain}  ","resource ""null_resource"" ""monitoring_provisioner"" {
  count = var.provisioner == ""salt"" ? 1 : 0


  triggers = {
    monitoring_id = azurerm_virtual_machine.monitoring.id
  }

  connection {
    host        = data.azurerm_public_ip.monitoring.ip_address
    type        = ""ssh""
    user        = var.admin_user
    private_key = file(var.private_key_location)
  }

  provisioner ""file"" {
    source      = ""../salt""
    destination = ""/tmp""
  }

  provisioner ""file"" {
    content     = data.template_file.salt_provisioner.rendered
    destination = ""/tmp/salt_provisioner.sh""
  }

// TODO: add or don't add this (from libvirt)
// network_domain: ${var.network_domain}


  provisioner ""file"" {
    content = <<EOF
provider: azure
role: monitoring
name_prefix: ${terraform.workspace}-${var.name}
hostname: ${terraform.workspace}-${var.name}${var.monitoring_count > 1 ? ""0${count.index + 1}"" : """"}
timezone: ${var.timezone}
reg_code: ${var.reg_code}
reg_email: ${var.reg_email}
reg_additional_modules: {${join("", "",formatlist(""'%s': '%s'"",keys(var.reg_additional_modules),values(var.reg_additional_modules),),)}}
additional_repos: {${join("", "",formatlist(""'%s': '%s'"",keys(var.additional_repos),values(var.additional_repos),),)}}
additional_packages: [${join("", "", formatlist(""'%s'"", var.additional_packages))}]
authorized_keys: [${trimspace(file(var.public_key_location))},${trimspace(file(var.public_key_location))}]
host_ips: [${join("", "", formatlist(""'%s'"", [var.monitoring_srv_ip]))}]
host_ip: ${var.monitoring_srv_ip}
role: monitoring
provider: libvirt
ha_sap_deployment_repo: ${var.ha_sap_deployment_repo}
monitored_services: [${join("", "", formatlist(""'%s'"", var.monitored_services))}]
EOF

destination = ""/tmp/grains""
}

provisioner ""remote-exec"" {
  inline = [
    ""${var.background ? ""nohup"" : """"} sudo sh /tmp/salt_provisioner.sh > /tmp/provisioning.log ${var.background ? ""&"" : """"}"",
    ""return_code=$? && sleep 1 && exit $return_code"",
  ] # Workaround to let the process start in background properly
}

}
",resource,"resource ""null_resource"" ""monitoring_provisioner"" {
  count = var.provisioner == ""salt"" ? 1 : 0


  triggers = {
    monitoring_id = azurerm_virtual_machine.monitoring.id
  }

  connection {
    host        = data.azurerm_public_ip.monitoring.ip_address
    type        = ""ssh""
    user        = var.admin_user
    private_key = file(var.private_key_location)
  }

  provisioner ""file"" {
    source      = ""../salt""
    destination = ""/tmp""
  }

  provisioner ""file"" {
    content     = data.template_file.salt_provisioner.rendered
    destination = ""/tmp/salt_provisioner.sh""
  }
  provisioner ""file"" {
    content = <<EOF
provider: azure
role: monitoring
name_prefix: ${terraform.workspace}-monitoring
hostname: ${terraform.workspace}-monitoring
timezone: ${var.timezone}
reg_code: ${var.reg_code}
reg_email: ${var.reg_email}
reg_additional_modules: {${join("", "", formatlist(""'%s': '%s'"", keys(var.reg_additional_modules), values(var.reg_additional_modules), ), )}}
additional_repos: {${join("", "", formatlist(""'%s': '%s'"", keys(var.additional_repos), values(var.additional_repos), ), )}}
additional_packages: [${join("", "", formatlist(""'%s'"", var.additional_packages))}]
authorized_keys: [${trimspace(file(var.public_key_location))},${trimspace(file(var.public_key_location))}]
host_ips: [${join("", "", formatlist(""'%s'"", [var.monitoring_srv_ip]))}]
host_ip: ${var.monitoring_srv_ip}
ha_sap_deployment_repo: ${var.ha_sap_deployment_repo}
monitored_services: [${join("", "", formatlist(""'%s'"", var.monitored_services))}]
network_domain: ""tf.local""
EOF

    destination = ""/tmp/grains""
  }

  provisioner ""remote-exec"" {
    inline = [
      ""${var.background ? ""nohup"" : """"} sudo sh /tmp/salt_provisioner.sh > /tmp/provisioning.log ${var.background ? ""&"" : """"}"",
      ""return_code=$? && sleep 1 && exit $return_code"",
    ] # Workaround to let the process start in background properly
  }

}
",resource,192,,f41baea2a7a45b527e944b62bcab73612c693e02,3d715fbde7f5b9ad25c6c317103955efc32e12b3,https://github.com/SUSE/ha-sap-terraform-deployments/blob/f41baea2a7a45b527e944b62bcab73612c693e02/azure/salt_provisioner.tf#L192,https://github.com/SUSE/ha-sap-terraform-deployments/blob/3d715fbde7f5b9ad25c6c317103955efc32e12b3/azure/salt_provisioner.tf,2019-09-05 00:01:31+02:00,2019-09-05 00:01:59+02:00,3,1,0,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,748,fast/stages/01-resman/organization.tf,fast/stages/01-resman/organization.tf,0,# todo,# TODO: implement tag-based conditions on this org role,# TODO: implement tag-based conditions on this org role,"module ""organization"" {
  source          = ""../../../modules/organization""
  organization_id = ""organizations/${var.organization.id}""
  # IAM additive bindings, granted via the restricted Organization Admin custom
  # role assigned in stage 00; they need to be additive to avoid conflicts
  iam_additive = merge(
    {
      ""roles/accesscontextmanager.policyAdmin"" = [
        module.branch-security-sa.iam_email
      ]
      ""roles/compute.orgFirewallPolicyAdmin"" = [
        module.branch-network-sa.iam_email
      ]
      ""roles/compute.xpnAdmin"" = [
        module.branch-network-sa.iam_email
      ]
      # TODO: implement tag-based conditions on this org role
      ""roles/orgpolicy.policyAdmin"" = concat(
        local.branch_teams_pf_sa_iam_emails,
        local.branch_dataplatform_sa_iam_emails,
      )
    },
    local.billing_org ? {
      ""roles/billing.costsManager"" = local.branch_teams_pf_sa_iam_emails
      ""roles/billing.user"" = concat(
        [
          module.branch-network-sa.iam_email,
          module.branch-security-sa.iam_email,
        ],
        local.branch_dataplatform_sa_iam_emails,
        # enable if individual teams can create their own projects
        # [
        #   for k, v in module.branch-teams-team-sa : v.iam_email
        # ],
        local.branch_teams_pf_sa_iam_emails
      )
    } : {}
  )
  # sample subset of useful organization policies, edit to suit requirements
  policy_boolean = {
    ""constraints/cloudfunctions.requireVPCConnector""              = true
    ""constraints/compute.disableGuestAttributesAccess""            = true
    ""constraints/compute.disableInternetNetworkEndpointGroup""     = true
    ""constraints/compute.disableNestedVirtualization""             = true
    ""constraints/compute.disableSerialPortAccess""                 = true
    ""constraints/compute.requireOsLogin""                          = true
    ""constraints/compute.restrictXpnProjectLienRemoval""           = true
    ""constraints/compute.skipDefaultNetworkCreation""              = true
    ""constraints/compute.setNewProjectDefaultToZonalDNSOnly""      = true
    ""constraints/iam.automaticIamGrantsForDefaultServiceAccounts"" = true
    ""constraints/iam.disableServiceAccountKeyCreation""            = true
    ""constraints/iam.disableServiceAccountKeyUpload""              = true
    ""constraints/sql.restrictPublicIp""                            = true
    ""constraints/sql.restrictAuthorizedNetworks""                  = true
    ""constraints/storage.uniformBucketLevelAccess""                = true
  }
  policy_list = {
    ""constraints/cloudfunctions.allowedIngressSettings"" = merge(
      local.list_allow, { values = [""is:ALLOW_INTERNAL_ONLY""] }
    )
    ""constraints/cloudfunctions.allowedVpcConnectorEgressSettings"" = merge(
      local.list_allow, { values = [""is:PRIVATE_RANGES_ONLY""] }
    )
    ""constraints/compute.restrictLoadBalancerCreationForTypes"" = merge(
      local.list_allow, { values = [""in:INTERNAL""] }
    )
    ""constraints/compute.vmExternalIpAccess"" = local.list_deny
    ""constraints/iam.allowedPolicyMemberDomains"" = merge(
      local.list_allow, {
        values = concat(
          [var.organization.customer_id],
          try(local.policy_configs.allowed_policy_member_domains, [])
        )
    })
    ""constraints/run.allowedIngress"" = merge(
      local.list_allow, { values = [""is:internal""] }
    )
    ""constraints/run.allowedVPCEgress"" = merge(
      local.list_allow, { values = [""is:private-ranges-only""] }
    )
    # ""constraints/compute.restrictCloudNATUsage""                      = local.list_deny
    # ""constraints/compute.restrictDedicatedInterconnectUsage""         = local.list_deny
    # ""constraints/compute.restrictPartnerInterconnectUsage""           = local.list_deny
    # ""constraints/compute.restrictProtocolForwardingCreationForTypes"" = local.list_deny
    # ""constraints/compute.restrictSharedVpcHostProjects""              = local.list_deny
    # ""constraints/compute.restrictSharedVpcSubnetworks""               = local.list_deny
    # ""constraints/compute.restrictVpcPeering"" = local.list_deny
    # ""constraints/compute.restrictVpnPeerIPs"" = local.list_deny
    # ""constraints/compute.vmCanIpForward""     = local.list_deny
    # ""constraints/gcp.resourceLocations"" = {
    #   inherit_from_parent = false
    #   suggested_value     = null
    #   status              = true
    #   values              = local.allowed_regions
    # }
  }
}
",module,"module ""organization"" {
  source          = ""../../../modules/organization""
  organization_id = ""organizations/${var.organization.id}""
  # IAM additive bindings, granted via the restricted Organization Admin custom
  # role assigned in stage 00; they need to be additive to avoid conflicts
  iam_additive = merge(
    {
      ""roles/accesscontextmanager.policyAdmin"" = [
        module.branch-security-sa.iam_email
      ]
      ""roles/compute.orgFirewallPolicyAdmin"" = [
        module.branch-network-sa.iam_email
      ]
      ""roles/compute.xpnAdmin"" = [
        module.branch-network-sa.iam_email
      ]
    },
    local.billing_org ? {
      ""roles/billing.costsManager"" = local.branch_teams_pf_sa_iam_emails
      ""roles/billing.user"" = concat(
        [
          module.branch-network-sa.iam_email,
          module.branch-security-sa.iam_email,
        ],
        local.branch_dataplatform_sa_iam_emails,
        # enable if individual teams can create their own projects
        # [
        #   for k, v in module.branch-teams-team-sa : v.iam_email
        # ],
        local.branch_teams_pf_sa_iam_emails
      )
    } : {}
  )
  # sample subset of useful organization policies, edit to suit requirements
  policy_boolean = {
    ""constraints/cloudfunctions.requireVPCConnector""              = true
    ""constraints/compute.disableGuestAttributesAccess""            = true
    ""constraints/compute.disableInternetNetworkEndpointGroup""     = true
    ""constraints/compute.disableNestedVirtualization""             = true
    ""constraints/compute.disableSerialPortAccess""                 = true
    ""constraints/compute.requireOsLogin""                          = true
    ""constraints/compute.restrictXpnProjectLienRemoval""           = true
    ""constraints/compute.skipDefaultNetworkCreation""              = true
    ""constraints/compute.setNewProjectDefaultToZonalDNSOnly""      = true
    ""constraints/iam.automaticIamGrantsForDefaultServiceAccounts"" = true
    ""constraints/iam.disableServiceAccountKeyCreation""            = true
    ""constraints/iam.disableServiceAccountKeyUpload""              = true
    ""constraints/sql.restrictPublicIp""                            = true
    ""constraints/sql.restrictAuthorizedNetworks""                  = true
    ""constraints/storage.uniformBucketLevelAccess""                = true
  }
  policy_list = {
    ""constraints/cloudfunctions.allowedIngressSettings"" = merge(
      local.list_allow, { values = [""is:ALLOW_INTERNAL_ONLY""] }
    )
    ""constraints/cloudfunctions.allowedVpcConnectorEgressSettings"" = merge(
      local.list_allow, { values = [""is:PRIVATE_RANGES_ONLY""] }
    )
    ""constraints/compute.restrictLoadBalancerCreationForTypes"" = merge(
      local.list_allow, { values = [""in:INTERNAL""] }
    )
    ""constraints/compute.vmExternalIpAccess"" = local.list_deny
    ""constraints/iam.allowedPolicyMemberDomains"" = merge(
      local.list_allow, {
        values = concat(
          [var.organization.customer_id],
          try(local.policy_configs.allowed_policy_member_domains, [])
        )
    })
    ""constraints/run.allowedIngress"" = merge(
      local.list_allow, { values = [""is:internal""] }
    )
    ""constraints/run.allowedVPCEgress"" = merge(
      local.list_allow, { values = [""is:private-ranges-only""] }
    )
    # ""constraints/compute.restrictCloudNATUsage""                      = local.list_deny
    # ""constraints/compute.restrictDedicatedInterconnectUsage""         = local.list_deny
    # ""constraints/compute.restrictPartnerInterconnectUsage""           = local.list_deny
    # ""constraints/compute.restrictProtocolForwardingCreationForTypes"" = local.list_deny
    # ""constraints/compute.restrictSharedVpcHostProjects""              = local.list_deny
    # ""constraints/compute.restrictSharedVpcSubnetworks""               = local.list_deny
    # ""constraints/compute.restrictVpcPeering"" = local.list_deny
    # ""constraints/compute.restrictVpnPeerIPs"" = local.list_deny
    # ""constraints/compute.vmCanIpForward""     = local.list_deny
    # ""constraints/gcp.resourceLocations"" = {
    #   inherit_from_parent = false
    #   suggested_value     = null
    #   status              = true
    #   values              = local.allowed_regions
    # }
  }
  tags = {
    context = {
      description = ""Resource management context.""
      iam         = {}
      values = {
        data       = null
        gke        = null
        networking = null
        sandbox    = null
        security   = null
        teams      = null
      }
    }
    environment = {
      description = ""Environment definition.""
      iam         = {}
      values = {
        development = null
        production  = null
      }
    }
  }
}
",module,66,,b9804d895b7debed37e41592484fbcb1dd9bd2f6,474bcbdd0e727c0974857c721a5288ff1cbf42f9,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b9804d895b7debed37e41592484fbcb1dd9bd2f6/fast/stages/01-resman/organization.tf#L66,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/474bcbdd0e727c0974857c721a5288ff1cbf42f9/fast/stages/01-resman/organization.tf,2022-02-18 14:39:33+01:00,2022-02-20 11:26:30+01:00,2,1,0,1,0,1,0,0,0,0
https://github.com/nasa/cumulus,301,tf-modules/archive/ingest-reporting.tf,tf-modules/archive/ingest-reporting.tf,0,todo,# TODO Remove this topic,# TODO Remove this topic,"resource ""aws_sns_topic"" ""report_collections_topic"" {
  name = ""${var.prefix}-report-collections-topic""
  tags = var.tags
}
",resource,"resource ""aws_sns_topic"" ""report_collections_topic"" {
  name = ""${var.prefix}-report-collections-topic""
  tags = var.tags
}
",resource,340,,413e8ef43461de1f3848756673a4a0ddd2e32506,f0fbbd070c6cf1398dbcfc02c3d62026ea9dbeba,https://github.com/nasa/cumulus/blob/413e8ef43461de1f3848756673a4a0ddd2e32506/tf-modules/archive/ingest-reporting.tf#L340,https://github.com/nasa/cumulus/blob/f0fbbd070c6cf1398dbcfc02c3d62026ea9dbeba/tf-modules/archive/ingest-reporting.tf,2023-02-20 10:55:23-05:00,2023-02-20 13:26:46-05:00,3,1,1,1,0,0,0,0,0,0
https://github.com/ministryofjustice/modernisation-platform,198,terraform/environments/data-platform-apps-and-tools/environment-configuration.tf,terraform/environments/data-platform-apps-and-tools/environment-configuration.tf,0,// todo,// TODO: Replace this with Obersevability Platform PRODUCTION Prometheus URL,// TODO: Replace this with Obersevability Platform PRODUCTION Prometheus URL,"locals {
  airflow_name              = ""${local.application_name}-${local.environment}""
  environment_configuration = local.environment_configurations[local.environment]
  environment_configurations = {
    development = {
      /* Route53 */
      route53_zone = ""apps-tools.development.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* SES */
      ses_domain_identity = ""apps-tools.development.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* VPC */
      vpc_cidr                   = ""10.26.128.0/21""
      vpc_private_subnets        = [""10.26.130.0/23"", ""10.26.132.0/23"", ""10.26.134.0/23""]
      vpc_public_subnets         = [""10.26.128.0/27"", ""10.26.128.32/27"", ""10.26.128.64/27""]
      vpc_database_subnets       = [""10.26.128.96/27"", ""10.26.128.128/27"", ""10.26.128.160/27""]
      vpc_enable_nat_gateway     = true
      vpc_one_nat_gateway_per_az = false

      /* EKS */
      eks_cluster_name = ""apps-tools-${local.environment}""
      eks_versions = {
        cluster                   = ""1.28""
        ami_release               = ""1.15.1-264e294c"" // [major version].[minor version].[patch version]-[first 8 chars of commit SHA]. Get the SHA from here: https://github.com/bottlerocket-os/bottlerocket/releases
        addon_coredns             = ""v1.10.1-eksbuild.4""
        addon_kube_proxy          = ""v1.28.2-eksbuild.2""
        addon_vpc_cni             = ""v1.15.1-eksbuild.1""
        addon_aws_guardduty_agent = ""v1.3.0-eksbuild.1""
        addon_ebs_csi_driver      = ""v1.23.1-eksbuild.1""
        addon_efs_csi_driver      = ""v1.7.0-eksbuild.1""
      }
      eks_sso_access_role = ""modernisation-platform-sandbox""

      /* Airflow */
      airflow_s3_bucket             = ""moj-data-platform-airflow-development20230627094128036000000001"" // This is defined in modernisation-platform-environments
      airflow_dag_s3_path           = ""dags/""                                                           // This is defined in modernisation-platform-environments
      airflow_requirements_s3_path  = ""requirements.txt""                                                // This is defined in modernisation-platform-environments
      airflow_execution_role_name   = ""${local.application_name}-${local.environment}-airflow-execution""
      airflow_name                  = ""${local.application_name}-${local.environment}""
      airflow_version               = ""2.6.3""
      airflow_environment_class     = ""mw1.medium""
      airflow_max_workers           = 2
      airflow_min_workers           = 1
      airflow_schedulers            = 2
      airflow_webserver_access_mode = ""PUBLIC_ONLY""
      airflow_configuration_options = {
        ""webserver.warn_deployment_exposure"" = 0
      }
      airflow_mail_from_address               = ""airflow""
      airflow_weekly_maintenance_window_start = ""SAT:00:00""

      /* Data Platform */
      data_platform_account_id        = local.environment_management.account_ids[""data-platform-development""]
      data_platform_openmetadata_role = ""openmetadata""

      /* Observability Platform */
      observability_platform_account_id     = local.environment_management.account_ids[""observability-platform-development""]
      observability_platform_role           = ""data-platform-apps-and-tools""
      observability_platform_prometheus_url = ""https://aps-workspaces.eu-west-2.amazonaws.com/workspaces/ws-464eea97-631a-4e5d-af22-4c5528d9e0e6/api/v1/remote_write""
    }
    production = {
      /* Route53 */
      route53_zone = ""apps-tools.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* SES */
      ses_domain_identity = ""apps-tools.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* VPC */
      vpc_cidr                   = ""10.27.128.0/21""
      vpc_private_subnets        = [""10.27.130.0/23"", ""10.27.132.0/23"", ""10.27.134.0/23""]
      vpc_public_subnets         = [""10.27.128.0/27"", ""10.27.128.32/27"", ""10.27.128.64/27""]
      vpc_database_subnets       = [""10.27.128.96/27"", ""10.27.128.128/27"", ""10.27.128.160/27""]
      vpc_enable_nat_gateway     = true
      vpc_one_nat_gateway_per_az = false

      /* EKS */
      eks_cluster_name = ""apps-tools-${local.environment}""
      eks_versions = {
        cluster                   = ""1.28""
        ami_release               = ""1.15.1-264e294c"" // [major version].[minor version].[patch version]-[first 8 chars of commit SHA]. Get the SHA from here: https://github.com/bottlerocket-os/bottlerocket/releases
        addon_coredns             = ""v1.10.1-eksbuild.4""
        addon_kube_proxy          = ""v1.28.2-eksbuild.2""
        addon_vpc_cni             = ""v1.15.1-eksbuild.1""
        addon_aws_guardduty_agent = ""v1.3.0-eksbuild.1""
        addon_ebs_csi_driver      = ""v1.23.1-eksbuild.1""
        addon_efs_csi_driver      = ""v1.7.0-eksbuild.1""
      }
      eks_sso_access_role = ""modernisation-platform-developer""

      /* Airflow */
      airflow_s3_bucket             = ""moj-data-platform-airflow-production20230908140747954800000002"" // This is defined in modernisation-platform-environments
      airflow_dag_s3_path           = ""dags/""                                                          // This is defined in modernisation-platform-environments
      airflow_requirements_s3_path  = ""requirements.txt""                                               // This is defined in modernisation-platform-environments
      airflow_execution_role_name   = ""${local.application_name}-${local.environment}-airflow-execution""
      airflow_name                  = ""${local.application_name}-${local.environment}""
      airflow_version               = ""2.6.3""
      airflow_environment_class     = ""mw1.medium""
      airflow_max_workers           = 2
      airflow_min_workers           = 1
      airflow_schedulers            = 2
      airflow_webserver_access_mode = ""PUBLIC_ONLY""
      airflow_configuration_options = {
        ""webserver.warn_deployment_exposure"" = 0
      }
      airflow_mail_from_address               = ""airflow""
      airflow_weekly_maintenance_window_start = ""SAT:00:00""

      /* Data Platform */
      data_platform_account_id        = local.environment_management.account_ids[""data-platform-production""]
      data_platform_openmetadata_role = ""openmetadata""

      /* Observability Platform */
      observability_platform_account_id = local.environment_management.account_ids[""observability-platform-production""]
      observability_platform_role       = ""data-platform-apps-and-tools""
      // TODO: Replace this with Obersevability Platform PRODUCTION Prometheus URL
      observability_platform_prometheus_url = ""https://aps-workspaces.eu-west-2.amazonaws.com/workspaces/ws-464eea97-631a-4e5d-af22-4c5528d9e0e6/api/v1/remote_write""
    }
  }
}
",locals,"locals {
  airflow_name              = ""${local.application_name}-${local.environment}""
  environment_configuration = local.environment_configurations[local.environment]
  environment_configurations = {
    development = {
      /* Route53 */
      route53_zone = ""apps-tools.development.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* SES */
      ses_domain_identity = ""apps-tools.development.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* VPC */
      vpc_cidr                   = ""10.26.128.0/21""
      vpc_private_subnets        = [""10.26.130.0/23"", ""10.26.132.0/23"", ""10.26.134.0/23""]
      vpc_public_subnets         = [""10.26.128.0/27"", ""10.26.128.32/27"", ""10.26.128.64/27""]
      vpc_database_subnets       = [""10.26.128.96/27"", ""10.26.128.128/27"", ""10.26.128.160/27""]
      vpc_enable_nat_gateway     = true
      vpc_one_nat_gateway_per_az = false

      /* EKS */
      eks_cluster_name = ""apps-tools-${local.environment}""
      eks_versions = {
        cluster                   = ""1.28""
        ami_release               = ""1.15.1-264e294c"" // [major version].[minor version].[patch version]-[first 8 chars of commit SHA]. Get the SHA from here: https://github.com/bottlerocket-os/bottlerocket/releases
        addon_coredns             = ""v1.10.1-eksbuild.4""
        addon_kube_proxy          = ""v1.28.2-eksbuild.2""
        addon_vpc_cni             = ""v1.15.1-eksbuild.1""
        addon_aws_guardduty_agent = ""v1.3.0-eksbuild.1""
        addon_ebs_csi_driver      = ""v1.23.1-eksbuild.1""
        addon_efs_csi_driver      = ""v1.7.0-eksbuild.1""
      }
      eks_sso_access_role = ""modernisation-platform-sandbox""

      /* Airflow */
      airflow_s3_bucket             = ""moj-data-platform-airflow-development20230627094128036000000001"" // This is defined in modernisation-platform-environments
      airflow_dag_s3_path           = ""dags/""                                                           // This is defined in modernisation-platform-environments
      airflow_requirements_s3_path  = ""requirements.txt""                                                // This is defined in modernisation-platform-environments
      airflow_execution_role_name   = ""${local.application_name}-${local.environment}-airflow-execution""
      airflow_name                  = ""${local.application_name}-${local.environment}""
      airflow_version               = ""2.6.3""
      airflow_environment_class     = ""mw1.medium""
      airflow_max_workers           = 2
      airflow_min_workers           = 1
      airflow_schedulers            = 2
      airflow_webserver_access_mode = ""PUBLIC_ONLY""
      airflow_configuration_options = {
        ""webserver.warn_deployment_exposure"" = 0
      }
      airflow_mail_from_address               = ""airflow""
      airflow_weekly_maintenance_window_start = ""SAT:00:00""

      /* Data Platform */
      data_platform_account_id        = local.environment_management.account_ids[""data-platform-development""]
      data_platform_openmetadata_role = ""openmetadata""

      /* Observability Platform */
      observability_platform_account_id     = local.environment_management.account_ids[""observability-platform-development""]
      observability_platform_role           = ""data-platform-apps-and-tools""
      observability_platform_prometheus_url = ""https://aps-workspaces.eu-west-2.amazonaws.com/workspaces/ws-464eea97-631a-4e5d-af22-4c5528d9e0e6/api/v1/remote_write""
    }
    production = {
      /* Route53 */
      route53_zone = ""apps-tools.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* SES */
      ses_domain_identity = ""apps-tools.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* VPC */
      vpc_cidr                   = ""10.27.128.0/21""
      vpc_private_subnets        = [""10.27.130.0/23"", ""10.27.132.0/23"", ""10.27.134.0/23""]
      vpc_public_subnets         = [""10.27.128.0/27"", ""10.27.128.32/27"", ""10.27.128.64/27""]
      vpc_database_subnets       = [""10.27.128.96/27"", ""10.27.128.128/27"", ""10.27.128.160/27""]
      vpc_enable_nat_gateway     = true
      vpc_one_nat_gateway_per_az = false

      /* EKS */
      eks_cluster_name = ""apps-tools-${local.environment}""
      eks_versions = {
        cluster                   = ""1.28""
        ami_release               = ""1.15.1-264e294c"" // [major version].[minor version].[patch version]-[first 8 chars of commit SHA]. Get the SHA from here: https://github.com/bottlerocket-os/bottlerocket/releases
        addon_coredns             = ""v1.10.1-eksbuild.4""
        addon_kube_proxy          = ""v1.28.2-eksbuild.2""
        addon_vpc_cni             = ""v1.15.1-eksbuild.1""
        addon_aws_guardduty_agent = ""v1.3.0-eksbuild.1""
        addon_ebs_csi_driver      = ""v1.23.1-eksbuild.1""
        addon_efs_csi_driver      = ""v1.7.0-eksbuild.1""
      }
      eks_sso_access_role = ""modernisation-platform-developer""

      /* Airflow */
      airflow_s3_bucket             = ""moj-data-platform-airflow-production20230908140747954800000002"" // This is defined in modernisation-platform-environments
      airflow_dag_s3_path           = ""dags/""                                                          // This is defined in modernisation-platform-environments
      airflow_requirements_s3_path  = ""requirements.txt""                                               // This is defined in modernisation-platform-environments
      airflow_execution_role_name   = ""${local.application_name}-${local.environment}-airflow-execution""
      airflow_name                  = ""${local.application_name}-${local.environment}""
      airflow_version               = ""2.6.3""
      airflow_environment_class     = ""mw1.medium""
      airflow_max_workers           = 2
      airflow_min_workers           = 1
      airflow_schedulers            = 2
      airflow_webserver_access_mode = ""PUBLIC_ONLY""
      airflow_configuration_options = {
        ""webserver.warn_deployment_exposure"" = 0
      }
      airflow_mail_from_address               = ""airflow""
      airflow_weekly_maintenance_window_start = ""SAT:00:00""

      /* Data Platform */
      data_platform_account_id        = local.environment_management.account_ids[""data-platform-production""]
      data_platform_openmetadata_role = ""openmetadata""

      /* Observability Platform */
      observability_platform_account_id     = local.environment_management.account_ids[""observability-platform-production""]
      observability_platform_role           = ""data-platform-apps-and-tools""
      observability_platform_prometheus_url = ""https://aps-workspaces.eu-west-2.amazonaws.com/workspaces/ws-55a65e9b-aab9-47a0-88b4-8275c50f1ff9/api/v1/remote_write""
    }
  }
}
",locals,115,,90002b7667baf7c1eb23d0e49dcfa9d9eaee7567,72b87280173d4417efe586f9faf8583a187e65f5,https://github.com/ministryofjustice/modernisation-platform/blob/90002b7667baf7c1eb23d0e49dcfa9d9eaee7567/terraform/environments/data-platform-apps-and-tools/environment-configuration.tf#L115,https://github.com/ministryofjustice/modernisation-platform/blob/72b87280173d4417efe586f9faf8583a187e65f5/terraform/environments/data-platform-apps-and-tools/environment-configuration.tf,2023-10-18 15:43:32+01:00,2023-10-19 10:41:31+01:00,2,1,0,1,0,0,0,1,1,0
https://github.com/terraform-google-modules/terraform-google-bigquery,23,main.tf,main.tf,0,# todo,# TODO: Should be configured to create multiple tables on central dataset,# TODO: Should be configured to create multiple tables on central dataset,"resource ""google_bigquery_table"" ""main"" {
  dataset_id = ""${google_bigquery_dataset.main.dataset_id}""
  table_id   = ""${var.table_id}""
  project    = ""${var.project_id}""

  #TODO: terraform 0.12 will enable ""time_partitioning ? time_partitioning_is_required : null"" (https://github.com/hashicorp/terraform/issues/17968)
  time_partitioning {
    type = ""${var.time_partitioning}""
  }

  labels = ""${var.table_labels}""

  schema = ""${file(""${var.schema_file}"")}""
}
",resource,"resource ""google_bigquery_table"" ""main"" {
  dataset_id = ""${google_bigquery_dataset.main.dataset_id}""
  table_id   = ""${var.table_id}""
  project    = ""${var.project_id}""

  time_partitioning {
    type = ""${var.time_partitioning}""
  }

  labels = ""${var.table_labels}""

  schema = ""${file(""${var.schema_file}"")}""
}
",resource,42,,7f922f7e9df197df38c9b09dfa6e3614d71f19f5,a7966ae4afc7bb73842fb9fd6f0f708b359cf36e,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/7f922f7e9df197df38c9b09dfa6e3614d71f19f5/main.tf#L42,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/a7966ae4afc7bb73842fb9fd6f0f708b359cf36e/main.tf,2019-01-16 18:10:54-05:00,2019-01-28 12:41:47-05:00,2,1,1,0,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,615,examples/data-solutions/data-platform-foundations/variables.tf,examples/data-solutions/data-platform-foundations/variables.tf,0,#todo,"#TODO Add Env variables, Airflow version","#TODO Add Env variables, Airflow version","variable ""composer_config"" {
  type = object({
    node_count = number
    #TODO Move to network
    ip_range_cloudsql   = string
    ip_range_gke_master = string
    ip_range_web_server = string
    #TODO hardcoded
    project_policy_boolean = map(bool)
    region                 = string
    ip_allocation_policy = object({
      use_ip_aliases                = string
      cluster_secondary_range_name  = string
      services_secondary_range_name = string
    })
    #TODO Add Env variables, Airflow version
  })
  default = {
    node_count             = 3
    ip_range_cloudsql      = ""10.20.10.0/24""
    ip_range_gke_master    = ""10.20.11.0/28""
    ip_range_web_server    = ""10.20.11.16/28""
    project_policy_boolean = null
    region                 = ""europe-west1""
    ip_allocation_policy = {
      use_ip_aliases                = ""true""
      cluster_secondary_range_name  = ""pods""
      services_secondary_range_name = ""services""
    }
  }
}
",variable,"variable ""composer_config"" {
  type = object({
    node_count      = number
    airflow_version = string
    env_variables   = map(string)
  })
  default = {
    node_count      = 3
    airflow_version = ""composer-1.17.5-airflow-2.1.4""
    env_variables   = {}
  }
}
",variable,45,,2e560407c118e7b7abc32f8ac1788a3f48563f21,d8bad5779036aa31639e4611e4935287fc79a4bc,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2e560407c118e7b7abc32f8ac1788a3f48563f21/examples/data-solutions/data-platform-foundations/variables.tf#L45,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d8bad5779036aa31639e4611e4935287fc79a4bc/examples/data-solutions/data-platform-foundations/variables.tf,2022-02-07 17:51:06+01:00,2022-02-07 21:28:54+01:00,2,1,0,1,0,0,0,0,0,0
https://github.com/nasa/cumulus,23,tf-modules/cumulus/ecs_cluster.tf,tf-modules/cumulus/ecs_cluster.tf,0,todo,# TODO Get this value dynamically instead of from a variable,# TODO Get this value dynamically instead of from a variable,"data ""aws_iam_policy_document"" ""ecs_cluster_instance_policy"" {
  statement {
    actions   = [""dynamodb:UpdateItem""]
    resources = [data.aws_dynamodb_table.async_operations.arn]
  }

  statement {
    actions = [
      ""autoscaling:CompleteLifecycleAction"",
      ""autoscaling:DescribeAutoScalingInstances"",
      ""autoscaling:DescribeLifecycleHooks"",
      ""autoscaling:RecordLifecycleActionHeartbeat"",
      ""cloudwatch:GetMetricStatistics"",
      ""ec2:DescribeInstances"",
      ""ecr:BatchCheckLayerAvailability"",
      ""ecr:BatchGetImage"",
      ""ecr:GetAuthorizationToken"",
      ""ecr:GetDownloadUrlForLayer"",
      ""ecs:DeregisterContainerInstance"",
      ""ecs:DescribeClusters"",
      ""ecs:DescribeContainerInstances"",
      ""ecs:DescribeServices"",
      ""ecs:DiscoverPollEndpoint"",
      ""ecs:ListContainerInstances"",
      ""ecs:ListServices"",
      ""ecs:ListTaskDefinitions"",
      ""ecs:ListTasks"",
      ""ecs:Poll"",
      ""ecs:RegisterContainerInstance"",
      ""ecs:RunTask"",
      ""ecs:StartTelemetrySession"",
      ""ecs:Submit*"",
      ""ecs:UpdateContainerInstancesState"",
      ""lambda:GetFunction"",
      ""lambda:invokeFunction"",
      ""logs:CreateLogGroup"",
      ""logs:CreateLogStream"",
      ""logs:DescribeLogStreams"",
      ""logs:PutLogEvents"",
      ""ssm:GetParameter""
    ]
    resources = [""*""]
  }

  statement {
    actions = [
      ""states:DescribeActivity"",
      ""states:GetActivityTask"",
      ""states:GetExecutionHistory"",
      ""states:SendTaskFailure"",
      ""states:SendTaskSuccess""
    ]
    resources = [""arn:aws:states:*:*:*""]
  }

  statement {
    actions = [
      ""s3:GetAccelerateConfiguration"",
      ""s3:GetBucket*"",
      ""s3:GetLifecycleConfiguration"",
      ""s3:GetReplicationConfiguration"",
      ""s3:ListBucket*"",
      ""s3:PutAccelerateConfiguration"",
      ""s3:PutBucket*"",
      ""s3:PutLifecycleConfiguration"",
      ""s3:PutReplicationConfiguration""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}""]
  }

  statement {
    actions = [
      ""s3:AbortMultipartUpload"",
      ""s3:DeleteObject"",
      ""s3:DeleteObjectVersion"",
      ""s3:GetObject*"",
      ""s3:ListMultipartUploadParts"",
      ""s3:PutObject*""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}/*""]
  }

  statement {
    actions = [""dynamodb:Scan""]
    # TODO I don't like the fact that we're making an assumption here about the names of our tables
    resources = [""arn:aws:dynamodb:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:table/${var.prefix}-*""]
  }

  statement {
    actions = [
      ""es:ESHttpDelete"",
      ""es:ESHttpGet"",
      ""es:ESHttpHead"",
      ""es:ESHttpPost"",
      ""es:ESHttpPut""
    ]
    # TODO Get this value dynamically instead of from a variable
    resources = [var.elasticsearch_arn]
  }
}
",data,"data ""aws_iam_policy_document"" ""ecs_cluster_instance_policy"" {
  statement {
    actions   = [""dynamodb:UpdateItem""]
    resources = [data.aws_dynamodb_table.async_operations.arn]
  }

  statement {
    actions = [
      ""autoscaling:CompleteLifecycleAction"",
      ""autoscaling:DescribeAutoScalingInstances"",
      ""autoscaling:DescribeLifecycleHooks"",
      ""autoscaling:RecordLifecycleActionHeartbeat"",
      ""cloudwatch:GetMetricStatistics"",
      ""ec2:DescribeInstances"",
      ""ecr:BatchCheckLayerAvailability"",
      ""ecr:BatchGetImage"",
      ""ecr:GetAuthorizationToken"",
      ""ecr:GetDownloadUrlForLayer"",
      ""ecs:DeregisterContainerInstance"",
      ""ecs:DescribeClusters"",
      ""ecs:DescribeContainerInstances"",
      ""ecs:DescribeServices"",
      ""ecs:DiscoverPollEndpoint"",
      ""ecs:ListContainerInstances"",
      ""ecs:ListServices"",
      ""ecs:ListTaskDefinitions"",
      ""ecs:ListTasks"",
      ""ecs:Poll"",
      ""ecs:RegisterContainerInstance"",
      ""ecs:RunTask"",
      ""ecs:StartTelemetrySession"",
      ""ecs:Submit*"",
      ""ecs:UpdateContainerInstancesState"",
      ""lambda:GetFunction"",
      ""lambda:invokeFunction"",
      ""logs:CreateLogGroup"",
      ""logs:CreateLogStream"",
      ""logs:DescribeLogStreams"",
      ""logs:PutLogEvents"",
      ""ssm:GetParameter""
    ]
    resources = [""*""]
  }

  statement {
    actions = [
      ""states:DescribeActivity"",
      ""states:GetActivityTask"",
      ""states:GetExecutionHistory"",
      ""states:SendTaskFailure"",
      ""states:SendTaskSuccess""
    ]
    resources = [""arn:aws:states:*:*:*""]
  }

  statement {
    actions = [
      ""s3:GetAccelerateConfiguration"",
      ""s3:GetBucket*"",
      ""s3:GetLifecycleConfiguration"",
      ""s3:GetReplicationConfiguration"",
      ""s3:ListBucket*"",
      ""s3:PutAccelerateConfiguration"",
      ""s3:PutBucket*"",
      ""s3:PutLifecycleConfiguration"",
      ""s3:PutReplicationConfiguration""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}""]
  }

  statement {
    actions = [
      ""s3:AbortMultipartUpload"",
      ""s3:DeleteObject"",
      ""s3:DeleteObjectVersion"",
      ""s3:GetObject*"",
      ""s3:ListMultipartUploadParts"",
      ""s3:PutObject*""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}/*""]
  }

  statement {
    actions = [""dynamodb:Scan""]
    # TODO I don't like the fact that we're making an assumption here about the names of our tables
    resources = [""arn:aws:dynamodb:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:table/${var.prefix}-*""]
  }

  statement {
    actions = [
      ""es:ESHttpDelete"",
      ""es:ESHttpGet"",
      ""es:ESHttpHead"",
      ""es:ESHttpPost"",
      ""es:ESHttpPut""
    ]
    resources = [var.elasticsearch_domain_arn]
  }
}
",data,102,,1da53282470313085da6e713a94458500df71f6c,ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7,https://github.com/nasa/cumulus/blob/1da53282470313085da6e713a94458500df71f6c/tf-modules/cumulus/ecs_cluster.tf#L102,https://github.com/nasa/cumulus/blob/ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7/tf-modules/cumulus/ecs_cluster.tf,2019-08-02 16:32:51-04:00,2019-08-14 14:23:38-04:00,3,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,19,community/modules/scheduler/gke-cluster/main.tf,community/modules/scheduler/gke-cluster/main.tf,0,take care,# is not allowed. Dataplane V2 will take care of network policy enforcement,"# Enabling NetworkPolicy for clusters with DatapathProvider=ADVANCED_DATAPATH 
 # is not allowed. Dataplane V2 will take care of network policy enforcement 
 # instead.","resource ""google_container_cluster"" ""gke_cluster"" {
  provider = google-beta

  project         = var.project_id
  name            = local.name
  location        = var.region
  resource_labels = var.labels

  # decouple node pool lifecyle from cluster life cycle
  remove_default_node_pool = true
  initial_node_count       = 1 # must be set when remove_default_node_pool is set

  network    = var.network_id
  subnetwork = var.subnetwork_self_link

  # Note: the existence of the ""master_authorized_networks_config"" block enables
  # the master authorized networks even if it's empty.
  master_authorized_networks_config {
  }

  private_ipv6_google_access = var.enable_private_ipv6_google_access ? ""PRIVATE_IPV6_GOOGLE_ACCESS_TO_GOOGLE"" : null

  master_auth {
    client_certificate_config {
      issue_client_certificate = false
    }
  }

  pod_security_policy_config {
    enabled = true
  }

  enable_shielded_nodes = true

  cluster_autoscaling { # Auto provisioning of node-pools
    enabled = false
    # Recomended profile if we ever turn on
    # autoscaling_profile = ""OPTIMIZE_UTILIZATION""
  }

  datapath_provider = var.enable_dataplane_v2 ? ""ADVANCED_DATAPATH"" : ""LEGACY_DATAPATH""

  network_policy {
    # Enabling NetworkPolicy for clusters with DatapathProvider=ADVANCED_DATAPATH
    # is not allowed. Dataplane V2 will take care of network policy enforcement
    # instead.
    enabled = false
    # GKE Dataplane V2 support. This must be set to PROVIDER_UNSPECIFIED in
    # order to let the datapath_provider take effect.
    # https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/issues/656#issuecomment-720398658
    provider = ""PROVIDER_UNSPECIFIED""
  }

  private_cluster_config {
    enable_private_nodes    = var.enable_private_nodes
    enable_private_endpoint = var.enable_private_endpoint
    master_ipv4_cidr_block  = var.master_ipv4_cidr_block
    master_global_access_config {
      enabled = var.enable_master_global_access
    }
  }

  ip_allocation_policy {
    cluster_secondary_range_name  = var.pods_ip_range_name
    services_secondary_range_name = var.services_ip_range_name
  }

  workload_identity_config {
    workload_pool = ""${var.project_id}.svc.id.goog""
  }

  dynamic ""authenticator_groups_config"" {
    for_each = local.cluster_authenticator_security_group
    content {
      security_group = authenticator_groups_config.value.security_group
    }
  }

  release_channel {
    channel = var.release_channel
  }

  maintenance_policy {
    daily_maintenance_window {
      start_time = var.maintenance_start_time
    }

    dynamic ""maintenance_exclusion"" {
      for_each = var.maintenance_exclusions
      content {
        exclusion_name = maintenance_exclusion.value.name
        start_time     = maintenance_exclusion.value.start_time
        end_time       = maintenance_exclusion.value.end_time
        exclusion_options {
          scope = maintenance_exclusion.value.exclusion_scope
        }
      }
    }
  }

  addons_config {
    # Istio is required if there is any pod-to-pod communication.
    istio_config {
      disabled = !var.enable_istio
      auth     = var.istio_auth
    }

    gce_persistent_disk_csi_driver_config {
      enabled = true
    }
  }

  lifecycle {
    # Ignore all changes to the default node pool. It's being removed after creation.
    ignore_changes = [
      node_config
    ]
  }

  logging_service    = ""logging.googleapis.com/kubernetes""
  monitoring_service = ""monitoring.googleapis.com/kubernetes""
}
",resource,"resource ""google_container_cluster"" ""gke_cluster"" {
  provider = google-beta

  project         = var.project_id
  name            = local.name
  location        = var.region
  resource_labels = local.labels

  # decouple node pool lifecycle from cluster life cycle
  remove_default_node_pool = true
  initial_node_count       = 1 # must be set when remove_default_node_pool is set

  network    = var.network_id
  subnetwork = var.subnetwork_self_link

  # Note: the existence of the ""master_authorized_networks_config"" block enables
  # the master authorized networks even if it's empty.
  master_authorized_networks_config {
    dynamic ""cidr_blocks"" {
      for_each = var.master_authorized_networks
      content {
        cidr_block   = cidr_blocks.value.cidr_block
        display_name = cidr_blocks.value.display_name
      }
    }
  }

  private_ipv6_google_access = var.enable_private_ipv6_google_access ? ""PRIVATE_IPV6_GOOGLE_ACCESS_TO_GOOGLE"" : null

  master_auth {
    client_certificate_config {
      issue_client_certificate = false
    }
  }

  enable_shielded_nodes = true

  cluster_autoscaling {
    # Controls auto provisioning of node-pools
    enabled = false

    # Controls autoscaling algorithm of node-pools
    autoscaling_profile = var.autoscaling_profile
  }

  datapath_provider = var.enable_dataplane_v2 ? ""ADVANCED_DATAPATH"" : ""LEGACY_DATAPATH""

  network_policy {
    # Enabling NetworkPolicy for clusters with DatapathProvider=ADVANCED_DATAPATH
    # is not allowed. Dataplane V2 will take care of network policy enforcement
    # instead.
    enabled = false
    # GKE Dataplane V2 support. This must be set to PROVIDER_UNSPECIFIED in
    # order to let the datapath_provider take effect.
    # https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/issues/656#issuecomment-720398658
    provider = ""PROVIDER_UNSPECIFIED""
  }

  private_cluster_config {
    enable_private_nodes    = var.enable_private_nodes
    enable_private_endpoint = var.enable_private_endpoint
    master_ipv4_cidr_block  = var.master_ipv4_cidr_block
    master_global_access_config {
      enabled = var.enable_master_global_access
    }
  }

  ip_allocation_policy {
    cluster_secondary_range_name  = var.pods_ip_range_name
    services_secondary_range_name = var.services_ip_range_name
  }

  workload_identity_config {
    workload_pool = ""${var.project_id}.svc.id.goog""
  }

  dynamic ""authenticator_groups_config"" {
    for_each = local.cluster_authenticator_security_group
    content {
      security_group = authenticator_groups_config.value.security_group
    }
  }

  release_channel {
    channel = var.release_channel
  }
  min_master_version = var.min_master_version

  maintenance_policy {
    daily_maintenance_window {
      start_time = var.maintenance_start_time
    }

    dynamic ""maintenance_exclusion"" {
      for_each = var.maintenance_exclusions
      content {
        exclusion_name = maintenance_exclusion.value.name
        start_time     = maintenance_exclusion.value.start_time
        end_time       = maintenance_exclusion.value.end_time
        exclusion_options {
          scope = maintenance_exclusion.value.exclusion_scope
        }
      }
    }
  }

  addons_config {
    gcp_filestore_csi_driver_config {
      enabled = var.enable_filestore_csi
    }
    gcs_fuse_csi_driver_config {
      enabled = var.enable_gcsfuse_csi
    }
    gce_persistent_disk_csi_driver_config {
      enabled = var.enable_persistent_disk_csi
    }
  }

  timeouts {
    create = var.timeout_create
    update = var.timeout_update
  }

  lifecycle {
    # Ignore all changes to the default node pool. It's being removed after creation.
    ignore_changes = [
      node_config
    ]
  }

  logging_service    = ""logging.googleapis.com/kubernetes""
  monitoring_service = ""monitoring.googleapis.com/kubernetes""
}
",resource,78,88.0,79f3f33f5570371dc5483ddec8c55a5400816aeb,a7adc269a3069a1ab27baed7c8f5e136a3f46f3e,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/79f3f33f5570371dc5483ddec8c55a5400816aeb/community/modules/scheduler/gke-cluster/main.tf#L78,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/a7adc269a3069a1ab27baed7c8f5e136a3f46f3e/community/modules/scheduler/gke-cluster/main.tf#L88,2023-04-04 15:04:33-07:00,2024-02-13 17:40:08-08:00,19,0,1,0,0,1,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1070,examples/gke-serverless/multitenant-fleet/variables.tf,blueprints/gke/multitenant-fleet/variables.tf,1,// todo,// TODO(jccb) is there any situation where the control plane VPC would export any routes?,// TODO(jccb) is there any situation where the control plane VPC would export any routes?,"variable ""peering_config"" {
  description = ""Configure peering with the control plane VPC. Requires compute.networks.updatePeering. Set to null if you don't want to update the default peering configuration.""
  type = object({
    export_routes = bool
    import_routes = bool
  })
  default = {
    export_routes = true
    // TODO(jccb) is there any situation where the control plane VPC would export any routes?
    import_routes = false
  }
}
",variable,the block associated got renamed or deleted,,231,,b1d9b27ac3087d9f2bcc592b2818da84602088f5,e8056577ce1767a975bece532dcddc481ff6bb37,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b1d9b27ac3087d9f2bcc592b2818da84602088f5/examples/gke-serverless/multitenant-fleet/variables.tf#L231,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/e8056577ce1767a975bece532dcddc481ff6bb37/blueprints/gke/multitenant-fleet/variables.tf,2022-08-30 20:39:47+02:00,2022-10-12 12:59:36+02:00,5,1,0,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1968,fast/stages/0-bootstrap/automation.tf,fast/stages/0-bootstrap/automation.tf,0,implemented,# account token generation events. This is implemented within the,"# Enable IAM data access logs to capture impersonation and service 
 # account token generation events. This is implemented within the 
 # automation project to limit log volume. For heightened security, 
 # consider enabling it at the organization level. A log sink within 
 # the organization will collect and store these logs in a logging 
 # bucket. See 
 # https://cloud.google.com/iam/docs/audit-logging#audited_operations","module ""automation-project"" {
  source          = ""../../../modules/project""
  billing_account = var.billing_account.id
  name            = ""iac-core-0""
  parent = coalesce(
    var.project_parent_ids.automation, ""organizations/${var.organization.id}""
  )
  prefix = local.prefix
  contacts = (
    var.bootstrap_user != null || var.essential_contacts == null
    ? {}
    : { (var.essential_contacts) = [""ALL""] }
  )
  # human (groups) IAM bindings
  iam_by_principals = {
    (local.principals.gcp-devops) = [
      ""roles/iam.serviceAccountAdmin"",
      ""roles/iam.serviceAccountTokenCreator"",
    ]
    (local.principals.gcp-organization-admins) = [
      ""roles/iam.serviceAccountTokenCreator"",
      ""roles/iam.workloadIdentityPoolAdmin""
    ]
  }
  # machine (service accounts) IAM bindings
  iam = {
    ""roles/browser"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/owner"" = [
      module.automation-tf-bootstrap-sa.iam_email
    ]
    ""roles/cloudbuild.builds.editor"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/cloudbuild.builds.viewer"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/iam.serviceAccountAdmin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/iam.serviceAccountViewer"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/iam.workloadIdentityPoolAdmin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/iam.workloadIdentityPoolViewer"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/source.admin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/source.reader"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/storage.admin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    (module.organization.custom_role_id[""storage_viewer""]) = [
      module.automation-tf-bootstrap-r-sa.iam_email,
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/viewer"" = [
      module.automation-tf-bootstrap-r-sa.iam_email,
      module.automation-tf-resman-r-sa.iam_email
    ]
  }
  iam_bindings = {
    delegated_grants_resman = {
      members = [module.automation-tf-resman-sa.iam_email]
      role    = ""roles/resourcemanager.projectIamAdmin""
      condition = {
        title       = ""resman_delegated_grant""
        description = ""Resource manager service account delegated grant.""
        expression = format(
          ""api.getAttribute('iam.googleapis.com/modifiedGrantsByRole', []).hasOnly(['%s'])"",
          ""roles/serviceusage.serviceUsageConsumer""
        )
      }
    }
  }
  iam_bindings_additive = {
    serviceusage_resman = {
      member = module.automation-tf-resman-sa.iam_email
      role   = ""roles/serviceusage.serviceUsageConsumer""
    }
    serviceusage_resman_r = {
      member = module.automation-tf-resman-r-sa.iam_email
      role   = ""roles/serviceusage.serviceUsageViewer""
    }
  }
  org_policies = var.bootstrap_user != null ? {} : {
    ""compute.skipDefaultNetworkCreation"" = {
      rules = [{ enforce = true }]
    }
    ""iam.automaticIamGrantsForDefaultServiceAccounts"" = {
      rules = [{ enforce = true }]
    }
    ""iam.disableServiceAccountKeyCreation"" = {
      rules = [{ enforce = true }]
    }
  }
  services = concat(
    [
      ""accesscontextmanager.googleapis.com"",
      ""bigquery.googleapis.com"",
      ""bigqueryreservation.googleapis.com"",
      ""bigquerystorage.googleapis.com"",
      ""billingbudgets.googleapis.com"",
      ""cloudasset.googleapis.com"",
      ""cloudbilling.googleapis.com"",
      ""cloudkms.googleapis.com"",
      ""cloudquotas.googleapis.com"",
      ""cloudresourcemanager.googleapis.com"",
      ""essentialcontacts.googleapis.com"",
      ""iam.googleapis.com"",
      ""iamcredentials.googleapis.com"",
      ""orgpolicy.googleapis.com"",
      ""pubsub.googleapis.com"",
      ""servicenetworking.googleapis.com"",
      ""serviceusage.googleapis.com"",
      ""sourcerepo.googleapis.com"",
      ""stackdriver.googleapis.com"",
      ""storage-component.googleapis.com"",
      ""storage.googleapis.com"",
      ""sts.googleapis.com""
    ],
    # enable specific service only after org policies have been applied
    var.bootstrap_user != null ? [] : [
      ""cloudbuild.googleapis.com"",
      ""compute.googleapis.com"",
      ""container.googleapis.com"",
    ]
  )

  # Enable IAM data access logs to capture impersonation and service
  # account token generation events. This is implemented within the
  # automation project to limit log volume. For heightened security,
  # consider enabling it at the organization level. A log sink within
  # the organization will collect and store these logs in a logging
  # bucket. See
  # https://cloud.google.com/iam/docs/audit-logging#audited_operations
  logging_data_access = {
    ""iam.googleapis.com"" = {
      # ADMIN_READ captures impersonation and token generation/exchanges
      ADMIN_READ = []
      # enable DATA_WRITE if you want to capture configuration changes
      # to IAM-related resources (roles, deny policies, service
      # accounts, identity pools, etc)
      # DATA_WRITE = []
    }
  }
}
",module,"module ""automation-project"" {
  source          = ""../../../modules/project""
  billing_account = var.billing_account.id
  name            = ""iac-core-0""
  parent = coalesce(
    var.project_parent_ids.automation, ""organizations/${var.organization.id}""
  )
  prefix = local.prefix
  contacts = (
    var.bootstrap_user != null || var.essential_contacts == null
    ? {}
    : { (var.essential_contacts) = [""ALL""] }
  )
  # human (groups) IAM bindings
  iam_by_principals = {
    (local.principals.gcp-devops) = [
      ""roles/iam.serviceAccountAdmin"",
      ""roles/iam.serviceAccountTokenCreator"",
    ]
    (local.principals.gcp-organization-admins) = [
      ""roles/iam.serviceAccountTokenCreator"",
      ""roles/iam.workloadIdentityPoolAdmin""
    ]
  }
  # machine (service accounts) IAM bindings
  iam = {
    ""roles/browser"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/owner"" = [
      module.automation-tf-bootstrap-sa.iam_email
    ]
    ""roles/cloudbuild.builds.editor"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/cloudbuild.builds.viewer"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/iam.serviceAccountAdmin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/iam.serviceAccountViewer"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/iam.workloadIdentityPoolAdmin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/iam.workloadIdentityPoolViewer"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/source.admin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/source.reader"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/storage.admin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    (module.organization.custom_role_id[""storage_viewer""]) = [
      module.automation-tf-bootstrap-r-sa.iam_email,
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/viewer"" = [
      module.automation-tf-bootstrap-r-sa.iam_email,
      module.automation-tf-resman-r-sa.iam_email
    ]
  }
  iam_bindings = {
    delegated_grants_resman = {
      members = [module.automation-tf-resman-sa.iam_email]
      role    = ""roles/resourcemanager.projectIamAdmin""
      condition = {
        title       = ""resman_delegated_grant""
        description = ""Resource manager service account delegated grant.""
        expression = format(
          ""api.getAttribute('iam.googleapis.com/modifiedGrantsByRole', []).hasOnly(['%s'])"",
          ""roles/serviceusage.serviceUsageConsumer""
        )
      }
    }
  }
  iam_bindings_additive = {
    serviceusage_resman = {
      member = module.automation-tf-resman-sa.iam_email
      role   = ""roles/serviceusage.serviceUsageConsumer""
    }
    serviceusage_resman_r = {
      member = module.automation-tf-resman-r-sa.iam_email
      role   = ""roles/serviceusage.serviceUsageViewer""
    }
  }
  org_policies = var.bootstrap_user != null ? {} : {
    ""compute.skipDefaultNetworkCreation"" = {
      rules = [{ enforce = true }]
    }
    ""iam.automaticIamGrantsForDefaultServiceAccounts"" = {
      rules = [{ enforce = true }]
    }
    ""iam.disableServiceAccountKeyCreation"" = {
      rules = [{ enforce = true }]
    }
  }
  services = concat(
    [
      ""accesscontextmanager.googleapis.com"",
      ""bigquery.googleapis.com"",
      ""bigqueryreservation.googleapis.com"",
      ""bigquerystorage.googleapis.com"",
      ""billingbudgets.googleapis.com"",
      ""cloudasset.googleapis.com"",
      ""cloudbilling.googleapis.com"",
      ""cloudkms.googleapis.com"",
      ""cloudquotas.googleapis.com"",
      ""cloudresourcemanager.googleapis.com"",
      ""essentialcontacts.googleapis.com"",
      ""iam.googleapis.com"",
      ""iamcredentials.googleapis.com"",
      ""orgpolicy.googleapis.com"",
      ""pubsub.googleapis.com"",
      ""servicenetworking.googleapis.com"",
      ""serviceusage.googleapis.com"",
      ""sourcerepo.googleapis.com"",
      ""stackdriver.googleapis.com"",
      ""storage-component.googleapis.com"",
      ""storage.googleapis.com"",
      ""sts.googleapis.com""
    ],
    # enable specific service only after org policies have been applied
    var.bootstrap_user != null ? [] : [
      ""cloudbuild.googleapis.com"",
      ""compute.googleapis.com"",
      ""container.googleapis.com"",
    ]
  )

  # Enable IAM data access logs to capture impersonation and service
  # account token generation events. This is implemented within the
  # automation project to limit log volume. For heightened security,
  # consider enabling it at the organization level. A log sink within
  # the organization will collect and store these logs in a logging
  # bucket. See
  # https://cloud.google.com/iam/docs/audit-logging#audited_operations
  logging_data_access = {
    ""iam.googleapis.com"" = {
      # ADMIN_READ captures impersonation and token generation/exchanges
      ADMIN_READ = []
      # enable DATA_WRITE if you want to capture configuration changes
      # to IAM-related resources (roles, deny policies, service
      # accounts, identity pools, etc)
      # DATA_WRITE = []
    }
  }
}
",module,161,163.0,99129d54a37da4c2d977b7db705de5024d530944,be9214f99a0718eb8698e9d539e2ad93cb442ac7,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/99129d54a37da4c2d977b7db705de5024d530944/fast/stages/0-bootstrap/automation.tf#L161,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/be9214f99a0718eb8698e9d539e2ad93cb442ac7/fast/stages/0-bootstrap/automation.tf#L163,2024-04-25 08:31:51+02:00,2024-05-21 10:39:47+02:00,2,0,0,0,0,1,0,0,1,0
https://github.com/awslabs/data-on-eks,18,ai-ml/trainium-inferentia/addons.tf,ai-ml/trainium-inferentia/addons.tf,0,implement,# Commented for visiblity to implement this feature in the future,"  # Commented for visiblity to implement this feature in the future
  #  placement {
  #   tenancy = ""default""
  #   availability_zone = ""${local.region}d""
  #   group_name        = local.karpenter_trn1_32xl_lt_name
  # }","resource ""aws_launch_template"" ""trn1_lt"" {
  name        = local.karpenter_trn1_32xl_lt_name
  description = ""Karpenter Trn1.32xlarge Launch Template""

  user_data = data.cloudinit_config.trn1_lt.rendered

  ebs_optimized = true

  image_id = data.aws_ami.eks_gpu.id

  iam_instance_profile {
    name = module.eks_blueprints_addons.karpenter.node_instance_profile_name
  }

  # Commented for visiblity to implement this feature in the future
  #  placement {
  #   tenancy = ""default""
  #   availability_zone = ""${local.region}d""
  #   group_name        = local.karpenter_trn1_32xl_lt_name
  # }

  metadata_options {
    http_endpoint               = ""enabled""
    http_tokens                 = ""required""
    http_put_response_hop_limit = 2
  }

  block_device_mappings {
    device_name = ""/dev/xvda""
    ebs {
      volume_size           = 100
      delete_on_termination = true
      volume_type           = ""gp3""
    }
  }

  monitoring {
    enabled = true
  }

  tag_specifications {
    resource_type = ""instance""

    tags = merge(local.tags, {
      ""karpenter.sh/discovery"" = local.name
    })
  }

  # First network interface with device_index=0 and network_card_index=0
  network_interfaces {
    device_index                = 0
    network_card_index          = 0
    associate_public_ip_address = false
    interface_type              = ""efa""
    delete_on_termination       = true
    security_groups             = [module.eks.node_security_group_id]
    description                 = ""Karpenter EFA config for Trainium""
  }

  # Additional network interfaces with device_index=1 and network_card_index ranging from 1 to 7
  dynamic ""network_interfaces"" {
    for_each = range(1, 8) # Create 7 additional network interfaces
    content {
      device_index                = 1
      network_card_index          = network_interfaces.value
      associate_public_ip_address = false
      interface_type              = ""efa""
      delete_on_termination       = true
      security_groups             = [module.eks.node_security_group_id]
      description                 = ""Karpenter EFA config for Trainium""
    }
  }
}
",resource,"resource ""aws_launch_template"" ""trn1_lt"" {
  name        = local.karpenter_trn1_32xl_lt_name
  description = ""Karpenter Trn1.32xlarge Launch Template""

  user_data = data.cloudinit_config.trn1_lt.rendered

  ebs_optimized = true

  image_id = data.aws_ami.eks_gpu.id

  iam_instance_profile {
    name = module.eks_blueprints_addons.karpenter.node_instance_profile_name
  }

  # Commented for visibility to implement this feature in the future
  #  placement {
  #   tenancy = ""default""
  #   availability_zone = ""${local.region}d""
  #   group_name        = local.karpenter_trn1_32xl_lt_name
  # }

  metadata_options {
    http_endpoint               = ""enabled""
    http_tokens                 = ""required""
    http_put_response_hop_limit = 2
  }

  block_device_mappings {
    device_name = ""/dev/xvda""
    ebs {
      volume_size           = 100
      delete_on_termination = true
      volume_type           = ""gp3""
    }
  }

  monitoring {
    enabled = true
  }

  tag_specifications {
    resource_type = ""instance""

    tags = merge(local.tags, {
      ""karpenter.sh/discovery"" = local.name
    })
  }

  # First network interface with device_index=0 and network_card_index=0
  network_interfaces {
    device_index                = 0
    network_card_index          = 0
    associate_public_ip_address = false
    interface_type              = ""efa""
    delete_on_termination       = true
    security_groups             = [module.eks.node_security_group_id]
    description                 = ""Karpenter EFA config for Trainium""
  }

  # Additional network interfaces with device_index=1 and network_card_index ranging from 1 to 7
  dynamic ""network_interfaces"" {
    for_each = range(1, 8) # Create 7 additional network interfaces
    content {
      device_index                = 1
      network_card_index          = network_interfaces.value
      associate_public_ip_address = false
      interface_type              = ""efa""
      delete_on_termination       = true
      security_groups             = [module.eks.node_security_group_id]
      description                 = ""Karpenter EFA config for Trainium""
    }
  }
}
",resource,424,,39e790ce0d4e45979d1374a86b2030e55a838441,352456833a6bee906dd49d28ad050c8f048c767b,https://github.com/awslabs/data-on-eks/blob/39e790ce0d4e45979d1374a86b2030e55a838441/ai-ml/trainium-inferentia/addons.tf#L424,https://github.com/awslabs/data-on-eks/blob/352456833a6bee906dd49d28ad050c8f048c767b/ai-ml/trainium-inferentia/addons.tf,2023-08-17 19:17:31+01:00,2023-12-08 15:47:41-05:00,9,1,1,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-sql-db,3,modules/backup/variables.tf,modules/backup/variables.tf,0,#todo,#TODO: test,"error_message = ""Must be a full GCS URI starting with gs://."" #TODO: test","variable ""export_uri"" {
  description = ""The bucket and path uri for exporting to GCS""
  type        = string
  validation {
    condition     = can(regex(""^gs:\\/\\/"", var.export_uri))
    error_message = ""Must be a full GCS URI starting with gs://."" #TODO: test
  }
}
",variable,"variable ""export_uri"" {
  description = ""The bucket and path uri for exporting to GCS""
  type        = string
  validation {
    condition     = can(regex(""^gs:\\/\\/"", var.export_uri))
    error_message = ""Must be a full GCS URI starting with gs://.""
  }
}
",variable,87,,c51bf296e392fca246aae1c9ba4299a5a97ef274,b1ef34d0ee1a84ef2c0be4141cb83448052264da,https://github.com/terraform-google-modules/terraform-google-sql-db/blob/c51bf296e392fca246aae1c9ba4299a5a97ef274/modules/backup/variables.tf#L87,https://github.com/terraform-google-modules/terraform-google-sql-db/blob/b1ef34d0ee1a84ef2c0be4141cb83448052264da/modules/backup/variables.tf,2022-05-13 10:59:53-05:00,2022-08-09 09:38:28-05:00,2,1,0,0,0,0,0,0,0,1
https://github.com/terraform-aws-modules/terraform-aws-eks,648,node_groups.tf,node_groups.tf,0,todo,# TODO - update this when `var.platform` is removed in v21.0,# TODO - update this when `var.platform` is removed in v21.0,"module ""self_managed_node_group"" {
  source = ""./modules/self-managed-node-group""

  for_each = { for k, v in var.self_managed_node_groups : k => v if var.create }

  create = try(each.value.create, true)

  cluster_name = time_sleep.this[0].triggers[""cluster_name""]

  # Autoscaling Group
  create_autoscaling_group = try(each.value.create_autoscaling_group, var.self_managed_node_group_defaults.create_autoscaling_group, true)

  name            = try(each.value.name, each.key)
  use_name_prefix = try(each.value.use_name_prefix, var.self_managed_node_group_defaults.use_name_prefix, true)

  availability_zones = try(each.value.availability_zones, var.self_managed_node_group_defaults.availability_zones, null)
  subnet_ids         = try(each.value.subnet_ids, var.self_managed_node_group_defaults.subnet_ids, var.subnet_ids)

  min_size                  = try(each.value.min_size, var.self_managed_node_group_defaults.min_size, 0)
  max_size                  = try(each.value.max_size, var.self_managed_node_group_defaults.max_size, 3)
  desired_size              = try(each.value.desired_size, var.self_managed_node_group_defaults.desired_size, 1)
  capacity_rebalance        = try(each.value.capacity_rebalance, var.self_managed_node_group_defaults.capacity_rebalance, null)
  min_elb_capacity          = try(each.value.min_elb_capacity, var.self_managed_node_group_defaults.min_elb_capacity, null)
  wait_for_elb_capacity     = try(each.value.wait_for_elb_capacity, var.self_managed_node_group_defaults.wait_for_elb_capacity, null)
  wait_for_capacity_timeout = try(each.value.wait_for_capacity_timeout, var.self_managed_node_group_defaults.wait_for_capacity_timeout, null)
  default_cooldown          = try(each.value.default_cooldown, var.self_managed_node_group_defaults.default_cooldown, null)
  default_instance_warmup   = try(each.value.default_instance_warmup, var.self_managed_node_group_defaults.default_instance_warmup, null)
  protect_from_scale_in     = try(each.value.protect_from_scale_in, var.self_managed_node_group_defaults.protect_from_scale_in, null)
  context                   = try(each.value.context, var.self_managed_node_group_defaults.context, null)

  target_group_arns         = try(each.value.target_group_arns, var.self_managed_node_group_defaults.target_group_arns, [])
  placement_group           = try(each.value.placement_group, var.self_managed_node_group_defaults.placement_group, null)
  health_check_type         = try(each.value.health_check_type, var.self_managed_node_group_defaults.health_check_type, null)
  health_check_grace_period = try(each.value.health_check_grace_period, var.self_managed_node_group_defaults.health_check_grace_period, null)

  force_delete           = try(each.value.force_delete, var.self_managed_node_group_defaults.force_delete, null)
  force_delete_warm_pool = try(each.value.force_delete_warm_pool, var.self_managed_node_group_defaults.force_delete_warm_pool, null)
  termination_policies   = try(each.value.termination_policies, var.self_managed_node_group_defaults.termination_policies, [])
  suspended_processes    = try(each.value.suspended_processes, var.self_managed_node_group_defaults.suspended_processes, [])
  max_instance_lifetime  = try(each.value.max_instance_lifetime, var.self_managed_node_group_defaults.max_instance_lifetime, null)

  enabled_metrics         = try(each.value.enabled_metrics, var.self_managed_node_group_defaults.enabled_metrics, [])
  metrics_granularity     = try(each.value.metrics_granularity, var.self_managed_node_group_defaults.metrics_granularity, null)
  service_linked_role_arn = try(each.value.service_linked_role_arn, var.self_managed_node_group_defaults.service_linked_role_arn, null)

  initial_lifecycle_hooks     = try(each.value.initial_lifecycle_hooks, var.self_managed_node_group_defaults.initial_lifecycle_hooks, [])
  instance_maintenance_policy = try(each.value.instance_maintenance_policy, var.self_managed_node_group_defaults.instance_maintenance_policy, {})
  instance_refresh            = try(each.value.instance_refresh, var.self_managed_node_group_defaults.instance_refresh, local.default_instance_refresh)
  use_mixed_instances_policy  = try(each.value.use_mixed_instances_policy, var.self_managed_node_group_defaults.use_mixed_instances_policy, false)
  mixed_instances_policy      = try(each.value.mixed_instances_policy, var.self_managed_node_group_defaults.mixed_instances_policy, null)
  warm_pool                   = try(each.value.warm_pool, var.self_managed_node_group_defaults.warm_pool, {})

  delete_timeout         = try(each.value.delete_timeout, var.self_managed_node_group_defaults.delete_timeout, null)
  autoscaling_group_tags = try(each.value.autoscaling_group_tags, var.self_managed_node_group_defaults.autoscaling_group_tags, {})

  # User data
  platform = try(each.value.platform, var.self_managed_node_group_defaults.platform, ""linux"")
  # TODO - update this when `var.platform` is removed in v21.0
  ami_type                 = try(each.value.ami_type, var.self_managed_node_group_defaults.ami_type, ""AL2_x86_64"")
  cluster_endpoint         = try(time_sleep.this[0].triggers[""cluster_endpoint""], """")
  cluster_auth_base64      = try(time_sleep.this[0].triggers[""cluster_certificate_authority_data""], """")
  cluster_service_cidr     = try(time_sleep.this[0].triggers[""cluster_service_cidr""], """")
  cluster_ip_family        = var.cluster_ip_family
  pre_bootstrap_user_data  = try(each.value.pre_bootstrap_user_data, var.self_managed_node_group_defaults.pre_bootstrap_user_data, """")
  post_bootstrap_user_data = try(each.value.post_bootstrap_user_data, var.self_managed_node_group_defaults.post_bootstrap_user_data, """")
  bootstrap_extra_args     = try(each.value.bootstrap_extra_args, var.self_managed_node_group_defaults.bootstrap_extra_args, """")
  user_data_template_path  = try(each.value.user_data_template_path, var.self_managed_node_group_defaults.user_data_template_path, """")
  cloudinit_pre_nodeadm    = try(each.value.cloudinit_pre_nodeadm, var.self_managed_node_group_defaults.cloudinit_pre_nodeadm, [])
  cloudinit_post_nodeadm   = try(each.value.cloudinit_post_nodeadm, var.self_managed_node_group_defaults.cloudinit_post_nodeadm, [])

  # Launch Template
  create_launch_template                 = try(each.value.create_launch_template, var.self_managed_node_group_defaults.create_launch_template, true)
  launch_template_id                     = try(each.value.launch_template_id, var.self_managed_node_group_defaults.launch_template_id, """")
  launch_template_name                   = try(each.value.launch_template_name, var.self_managed_node_group_defaults.launch_template_name, each.key)
  launch_template_use_name_prefix        = try(each.value.launch_template_use_name_prefix, var.self_managed_node_group_defaults.launch_template_use_name_prefix, true)
  launch_template_version                = try(each.value.launch_template_version, var.self_managed_node_group_defaults.launch_template_version, null)
  launch_template_default_version        = try(each.value.launch_template_default_version, var.self_managed_node_group_defaults.launch_template_default_version, null)
  update_launch_template_default_version = try(each.value.update_launch_template_default_version, var.self_managed_node_group_defaults.update_launch_template_default_version, true)
  launch_template_description            = try(each.value.launch_template_description, var.self_managed_node_group_defaults.launch_template_description, ""Custom launch template for ${try(each.value.name, each.key)} self managed node group"")
  launch_template_tags                   = try(each.value.launch_template_tags, var.self_managed_node_group_defaults.launch_template_tags, {})
  tag_specifications                     = try(each.value.tag_specifications, var.self_managed_node_group_defaults.tag_specifications, [""instance"", ""volume"", ""network-interface""])

  ebs_optimized   = try(each.value.ebs_optimized, var.self_managed_node_group_defaults.ebs_optimized, null)
  ami_id          = try(each.value.ami_id, var.self_managed_node_group_defaults.ami_id, """")
  cluster_version = try(each.value.cluster_version, var.self_managed_node_group_defaults.cluster_version, time_sleep.this[0].triggers[""cluster_version""])
  instance_type   = try(each.value.instance_type, var.self_managed_node_group_defaults.instance_type, ""m6i.large"")
  key_name        = try(each.value.key_name, var.self_managed_node_group_defaults.key_name, null)

  disable_api_termination              = try(each.value.disable_api_termination, var.self_managed_node_group_defaults.disable_api_termination, null)
  instance_initiated_shutdown_behavior = try(each.value.instance_initiated_shutdown_behavior, var.self_managed_node_group_defaults.instance_initiated_shutdown_behavior, null)
  kernel_id                            = try(each.value.kernel_id, var.self_managed_node_group_defaults.kernel_id, null)
  ram_disk_id                          = try(each.value.ram_disk_id, var.self_managed_node_group_defaults.ram_disk_id, null)

  block_device_mappings              = try(each.value.block_device_mappings, var.self_managed_node_group_defaults.block_device_mappings, {})
  capacity_reservation_specification = try(each.value.capacity_reservation_specification, var.self_managed_node_group_defaults.capacity_reservation_specification, {})
  cpu_options                        = try(each.value.cpu_options, var.self_managed_node_group_defaults.cpu_options, {})
  credit_specification               = try(each.value.credit_specification, var.self_managed_node_group_defaults.credit_specification, {})
  elastic_gpu_specifications         = try(each.value.elastic_gpu_specifications, var.self_managed_node_group_defaults.elastic_gpu_specifications, {})
  elastic_inference_accelerator      = try(each.value.elastic_inference_accelerator, var.self_managed_node_group_defaults.elastic_inference_accelerator, {})
  enclave_options                    = try(each.value.enclave_options, var.self_managed_node_group_defaults.enclave_options, {})
  hibernation_options                = try(each.value.hibernation_options, var.self_managed_node_group_defaults.hibernation_options, {})
  instance_requirements              = try(each.value.instance_requirements, var.self_managed_node_group_defaults.instance_requirements, {})
  instance_market_options            = try(each.value.instance_market_options, var.self_managed_node_group_defaults.instance_market_options, {})
  license_specifications             = try(each.value.license_specifications, var.self_managed_node_group_defaults.license_specifications, {})
  metadata_options                   = try(each.value.metadata_options, var.self_managed_node_group_defaults.metadata_options, local.metadata_options)
  enable_monitoring                  = try(each.value.enable_monitoring, var.self_managed_node_group_defaults.enable_monitoring, true)
  enable_efa_support                 = try(each.value.enable_efa_support, var.self_managed_node_group_defaults.enable_efa_support, false)
  network_interfaces                 = try(each.value.network_interfaces, var.self_managed_node_group_defaults.network_interfaces, [])
  placement                          = try(each.value.placement, var.self_managed_node_group_defaults.placement, {})
  maintenance_options                = try(each.value.maintenance_options, var.self_managed_node_group_defaults.maintenance_options, {})
  private_dns_name_options           = try(each.value.private_dns_name_options, var.self_managed_node_group_defaults.private_dns_name_options, {})

  # IAM role
  create_iam_instance_profile   = try(each.value.create_iam_instance_profile, var.self_managed_node_group_defaults.create_iam_instance_profile, true)
  iam_instance_profile_arn      = try(each.value.iam_instance_profile_arn, var.self_managed_node_group_defaults.iam_instance_profile_arn, null)
  iam_role_name                 = try(each.value.iam_role_name, var.self_managed_node_group_defaults.iam_role_name, null)
  iam_role_use_name_prefix      = try(each.value.iam_role_use_name_prefix, var.self_managed_node_group_defaults.iam_role_use_name_prefix, true)
  iam_role_path                 = try(each.value.iam_role_path, var.self_managed_node_group_defaults.iam_role_path, null)
  iam_role_description          = try(each.value.iam_role_description, var.self_managed_node_group_defaults.iam_role_description, ""Self managed node group IAM role"")
  iam_role_permissions_boundary = try(each.value.iam_role_permissions_boundary, var.self_managed_node_group_defaults.iam_role_permissions_boundary, null)
  iam_role_tags                 = try(each.value.iam_role_tags, var.self_managed_node_group_defaults.iam_role_tags, {})
  iam_role_attach_cni_policy    = try(each.value.iam_role_attach_cni_policy, var.self_managed_node_group_defaults.iam_role_attach_cni_policy, true)
  # To better understand why this `lookup()` logic is required, see:
  # https://github.com/hashicorp/terraform/issues/31646#issuecomment-1217279031
  iam_role_additional_policies = lookup(each.value, ""iam_role_additional_policies"", lookup(var.self_managed_node_group_defaults, ""iam_role_additional_policies"", {}))

  # Access entry
  create_access_entry = try(each.value.create_access_entry, var.self_managed_node_group_defaults.create_access_entry, true)
  iam_role_arn        = try(each.value.iam_role_arn, var.self_managed_node_group_defaults.iam_role_arn, null)

  # Autoscaling group schedule
  create_schedule = try(each.value.create_schedule, var.self_managed_node_group_defaults.create_schedule, true)
  schedules       = try(each.value.schedules, var.self_managed_node_group_defaults.schedules, {})

  # Security group
  vpc_security_group_ids            = compact(concat([local.node_security_group_id], try(each.value.vpc_security_group_ids, var.self_managed_node_group_defaults.vpc_security_group_ids, [])))
  cluster_primary_security_group_id = try(each.value.attach_cluster_primary_security_group, var.self_managed_node_group_defaults.attach_cluster_primary_security_group, false) ? aws_eks_cluster.this[0].vpc_config[0].cluster_security_group_id : null

  tags = merge(var.tags, try(each.value.tags, var.self_managed_node_group_defaults.tags, {}))
}
",module,"module ""self_managed_node_group"" {
  source = ""./modules/self-managed-node-group""

  for_each = { for k, v in var.self_managed_node_groups : k => v if var.create }

  create = try(each.value.create, true)

  cluster_name = time_sleep.this[0].triggers[""cluster_name""]

  # Autoscaling Group
  create_autoscaling_group = try(each.value.create_autoscaling_group, var.self_managed_node_group_defaults.create_autoscaling_group, true)

  name            = try(each.value.name, each.key)
  use_name_prefix = try(each.value.use_name_prefix, var.self_managed_node_group_defaults.use_name_prefix, true)

  availability_zones = try(each.value.availability_zones, var.self_managed_node_group_defaults.availability_zones, null)
  subnet_ids         = try(each.value.subnet_ids, var.self_managed_node_group_defaults.subnet_ids, var.subnet_ids)

  min_size                  = try(each.value.min_size, var.self_managed_node_group_defaults.min_size, 0)
  max_size                  = try(each.value.max_size, var.self_managed_node_group_defaults.max_size, 3)
  desired_size              = try(each.value.desired_size, var.self_managed_node_group_defaults.desired_size, 1)
  capacity_rebalance        = try(each.value.capacity_rebalance, var.self_managed_node_group_defaults.capacity_rebalance, null)
  min_elb_capacity          = try(each.value.min_elb_capacity, var.self_managed_node_group_defaults.min_elb_capacity, null)
  wait_for_elb_capacity     = try(each.value.wait_for_elb_capacity, var.self_managed_node_group_defaults.wait_for_elb_capacity, null)
  wait_for_capacity_timeout = try(each.value.wait_for_capacity_timeout, var.self_managed_node_group_defaults.wait_for_capacity_timeout, null)
  default_cooldown          = try(each.value.default_cooldown, var.self_managed_node_group_defaults.default_cooldown, null)
  default_instance_warmup   = try(each.value.default_instance_warmup, var.self_managed_node_group_defaults.default_instance_warmup, null)
  protect_from_scale_in     = try(each.value.protect_from_scale_in, var.self_managed_node_group_defaults.protect_from_scale_in, null)
  context                   = try(each.value.context, var.self_managed_node_group_defaults.context, null)

  target_group_arns         = try(each.value.target_group_arns, var.self_managed_node_group_defaults.target_group_arns, [])
  placement_group           = try(each.value.placement_group, var.self_managed_node_group_defaults.placement_group, null)
  health_check_type         = try(each.value.health_check_type, var.self_managed_node_group_defaults.health_check_type, null)
  health_check_grace_period = try(each.value.health_check_grace_period, var.self_managed_node_group_defaults.health_check_grace_period, null)

  force_delete           = try(each.value.force_delete, var.self_managed_node_group_defaults.force_delete, null)
  force_delete_warm_pool = try(each.value.force_delete_warm_pool, var.self_managed_node_group_defaults.force_delete_warm_pool, null)
  termination_policies   = try(each.value.termination_policies, var.self_managed_node_group_defaults.termination_policies, [])
  suspended_processes    = try(each.value.suspended_processes, var.self_managed_node_group_defaults.suspended_processes, [])
  max_instance_lifetime  = try(each.value.max_instance_lifetime, var.self_managed_node_group_defaults.max_instance_lifetime, null)

  enabled_metrics         = try(each.value.enabled_metrics, var.self_managed_node_group_defaults.enabled_metrics, [])
  metrics_granularity     = try(each.value.metrics_granularity, var.self_managed_node_group_defaults.metrics_granularity, null)
  service_linked_role_arn = try(each.value.service_linked_role_arn, var.self_managed_node_group_defaults.service_linked_role_arn, null)

  initial_lifecycle_hooks     = try(each.value.initial_lifecycle_hooks, var.self_managed_node_group_defaults.initial_lifecycle_hooks, [])
  instance_maintenance_policy = try(each.value.instance_maintenance_policy, var.self_managed_node_group_defaults.instance_maintenance_policy, {})
  instance_refresh            = try(each.value.instance_refresh, var.self_managed_node_group_defaults.instance_refresh, local.default_instance_refresh)
  use_mixed_instances_policy  = try(each.value.use_mixed_instances_policy, var.self_managed_node_group_defaults.use_mixed_instances_policy, false)
  mixed_instances_policy      = try(each.value.mixed_instances_policy, var.self_managed_node_group_defaults.mixed_instances_policy, null)
  warm_pool                   = try(each.value.warm_pool, var.self_managed_node_group_defaults.warm_pool, {})

  delete_timeout         = try(each.value.delete_timeout, var.self_managed_node_group_defaults.delete_timeout, null)
  autoscaling_group_tags = try(each.value.autoscaling_group_tags, var.self_managed_node_group_defaults.autoscaling_group_tags, {})

  # User data
  platform = try(each.value.platform, var.self_managed_node_group_defaults.platform, ""linux"")
  # TODO - update this when `var.platform` is removed in v21.0
  ami_type                 = try(each.value.ami_type, var.self_managed_node_group_defaults.ami_type, ""AL2_x86_64"")
  cluster_endpoint         = try(time_sleep.this[0].triggers[""cluster_endpoint""], """")
  cluster_auth_base64      = try(time_sleep.this[0].triggers[""cluster_certificate_authority_data""], """")
  cluster_service_cidr     = try(time_sleep.this[0].triggers[""cluster_service_cidr""], """")
  cluster_ip_family        = var.cluster_ip_family
  pre_bootstrap_user_data  = try(each.value.pre_bootstrap_user_data, var.self_managed_node_group_defaults.pre_bootstrap_user_data, """")
  post_bootstrap_user_data = try(each.value.post_bootstrap_user_data, var.self_managed_node_group_defaults.post_bootstrap_user_data, """")
  bootstrap_extra_args     = try(each.value.bootstrap_extra_args, var.self_managed_node_group_defaults.bootstrap_extra_args, """")
  user_data_template_path  = try(each.value.user_data_template_path, var.self_managed_node_group_defaults.user_data_template_path, """")
  cloudinit_pre_nodeadm    = try(each.value.cloudinit_pre_nodeadm, var.self_managed_node_group_defaults.cloudinit_pre_nodeadm, [])
  cloudinit_post_nodeadm   = try(each.value.cloudinit_post_nodeadm, var.self_managed_node_group_defaults.cloudinit_post_nodeadm, [])

  # Launch Template
  create_launch_template                 = try(each.value.create_launch_template, var.self_managed_node_group_defaults.create_launch_template, true)
  launch_template_id                     = try(each.value.launch_template_id, var.self_managed_node_group_defaults.launch_template_id, """")
  launch_template_name                   = try(each.value.launch_template_name, var.self_managed_node_group_defaults.launch_template_name, each.key)
  launch_template_use_name_prefix        = try(each.value.launch_template_use_name_prefix, var.self_managed_node_group_defaults.launch_template_use_name_prefix, true)
  launch_template_version                = try(each.value.launch_template_version, var.self_managed_node_group_defaults.launch_template_version, null)
  launch_template_default_version        = try(each.value.launch_template_default_version, var.self_managed_node_group_defaults.launch_template_default_version, null)
  update_launch_template_default_version = try(each.value.update_launch_template_default_version, var.self_managed_node_group_defaults.update_launch_template_default_version, true)
  launch_template_description            = try(each.value.launch_template_description, var.self_managed_node_group_defaults.launch_template_description, ""Custom launch template for ${try(each.value.name, each.key)} self managed node group"")
  launch_template_tags                   = try(each.value.launch_template_tags, var.self_managed_node_group_defaults.launch_template_tags, {})
  tag_specifications                     = try(each.value.tag_specifications, var.self_managed_node_group_defaults.tag_specifications, [""instance"", ""volume"", ""network-interface""])

  ebs_optimized   = try(each.value.ebs_optimized, var.self_managed_node_group_defaults.ebs_optimized, null)
  ami_id          = try(each.value.ami_id, var.self_managed_node_group_defaults.ami_id, """")
  cluster_version = try(each.value.cluster_version, var.self_managed_node_group_defaults.cluster_version, time_sleep.this[0].triggers[""cluster_version""])
  instance_type   = try(each.value.instance_type, var.self_managed_node_group_defaults.instance_type, ""m6i.large"")
  key_name        = try(each.value.key_name, var.self_managed_node_group_defaults.key_name, null)

  disable_api_termination              = try(each.value.disable_api_termination, var.self_managed_node_group_defaults.disable_api_termination, null)
  instance_initiated_shutdown_behavior = try(each.value.instance_initiated_shutdown_behavior, var.self_managed_node_group_defaults.instance_initiated_shutdown_behavior, null)
  kernel_id                            = try(each.value.kernel_id, var.self_managed_node_group_defaults.kernel_id, null)
  ram_disk_id                          = try(each.value.ram_disk_id, var.self_managed_node_group_defaults.ram_disk_id, null)

  block_device_mappings              = try(each.value.block_device_mappings, var.self_managed_node_group_defaults.block_device_mappings, {})
  capacity_reservation_specification = try(each.value.capacity_reservation_specification, var.self_managed_node_group_defaults.capacity_reservation_specification, {})
  cpu_options                        = try(each.value.cpu_options, var.self_managed_node_group_defaults.cpu_options, {})
  credit_specification               = try(each.value.credit_specification, var.self_managed_node_group_defaults.credit_specification, {})
  elastic_gpu_specifications         = try(each.value.elastic_gpu_specifications, var.self_managed_node_group_defaults.elastic_gpu_specifications, {})
  elastic_inference_accelerator      = try(each.value.elastic_inference_accelerator, var.self_managed_node_group_defaults.elastic_inference_accelerator, {})
  enclave_options                    = try(each.value.enclave_options, var.self_managed_node_group_defaults.enclave_options, {})
  hibernation_options                = try(each.value.hibernation_options, var.self_managed_node_group_defaults.hibernation_options, {})
  instance_requirements              = try(each.value.instance_requirements, var.self_managed_node_group_defaults.instance_requirements, {})
  instance_market_options            = try(each.value.instance_market_options, var.self_managed_node_group_defaults.instance_market_options, {})
  license_specifications             = try(each.value.license_specifications, var.self_managed_node_group_defaults.license_specifications, {})
  metadata_options                   = try(each.value.metadata_options, var.self_managed_node_group_defaults.metadata_options, local.metadata_options)
  enable_monitoring                  = try(each.value.enable_monitoring, var.self_managed_node_group_defaults.enable_monitoring, true)
  enable_efa_support                 = try(each.value.enable_efa_support, var.self_managed_node_group_defaults.enable_efa_support, false)
  network_interfaces                 = try(each.value.network_interfaces, var.self_managed_node_group_defaults.network_interfaces, [])
  placement                          = try(each.value.placement, var.self_managed_node_group_defaults.placement, {})
  maintenance_options                = try(each.value.maintenance_options, var.self_managed_node_group_defaults.maintenance_options, {})
  private_dns_name_options           = try(each.value.private_dns_name_options, var.self_managed_node_group_defaults.private_dns_name_options, {})

  # IAM role
  create_iam_instance_profile   = try(each.value.create_iam_instance_profile, var.self_managed_node_group_defaults.create_iam_instance_profile, true)
  iam_instance_profile_arn      = try(each.value.iam_instance_profile_arn, var.self_managed_node_group_defaults.iam_instance_profile_arn, null)
  iam_role_name                 = try(each.value.iam_role_name, var.self_managed_node_group_defaults.iam_role_name, null)
  iam_role_use_name_prefix      = try(each.value.iam_role_use_name_prefix, var.self_managed_node_group_defaults.iam_role_use_name_prefix, true)
  iam_role_path                 = try(each.value.iam_role_path, var.self_managed_node_group_defaults.iam_role_path, null)
  iam_role_description          = try(each.value.iam_role_description, var.self_managed_node_group_defaults.iam_role_description, ""Self managed node group IAM role"")
  iam_role_permissions_boundary = try(each.value.iam_role_permissions_boundary, var.self_managed_node_group_defaults.iam_role_permissions_boundary, null)
  iam_role_tags                 = try(each.value.iam_role_tags, var.self_managed_node_group_defaults.iam_role_tags, {})
  iam_role_attach_cni_policy    = try(each.value.iam_role_attach_cni_policy, var.self_managed_node_group_defaults.iam_role_attach_cni_policy, true)
  # To better understand why this `lookup()` logic is required, see:
  # https://github.com/hashicorp/terraform/issues/31646#issuecomment-1217279031
  iam_role_additional_policies = lookup(each.value, ""iam_role_additional_policies"", lookup(var.self_managed_node_group_defaults, ""iam_role_additional_policies"", {}))

  # Access entry
  create_access_entry = try(each.value.create_access_entry, var.self_managed_node_group_defaults.create_access_entry, true)
  iam_role_arn        = try(each.value.iam_role_arn, var.self_managed_node_group_defaults.iam_role_arn, null)

  # Autoscaling group schedule
  create_schedule = try(each.value.create_schedule, var.self_managed_node_group_defaults.create_schedule, true)
  schedules       = try(each.value.schedules, var.self_managed_node_group_defaults.schedules, {})

  # Security group
  vpc_security_group_ids            = compact(concat([local.node_security_group_id], try(each.value.vpc_security_group_ids, var.self_managed_node_group_defaults.vpc_security_group_ids, [])))
  cluster_primary_security_group_id = try(each.value.attach_cluster_primary_security_group, var.self_managed_node_group_defaults.attach_cluster_primary_security_group, false) ? aws_eks_cluster.this[0].vpc_config[0].cluster_security_group_id : null

  tags = merge(var.tags, try(each.value.tags, var.self_managed_node_group_defaults.tags, {}))
}
",module,469,469.0,74d39187d855932dd976da6180eda42dcfe09873,74d39187d855932dd976da6180eda42dcfe09873,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/74d39187d855932dd976da6180eda42dcfe09873/node_groups.tf#L469,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/74d39187d855932dd976da6180eda42dcfe09873/node_groups.tf#L469,2024-05-08 08:04:19-04:00,2024-05-08 08:04:19-04:00,1,0,1,1,0,0,0,0,0,0
https://github.com/chanzuckerberg/cztack,102,aws-lambda-function/main.tf,aws-lambda-function/main.tf,0,todo,# TODO scope this policy down,# TODO scope this policy down,"resource aws_iam_policy lambda_logging {
  name_prefix = ""${local.name}-lambda-logging""
  path        = ""/""
  description = ""IAM policy for logging from the ${local.name} lambda.""

  # TODO scope this policy down
  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": [
        ""logs:CreateLogStream"",
        ""logs:PutLogEvents""
      ],
      ""Resource"": ""${aws_cloudwatch_log_group.log.arn}"",
      ""Effect"": ""Allow""
    }
  ]
}
EOF

}
",resource,"data ""aws_iam_policy_document"" ""lambda_logging_policy"" {
  statement {
    effect = ""Allow""
    actions = compact([
      ""logs:CreateLogStream"",
      ""logs:PutLogEvents"",
      var.at_edge ? ""logs:CreateLogGroup"" : """"
    ])

    resources = [
      var.at_edge ?
      ""*"" :
      ""${aws_cloudwatch_log_group.log.arn}:*""
    ]
  }
}
",data,73,84.0,0ce8face5837bbe379a9dbf3ce1b7820e4b7d100,1882eb86cffd9382dc75c0c5d5b89116324fb87f,https://github.com/chanzuckerberg/cztack/blob/0ce8face5837bbe379a9dbf3ce1b7820e4b7d100/aws-lambda-function/main.tf#L73,https://github.com/chanzuckerberg/cztack/blob/1882eb86cffd9382dc75c0c5d5b89116324fb87f/aws-lambda-function/main.tf#L84,2020-07-22 11:53:07-07:00,2023-03-31 14:28:49+00:00,13,0,0,1,0,1,0,0,1,0
https://github.com/ministryofjustice/modernisation-platform,199,terraform/environments/data-platform-apps-and-tools/kubernetes-secrets.tf,terraform/environments/data-platform-apps-and-tools/kubernetes-secrets.tf,0,// todo,// TODO: Rename this?,// TODO: Rename this?,"resource ""kubernetes_secret"" ""openmetadata_airflow"" {
  metadata {
    name      = ""openmetadata-airflow""
    namespace = kubernetes_namespace.openmetadata.metadata[0].name
  }
  data = {
    openmetadata-airflow-password = random_password.openmetadata_airflow.result
  }
  type = ""Opaque""
}
",resource,,,1,0.0,90002b7667baf7c1eb23d0e49dcfa9d9eaee7567,951ffdb805c4255e3bb6d0a5febe5f1bb21407c9,https://github.com/ministryofjustice/modernisation-platform/blob/90002b7667baf7c1eb23d0e49dcfa9d9eaee7567/terraform/environments/data-platform-apps-and-tools/kubernetes-secrets.tf#L1,https://github.com/ministryofjustice/modernisation-platform/blob/951ffdb805c4255e3bb6d0a5febe5f1bb21407c9/terraform/environments/data-platform-apps-and-tools/kubernetes-secrets.tf#L0,2023-10-18 15:43:32+01:00,2023-12-19 15:49:47+00:00,2,2,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1888,modules/dataproc/outputs.tf,modules/dataproc/outputs.tf,0,fix,# FIXME: 2024-03-08: broken in provider,"# FIXME: 2024-03-08: broken in provider 
 #output ""instance_names"" { 
 #  description = ""List of instance names which have been assigned to the cluster."" 
 #  value = { 
 #    master             = google_dataproc_cluster.cluster.cluster_config.0.master_config.0.instance_names 
 #    worker             = google_dataproc_cluster.cluster.cluster_config.0.worker_config.0.instance_names 
 #    preemptible_worker = google_dataproc_cluster.cluster.cluster_config.0.preemptible_worker_config.0.instance_names 
 #  } 
 #} ","output ""name"" {
  description = ""The name of the cluster.""
  value       = google_dataproc_cluster.cluster.name
}
",output,"output ""name"" {
  description = ""The name of the cluster.""
  value       = google_dataproc_cluster.cluster.name
}
",output,35,35.0,1a235cbcecef59886a3e5caa498cf1f7cdada943,3af7e257d21f889ffaf7b32a3bab974fdbfda6e4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/1a235cbcecef59886a3e5caa498cf1f7cdada943/modules/dataproc/outputs.tf#L35,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3af7e257d21f889ffaf7b32a3bab974fdbfda6e4/modules/dataproc/outputs.tf#L35,2024-03-11 11:05:33+01:00,2024-04-17 10:23:48+02:00,2,0,0,0,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1891,modules/project-factory/variables.tf,modules/project-factory/variables.tf,0,# todo,# TODO: allow defining notification channels via YAML files,# TODO: allow defining notification channels via YAML files,"variable ""factories_config"" {
  description = ""Path to folder with YAML resource description data files.""
  type = object({
    hierarchy = optional(object({
      folders_data_path = string
      parent_ids        = optional(map(string), {})
    }))
    projects_data_path = optional(string)
    budgets = optional(object({
      billing_account   = string
      budgets_data_path = string
      # TODO: allow defining notification channels via YAML files
      notification_channels = optional(map(any), {})
    }))
  })
  nullable = false
}
",variable,"variable ""factories_config"" {
  description = ""Path to folder with YAML resource description data files.""
  type = object({
    hierarchy = optional(object({
      folders_data_path = string
      parent_ids        = optional(map(string), {})
    }))
    projects_data_path = optional(string)
    budgets = optional(object({
      billing_account   = string
      budgets_data_path = string
      # TODO: allow defining notification channels via YAML files
      notification_channels = optional(map(any), {})
    }))
  })
  nullable = false
}
",variable,102,107.0,28f02688eeb48f29e5b8640168a5ee569820708d,309792c559bde5c1dbae1c0550f62a28df85cd79,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/28f02688eeb48f29e5b8640168a5ee569820708d/modules/project-factory/variables.tf#L102,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/309792c559bde5c1dbae1c0550f62a28df85cd79/modules/project-factory/variables.tf#L107,2024-03-14 15:03:42+03:00,2024-04-22 09:28:01+02:00,2,0,0,1,0,1,0,0,0,0
https://github.com/rust-lang/simpleinfra,13,terragrunt/modules/docs-rs/cloudfront.tf,terragrunt/modules/docs-rs/cloudfront.tf,0,// todo,// TODO: replace with real origin,// TODO: replace with real origin,"resource ""aws_cloudfront_distribution"" ""webapp"" {
  comment = var.domain

  enabled             = true
  wait_for_deployment = false
  is_ipv6_enabled     = true
  price_class         = ""PriceClass_All""
  http_version        = ""http2and3""

  aliases = [var.domain]
  viewer_certificate {
    acm_certificate_arn      = module.certificate.arn
    ssl_support_method       = ""sni-only""
    minimum_protocol_version = ""TLSv1.1_2016""
  }

  default_cache_behavior {
    target_origin_id       = ""ec2""
    allowed_methods        = [""GET"", ""HEAD"", ""OPTIONS""]
    cached_methods         = [""GET"", ""HEAD"", ""OPTIONS""]
    compress               = true
    viewer_protocol_policy = ""redirect-to-https""

    default_ttl = 900 // 15 minutes
    min_ttl     = 0
    max_ttl     = 31536000 // 1 year

    forwarded_values {
      headers = [
        // Allow detecting HTTPS from the webapp
        ""CloudFront-Forwarded-Proto"",
        // Allow detecting the domain name from the webapp
        ""Host"",
      ]
      query_string = true
      cookies {
        forward = ""none""
      }
    }
  }

  origin {
    origin_id = ""ec2""
    // TODO: replace with real origin
    domain_name = ""http://example.com""

    custom_origin_config {
      http_port              = 80
      https_port             = 443
      origin_protocol_policy = ""http-only""
      origin_ssl_protocols   = [""TLSv1.2""]
    }
  }

  restrictions {
    geo_restriction {
      restriction_type = ""none""
    }
  }

  // Stop CloudFront from caching error responses.
  //
  // Before we did this users were seeing error pages even after we resolved
  // outages, forcing us to invalidate the caches every time. The team agreed
  // the best solution was instead to stop CloudFront from caching error
  // responses altogether.
  dynamic ""custom_error_response"" {
    for_each = toset([400, 403, 404, 405, 414, 500, 501, 502, 503, 504])
    content {
      error_code            = custom_error_response.value
      error_caching_min_ttl = 0
    }
  }
}
",resource,"resource ""aws_cloudfront_distribution"" ""webapp"" {
  comment = var.domain

  enabled             = true
  wait_for_deployment = false
  is_ipv6_enabled     = true
  price_class         = ""PriceClass_All""
  http_version        = ""http2and3""

  aliases = [var.domain]
  viewer_certificate {
    acm_certificate_arn      = module.certificate.arn
    ssl_support_method       = ""sni-only""
    minimum_protocol_version = ""TLSv1.1_2016""
  }

  default_cache_behavior {
    target_origin_id       = ""ec2""
    allowed_methods        = [""GET"", ""HEAD"", ""OPTIONS""]
    cached_methods         = [""GET"", ""HEAD"", ""OPTIONS""]
    compress               = true
    viewer_protocol_policy = ""redirect-to-https""

    default_ttl = 900 // 15 minutes
    min_ttl     = 0
    max_ttl     = 31536000 // 1 year

    forwarded_values {
      headers = [
        // Allow detecting HTTPS from the webapp
        ""CloudFront-Forwarded-Proto"",
        // Allow detecting the domain name from the webapp
        ""Host"",
      ]
      query_string = true
      cookies {
        forward = ""none""
      }
    }
  }

  origin {
    origin_id   = ""ec2""
    domain_name = local.web_domain

    custom_origin_config {
      http_port              = 80
      https_port             = 443
      origin_protocol_policy = ""http-only""
      origin_ssl_protocols   = [""TLSv1.2""]
    }
  }

  restrictions {
    geo_restriction {
      restriction_type = ""none""
    }
  }

  // Stop CloudFront from caching error responses.
  //
  // Before we did this users were seeing error pages even after we resolved
  // outages, forcing us to invalidate the caches every time. The team agreed
  // the best solution was instead to stop CloudFront from caching error
  // responses altogether.
  dynamic ""custom_error_response"" {
    for_each = toset([400, 403, 404, 405, 414, 500, 501, 502, 503, 504])
    content {
      error_code            = custom_error_response.value
      error_caching_min_ttl = 0
    }
  }
}
",resource,54,,0b061f4b31ad08a3761960de431377549e76d0c4,8d588e18da3b236eaa53a9920a5af7219450cf75,https://github.com/rust-lang/simpleinfra/blob/0b061f4b31ad08a3761960de431377549e76d0c4/terragrunt/modules/docs-rs/cloudfront.tf#L54,https://github.com/rust-lang/simpleinfra/blob/8d588e18da3b236eaa53a9920a5af7219450cf75/terragrunt/modules/docs-rs/cloudfront.tf,2023-01-04 17:28:18+01:00,2023-01-10 20:16:01+01:00,3,1,1,1,0,0,1,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,304,libvirt/main.tf,libvirt/main.tf,0,// todo,// TODO: this can moved to a tfvars,// TODO: this can moved to a tfvars,"resource ""libvirt_volume"" ""base_image"" {
  // the base image will be ""cloned"" and used by other domains, 
  // it is the central  image.
  name   = ""${terraform.workspace}-baseimage""
  source = var.base_image
  // TODO: this can moved to a tfvars
  pool   = var.storage_pool
}
",resource,"resource ""libvirt_volume"" ""base_image"" {
  // baseimage is ""cloned"" and used centrally by other domains
  name   = ""${terraform.workspace}-baseimage""
  source = var.base_image
  pool   = var.storage_pool
}
",resource,17,,b11930acacfa1ed7479d9490adc7b066a3973790,03ab687eae834b6cef248743e4d67ea04430bb3c,https://github.com/SUSE/ha-sap-terraform-deployments/blob/b11930acacfa1ed7479d9490adc7b066a3973790/libvirt/main.tf#L17,https://github.com/SUSE/ha-sap-terraform-deployments/blob/03ab687eae834b6cef248743e4d67ea04430bb3c/libvirt/main.tf,2019-08-28 13:42:24+02:00,2019-08-28 13:49:04+02:00,2,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,607,examples/factories/project-factory/main.tf,examples/factories/project-factory/main.tf,0,# todo,# TODO(jccb): we should probably change this to non-authoritative bindings,# TODO(jccb): we should probably change this to non-authoritative bindings,"resource ""google_compute_subnetwork_iam_binding"" ""binding"" {
  for_each   = local.vpc_setup ? coalesce(var.vpc.subnets_iam, {}) : {}
  project    = local.vpc_host_project
  subnetwork = ""projects/${local.vpc_host_project}/regions/${split(""/"", each.key)[0]}/subnetworks/${split(""/"", each.key)[1]}""
  region     = split(""/"", each.key)[0]
  role       = ""roles/compute.networkUser""
  members    = concat(each.value, local.network_user_service_accounts)
}
",resource,the block associated got renamed or deleted,,138,,82b181f34e832a67dd99f8eb5355cca0bf8585e9,40cb46e1cc59c36cac9dd3198c841f32cee11733,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/82b181f34e832a67dd99f8eb5355cca0bf8585e9/examples/factories/project-factory/main.tf#L138,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/40cb46e1cc59c36cac9dd3198c841f32cee11733/examples/factories/project-factory/main.tf,2022-02-07 11:54:19+01:00,2022-02-09 11:06:51+01:00,2,1,0,1,0,1,1,0,0,0
https://github.com/cattle-ops/terraform-aws-gitlab-runner,3,variables.tf,variables.tf,0,todo,# TODO remove as soon as subnet_id_runners and subnet_ids_gitlab_runner are gone. Variable is mandatory now.,"default     = """" # TODO remove as soon as subnet_id_runners and subnet_ids_gitlab_runner are gone. Variable is mandatory now.","variable ""subnet_id"" {
  description = ""Subnet id used for the runner and executors. Must belong to the VPC specified above.""
  type        = string
  default     = """" # TODO remove as soon as subnet_id_runners and subnet_ids_gitlab_runner are gone. Variable is mandatory now.
}
",variable,"variable ""subnet_id"" {
  description = <<-EOT
    Subnet id used for the Runner and Runner Workers. Must belong to the `vpc_id`. In case the fleet mode is used, multiple subnets for
    the Runner Workers can be provided with runner_worker_docker_machine_instance.subnet_ids.
  EOT
  type        = string
}
",variable,25,,36633fa6286343cce72b0d9c0c3451305a36be11,c8a3b89c46f749214461bade8e1e6d161d0ef860,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/36633fa6286343cce72b0d9c0c3451305a36be11/variables.tf#L25,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/c8a3b89c46f749214461bade8e1e6d161d0ef860/variables.tf,2022-01-13 23:50:43+01:00,2023-09-07 17:14:21+02:00,54,1,0,1,0,0,1,0,0,0
https://github.com/chanzuckerberg/cztack,17,aws-ecs-job-fargate/iam.tf,aws-ecs-job-fargate/iam.tf,0,# todo,# TODO(mbarrien): We can probably narrow this down to allowing access to only,"# TODO(mbarrien): We can probably narrow this down to allowing access to only 
 # the specific ECR arn if applicable, and the specific cloudwatch log group. 
 # Either pass both identifiers in, or pass the entire role ARN as an argument","resource ""aws_iam_role_policy_attachment"" ""task_execution_role"" {
  count      = var.registry_secretsmanager_arn != null ? 1 : 0
  role       = aws_iam_role.task_execution_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy""
}
",resource,"resource ""aws_iam_role_policy_attachment"" ""task_execution_role"" {
  role       = aws_iam_role.task_execution_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy""
}
",resource,18,19.0,6918848f1dab99c67e49d21bdc839d907ff8b647,5696d1db18ded14983fcd5c480468753c20eef53,https://github.com/chanzuckerberg/cztack/blob/6918848f1dab99c67e49d21bdc839d907ff8b647/aws-ecs-job-fargate/iam.tf#L18,https://github.com/chanzuckerberg/cztack/blob/5696d1db18ded14983fcd5c480468753c20eef53/aws-ecs-job-fargate/iam.tf#L19,2019-09-25 09:47:44-07:00,2022-07-07 15:58:52-07:00,5,0,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,662,examples/data-solutions/data-platform-foundations/03-composer.tf,examples/data-solutions/data-platform-foundations/03-composer.tf,0,# todo,# TODO: descriptive name,# TODO: descriptive name,"module ""orch-sa-cmp-0"" {
  source     = ""../../../modules/iam-service-account""
  project_id = module.orch-project.project_id
  prefix     = var.prefix
  name       = ""orc-cmp-0""
  # TODO: descriptive name
  display_name = ""TODO""
  iam = {
    ""roles/iam.serviceAccountTokenCreator"" = [local.groups_iam.data-engineers]
    ""roles/iam.serviceAccountUser""         = [module.orch-sa-cmp-0.iam_email]
  }
}
",module,"module ""orch-sa-cmp-0"" {
  source       = ""../../../modules/iam-service-account""
  project_id   = module.orch-project.project_id
  prefix       = var.prefix
  name         = ""orc-cmp-0""
  display_name = ""Data platform Composer service account""
  iam = {
    ""roles/iam.serviceAccountTokenCreator"" = [local.groups_iam.data-engineers]
    ""roles/iam.serviceAccountUser""         = [module.orch-sa-cmp-0.iam_email]
  }
}
",module,22,,db1dc76e74850afc59c67fba0f21c8730687a56a,cdc6c7fc94985fea9e36ee79472275076a4cf95e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/db1dc76e74850afc59c67fba0f21c8730687a56a/examples/data-solutions/data-platform-foundations/03-composer.tf#L22,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/cdc6c7fc94985fea9e36ee79472275076a4cf95e/examples/data-solutions/data-platform-foundations/03-composer.tf,2022-02-10 07:54:52+01:00,2022-02-12 10:20:14+01:00,4,1,0,1,0,0,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,149,modules/node_groups/variables.tf,modules/node_groups/variables.tf,0,hack,# Hack for a homemade `depends_on` https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2,"# Hack for a homemade `depends_on` https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2 
 # Will be removed in Terraform 0.13 with the support of module's `depends_on` https://github.com/hashicorp/terraform/issues/10462","variable ""ng_depends_on"" {
  description = ""List of references to other resources this submodule depends on""
  type        = any
  default     = null
}
",variable,the block associated got renamed or deleted,,38,,616d30ec674ff1d125710755f5073b1665bbd1af,56e93d77de58f311f1d1d7051f40bf77e7b03524,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/616d30ec674ff1d125710755f5073b1665bbd1af/modules/node_groups/variables.tf#L38,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/56e93d77de58f311f1d1d7051f40bf77e7b03524/modules/node_groups/variables.tf,2020-06-28 02:31:23+02:00,2021-11-06 20:19:03+01:00,7,1,0,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,617,examples/data-solutions/data-platform-foundations/variables.tf,examples/data-solutions/data-platform-foundations/variables.tf,0,#todo,#TODO hardcoded VPC ranges,#TODO hardcoded VPC ranges,"variable ""network_config"" {
  description = ""Network configurations to use. Specify a shared VPC to use, if null networks will be created in projects.""
  type = object({
    #TODO hardcoded Cloud NAT
    network_self_link = string
    #TODO hardcoded VPC ranges
    subnet_self_links = object({
      load           = string
      transformation = string
      orchestration  = string
    })
    composer_ip_ranges = object({
      cloudsql   = string
      gke_master = string
      web_server = string
    })
    composer_secondary_ranges = object({
      pods     = string
      services = string
    })
  })
  default = {
    enable_cloud_nat = false
    host_project     = null
    network          = null

    vpc_subnet = {
      load = {
        range           = ""10.10.0.0/24""
        secondary_range = null
      }
      transformation = {
        range           = ""10.10.0.0/24""
        secondary_range = null
      }
      orchestration = {
        range = ""10.10.0.0/24""
        secondary_range = {
          pods     = ""10.10.8.0/22""
          services = ""10.10.12.0/24""
        }
      }
    }
    vpc_subnet_self_link = null
  }
}
",variable,"variable ""network_config"" {
  description = ""Shared VPC network configurations to use. If null networks will be created in projects with preconfigured values.""
  type = object({
    network_self_link = string
    subnet_self_links = object({
      load           = string
      transformation = string
      orchestration  = string
    })
    composer_ip_ranges = object({
      cloudsql   = string
      gke_master = string
      web_server = string
    })
    composer_secondary_ranges = object({
      pods     = string
      services = string
    })
  })

  default = {
    network_self_link         = null
    subnet_self_links         = null
    composer_ip_ranges        = null
    composer_secondary_ranges = null
  }
}
",variable,83,,2e560407c118e7b7abc32f8ac1788a3f48563f21,d8bad5779036aa31639e4611e4935287fc79a4bc,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2e560407c118e7b7abc32f8ac1788a3f48563f21/examples/data-solutions/data-platform-foundations/variables.tf#L83,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d8bad5779036aa31639e4611e4935287fc79a4bc/examples/data-solutions/data-platform-foundations/variables.tf,2022-02-07 17:51:06+01:00,2022-02-07 21:28:54+01:00,2,1,0,1,0,0,1,0,0,0
https://github.com/apache/beam,7,playground/terraform/provider.tf,playground/terraform/provider.tf,0,todo,# TODO Please remove it when all resources are available in the stable version,# TODO Please remove it when all resources are available in the stable version,"provider ""google-beta"" {
  region = ""us-central""
}
",provider,"provider ""google-beta"" {
  region = var.region
  project = var.project_id
  // TODO may need to run module.setup first independent of this solution and add the terraform service account as a variable
  // This allows us to use a service account to provision resources without downloading or storing service account keys
  #  impersonate_service_account = module.setup.terraform_service_account_email
}",provider,23,,675c0bc10f813ea593702f5e6a0fd2ce38caf720,ad21d8353c856152346408f1d5029c9af05957c8,https://github.com/apache/beam/blob/675c0bc10f813ea593702f5e6a0fd2ce38caf720/playground/terraform/provider.tf#L23,https://github.com/apache/beam/blob/ad21d8353c856152346408f1d5029c9af05957c8/playground/terraform/provider.tf,2022-02-22 10:04:20-08:00,2022-03-16 14:13:22-07:00,2,1,0,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-lb-http,1,main.tf,main.tf,0,broken,# as the cross-project reference broken in https://github.com/terraform-google-modules/terraform-google-vm/issues/29,"# as the cross-project reference broken in https://github.com/terraform-google-modules/terraform-google-vm/issues/29 
 # and as there are some level of uncertenity that support for more than one is cross-referenced project ever worked 
 # we're limiting cross-referenced project to only one replacing list with string","resource ""google_compute_firewall"" ""default"" {
  for_each = toset(var.firewall_networks)

  name          = ""${var.name}-${each.value}-hc""
  # as the cross-project reference broken in https://github.com/terraform-google-modules/terraform-google-vm/issues/29
  # and as there are some level of uncertenity that support for more than one is cross-referenced project ever worked
  # we're limiting cross-referenced project to only one replacing list with string
  project       = var.firewall_projects != null ? var.firewall_projects : var.project
  network       = each.value
  source_ranges = [
    ""130.211.0.0/22"",
    ""35.191.0.0/16"",
    ""209.85.152.0/22"",
    ""209.85.204.0/22""]
  target_tags   = var.target_tags

  dynamic ""allow"" {
    for_each = distinct(var.backend_params)
    content {
      protocol = ""tcp""
      ports    = [
        split("","", allow.value)[2]]
    }
  }
}
",resource,the block associated got renamed or deleted,,155,,26efad92aea00aad9f7899670d843500b87dd988,fa368c1ab56dd6737a2771f89f33cb97f6c8c024,https://github.com/terraform-google-modules/terraform-google-lb-http/blob/26efad92aea00aad9f7899670d843500b87dd988/main.tf#L155,https://github.com/terraform-google-modules/terraform-google-lb-http/blob/fa368c1ab56dd6737a2771f89f33cb97f6c8c024/main.tf,2019-09-06 19:30:08+03:00,2019-09-10 13:06:05+03:00,2,1,0,1,1,1,0,0,0,0
https://github.com/ministryofjustice/modernisation-platform,173,terraform/environments/bootstrap/member-bootstrap/iam.tf,terraform/environments/bootstrap/member-bootstrap/iam.tf,0,//todo,"#route53:* and ec2:Describe* are already allowed, but keeping the below for now, in case this is going to be moved elsewhere //TODO cleanup the permissions here","#route53:* and ec2:Describe* are already allowed, but keeping the below for now, in case this is going to be moved elsewhere //TODO cleanup the permissions here 
 //      ""route53:CreateHostedZone"", 
 //      ""route53:GetHostedZone"", 
 //      ""route53:DeleteHostedZone"", 
 //      ""route53:ListHostedZonesByName"", 
 //      ""route53:CreateHealthCheck"", 
 //      ""route53:GetHealthCheck"", 
 //      ""route53:DeleteHealthCheck"", 
 //      ""route53:UpdateHealthCheck"", 
 //      ""route53:ChangeResourceRecordSets"", 
 //      ""ec2:DescribeInstances"", 
 //      ""ec2:DescribeVpcs"", 
 //      ""ec2:DescribeRegions"" 
 # This is the end of permissions needed for the ServiceDiscovery and the AWS Cloud Map, see the doc: https://docs.aws.amazon.com/cloud-map/latest/dg/cloud-map-api-permissions-ref.html","data ""aws_iam_policy_document"" ""member-access"" {
  statement {
    #checkov:skip=CKV_AWS_108
    #checkov:skip=CKV_AWS_111
    #checkov:skip=CKV_AWS_107
    #checkov:skip=CKV_AWS_109
    #checkov:skip=CKV_AWS_110
    #checkov:skip=CKV2_AWS_40
    effect = ""Allow""
    actions = [
      ""acm-pca:*"",
      ""acm:*"",
      ""application-autoscaling:*"",
      ""applicationinsights:*"",
      ""athena:*"",
      ""autoscaling:*"",
      ""backup:*"",
      ""cloudfront:*"",
      ""cloudwatch:*"",
      ""codebuild:*"",
      ""codedeploy:*"",
      ""codepipeline:*"",
      ""dbqms:*"",
      ""dlm:*"",
      ""dms:*"",
      ""ds:CheckAlias"",
      ""ds:Describe*"",
      ""ds:List*"",
      ""ds:*Tags*"",
      ""ds:CancelSchemaExtension"",
      ""ds:CreateComputer"",
      ""ds:CreateAlias"",
      ""ds:CreateDirectory"",
      ""ds:CreateLogSubscription"",
      ""ds:CreateMicrosoftAD"",
      ""ds:CreateSnapshot"",
      ""ds:DeleteDirectory"",
      ""ds:DeleteLogSubscription"",
      ""ds:DeleteSnapshot"",
      ""ds:DeregisterCertificate"",
      ""ds:DeregisterEventTopic"",
      ""ds:DisableClientAuthentication"",
      ""ds:DisableLDAPS"",
      ""ds:DisableRadius"",
      ""ds:EnableClientAuthentication"",
      ""ds:EnableLDAPS"",
      ""ds:EnableRadius"",
      ""ds:RegisterCertificate"",
      ""ds:RegisterEventTopic"",
      ""ds:ResetUserPassword"",
      ""ds:RestoreFromSnapshot"",
      ""ds:StartSchemaExtension"",
      ""ds:UpdateDirectorySetup"",
      ""ds:UpdateNumberOfDomainControllers"",
      ""ds:UpdateRadius"",
      ""ds:UpdateSettings"",
      ""dynamodb:*"",
      ""ebs:*"",
      ""ec2:Describe*"",
      ""ec2:*SecurityGroup*"",
      ""ec2:*KeyPair*"",
      ""ec2:*Tags*"",
      ""ec2:*Volume*"",
      ""ec2:*Snapshot*"",
      ""ec2:*Ebs*"",
      ""ec2:*NetworkInterface*"",
      ""ec2:*Address*"",
      ""ec2:*Image*"",
      ""ec2:*Event*"",
      ""ec2:*Instance*"",
      ""ec2:*CapacityReservation*"",
      ""ec2:*Fleet*"",
      ""ec2:Get*"",
      ""ec2:SendDiagnosticInterrupt"",
      ""ec2:*LaunchTemplate*"",
      ""ec2:*PlacementGroup*"",
      ""ec2:*IdFormat*"",
      ""ec2:*Spot*"",
      ""ecr-public:*"",
      ""ecr:*"",
      ""ecs:*"",
      ""elasticfilesystem:*"",
      ""elasticloadbalancing:*"",
      ""events:*"",
      ""firehose:*"",
      ""glacier:*"",
      ""glue:*"",
      ""guardduty:get*"",
      ""iam:*"",
      ""kinesis:*"",
      ""kms:*"",
      ""lambda:*"",
      ""logs:*"",
      ""organizations:Describe*"",
      ""organizations:List*"",
      ""quicksight:*"",
      ""rds-db:*"",
      ""rds:*"",
      ""rds-data:*"",
      ""route53:*"",
      ""s3:*"",
      ""secretsmanager:*"",
      ""ses:*"",
      ""sns:*"",
      ""sqs:*"",
      ""ssm:*"",
      ""waf:*"",
      ""wafv2:*"",
      ""resource-groups:*"",
      ""redshift:*"",
      ""redshift-data:*"",
      ""redshift-serverless:*"",
      # The following permissions are needed for the ServiceDiscovery and the AWS Cloud Map, see the doc: https://docs.aws.amazon.com/cloud-map/latest/dg/cloud-map-api-permissions-ref.html
      ""servicediscovery:CreateHttpNamespace"",
      ""servicediscovery:UpdateHttpNamespace"",
      ""servicediscovery:DeleteNamespace"",
      ""servicediscovery:GetNamespace"",
      ""servicediscovery:ListNamespaces"",
      ""servicediscovery:CreatePrivateDnsNamespace"",
      ""servicediscovery:CreatePublicDnsNamespace"",
      ""servicediscovery:UpdatePrivateDnsNamespace"",
      ""servicediscovery:UpdatePublicDnsNamespace"",
      ""servicediscovery:CreateService"",
      ""servicediscovery:DeleteService"",
      ""servicediscovery:UpdateService"",
      ""servicediscovery:GetService"",
      ""servicediscovery:ListServices"",
      ""servicediscovery:DeregisterInstance"",
      ""servicediscovery:DiscoverInstances"",
      ""servicediscovery:RegisterInstance"",
      ""servicediscovery:GetInstance"",
      ""servicediscovery:ListInstances"",
      ""servicediscovery:GetInstancesHealthStatus"",
      ""servicediscovery:UpdateInstanceCustomHealthStatus"",
      ""servicediscovery:GetOperation"",
      ""servicediscovery:ListOperations""
      #route53:* and ec2:Describe* are already allowed, but keeping the below for now, in case this is going to be moved elsewhere //TODO cleanup the permissions here
//      ""route53:CreateHostedZone"",
//      ""route53:GetHostedZone"",
//      ""route53:DeleteHostedZone"",
//      ""route53:ListHostedZonesByName"",
//      ""route53:CreateHealthCheck"",
//      ""route53:GetHealthCheck"",
//      ""route53:DeleteHealthCheck"",
//      ""route53:UpdateHealthCheck"",
//      ""route53:ChangeResourceRecordSets"",
//      ""ec2:DescribeInstances"",
//      ""ec2:DescribeVpcs"",
//      ""ec2:DescribeRegions""
      # This is the end of permissions needed for the ServiceDiscovery and the AWS Cloud Map, see the doc: https://docs.aws.amazon.com/cloud-map/latest/dg/cloud-map-api-permissions-ref.html
    ]
    resources = [""*""] #tfsec:ignore:AWS099 tfsec:ignore:AWS097
  }

  statement {
    effect = ""Deny""
    actions = [
      ""ec2:CreateVpc"",
      ""ec2:CreateSubnet"",
      ""ec2:CreateVpcPeeringConnection"",
      ""iam:AddClientIDToOpenIDConnectProvider"",
      ""iam:AddUserToGroup"",
      ""iam:AttachGroupPolicy"",
      ""iam:AttachUserPolicy"",
      ""iam:CreateAccountAlias"",
      ""iam:CreateGroup"",
      ""iam:CreateLoginProfile"",
      ""iam:CreateOpenIDConnectProvider"",
      ""iam:CreateSAMLProvider"",
      ""iam:CreateUser"",
      ""iam:CreateVirtualMFADevice"",
      ""iam:DeactivateMFADevice"",
      ""iam:DeleteAccountAlias"",
      ""iam:DeleteAccountPasswordPolicy"",
      ""iam:DeleteGroup"",
      ""iam:DeleteGroupPolicy"",
      ""iam:DeleteLoginProfile"",
      ""iam:DeleteOpenIDConnectProvider"",
      ""iam:DeleteSAMLProvider"",
      ""iam:DeleteUser"",
      ""iam:DeleteUserPermissionsBoundary"",
      ""iam:DeleteUserPolicy"",
      ""iam:DeleteVirtualMFADevice"",
      ""iam:DetachGroupPolicy"",
      ""iam:DetachUserPolicy"",
      ""iam:EnableMFADevice"",
      ""iam:RemoveClientIDFromOpenIDConnectProvider"",
      ""iam:RemoveUserFromGroup"",
      ""iam:ResyncMFADevice"",
      ""iam:UpdateAccountPasswordPolicy"",
      ""iam:UpdateGroup"",
      ""iam:UpdateLoginProfile"",
      ""iam:UpdateOpenIDConnectProviderThumbprint"",
      ""iam:UpdateSAMLProvider"",
      ""iam:UpdateUser""
    ]
    resources = [""*""]
  }

  statement {
    effect = ""Deny""
    actions = [
      ""iam:AttachRolePolicy"",
      ""iam:DeleteRole"",
      ""iam:DeleteRolePermissionsBoundary"",
      ""iam:DeleteRolePolicy"",
      ""iam:DetachRolePolicy"",
      ""iam:PutRolePermissionsBoundary"",
      ""iam:PutRolePolicy"",
      ""iam:UpdateAssumeRolePolicy"",
      ""iam:UpdateRole"",
      ""iam:UpdateRoleDescription""
    ]
    resources = [""arn:aws:iam::*:user/cicd-member-user""]
  }
}
",data,"data ""aws_iam_policy_document"" ""member-access"" {
  statement {
    #checkov:skip=CKV_AWS_108
    #checkov:skip=CKV_AWS_111
    #checkov:skip=CKV_AWS_107
    #checkov:skip=CKV_AWS_109
    #checkov:skip=CKV_AWS_110
    #checkov:skip=CKV2_AWS_40
    effect = ""Allow""
    actions = [
      ""acm-pca:*"",
      ""acm:*"",
      ""application-autoscaling:*"",
      ""applicationinsights:*"",
      ""athena:*"",
      ""autoscaling:*"",
      ""backup:*"",
      ""cloudfront:*"",
      ""cloudwatch:*"",
      ""codebuild:*"",
      ""codedeploy:*"",
      ""codepipeline:*"",
      ""dbqms:*"",
      ""dlm:*"",
      ""dms:*"",
      ""ds:CheckAlias"",
      ""ds:Describe*"",
      ""ds:List*"",
      ""ds:*Tags*"",
      ""ds:CancelSchemaExtension"",
      ""ds:CreateComputer"",
      ""ds:CreateAlias"",
      ""ds:CreateDirectory"",
      ""ds:CreateLogSubscription"",
      ""ds:CreateMicrosoftAD"",
      ""ds:CreateSnapshot"",
      ""ds:DeleteDirectory"",
      ""ds:DeleteLogSubscription"",
      ""ds:DeleteSnapshot"",
      ""ds:DeregisterCertificate"",
      ""ds:DeregisterEventTopic"",
      ""ds:DisableClientAuthentication"",
      ""ds:DisableLDAPS"",
      ""ds:DisableRadius"",
      ""ds:EnableClientAuthentication"",
      ""ds:EnableLDAPS"",
      ""ds:EnableRadius"",
      ""ds:RegisterCertificate"",
      ""ds:RegisterEventTopic"",
      ""ds:ResetUserPassword"",
      ""ds:RestoreFromSnapshot"",
      ""ds:StartSchemaExtension"",
      ""ds:UpdateDirectorySetup"",
      ""ds:UpdateNumberOfDomainControllers"",
      ""ds:UpdateRadius"",
      ""ds:UpdateSettings"",
      ""dynamodb:*"",
      ""ebs:*"",
      ""ec2:Describe*"",
      ""ec2:*SecurityGroup*"",
      ""ec2:*KeyPair*"",
      ""ec2:*Tags*"",
      ""ec2:*Volume*"",
      ""ec2:*Snapshot*"",
      ""ec2:*Ebs*"",
      ""ec2:*NetworkInterface*"",
      ""ec2:*Address*"",
      ""ec2:*Image*"",
      ""ec2:*Event*"",
      ""ec2:*Instance*"",
      ""ec2:*CapacityReservation*"",
      ""ec2:*Fleet*"",
      ""ec2:Get*"",
      ""ec2:SendDiagnosticInterrupt"",
      ""ec2:*LaunchTemplate*"",
      ""ec2:*PlacementGroup*"",
      ""ec2:*IdFormat*"",
      ""ec2:*Spot*"",
      ""ecr-public:*"",
      ""ecr:*"",
      ""ecs:*"",
      ""elasticfilesystem:*"",
      ""elasticloadbalancing:*"",
      ""events:*"",
      ""firehose:*"",
      ""glacier:*"",
      ""glue:*"",
      ""guardduty:get*"",
      ""iam:*"",
      ""kinesis:*"",
      ""kms:*"",
      ""lambda:*"",
      ""logs:*"",
      ""organizations:Describe*"",
      ""organizations:List*"",
      ""quicksight:*"",
      ""rds-db:*"",
      ""rds:*"",
      ""rds-data:*"",
      ""route53:*"",
      ""s3:*"",
      ""secretsmanager:*"",
      ""ses:*"",
      ""sns:*"",
      ""sqs:*"",
      ""ssm:*"",
      ""waf:*"",
      ""wafv2:*"",
      ""resource-groups:*"",
      ""redshift:*"",
      ""redshift-data:*"",
      ""redshift-serverless:*""
    ]
    resources = [""*""] #tfsec:ignore:AWS099 tfsec:ignore:AWS097
  }

  statement {
    effect = ""Deny""
    actions = [
      ""ec2:CreateVpc"",
      ""ec2:CreateSubnet"",
      ""ec2:CreateVpcPeeringConnection"",
      ""iam:AddClientIDToOpenIDConnectProvider"",
      ""iam:AddUserToGroup"",
      ""iam:AttachGroupPolicy"",
      ""iam:AttachUserPolicy"",
      ""iam:CreateAccountAlias"",
      ""iam:CreateGroup"",
      ""iam:CreateLoginProfile"",
      ""iam:CreateOpenIDConnectProvider"",
      ""iam:CreateSAMLProvider"",
      ""iam:CreateUser"",
      ""iam:CreateVirtualMFADevice"",
      ""iam:DeactivateMFADevice"",
      ""iam:DeleteAccountAlias"",
      ""iam:DeleteAccountPasswordPolicy"",
      ""iam:DeleteGroup"",
      ""iam:DeleteGroupPolicy"",
      ""iam:DeleteLoginProfile"",
      ""iam:DeleteOpenIDConnectProvider"",
      ""iam:DeleteSAMLProvider"",
      ""iam:DeleteUser"",
      ""iam:DeleteUserPermissionsBoundary"",
      ""iam:DeleteUserPolicy"",
      ""iam:DeleteVirtualMFADevice"",
      ""iam:DetachGroupPolicy"",
      ""iam:DetachUserPolicy"",
      ""iam:EnableMFADevice"",
      ""iam:RemoveClientIDFromOpenIDConnectProvider"",
      ""iam:RemoveUserFromGroup"",
      ""iam:ResyncMFADevice"",
      ""iam:UpdateAccountPasswordPolicy"",
      ""iam:UpdateGroup"",
      ""iam:UpdateLoginProfile"",
      ""iam:UpdateOpenIDConnectProviderThumbprint"",
      ""iam:UpdateSAMLProvider"",
      ""iam:UpdateUser""
    ]
    resources = [""*""]
  }

  statement {
    effect = ""Deny""
    actions = [
      ""iam:AttachRolePolicy"",
      ""iam:DeleteRole"",
      ""iam:DeleteRolePermissionsBoundary"",
      ""iam:DeleteRolePolicy"",
      ""iam:DetachRolePolicy"",
      ""iam:PutRolePermissionsBoundary"",
      ""iam:PutRolePolicy"",
      ""iam:UpdateAssumeRolePolicy"",
      ""iam:UpdateRole"",
      ""iam:UpdateRoleDescription""
    ]
    resources = [""arn:aws:iam::*:user/cicd-member-user""]
  }
}
",data,153,,1a3f6074f866a5302b6a4f917ccfb5dc6b66e751,82b96b6253a012664e19390dbc00cec75b278963,https://github.com/ministryofjustice/modernisation-platform/blob/1a3f6074f866a5302b6a4f917ccfb5dc6b66e751/terraform/environments/bootstrap/member-bootstrap/iam.tf#L153,https://github.com/ministryofjustice/modernisation-platform/blob/82b96b6253a012664e19390dbc00cec75b278963/terraform/environments/bootstrap/member-bootstrap/iam.tf,2023-03-21 15:25:00+00:00,2023-03-30 11:27:01+01:00,4,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,147,modules/vpc-sc/outputs.tf,modules/vpc-sc/outputs.tf,0,fix,# FIXME(jccb): these deps don't exist (??),"# FIXME(jccb): these deps don't exist (??) 
 # depends_on = [ 
 #   google_organization_iam_audit_config, 
 #   google_organization_iam_binding.authoritative, 
 #   google_organization_iam_custom_role.roles, 
 #   google_organization_iam_member.additive, 
 #   google_organization_policy.boolean, 
 #   google_organization_policy.list 
 # ]","output ""org_id"" {
  description = ""Organization id dependent on module resources.""
  value       = var.org_id
  # FIXME(jccb): these deps don't exist (??)
  # depends_on = [
  #   google_organization_iam_audit_config,
  #   google_organization_iam_binding.authoritative,
  #   google_organization_iam_custom_role.roles,
  #   google_organization_iam_member.additive,
  #   google_organization_policy.boolean,
  #   google_organization_policy.list
  # ]
}
",output,the block associated got renamed or deleted,,20,,eecdee63e60d8d2d2768a0696caa3a2ae124a290,dda715670cfa1a40aa94929b8f8383ea03008269,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/eecdee63e60d8d2d2768a0696caa3a2ae124a290/modules/vpc-sc/outputs.tf#L20,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/dda715670cfa1a40aa94929b8f8383ea03008269/modules/vpc-sc/outputs.tf,2020-11-07 10:28:33+01:00,2020-11-16 18:04:12+01:00,2,1,0,1,0,0,0,0,0,1
https://github.com/ministryofjustice/modernisation-platform,196,terraform/environments/data-platform-apps-and-tools/eks-cluster.tf,terraform/environments/data-platform-apps-and-tools/eks-cluster.tf,0,// todo,// TODO: Review these settings,// TODO: Review these settings,"module ""eks"" {
  #checkov:skip=CKV_TF_1:Module is from Terraform registry

  source  = ""terraform-aws-modules/eks/aws""
  version = ""19.16.0""

  cluster_name    = local.environment_configuration.eks_cluster_name
  cluster_version = local.environment_configuration.eks_versions.cluster

  cluster_endpoint_private_access = true
  cluster_endpoint_public_access  = true

  vpc_id                   = module.vpc.vpc_id
  control_plane_subnet_ids = module.vpc.private_subnets
  subnet_ids               = module.vpc.private_subnets

  cluster_enabled_log_types = [""api"", ""audit"", ""authenticator"", ""controllerManager"", ""scheduler""]

  cluster_addons = {
    coredns = {
      addon_version = local.environment_configuration.eks_versions.addon_coredns
    }
    kube-proxy = {
      addon_version = local.environment_configuration.eks_versions.addon_kube_proxy
    }
    vpc-cni = {
      addon_version = local.environment_configuration.eks_versions.addon_vpc_cni
    }
    aws-guardduty-agent = {
      addon_version = local.environment_configuration.eks_versions.addon_aws_guardduty_agent
    }
  }

  eks_managed_node_group_defaults = {
    ami_release_version = local.environment_configuration.eks_versions.ami_release
    ami_type            = ""BOTTLEROCKET_x86_64""
    platform            = ""bottlerocket""
    metadata_options = {
      http_endpoint               = ""enabled""
      http_put_response_hop_limit = 2
      http_tokens                 = ""required""
      instance_metadata_tags      = ""enabled""
    }

    block_device_mappings = {
      xvda = {
        device_name = ""/dev/xvda""
        ebs = {
          volume_size = 100
        }
      }
    }

    iam_role_additional_policies = {
      AmazonSSMManagedInstanceCore = ""arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore""
      CloudWatchAgentServerPolicy  = ""arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy""
    }
  }

  // TODO: Review these settings
  eks_managed_node_groups = {
    general = {
      min_size       = 3
      max_size       = 10
      desired_size   = 5
      instance_types = [""t3.xlarge""]
    }
  }

  manage_aws_auth_configmap = true

  aws_auth_roles = [
    {
      rolearn  = ""arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${one(data.aws_iam_roles.eks_sso_access_role.names)}""
      groups   = [""system:masters""]
      username = ""administrator""
    },
    {
      // rolearn cannot consume module.airflow_execution_role.arn because that role consumes module.eks.cluster_arn, but we can construct the ARN manually
      rolearn  = ""arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.environment_configuration.airflow_execution_role_name}""
      groups   = [""airflow""]
      username = ""airflow""
    }
  ]

  tags = local.tags
}
",module,,,63,0.0,90002b7667baf7c1eb23d0e49dcfa9d9eaee7567,951ffdb805c4255e3bb6d0a5febe5f1bb21407c9,https://github.com/ministryofjustice/modernisation-platform/blob/90002b7667baf7c1eb23d0e49dcfa9d9eaee7567/terraform/environments/data-platform-apps-and-tools/eks-cluster.tf#L63,https://github.com/ministryofjustice/modernisation-platform/blob/951ffdb805c4255e3bb6d0a5febe5f1bb21407c9/terraform/environments/data-platform-apps-and-tools/eks-cluster.tf#L0,2023-10-18 15:43:32+01:00,2023-12-19 15:49:47+00:00,9,2,1,1,0,0,0,0,0,0
https://github.com/chanzuckerberg/cztack,101,aws-iam-secrets-reader-policy/main.tf,aws-iam-secrets-reader-policy/main.tf,0,todo,# TODO KMS permissions,# TODO KMS permissions,"data aws_iam_policy_document policy {
  statement {
    actions = [
      ""secretsmanager:GetSecretValue"",
    ]

    resources = var.secrets_arns
  }
}
",data,"data ""aws_iam_policy_document"" ""policy"" {
  statement {
    actions = [
      ""secretsmanager:GetSecretValue"",
    ]

    resources = var.secrets_arns
  }
}
",data,1,1.0,817778260c469703be66e3fb07ed6228f33c1960,9df439500dee7468643ca03a844cf7a5b1e1b313,https://github.com/chanzuckerberg/cztack/blob/817778260c469703be66e3fb07ed6228f33c1960/aws-iam-secrets-reader-policy/main.tf#L1,https://github.com/chanzuckerberg/cztack/blob/9df439500dee7468643ca03a844cf7a5b1e1b313/aws-iam-secrets-reader-policy/main.tf#L1,2020-07-22 10:22:01-07:00,2021-04-13 14:52:04-04:00,2,0,0,1,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,339,infra/gcp/terraform/modules/oci-proxy/oci-proxy.tf,infra/gcp/terraform/modules/oci-proxy/main.tf,1,fix,// - We need to be able to deploy registry fixes ASAP,"// NOTE: We deploy from staging because: 
 // - We pin by digest anyhow (so it's comparably secure) 
 // - We need to be able to deploy registry fixes ASAP 
 // - We will eventually auto-deploy staging by overriding the project and digest on the production config to avoid skew 
 // If you're interested in running this image yourself releases are available at registry.k8s.io/infra-tools/archeio","resource ""google_cloud_run_service"" ""oci-proxy"" {
  project  = google_project.project.project_id
  for_each = var.cloud_run_config
  name     = ""${var.project_id}-${each.key}""
  location = each.key

  template {
    metadata {
      annotations = {
        ""autoscaling.knative.dev/maxScale"" = ""10"" // TODO: adjust to control costs
        ""run.googleapis.com/launch-stage""  = ""BETA""
      }
    }
    spec {
      service_account_name = google_service_account.oci-proxy.email
      containers {
        // NOTE: We deploy from staging because:
        // - We pin by digest anyhow (so it's comparably secure)
        // - We need to be able to deploy registry fixes ASAP
        // - We will eventually auto-deploy staging by overriding the project and digest on the production config to avoid skew
        // If you're interested in running this image yourself releases are available at registry.k8s.io/infra-tools/archeio
        image = ""gcr.io/k8s-staging-infra-tools/archeio@${var.digest}""
        args  = [""-v=${var.verbosity}""]

        dynamic ""env"" {
          for_each = each.value.environment_variables
          content {
            name  = env.value[""name""]
            value = env.value[""value""]
          }
        }

        // ensure this match the value for template.spec.containers.resources.limits
        env {
          name  = ""GOMAXPROCS""
          value = ""1""
        }

        resources {
          limits = {
            ""cpu"" = ""1000m""
          }
        }
      }

      # we can probably hit 1k QPS/core (cloud run's maximum configurable)
      # but we are leaving in a little overhead, if we actually hit 1k qps in
      # a region we can scale to another 1 core instance
      container_concurrency = 800

      // we only serve cheap redirects, 60s is a rather long request
      timeout_seconds = 60
    }
  }

  traffic {
    percent         = 100
    latest_revision = true
  }

  depends_on = [
    google_project_service.project[""run.googleapis.com""]
  ]

  lifecycle {
    ignore_changes = [
      // This gets added by the Cloud Run API post deploy and causes diffs, can be ignored...
      template[0].metadata[0].annotations[""client.knative.dev/sandbox""],
      template[0].metadata[0].annotations[""run.googleapis.com/user-image""],
      template[0].metadata[0].annotations[""run.googleapis.com/client-name""],
      template[0].metadata[0].annotations[""run.googleapis.com/client-version""],
    ]
  }
}
",resource,"resource ""google_cloud_run_service"" ""oci-proxy"" {
  project  = var.project_id
  for_each = local.cloud_run_config
  name     = ""${var.project_id}-${each.key}""
  location = each.key

  template {
    metadata {
      annotations = {
        ""autoscaling.knative.dev/maxScale"" = ""10"" // TODO: adjust to control costs
        ""run.googleapis.com/launch-stage""  = ""BETA""
      }
      labels = {
        ""run.googleapis.com/startupProbeType"" = ""Default""
      }
    }
    spec {
      service_account_name = google_service_account.oci-proxy.email
      containers {
        // NOTE: We deploy from staging because:
        // - We pin by digest anyhow (so it's comparably secure)
        // - We need to be able to deploy registry fixes ASAP
        // - We will eventually auto-deploy staging by overriding the project and digest on the production config to avoid skew
        // If you're interested in running this image yourself releases are available at registry.k8s.io/infra-tools/archeio
        image = ""gcr.io/k8s-staging-infra-tools/archeio@${var.digest}""
        args  = [""-v=${var.verbosity}""]

        dynamic ""env"" {
          for_each = each.value.environment_variables
          content {
            name  = env.value[""name""]
            value = env.value[""value""]
          }
        }

        // ensure this match the value for template.spec.containers.resources.limits
        env {
          name  = ""GOMAXPROCS""
          value = ""1""
        }

        resources {
          limits = {
            ""cpu"" = ""1000m""
            // default, also the minimum permitted for second generation
            // https://cloud.google.com/run/docs/about-execution-environments
            ""memory"" = ""512Mi""
          }
        }

        startup_probe {
          failure_threshold     = 1
          initial_delay_seconds = 0
          period_seconds        = 240
          timeout_seconds       = 240
          tcp_socket {
            port = 8080
          }
        }
      }

      # we can probably hit 1k QPS/core (cloud run's maximum configurable)
      # but we are leaving in a little overhead, if we actually hit 1k qps in
      # a region we can scale to another 1 core instance
      container_concurrency = 800

      // we only serve cheap redirects, 60s is a rather long request
      timeout_seconds = 60
    }
  }

  traffic {
    percent         = 100
    latest_revision = true
  }

  depends_on = [
    google_project_service.project[""run.googleapis.com""]
  ]

  lifecycle {
    ignore_changes = [
      // This gets added by the Cloud Run API post deploy and causes diffs, can be ignored...
      template[0].metadata[0].annotations[""client.knative.dev/sandbox""],
      template[0].metadata[0].annotations[""run.googleapis.com/user-image""],
      template[0].metadata[0].annotations[""run.googleapis.com/client-name""],
      template[0].metadata[0].annotations[""run.googleapis.com/client-version""],
    ]
  }
}
",resource,91,483.0,8d0eae1fb71ae107c9add98ecde6740eecaccdab,b41eb06d1e7666284e2221d8105ac1f37d40f16f,https://github.com/kubernetes/k8s.io/blob/8d0eae1fb71ae107c9add98ecde6740eecaccdab/infra/gcp/terraform/modules/oci-proxy/oci-proxy.tf#L91,https://github.com/kubernetes/k8s.io/blob/b41eb06d1e7666284e2221d8105ac1f37d40f16f/infra/gcp/terraform/modules/oci-proxy/main.tf#L483,2023-04-02 17:09:27-07:00,2024-03-12 19:30:59+01:00,9,0,1,1,0,0,0,1,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,395,azure/network.tf,azure/network.tf,0,todo,// TODO ALL THIS RULES NEED A PROPER NAME. Port number isn't human friendly,"# Load balancing rules for HANA 2.0 
 // TODO ALL THIS RULES NEED A PROPER NAME. Port number isn't human friendly 
 // use services name","resource ""azurerm_lb_rule"" ""lb_30313"" {
  resource_group_name            = azurerm_resource_group.myrg.name
  loadbalancer_id                = azurerm_lb.mylb.id
  name                           = ""hana-lb-30313""
  protocol                       = ""Tcp""
  frontend_ip_configuration_name = ""mylb-frontend""
  frontend_port                  = 30313
  backend_port                   = 30313
  backend_address_pool_id        = azurerm_lb_backend_address_pool.mylb.id
  probe_id                       = azurerm_lb_probe.mylb.id
  idle_timeout_in_minutes        = 30
  enable_floating_ip             = ""true""
}
",resource,"resource ""azurerm_lb_rule"" ""lb_30313"" {
  resource_group_name            = azurerm_resource_group.myrg.name
  loadbalancer_id                = azurerm_lb.mylb.id
  name                           = ""hana-lb-30313""
  protocol                       = ""Tcp""
  frontend_ip_configuration_name = ""mylb-frontend""
  frontend_port                  = 30313
  backend_port                   = 30313
  backend_address_pool_id        = azurerm_lb_backend_address_pool.mylb.id
  probe_id                       = azurerm_lb_probe.mylb.id
  idle_timeout_in_minutes        = 30
  enable_floating_ip             = ""true""
}
",resource,70,,5daf59b3bbcb57130d80f6d844ad35171f7c010a,9adcf152486838f9f5b600ead5e4ef373918a786,https://github.com/SUSE/ha-sap-terraform-deployments/blob/5daf59b3bbcb57130d80f6d844ad35171f7c010a/azure/network.tf#L70,https://github.com/SUSE/ha-sap-terraform-deployments/blob/9adcf152486838f9f5b600ead5e4ef373918a786/azure/network.tf,2019-09-05 00:05:54+02:00,2019-09-05 18:08:13+02:00,2,1,0,1,0,1,1,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,35,variables.tf,variables.tf,0,implemented,# placeholder variable for debugging scripts. To be implemented in future,# placeholder variable for debugging scripts. To be implemented in future,"variable ""debug_mode"" {
  default     = false
  description = ""Whether to turn on debug mode.""
  type        = bool
}
",variable,,,791,0.0,2b2991c788c70e2267603919f84a6d916b66baf9,6c867cd8e9cbf559742f56658989bcded0d1fd89,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/2b2991c788c70e2267603919f84a6d916b66baf9/variables.tf#L791,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/variables.tf#L0,2021-10-26 10:35:29+11:00,2023-10-25 16:40:02+11:00,45,2,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,22,modules/gke-cluster/main.tf,modules/gke-cluster/main.tf,0,# todo,"# TODO(ludomagno): make optional, and support beta feature","# TODO(ludomagno): make optional, and support beta feature 
 # https://www.terraform.io/docs/providers/google/r/container_cluster.html#daily_maintenance_window","resource ""google_container_cluster"" ""cluster"" {
  provider                    = google-beta
  project                     = var.project_id
  name                        = var.name
  description                 = var.description
  location                    = var.location
  node_locations              = length(var.node_locations) == 0 ? null : var.node_locations
  min_master_version          = var.min_master_version
  network                     = var.network
  subnetwork                  = var.subnetwork
  logging_service             = var.logging_service
  monitoring_service          = var.monitoring_service
  resource_labels             = var.labels
  default_max_pods_per_node   = var.default_max_pods_per_node
  enable_binary_authorization = var.enable_binary_authorization
  enable_intranode_visibility = var.enable_intranode_visibility
  enable_shielded_nodes       = var.enable_shielded_nodes
  enable_tpu                  = var.enable_tpu
  initial_node_count          = 1
  remove_default_node_pool    = true

  # node_config

  addons_config {
    http_load_balancing {
      disabled = ! var.addons.http_load_balancing
    }
    horizontal_pod_autoscaling {
      disabled = ! var.addons.horizontal_pod_autoscaling
    }
    network_policy_config {
      disabled = ! var.addons.network_policy_config
    }
    # beta addons
    # cloudrun is dynamic as it tends to trigger cluster recreation on change
    dynamic cloudrun_config {
      for_each = var.addons.istio_config.enabled && var.addons.cloudrun_config ? [""""] : []
      content {
        disabled = false
      }
    }
    istio_config {
      disabled = ! var.addons.istio_config.enabled
      auth     = var.addons.istio_config.tls ? ""AUTH_MUTUAL_TLS"" : ""AUTH_NONE""
    }
  }

  # TODO(ludomagno): support setting address ranges instead of range names
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#cluster_ipv4_cidr_block
  ip_allocation_policy {
    cluster_secondary_range_name  = var.secondary_range_pods
    services_secondary_range_name = var.secondary_range_services
  }

  # TODO(ludomagno): make optional, and support beta feature
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#daily_maintenance_window
  maintenance_policy {
    daily_maintenance_window {
      start_time = var.maintenance_start_time
    }
  }

  master_auth {
    client_certificate_config {
      issue_client_certificate = false
    }
  }

  dynamic master_authorized_networks_config {
    for_each = length(var.master_authorized_ranges) == 0 ? [] : list(var.master_authorized_ranges)
    iterator = ranges
    content {
      dynamic cidr_blocks {
        for_each = ranges.value
        iterator = range
        content {
          cidr_block   = range.value
          display_name = range.key
        }
      }
    }
  }

  dynamic network_policy {
    for_each = var.addons.network_policy_config ? [""""] : []
    content {
      enabled  = true
      provider = ""CALICO""
    }
  }

  dynamic private_cluster_config {
    for_each = var.private_cluster_config != null ? [var.private_cluster_config] : []
    iterator = config
    content {
      enable_private_nodes    = config.value.enable_private_nodes
      enable_private_endpoint = config.value.enable_private_endpoint
      master_ipv4_cidr_block  = config.value.master_ipv4_cidr_block
    }
  }

  # beta features

  dynamic authenticator_groups_config {
    for_each = var.authenticator_security_group == null ? [] : [""""]
    content {
      security_group = var.authenticator_security_group
    }
  }

  dynamic cluster_autoscaling {
    for_each = var.cluster_autoscaling.enabled ? [var.cluster_autoscaling] : []
    iterator = config
    content {
      enabled = true
      resource_limits {
        resource_type = ""cpu""
        minimum       = config.cpu_min
        maximum       = config.cpu_max
      }
      resource_limits {
        resource_type = ""memory""
        minimum       = config.memory_min
        maximum       = config.memory_max
      }
    }
  }

  dynamic database_encryption {
    for_each = var.database_encryption.enabled ? [var.database_encryption] : []
    iterator = config
    content {
      state    = config.value.state
      key_name = config.value.key_name
    }
  }

  dynamic pod_security_policy_config {
    for_each = var.pod_security_policy != null ? [""""] : []
    content {
      enabled = var.pod_security_policy
    }
  }

  dynamic release_channel {
    for_each = var.release_channel != null ? [""""] : []
    content {
      channel = var.release_channel
    }
  }

  dynamic resource_usage_export_config {
    for_each = (
      var.resource_usage_export_config.enabled != null
      &&
      var.resource_usage_export_config.dataset != null
      ? [""""] : []
    )
    content {
      enable_network_egress_metering = var.resource_usage_export_config.enabled
      bigquery_destination {
        dataset_id = var.resource_usage_export_config.dataset
      }
    }
  }

  dynamic vertical_pod_autoscaling {
    for_each = var.vertical_pod_autoscaling == null ? [] : [""""]
    content {
      enabled = var.vertical_pod_autoscaling
    }
  }

  dynamic workload_identity_config {
    for_each = var.workload_identity ? [""""] : []
    content {
      identity_namespace = ""${var.project_id}.svc.id.goog""
    }
  }

}
",resource,"resource ""google_container_cluster"" ""cluster"" {
  provider                    = google-beta
  project                     = var.project_id
  name                        = var.name
  description                 = var.description
  location                    = var.location
  node_locations              = length(var.node_locations) == 0 ? null : var.node_locations
  min_master_version          = var.min_master_version
  network                     = var.network
  subnetwork                  = var.subnetwork
  logging_service             = var.logging_config == null ? var.logging_service : null
  monitoring_service          = var.monitoring_config == null ? var.monitoring_service : null
  resource_labels             = var.labels
  default_max_pods_per_node   = var.enable_autopilot ? null : var.default_max_pods_per_node
  enable_binary_authorization = var.enable_binary_authorization
  enable_intranode_visibility = var.enable_intranode_visibility
  enable_l4_ilb_subsetting    = var.enable_l4_ilb_subsetting
  enable_shielded_nodes       = var.enable_shielded_nodes
  enable_tpu                  = var.enable_tpu
  initial_node_count          = 1
  remove_default_node_pool    = var.enable_autopilot ? null : true
  datapath_provider           = var.enable_dataplane_v2 ? ""ADVANCED_DATAPATH"" : ""DATAPATH_PROVIDER_UNSPECIFIED""
  enable_autopilot            = var.enable_autopilot == true ? true : null

  # node_config {}
  # NOTE: Default node_pool is deleted, so node_config (here) is extranneous.
  # Specify that node_config as an parameter to gke-nodepool module instead.

  # TODO(ludomagno): compute addons map in locals and use a single dynamic block
  addons_config {
    dns_cache_config {
      enabled = var.addons.dns_cache_config
    }
    http_load_balancing {
      disabled = !var.addons.http_load_balancing
    }
    horizontal_pod_autoscaling {
      disabled = !var.addons.horizontal_pod_autoscaling
    }
    dynamic ""network_policy_config"" {
      for_each = !var.enable_autopilot ? [""""] : []
      content {
        disabled = !var.addons.network_policy_config
      }
    }
    cloudrun_config {
      disabled = !var.addons.cloudrun_config
    }
    istio_config {
      disabled = !var.addons.istio_config.enabled
      auth     = var.addons.istio_config.tls ? ""AUTH_MUTUAL_TLS"" : ""AUTH_NONE""
    }
    gce_persistent_disk_csi_driver_config {
      enabled = var.addons.gce_persistent_disk_csi_driver_config
    }
  }

  # TODO(ludomagno): support setting address ranges instead of range names
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#cluster_ipv4_cidr_block
  ip_allocation_policy {
    cluster_secondary_range_name  = var.secondary_range_pods
    services_secondary_range_name = var.secondary_range_services
  }

  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#daily_maintenance_window
  maintenance_policy {
    dynamic ""daily_maintenance_window"" {
      for_each = var.maintenance_config != null && lookup(var.maintenance_config, ""daily_maintenance_window"", null) != null ? [var.maintenance_config.daily_maintenance_window] : []
      iterator = config
      content {
        start_time = config.value.start_time
      }
    }

    dynamic ""recurring_window"" {
      for_each = var.maintenance_config != null && lookup(var.maintenance_config, ""recurring_window"", null) != null ? [var.maintenance_config.recurring_window] : []
      iterator = config
      content {
        start_time = config.value.start_time
        end_time   = config.value.end_time
        recurrence = config.value.recurrence
      }
    }

    dynamic ""maintenance_exclusion"" {
      for_each = var.maintenance_config != null && lookup(var.maintenance_config, ""maintenance_exclusion"", null) != null ? var.maintenance_config.maintenance_exclusion : []
      iterator = config
      content {
        exclusion_name = config.value.exclusion_name
        start_time     = config.value.start_time
        end_time       = config.value.end_time
      }
    }
  }

  master_auth {
    client_certificate_config {
      issue_client_certificate = false
    }
  }

  dynamic ""master_authorized_networks_config"" {
    for_each = (
      length(var.master_authorized_ranges) == 0
      ? []
      : [var.master_authorized_ranges]
    )
    iterator = ranges
    content {
      dynamic ""cidr_blocks"" {
        for_each = ranges.value
        iterator = range
        content {
          cidr_block   = range.value
          display_name = range.key
        }
      }
    }
  }

  #the network_policy block is enabled if network_policy_config and network_dataplane_v2 is set to false. Dataplane V2 has built-in network policies.
  dynamic ""network_policy"" {
    for_each = var.addons.network_policy_config ? [""""] : []
    content {
      enabled  = var.enable_dataplane_v2 ? false : true
      provider = var.enable_dataplane_v2 ? ""PROVIDER_UNSPECIFIED"" : ""CALICO""
    }
  }

  dynamic ""private_cluster_config"" {
    for_each = local.is_private ? [var.private_cluster_config] : []
    iterator = config
    content {
      enable_private_nodes    = config.value.enable_private_nodes
      enable_private_endpoint = config.value.enable_private_endpoint
      master_ipv4_cidr_block  = config.value.master_ipv4_cidr_block
      master_global_access_config {
        enabled = config.value.master_global_access
      }
    }
  }

  # beta features

  dynamic ""authenticator_groups_config"" {
    for_each = var.authenticator_security_group == null ? [] : [""""]
    content {
      security_group = var.authenticator_security_group
    }
  }

  dynamic ""cluster_autoscaling"" {
    for_each = var.cluster_autoscaling.enabled ? [var.cluster_autoscaling] : []
    iterator = config
    content {
      enabled = true
      resource_limits {
        resource_type = ""cpu""
        minimum       = config.value.cpu_min
        maximum       = config.value.cpu_max
      }
      resource_limits {
        resource_type = ""memory""
        minimum       = config.value.memory_min
        maximum       = config.value.memory_max
      }
      // TODO: support GPUs too
    }
  }

  dynamic ""database_encryption"" {
    for_each = var.database_encryption.enabled ? [var.database_encryption] : []
    iterator = config
    content {
      state    = config.value.state
      key_name = config.value.key_name
    }
  }

  dynamic ""pod_security_policy_config"" {
    for_each = var.pod_security_policy != null ? [""""] : []
    content {
      enabled = var.pod_security_policy
    }
  }

  dynamic ""release_channel"" {
    for_each = var.release_channel != null ? [""""] : []
    content {
      channel = var.release_channel
    }
  }

  dynamic ""resource_usage_export_config"" {
    for_each = (
      var.resource_usage_export_config.enabled != null
      &&
      var.resource_usage_export_config.dataset != null
      ? [""""] : []
    )
    content {
      enable_network_egress_metering = var.resource_usage_export_config.enabled
      bigquery_destination {
        dataset_id = var.resource_usage_export_config.dataset
      }
    }
  }

  dynamic ""vertical_pod_autoscaling"" {
    for_each = var.vertical_pod_autoscaling == null ? [] : [""""]
    content {
      enabled = var.vertical_pod_autoscaling
    }
  }

  dynamic ""workload_identity_config"" {
    for_each = var.workload_identity && !var.enable_autopilot ? [""""] : []
    content {
      identity_namespace = ""${var.project_id}.svc.id.goog""
    }
  }

  dynamic ""monitoring_config"" {
    for_each = var.monitoring_config != null ? [""""] : []
    content {
      enable_components = var.monitoring_config
    }
  }

  dynamic ""logging_config"" {
    for_each = var.logging_config != null ? [""""] : []
    content {
      enable_components = var.logging_config
    }
  }

  dynamic ""dns_config"" {
    for_each = var.dns_config != null ? [var.dns_config] : []
    iterator = config
    content {
      cluster_dns        = config.value.cluster_dns
      cluster_dns_scope  = config.value.cluster_dns_scope
      cluster_dns_domain = config.value.cluster_dns_domain
    }
  }
}
",resource,71,,c486bfc66f9814e33b410602cb557a5e4d532912,e1b79bc7f6e2e3361e0ec3ee4256dd44b98b26fe,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c486bfc66f9814e33b410602cb557a5e4d532912/modules/gke-cluster/main.tf#L71,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/e1b79bc7f6e2e3361e0ec3ee4256dd44b98b26fe/modules/gke-cluster/main.tf,2020-04-03 14:06:48+02:00,2021-10-20 18:21:05+02:00,17,1,1,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,1,main.tf,main.tf,0,#todo,#TODO: add if condition to validate if neither US or EU are supplied,#TODO: add if condition to validate if neither US or EU are supplied,"resource ""google_bigquery_dataset"" ""default"" {
  dataset_id                  = ""${var.dataset_id}""
  friendly_name               = ""${var.dataset_name}""
  description                 = ""${var.description}""
  #TODO: add if condition to validate if neither US or EU are supplied
  location                    = ""${var.region}""
  #TODO: format this ne excluded by default but can optionally be defined if the user wishes
  default_table_expiration_ms = ""${var.expiration}""
  project                     = ""${var.project_id}""

  #TODO: Need to find a way to dynamically assign a dict object(s)
  labels {
    env = ""default""
    foo = ""bar""
    tonyd = ""tonyd""
  }

  //TODO: array of users or groups needs to be added to have access. Need to figure out the best method of customers to allocate users or groups.
  # access {
  #   role   = ""READER""
  #   domain = ""adigangi.com""
  # }
  #
  # access {
  #   role           = ""WRITER""
  #   user_by_email = ""adigangi@adigangi.com""
  # }
  #
  # access {
  #   role           = ""OWNER""
  #   special_group  = ""projectOwners""
  # }
}
",resource,the block associated got renamed or deleted,,26,,d56aa2c9a80343d60eed3e1a7d24962be31ee0b6,7f922f7e9df197df38c9b09dfa6e3614d71f19f5,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/d56aa2c9a80343d60eed3e1a7d24962be31ee0b6/main.tf#L26,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/7f922f7e9df197df38c9b09dfa6e3614d71f19f5/main.tf,2018-11-20 10:30:15-05:00,2019-01-16 18:10:54-05:00,3,1,0,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,698,infra/modules/aws-cognito-identity-cli/variables.tf,infra/modules/aws-cognito-identity-cli/variables.tf,0,# todo,"# TODO: expect value format ""${module.cognito-identity-pool.developer_provider_name}=${module.msft-connection[k].connector.application_id}""","# TODO: expect value format ""${module.cognito-identity-pool.developer_provider_name}=${module.msft-connection[k].connector.application_id}"" 
 # --> add validation of that OR split those components in properties of an object??","variable ""login_ids"" {
  type = map(string)
  description = ""Map of connector id => login id for which to create identities in pool""
}
",variable,"variable ""login_ids"" {
  type        = map(string)
  description = ""Map of connector id => login id for which to create identities in pool""
}",variable,16,17.0,03edc740757710dc38e422206367817e234b7a8e,ea4455ba9b9a4fd4b9009750d297f8fc83482ad0,https://github.com/Worklytics/psoxy/blob/03edc740757710dc38e422206367817e234b7a8e/infra/modules/aws-cognito-identity-cli/variables.tf#L16,https://github.com/Worklytics/psoxy/blob/ea4455ba9b9a4fd4b9009750d297f8fc83482ad0/infra/modules/aws-cognito-identity-cli/variables.tf#L17,2023-02-15 12:34:03-08:00,2023-02-16 07:54:57-08:00,3,0,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,760,examples/cloud-operations/network-dashboard/main.tf,examples/cloud-operations/network-dashboard/main.tf,0,# todo,"# TODO: How to secure Cloud Function invokation? Not   member = ""allUsers"" but specific Scheduler service Account?","# TODO: How to secure Cloud Function invokation? Not   member = ""allUsers"" but specific Scheduler service Account? 
 # Maybe ""service-YOUR_PROJECT_NUMBER@gcp-sa-cloudscheduler.iam.gserviceaccount.com""? ","resource ""google_cloudfunctions_function_iam_member"" ""invoker"" {
  project        = module.project-monitoring.project_id
  region         = var.region
  cloud_function = google_cloudfunctions_function.function_quotas.name

  role   = ""roles/cloudfunctions.invoker""
  member = ""allUsers""
}
",resource,the block associated got renamed or deleted,,164,,9f3ee4dc2299a7e9034cf25988d9c4b59c0fd70b,65172031f0f525adfe6d56aa050aab606d6ce8e7,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9f3ee4dc2299a7e9034cf25988d9c4b59c0fd70b/examples/cloud-operations/network-dashboard/main.tf#L164,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/65172031f0f525adfe6d56aa050aab606d6ce8e7/examples/cloud-operations/network-dashboard/main.tf,2022-03-08 18:36:02+01:00,2022-03-17 20:08:58+01:00,6,1,1,1,0,1,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,141,modules/oke/cluster.tf,modules/oke/cluster.tf,0,//todo,//TODO: remove this option when the relevant Kubernetes version (v1.25) is no longer supported by OKE,//TODO: remove this option when the relevant Kubernetes version (v1.25) is no longer supported by OKE,"resource ""oci_containerengine_cluster"" ""k8s_cluster"" {
  compartment_id     = var.compartment_id
  kubernetes_version = var.cluster_kubernetes_version
  kms_key_id         = var.use_cluster_encryption == true ? var.cluster_kms_key_id : null
  name               = var.label_prefix == ""none"" ? var.cluster_name : ""${var.label_prefix}-${var.cluster_name}""

  depends_on = [time_sleep.wait_30_seconds]

  cluster_pod_network_options {
    cni_type = var.cni_type == ""flannel"" ? ""FLANNEL_OVERLAY"" : ""OCI_VCN_IP_NATIVE""
  }

  endpoint_config {
    is_public_ip_enabled = var.control_plane_type == ""public"" ? true : false
    nsg_ids              = var.control_plane_nsgs
    subnet_id            = var.cluster_subnets[""cp""]
  }

  dynamic ""image_policy_config"" {
    for_each = var.use_signed_images == true ? [1] : []

    content {
      is_policy_enabled = true

      dynamic ""key_details"" {
        iterator = signing_keys_iterator
        for_each = var.image_signing_keys

        content {
          kms_key_id = signing_keys_iterator.value
        }
      }
    }
  }

  freeform_tags = lookup(var.freeform_tags, ""cluster"", {})
  defined_tags  = lookup(var.defined_tags, ""cluster"", {})

  options {
    add_ons {
      is_kubernetes_dashboard_enabled = var.cluster_options_add_ons_is_kubernetes_dashboard_enabled
      is_tiller_enabled               = false
    }

    //TODO: remove this option when the relevant Kubernetes version (v1.25) is no longer supported by OKE
    admission_controller_options {
      is_pod_security_policy_enabled = lookup(var.admission_controller_options, ""PodSecurityPolicy"", false)
    }

    kubernetes_network_config {
      pods_cidr     = var.cluster_options_kubernetes_network_config_pods_cidr
      services_cidr = var.cluster_options_kubernetes_network_config_services_cidr
    }

    persistent_volume_config {
      freeform_tags = lookup(var.freeform_tags, ""persistent_volume"", {})
      defined_tags  = lookup(var.defined_tags, ""persistent_volume"", {})
    }

    service_lb_config {
      freeform_tags = lookup(var.freeform_tags, ""service_lb"", {})
      defined_tags  = lookup(var.defined_tags, ""service_lb"", {})
    }

    service_lb_subnet_ids = [var.cluster_subnets[local.lb_subnet]]
  }

  lifecycle {
    ignore_changes = [defined_tags, freeform_tags, cluster_pod_network_options]

    precondition {
      condition     = var.cluster_subnets[local.lb_subnet] != """"
      error_message = ""Preferred load balancer references unexisting load balancer. Please check variables preferred_load_balancer and load_balancers.""
    }
  }

  vcn_id = var.vcn_id

}
",resource,,,55,0.0,23a6c53575badeea9f8bf2fd815fca59f843e677,6c867cd8e9cbf559742f56658989bcded0d1fd89,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/23a6c53575badeea9f8bf2fd815fca59f843e677/modules/oke/cluster.tf#L55,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/modules/oke/cluster.tf#L0,2023-03-23 09:58:36+11:00,2023-10-25 16:40:02+11:00,5,2,1,1,1,0,0,0,0,0
https://github.com/CDCgov/prime-simplereport,40,ops/services/postgres_db/monitor.tf,ops/services/postgres_db/monitor.tf,0,// todo,// TODO: delete the current target_resource_id line and uncomment the line below it,"// TODO: delete the current target_resource_id line and uncomment the line below it 
 // when removing old DB configuration.","resource ""azurerm_monitor_diagnostic_setting"" ""postgres"" {
  name               = ""simple-report-${var.env}-db-diag""
  target_resource_id = azurerm_postgresql_server.db.id
  //target_resource_id         = azurerm_postgresql_flexible_server.db.id
  log_analytics_workspace_id = var.log_workspace_id

  log {
    category = ""PostgreSQLLogs""
    enabled  = true

    retention_policy {
      enabled = false
    }
  }

  metric {
    category = ""AllMetrics""
    enabled  = true

    retention_policy {
      enabled = false
    }
  }

  log {
    category = ""QueryStoreRuntimeStatistics""
    enabled  = false

    retention_policy {
      enabled = false
    }
  }

  log {
    category = ""QueryStoreWaitStatistics""
    enabled  = false

    retention_policy {
      enabled = false
    }
  }
}
",resource,"resource ""azurerm_monitor_diagnostic_setting"" ""postgres"" {
  name                       = ""simple-report-${var.env}-db-diag""
  target_resource_id         = azurerm_postgresql_flexible_server.db.id
  log_analytics_workspace_id = var.log_workspace_id

  log {
    category = ""PostgreSQLLogs""
    enabled  = true

    retention_policy {
      enabled = false
    }
  }

  metric {
    category = ""AllMetrics""
    enabled  = true

    retention_policy {
      enabled = false
    }
  }
}
",resource,1,,3e4185fa549937cff9213c325bc0fee780e41eb0,fb9a18eb71f31044ca7a58958a25dea489263ca7,https://github.com/CDCgov/prime-simplereport/blob/3e4185fa549937cff9213c325bc0fee780e41eb0/ops/services/postgres_db/monitor.tf#L1,https://github.com/CDCgov/prime-simplereport/blob/fb9a18eb71f31044ca7a58958a25dea489263ca7/ops/services/postgres_db/monitor.tf,2022-02-16 12:59:39-05:00,2022-04-28 23:27:57-04:00,2,1,1,1,0,0,0,0,1,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,959,tests/modules/organization-policy/fixture/variables.tf,tests/modules/organization-policy/fixture/variables.tf,0,# todo,"# TODO: convert to a proper data structure map(map(object({...}))) once tf1.3 is released and optional object keys are avaliable,","# TODO: convert to a proper data structure map(map(object({...}))) once tf1.3 is released and optional object keys are avaliable, 
 # for now it will cause multiple keys to be set to null for every policy definition 
 # https://github.com/hashicorp/terraform/releases/tag/v1.3.0-alpha20220622","variable ""organization_policies"" {
  description = ""Organization policies keyed by parent in format `projects/project-id`, `folders/1234567890` or `organizations/1234567890`.""
  type        = any
  default     = {}
}
",variable,the block associated got renamed or deleted,,24,,a34983b2e960ec5d2e1539f6c781248068d22ef3,b8fae0fbf064d881e70b7958685006c3c084ff10,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/a34983b2e960ec5d2e1539f6c781248068d22ef3/tests/modules/organization-policy/fixture/variables.tf#L24,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b8fae0fbf064d881e70b7958685006c3c084ff10/tests/modules/organization-policy/fixture/variables.tf,2022-07-06 19:41:18+02:00,2022-07-08 14:55:28+02:00,3,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd,4,modules/secure-cd/main.tf,modules/secure-cd/main.tf,0,// todo,// TODO: is this necessary or can we just know the branch inherently,_ENVIRONMENT       = each.value // TODO: is this necessary or can we just know the branch inherently,"resource ""google_cloudbuild_trigger"" ""deploy_trigger"" {
  for_each = toset(var.deploy_branches)
  project = var.project_id
  name    = ""deploy-trigger-${each.value}""
  trigger_template {
    branch_name = each.value
    repo_name   = var.manifest_wet_repo
  }
  substitutions = merge(
    {
      _GAR_REPOSITORY    = local.gar_name
      _DEFAULT_REGION    = var.primary_location
      _MANIFEST_WET_REPO = var.manifest_wet_repo
      _ENVIRONMENT       = each.value // TODO: is this necessary or can we just know the branch inherently
    },
    var.additional_substitutions
  )
  filename   = var.app_deploy_trigger_yaml
  depends_on = [google_sourcerepo_repository.repos]
}
",resource,"resource ""google_cloudbuild_trigger"" ""deploy_trigger"" {
  for_each = var.deploy_branch_clusters
  project  = var.project_id
  name     = ""deploy-trigger-${each.key}""

  trigger_template {
    branch_name = each.key
    repo_name   = var.manifest_wet_repo
  }
  substitutions = merge(
    {
      _GAR_REPOSITORY      = var.gar_repo_name
      _DEFAULT_REGION      = each.value.location
      _MANIFEST_WET_REPO   = var.manifest_wet_repo
      _CLUSTER_NAME        = each.value.cluster
      _CLUSTER_PROJECT     = each.value.project_id
      _CLOUDBUILD_FILENAME = var.app_deploy_trigger_yaml
      _CACHE_BUCKET_NAME   = var.cache_bucket_name
      _NEXT_ENV            = each.value.next_env
      _ATTESTOR_NAME       = var.deploy_branch_clusters[""${each.value.next_env}""].attestations[0]
    }
  )
  filename = var.app_deploy_trigger_yaml

}
",resource,38,,75fd6e2deaa66330698ccc35ac59bd65b827cb93,8c54ab472287c4a642c961dcdf44f71626f956af,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/75fd6e2deaa66330698ccc35ac59bd65b827cb93/modules/secure-cd/main.tf#L38,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/8c54ab472287c4a642c961dcdf44f71626f956af/modules/secure-cd/main.tf,2021-10-14 17:52:10-05:00,2021-11-12 15:01:06-06:00,9,1,0,1,0,0,0,1,0,0
https://github.com/chanzuckerberg/cztack,1,aws-default-vpc-security/main.tf,aws-default-vpc-security/main.tf,0,hack,# count hack here based on https://github.com/hashicorp/terraform/issues/11574#issuecomment-365690226,"# count hack here based on https://github.com/hashicorp/terraform/issues/11574#issuecomment-365690226 
 # :(","resource ""aws_default_subnet"" ""default"" {
  count = ""${length(split("","", join("","", flatten(data.aws_availability_zones.available.*.names))))}""

  availability_zone = ""${data.aws_availability_zones.available.names[count.index]}""
}
",resource,"resource ""aws_default_subnet"" ""default"" {
  count = length(split("","", join("","", flatten(data.aws_availability_zones.available.*.names))))

  availability_zone = data.aws_availability_zones.available.names[count.index]
}
",resource,40,40.0,038021ff34e8b2c5bce7a32400dca526838790df,b71a885f73472340541c8cb9f7bc1f5e279538a4,https://github.com/chanzuckerberg/cztack/blob/038021ff34e8b2c5bce7a32400dca526838790df/aws-default-vpc-security/main.tf#L40,https://github.com/chanzuckerberg/cztack/blob/b71a885f73472340541c8cb9f7bc1f5e279538a4/aws-default-vpc-security/main.tf#L40,2018-07-27 14:32:44-07:00,2021-06-25 11:33:29-04:00,4,0,0,1,1,0,1,0,0,0
https://github.com/uyuni-project/sumaform,204,modules/libvirt/package_mirror/main.tf,modules/libvirt/package_mirror/main.tf,0,hack,"// HACK: evaluates to ""nat_network"" if bridge is empty, """" otherwise","// HACK: evaluates to ""nat_network"" if bridge is empty, """" otherwise","resource ""libvirt_domain"" ""domain"" {
  name = ""package-mirror""
  memory = 512
  vcpu = 1
  running = ""${var.running}""

  disk {
    volume_id = ""${libvirt_volume.main_disk.id}""
  }
  disk {
    volume_id = ""${libvirt_volume.data_disk.id}""
  }

  network_interface {
    wait_for_lease = true
    // HACK: evaluates to ""nat_network"" if bridge is empty, """" otherwise
    network_name = ""${element(list(""${var.name_prefix}nat_network"", """"), replace(replace(var.bridge, ""/.+/"", ""1""), ""/^$/"", ""0""))}""
    bridge = ""${var.bridge}""
    mac = ""${var.mac}""
  }

  connection {
    user = ""root""
    password = ""linux""
  }

  provisioner ""file"" {
    source = ""salt""
    destination = ""/srv""
  }

  provisioner ""file"" {
    content = <<EOF

hostname: package-mirror
domain: ${var.domain}
use-avahi: True
role: package-mirror
cc_username: ${var.cc_username}
cc_password: ${var.cc_password}
data_disk_device: vdb

EOF

    destination = ""/etc/salt/grains""
  }

  provisioner ""remote-exec"" {
    inline = [
      ""salt-call --local state.sls terraform-resource"",
      ""salt-call --local state.highstate""
    ]
  }
}
",resource,the block associated got renamed or deleted,,31,,2caf37c92febd157d60a29f7c1151bee01524c62,0d3a83d22f57360dec522bcf1e5dd7adb3b9f9c3,https://github.com/uyuni-project/sumaform/blob/2caf37c92febd157d60a29f7c1151bee01524c62/modules/libvirt/package_mirror/main.tf#L31,https://github.com/uyuni-project/sumaform/blob/0d3a83d22f57360dec522bcf1e5dd7adb3b9f9c3/modules/libvirt/package_mirror/main.tf,2016-11-04 16:33:43+01:00,2016-11-04 17:34:25+01:00,2,1,1,1,0,0,1,0,0,0
https://github.com/Worklytics/psoxy,2496,infra/modules/psoxy-constants/main.tf,infra/modules/psoxy-constants/main.tf,0,# todo,# TODO: confirm that this is indeed the same list (believe it is),"# TODO: add list of permissions, which customer could use to create custom role as alternative   
 # TODO: confirm that this is indeed the same list (believe it is)","locals {

  # AWS Managed polices
  # see: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#aws-managed-policies
  required_aws_roles_to_provision_host = {
    ""arn:aws:iam::aws:policy/IAMFullAccess""        = ""IAMFullAccess""
    ""arn:aws:iam::aws:policy/AmazonS3FullAccess""   = ""AmazonS3FullAccess""
    ""arn:aws:iam::aws:policy/CloudWatchFullAccess"" = ""CloudWatchFullAccess""
    ""arn:aws:iam::aws:policy/AmazonSSMFullAccess""  = ""AmazonSSMFullAccess""
    ""arn:aws:iam::aws:policy/AWSLambda_FullAccess"" = ""AWSLambda_FullAccess""
  }
  # AWS managed policy required to consume Microsoft 365 data
  # (in addition to above)
  required_aws_managed_policies_to_consume_msft_365_source = {
    ""arn:aws:iam::aws:policy/AmazonCognitoPowerUser"" = ""AmazonCognitoPowerUser""
  }

  # TODO: create IAM policy document, which installer could use to create their own policy as
  # alternative to using AWS Managed policies

  # initial GCP APIs that must be enabled in projects that will host the proxy.
  # (Terraform apply will enabled additional ones)
  required_gcp_apis_to_host = {
    # https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com
    ""iamcredentials.googleapis.com"" = ""IAM Service Account Credentials API"",
    # https://console.cloud.google.com/apis/library/serviceusage.googleapis.com
    ""serviceusage.googleapis.com""   = ""Service Usage API"",
  }

  required_gcp_roles_to_provision_host = {
    ""roles/storage.admin"" = {
      display_name    = ""Storage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#storage.admin""
    },
    ""roles/iam.roleAdmin"" = {
      display_name    = ""IAM Role Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.roleAdmin""
    },
    ""roles/secretmanager.admin"" = {
      display_name    = ""Secret Manager Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#secretmanager.admin""
    },
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    },
    ""roles/cloudfunctions.admin"" = {
      display_name    = ""Cloud Functions Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#cloudfunctions.admin""
    },
  }
  # TODO: add list of permissions, which customer could use to create custom role as alternative


  # TODO: confirm that this is indeed the same list (believe it is)
  required_gcp_apis_to_provision_google_workspace_source = local.required_gcp_apis_to_host

  required_gcp_roles_to_provision_google_workspace_source = {
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    }
  }
  # TODO: add list of permissions, which customer could use to create custom role as alternative

  required_azuread_roles_to_provision_msft_365_source = {
    ""7ab1d382-f21e-4acd-a863-ba3e13f7da61"" = ""Cloud Application Administrator"",
  }
}
",locals,"locals {

  # AWS Managed polices
  # see: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#aws-managed-policies
  required_aws_roles_to_provision_host = {
    ""arn:aws:iam::aws:policy/IAMFullAccess""        = ""IAMFullAccess""
    ""arn:aws:iam::aws:policy/AmazonS3FullAccess""   = ""AmazonS3FullAccess"" # only if using bulk sources, although 95% do
    ""arn:aws:iam::aws:policy/CloudWatchFullAccess"" = ""CloudWatchFullAccess""
    ""arn:aws:iam::aws:policy/AmazonSSMFullAccess""  = ""AmazonSSMFullAccess""
    ""arn:aws:iam::aws:policy/AWSLambda_FullAccess"" = ""AWSLambda_FullAccess""
  }
  # AWS managed policy required to consume Microsoft 365 data
  # (in addition to above)
  required_aws_managed_policies_to_consume_msft_365_source = {
    ""arn:aws:iam::aws:policy/AmazonCognitoPowerUser"" = ""AmazonCognitoPowerUser""
  }

  # subset of https://docs.aws.amazon.com/aws-managed-policy/latest/reference/SecretsManagerReadWrite.html
  # as that seems like overkill
  #  - if you're going to use KMS to encrypt the secrets, then you'll need to add the KMS permissions
  #    on the key you intend to use.
  #  - you can/should modify the Resource part of this to limit to a subset of secrets, if this
  #    is being deployed to an AWS account that's used for purposes beyond this proxy deployment
  required_aws_policy_to_use_secrets_manager = {
    ""Version"" : ""2012-10-17"",
    ""Statement"" : [
      {
        ""Effect"" : ""Allow"",
        ""Action"" : [
          ""secretsmanager:*"",
          ""tag:GetResources""
        ],
        ""Resource"" : ""*""
      }
    ]
  }


  # TODO: create IAM policy document, which installer could use to create their own policy as
  # alternative to using AWS Managed policies

  # initial GCP APIs that must be enabled in projects that will host the proxy.
  # (Terraform apply will enabled additional ones)
  required_gcp_apis_to_host = {
    # https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com
    ""iamcredentials.googleapis.com"" = ""IAM Service Account Credentials API"",
    # https://console.cloud.google.com/apis/library/serviceusage.googleapis.com
    ""serviceusage.googleapis.com"" = ""Service Usage API"",
  }

  required_gcp_roles_to_provision_host = {
    ""roles/storage.admin"" = {
      display_name    = ""Storage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#storage.admin""
    },
    ""roles/iam.roleAdmin"" = {
      display_name    = ""IAM Role Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.roleAdmin""
    },
    ""roles/secretmanager.admin"" = {
      display_name    = ""Secret Manager Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#secretmanager.admin""
    },
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    },
    ""roles/cloudfunctions.admin"" = {
      display_name    = ""Cloud Functions Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#cloudfunctions.admin""
    },
  }
  # TODO: add list of permissions, which customer could use to create custom role as alternative


  # TODO: confirm that this is indeed the same list (believe it is)
  required_gcp_apis_to_provision_google_workspace_source = local.required_gcp_apis_to_host

  required_gcp_roles_to_provision_google_workspace_source = {
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    }
  }
  # TODO: add list of permissions, which customer could use to create custom role as alternative

  required_azuread_roles_to_provision_msft_365_source = {
    ""7ab1d382-f21e-4acd-a863-ba3e13f7da61"" = ""Cloud Application Administrator"",
  }
}
",locals,59,80.0,bcddaabc129dfb7cce0bc825f95b49ce878a0d55,005e1fed5f46b4310d81d41d29862bb1c4f360b0,https://github.com/Worklytics/psoxy/blob/bcddaabc129dfb7cce0bc825f95b49ce878a0d55/infra/modules/psoxy-constants/main.tf#L59,https://github.com/Worklytics/psoxy/blob/005e1fed5f46b4310d81d41d29862bb1c4f360b0/infra/modules/psoxy-constants/main.tf#L80,2023-07-21 10:14:50-07:00,2024-02-06 19:07:07+00:00,3,0,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1607,modules/net-vpc/subnets.tf,modules/net-vpc/subnets.tf,0,# todo,# TODO: merge factory subnet IAM members,# TODO: merge factory subnet IAM members,"resource ""google_compute_subnetwork_iam_member"" ""bindings"" {
  for_each   = var.subnet_iam_bindings_additive
  project    = var.project_id
  subnetwork = google_compute_subnetwork.subnetwork[each.value.subnet].name
  region     = google_compute_subnetwork.subnetwork[each.value.subnet].region
  role       = each.value.role
  member     = each.value.member
  dynamic ""condition"" {
    for_each = each.value.condition == null ? [] : [""""]
    content {
      expression  = each.value.condition.expression
      title       = each.value.condition.title
      description = each.value.condition.description
    }
  }
}
",resource,"resource ""google_compute_subnetwork_iam_member"" ""bindings"" {
  for_each   = local.subnet_iam_bindings_additive
  project    = var.project_id
  subnetwork = local.all_subnets[each.value.subnet].name
  region     = local.all_subnets[each.value.subnet].region
  role       = each.value.role
  member     = each.value.member
  dynamic ""condition"" {
    for_each = each.value.condition == null ? [] : [""""]
    content {
      expression  = each.value.condition.expression
      title       = each.value.condition.title
      description = each.value.condition.description
    }
  }
}
",resource,187,,819894d2bab4b440f1b52b1ac8035912fb107004,f19ab4872f816cecb866e0b635e302236a444eef,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/819894d2bab4b440f1b52b1ac8035912fb107004/modules/net-vpc/subnets.tf#L187,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/f19ab4872f816cecb866e0b635e302236a444eef/modules/net-vpc/subnets.tf,2023-08-20 09:44:20+02:00,2023-09-15 00:27:55+02:00,11,1,0,1,0,1,1,0,0,0
https://github.com/Worklytics/psoxy,832,infra/modules/google-workspace-dwd-connection/main.tf,infra/modules/google-workspace-dwd-connection/main.tf,0,# todo,"# TODO: md5 here is 32 chars of hex, so some risk of collision by truncating, while could use","# TODO: md5 here is 32 chars of hex, so some risk of collision by truncating, while could use","locals {
  # sa_account_ids must be 6-30 chars, and must start with a letter, use only lowercase letters,
  # numbers and - (inside)
  # see https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_service_account

  trimmed_id = trim(var.connector_service_account_id, "" "")

  # TODO: md5 here is 32 chars of hex, so some risk of collision by truncating, while could use
  sa_account_id = local.trimmed_id < 31 ? lower(replace(local.trimmed_id , "" "", ""-"")) : substr(md5(local.trimmed_id), 0, 30)
}
",locals,"locals {
  # sa_account_ids must be 6-30 chars, and must start with a letter, use only lowercase letters,
  # numbers and - (inside)
  # see https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_service_account

  # trim trailing replaces, replace enclosed spaces with dashes, and everything as lower case
  trimmed_id = lower(replace(trim(var.connector_service_account_id, "" ""), "" "", ""-""))

  # pad if too short
  padded_id = length(local.trimmed_id) < 6 ? ""psoxy-${local.trimmed_id}"" : local.trimmed_id

  # hash if too long
  # TODO: md5 here is 32 chars of hex, so some risk of collision by truncating
  sa_account_id = length(local.padded_id) < 31 ? local.padded_id : substr(md5(local.padded_id), 0, 30)

  instance_id = coalesce(var.instance_id, var.display_name)
}
",locals,14,,674bb3cc40b9675953558a8fe9f432c2298ba3a0,2f240146ca64d11927aac88550c4df4b094c45ee,https://github.com/Worklytics/psoxy/blob/674bb3cc40b9675953558a8fe9f432c2298ba3a0/infra/modules/google-workspace-dwd-connection/main.tf#L14,https://github.com/Worklytics/psoxy/blob/2f240146ca64d11927aac88550c4df4b094c45ee/infra/modules/google-workspace-dwd-connection/main.tf,2023-03-22 16:22:58-07:00,2023-06-20 14:50:05+00:00,5,1,0,1,0,1,0,0,0,0
https://github.com/kbst/terraform-kubestack,29,google/cluster-local/configuration.tf,google/cluster-local/configuration.tf,0,implementation,# to align with the local implementations,"# while we have the real region for GKE 
 # we still hash and prefix it with gke- 
 # to align with the local implementations 
 # for AKS end EKS","locals {
  # current workspace config
  cfg = module.configuration.merged[terraform.workspace]

  name_prefix = local.cfg[""name_prefix""]

  base_domain = local.cfg[""base_domain""]

  # while we have the real region for GKE
  # we still hash and prefix it with gke-
  # to align with the local implementations
  # for AKS end EKS
  fake_region_hash = substr(sha256(local.cfg[""region""]), 0, 7)
  fake_region      = ""gke-${local.fake_region_hash}""

  http_port_default = terraform.workspace == ""apps"" ? 80 : 8080
  http_port         = lookup(local.cfg, ""http_port"", local.http_port_default)

  https_port_default = terraform.workspace == ""apps"" ? 443 : 8443
  https_port         = lookup(local.cfg, ""https_port"", local.https_port_default)

  manifest_path_default = ""manifests/overlays/${terraform.workspace}""
  manifest_path         = var.manifest_path != null ? var.manifest_path : local.manifest_path_default

  disable_default_ingress = lookup(local.cfg, ""disable_default_ingress"", false)

  node_image = lookup(local.cfg, ""node_image"", ""kindest/node:v1.18.0"")

  # technically it should be min_node_count times number of AZs
  # but it seems better to keep node count low in the dev env
  node_count = lookup(local.cfg, ""cluster_min_node_count"", 1)
  nodes = [
    for node, _ in range(local.node_count) :
    ""worker""
  ]
  extra_nodes = join("","", local.nodes)

}
",locals,"locals {
  # current workspace config
  cfg = module.configuration.merged[terraform.workspace]

  name_prefix = local.cfg[""name_prefix""]

  base_domain = local.cfg[""base_domain""]

  # while we have the real region for GKE
  # we still hash and prefix it with gke-
  # to align with the local implementations
  # for AKS end EKS
  fake_region_hash = substr(sha256(local.cfg[""region""]), 0, 7)
  fake_region      = ""gke-${local.fake_region_hash}""

  http_port_default = terraform.workspace == ""apps"" ? 80 : 8080
  http_port         = lookup(local.cfg, ""http_port"", local.http_port_default)

  https_port_default = terraform.workspace == ""apps"" ? 443 : 8443
  https_port         = lookup(local.cfg, ""https_port"", local.https_port_default)

  disable_default_ingress = lookup(local.cfg, ""disable_default_ingress"", false)

  node_image = lookup(local.cfg, ""node_image"", null)

  # technically it should be min_node_count times number of AZs
  # but it seems better to keep node count low in the dev env
  node_count = lookup(local.cfg, ""cluster_min_node_count"", 1)
  nodes = [
    for node, _ in range(local.node_count) :
    ""worker""
  ]
  extra_nodes = join("","", local.nodes)

}
",locals,18,18.0,40af925aa6e6543373a298870aadf27d1672f58d,67b7dfce00f2dc67a9293d74ef5ac80879de5a2b,https://github.com/kbst/terraform-kubestack/blob/40af925aa6e6543373a298870aadf27d1672f58d/google/cluster-local/configuration.tf#L18,https://github.com/kbst/terraform-kubestack/blob/67b7dfce00f2dc67a9293d74ef5ac80879de5a2b/google/cluster-local/configuration.tf#L18,2020-09-29 15:18:43+02:00,2021-05-25 21:22:58+02:00,3,0,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,589,examples/data-solutions/dp-foundation/07-exposure.tf,examples/data-solutions/data-platform-foundations/07-exposure.tf,1,#todo,#TODO add role => service account mapping to assign roles on exposure project,#TODO add role => service account mapping to assign roles on exposure project,"locals {
  group_iam_exp = {
    #TODO add group => role mapping to asign on exposure project
  }
  iam_exp = {
    #TODO add role => service account mapping to assign roles on exposure project
  }
  prefix_exp = ""${var.prefix}-exp""
}
",locals,the block associated got renamed or deleted,,22,,3c99074b3ff652827d277217e4f84b48a713b224,4f4a9cd7ac2cee3a1b2e413cd19f6b6b07c35622,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3c99074b3ff652827d277217e4f84b48a713b224/examples/data-solutions/dp-foundation/07-exposure.tf#L22,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/4f4a9cd7ac2cee3a1b2e413cd19f6b6b07c35622/examples/data-solutions/data-platform-foundations/07-exposure.tf,2022-02-02 15:31:54+01:00,2022-02-09 17:01:25+01:00,3,1,0,1,0,1,0,0,0,0
https://github.com/CDCgov/prime-simplereport,24,ops/dev/persistent/main.tf,ops/dev/persistent/main.tf,0,// todo,// TODO: delete old_subnet_id when removing the old DB configuration,// TODO: delete old_subnet_id when removing the old DB configuration,"module ""db"" {
  source      = ""../../services/postgres_db""
  env         = local.env
  rg_location = local.rg_location
  rg_name     = local.rg_name

  global_vault_id = data.azurerm_key_vault.global.id
  db_vault_id     = data.azurerm_key_vault.db_keys.id
  // TODO: delete old_subnet_id when removing the old DB configuration
  old_subnet_id = module.vnet.subnet_vm_id
  subnet_id     = module.vnet.subnet_db_id
  // TODO: remove this when removing old DB config
  dns_zone_id      = module.vnet.private_dns_zone_id
  log_workspace_id = module.monitoring.log_analytics_workspace_id
  // TODO: remove this when removing old DB config
  nophi_user_password = random_password.random_nophi_password.result

  tags = local.management_tags
}
",module,"module ""db"" {
  source      = ""../../services/postgres_db""
  env         = local.env
  rg_location = local.rg_location
  rg_name     = local.rg_name

  global_vault_id  = data.azurerm_key_vault.global.id
  db_vault_id      = data.azurerm_key_vault.db_keys.id
  subnet_id        = module.vnet.subnet_db_id
  log_workspace_id = module.monitoring.log_analytics_workspace_id

  nophi_user_password = random_password.random_nophi_password.result

  tags = local.management_tags
}
",module,54,,3e4185fa549937cff9213c325bc0fee780e41eb0,fb9a18eb71f31044ca7a58958a25dea489263ca7,https://github.com/CDCgov/prime-simplereport/blob/3e4185fa549937cff9213c325bc0fee780e41eb0/ops/dev/persistent/main.tf#L54,https://github.com/CDCgov/prime-simplereport/blob/fb9a18eb71f31044ca7a58958a25dea489263ca7/ops/dev/persistent/main.tf,2022-02-16 12:59:39-05:00,2022-04-28 23:27:57-04:00,2,1,1,1,0,0,1,0,0,0
https://github.com/Worklytics/psoxy,744,infra/modules/worklytics-connector-specs/main.tf,infra/modules/worklytics-connector-specs/main.tf,0,# todo,"# TODO: arguably it does make sense to have these in yaml, and read them from there; bc YAML gives","# TODO: arguably it does make sense to have these in yaml, and read them from there; bc YAML gives 
 # more interoperability than in a .tf file  ","locals {

  google_workspace_sources = {
    # GDirectory connections are a PRE-REQ for gmail, gdrive, and gcal connections. remove only
    # if you plan to directly connect Directory to worklytics (without proxy). such a scenario is
    # used for customers who care primarily about pseudonymizing PII of external subjects with whom
    # they collaborate in GMail/GCal/GDrive. the Directory does not contain PII of subjects external
    # to the Google Workspace, so may be directly connected in such scenarios.
    ""gdirectory"" : {
      source_kind : ""gdirectory"",
      worklytics_connector_id : ""gdirectory-psoxy"",
      display_name : ""Google Directory""
      identifier_scope_id : ""gapps""
      apis_consumed : [
        ""admin.googleapis.com""
      ]
      oauth_scopes_needed : [
        ""https://www.googleapis.com/auth/admin.directory.user.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.user.alias.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.domain.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.group.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.group.member.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.orgunit.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.rolemanagement.readonly""
      ]
      source_auth_strategy : ""gcp_service_account_key""
      target_host : ""admin.googleapis.com""
      environment_variables : {}
      example_api_calls : [
        ""/admin/directory/v1/users?customer=my_customer&maxResults=10"",
        ""/admin/directory/v1/groups?customer=my_customer&maxResults=10"",
        ""/admin/directory/v1/customer/my_customer/domains"",
        ""/admin/directory/v1/customer/my_customer/roles?maxResults=10"",
        ""/admin/directory/v1/customer/my_customer/roleassignments?maxResults=10""
      ]
      example_api_calls_user_to_impersonate : var.google_workspace_example_admin
    },
    ""gcal"" : {
      source_kind : ""gcal"",
      worklytics_connector_id : ""gcal-psoxy"",
      display_name : ""Google Calendar""
      identifier_scope_id : ""gapps""
      apis_consumed : [
        ""calendar-json.googleapis.com""
      ]
      source_auth_strategy : ""gcp_service_account_key""
      target_host : ""www.googleapis.com""
      oauth_scopes_needed : [
        ""https://www.googleapis.com/auth/calendar.readonly""
      ],
      environment_variables : {}
      example_api_calls : [
        ""/calendar/v3/calendars/primary"",
        ""/calendar/v3/users/me/settings"",
        ""/calendar/v3/calendars/primary/events?maxResults=10""
      ]
      example_api_calls_user_to_impersonate : var.google_workspace_example_user
    },
    ""gmail"" : {
      source_kind : ""gmail"",
      worklytics_connector_id : ""gmail-meta-psoxy"",
      display_name : ""GMail""
      identifier_scope_id : ""gapps""
      apis_consumed : [
        ""gmail.googleapis.com""
      ]
      source_auth_strategy : ""gcp_service_account_key""
      target_host : ""www.googleapis.com""
      oauth_scopes_needed : [
        ""https://www.googleapis.com/auth/gmail.metadata""
      ],
      environment_variables : {},
      example_api_calls : [
        ""/gmail/v1/users/me/messages?maxResults=10""
      ]
      example_api_calls_user_to_impersonate : var.google_workspace_example_user
    },
    ""google-chat"" : {
      source_kind : ""google-chat"",
      worklytics_connector_id : ""google-chat-psoxy"",
      display_name : ""Google Chat""
      identifier_scope_id : ""gapps""
      apis_consumed : [
        ""admin.googleapis.com""
      ]
      source_auth_strategy : ""gcp_service_account_key""
      target_host : ""admin.googleapis.com""
      oauth_scopes_needed : [
        ""https://www.googleapis.com/auth/admin.reports.audit.readonly""
      ]
      environment_variables : {}
      example_api_calls : [
        ""/admin/reports/v1/activity/users/all/applications/chat?maxResults=10""
      ]
      example_api_calls_user_to_impersonate : var.google_workspace_example_user
    },
    ""google-meet"" : {
      source_kind : ""google-meet""
      worklytics_connector_id : ""google-meet-psoxy""
      display_name : ""Google Meet""
      identifier_scope_id : ""gapps""
      apis_consumed : [
        ""admin.googleapis.com""
      ]
      source_auth_strategy : ""gcp_service_account_key""
      target_host : ""admin.googleapis.com""
      oauth_scopes_needed : [
        ""https://www.googleapis.com/auth/admin.reports.audit.readonly""
      ]
      environment_variables : {}
      example_api_calls : [
        ""/admin/reports/v1/activity/users/all/applications/meet?maxResults=10""
      ]
      example_api_calls_user_to_impersonate : var.google_workspace_example_user
    },
    ""gdrive"" : {
      source_kind : ""gdrive"",
      worklytics_connector_id : ""gdrive-psoxy"",
      display_name : ""Google Drive""
      identifier_scope_id : ""gapps""
      apis_consumed : [
        ""drive.googleapis.com""
      ]
      source_auth_strategy : ""gcp_service_account_key""
      target_host : ""www.googleapis.com""
      oauth_scopes_needed : [
        ""https://www.googleapis.com/auth/drive.metadata.readonly""
      ],
      environment_variables : {}
      example_api_calls : [
        ""/drive/v2/files"",
        ""/drive/v3/files""
      ],
      example_api_calls_user_to_impersonate : var.google_workspace_example_user
    }
  }

  # Microsoft 365 sources; add/remove as you wish
  # See https://docs.microsoft.com/en-us/graph/permissions-reference for all the permissions available in AAD Graph API
  msft_365_connectors = {
    ""azure-ad"" : {
      enabled : true,
      worklytics_connector_id : ""azure-ad-psoxy"",
      source_kind : ""azure-ad"",
      display_name : ""Azure Directory""
      identifier_scope_id : ""azure-ad""
      source_auth_strategy : ""oauth2_refresh_token""
      target_host : ""graph.microsoft.com""
      required_oauth2_permission_scopes : [],
      # Delegated permissions (from `az ad sp list --query ""[?appDisplayName=='Microsoft Graph'].oauth2Permissions"" --all`)
      required_app_roles : [
        # Application permissions (form az ad sp list --query ""[?appDisplayName=='Microsoft Graph'].appRoles"" --all
        ""User.Read.All"",
        ""Group.Read.All""
      ]
      environment_variables : {
        GRANT_TYPE : ""workload_identity_federation"" # by default, assumed to be of type 'urn:ietf:params:oauth:client-assertion-type:jwt-bearer'
        TOKEN_SCOPE : ""https://graph.microsoft.com/.default""
        REFRESH_ENDPOINT = ""https://login.microsoftonline.com/${var.msft_tenant_id}/oauth2/v2.0/token""
      },
      example_api_calls : [
        ""/v1.0/users"",
        ""/v1.0/users/${var.example_msft_user_guid}"",
        ""/v1.0/groups"",
        ""/v1.0/groups/{group-id}/members""
      ]
    },
    ""outlook-cal"" : {
      enabled : true,
      source_kind : ""outlook-cal"",
      worklytics_connector_id : ""outlook-cal-psoxy"",
      display_name : ""Outlook Calendar""
      identifier_scope_id : ""azure-ad""
      source_auth_strategy : ""oauth2_refresh_token""
      target_host : ""graph.microsoft.com""
      required_oauth2_permission_scopes : [],
      required_app_roles : [
        ""OnlineMeetings.Read.All"",
        ""Calendars.Read"",
        ""MailboxSettings.Read"",
        ""Group.Read.All"",
        ""User.Read.All""
      ],
      environment_variables : {
        GRANT_TYPE : ""workload_identity_federation"" # by default, assumed to be of type 'urn:ietf:params:oauth:client-assertion-type:jwt-bearer'
        TOKEN_SCOPE : ""https://graph.microsoft.com/.default""
        REFRESH_ENDPOINT = ""https://login.microsoftonline.com/${var.msft_tenant_id}/oauth2/v2.0/token""
      },
      example_api_calls : [
        ""/v1.0/users"",
        ""/v1.0/users/${var.example_msft_user_guid}/events"",
        ""/v1.0/users/${var.example_msft_user_guid}/calendarView?startDateTime=2022-10-01T00:00:00Z&endDateTime=${timestamp()}"",
        ""/v1.0/users/${var.example_msft_user_guid}/mailboxSettings"",
        ""/v1.0/groups"",
        ""/v1.0/groups/{group-id}/members""
      ]
    },
    ""outlook-mail"" : {
      enabled : true,
      source_kind : ""outlook-mail""
      worklytics_connector_id : ""outlook-mail-psoxy"",
      display_name : ""Outlook Mail""
      identifier_scope_id : ""azure-ad""
      source_auth_strategy : ""oauth2_refresh_token""
      target_host : ""graph.microsoft.com""
      required_oauth2_permission_scopes : [],
      required_app_roles : [
        ""Mail.ReadBasic.All"",
        ""MailboxSettings.Read"",
        ""Group.Read.All"",
        ""User.Read.All""
      ],
      environment_variables : {
        GRANT_TYPE : ""workload_identity_federation"" # by default, assumed to be of type 'urn:ietf:params:oauth:client-assertion-type:jwt-bearer'
        TOKEN_SCOPE : ""https://graph.microsoft.com/.default""
        REFRESH_ENDPOINT : ""https://login.microsoftonline.com/${var.msft_tenant_id}/oauth2/v2.0/token""
      }
      example_api_calls : [
        ""/beta/users"",
        ""/beta/users/${var.example_msft_user_guid}/mailboxSettings"",
        ""/beta/users/${var.example_msft_user_guid}/mailFolders/SentItems/messages"",
        ""/v1.0/groups"",
        ""/v1.0/groups/{group-id}/members""
      ]
    }
  }
  oauth_long_access_connectors = {
    asana = {
      source_kind : ""asana"",
      worklytics_connector_id : ""asana-psoxy""
      display_name : ""Asana""
      identifier_scope_id : ""asana""
      worklytics_connector_name : ""Asana via Psoxy""
      target_host : ""app.asana.com""
      source_auth_strategy : ""oauth2_access_token""
      environment_variables : {}
      secured_variables : [
        { name : ""ACCESS_TOKEN"", writable : false },
      ],
      reserved_concurrent_executions : null
      example_api_calls_user_to_impersonate : null
      example_api_calls : [
        ""/api/1.0/workspaces"",
        ""/api/1.0/users?workspace={ANY_WORKSPACE_GID}&limit=10"",
        ""/api/1.0/workspaces/{ANY_WORKSPACE_GID}/teams&limit=10"",
        ""/api/1.0/teams/{ANY_TEAM_GID}/projects?limit=20"",
        ""/api/1.0/tasks?project={ANY_PROJECT_GID}"",
        ""/api/1.0/tasks/{ANY_TASK_GID}"",
        ""/api/1.0/tasks/{ANY_TASK_GID}/stories"",
      ],
      secured_variables : [
        { name : ""ACCESS_TOKEN"", writable : false },
      ],
      external_token_todo : <<EOT
  1. Create a [Service Account User + token](https://asana.com/guide/help/premium/service-accounts)
     or a sufficiently [Personal Access Token]() for a sufficiently privileged user (who can see all
     the workspaces/teams/projects/tasks you wish to import to Worklytics via this connection).
  2. Update the content of PSOXY_ASANA_ACCESS_TOKEN variable with the previous token value obtained
EOT
    }
    slack-discovery-api = {
      source_kind : ""slack""
      identifier_scope_id : ""slack""
      worklytics_connector_id : ""slack-discovery-api-psoxy"",
      worklytics_connector_name : ""Slack via Psoxy"",
      display_name : ""Slack Discovery API""
      target_host : ""www.slack.com""
      source_auth_strategy : ""oauth2_access_token""
      oauth_scopes_needed : [
        ""discovery:read"",
      ]
      environment_variables : {}
      secured_variables : [
        { name : ""ACCESS_TOKEN"", writable : false },
      ]
      reserved_concurrent_executions : null
      example_api_calls_user_to_impersonate : null
      example_api_calls : [
        ""/api/discovery.enterprise.info"",
        ""/api/discovery.conversations.list"",
        ""/api/discovery.conversations.history?channel={CHANNEL_ID}&limit=10"",
        ""/api/discovery.users.list"",
      ]
      external_token_todo : <<EOT
## Slack Discovery Setup
For enabling Slack Discovery with the Psoxy you must first setup an app on your Slack Enterprise
instance.
  1. Go to https://api.slack.com/apps and create an app, select name a development workspace
  2. Take note of your App ID and contact your Slack rep and ask them to enable `discovery:read` scope for the app.
If they also enable `discovery:write` then delete it for safety, the app just needs read access.
3. Generate the following URL replacing the placeholders for *YOUR_CLIENT_ID* and *YOUR_APP_SECRET* and save it for later
`https://api.slack.com/api/oauth.v2.access?client_id=YOUR_CLIENT_ID&client_secret=YOUR_APP_SECRET`
4. Go to OAuth & Permissions > Redirect URLs and add the previous URL there
The next step depends on your installation approach you might need to change slightly
### Org wide install
Use this step if you want to install in the whole org, across multiple workspaces.
  1. Add a bot scope (not really used, but Slack doesn't allow org-wide without a bot scope requested).
     Just add `users:read`, something that is read-only and we already have access through discovery.
  2. Go to *Org Level Apps* and Opt-in to the program
  3. Go to Settings > Install App
  4. Install into *organization*
  5. Copy the User OAuth Token and store it in secret manager.
  Otherwise, share the token with the AWS/GCP administrator completing the implementation.
### Workspace install
Use this steps if you intend to install in just one workspace within your org.
  1. Go to Settings > Install App
  2. Install into *workspace*
  3. Copy the User OAuth Token and store it in the secret manager (or share with the administrator completing the implementation)
EOT
    }
    zoom = {
      source_kind : ""zoom""
      worklytics_connector_id : ""zoom-psoxy""
      display_name : ""Zoom""
      worklytics_connector_name : ""Zoom via Psoxy""
      identifier_scope_id : ""zoom""
      source_auth_strategy: ""oauth2_refresh_token""
      target_host: ""api.zoom.us""
      environment_variables : {
        GRANT_TYPE: ""account_credentials""
        REFRESH_ENDPOINT: ""https://zoom.us/oauth/token""
      }
      secured_variables : [
        { name : ""CLIENT_SECRET"", writable : false },
        { name : ""CLIENT_ID"", writable : false },
        { name : ""ACCOUNT_ID"", writable : false },
        { name : ""ACCESS_TOKEN"", writable : true },
      ],
      reserved_concurrent_executions : null # 1
      example_api_calls_user_to_impersonate : null
      example_api_calls : [
        ""/v2/users"",
        ""/v2/users/{USER_ID}/meetings"",
        ""/v2/past_meetings/{MEETING_ID}"",
        ""/v2/past_meetings/{MEETING_ID}/instances"",
        ""/v2/past_meetings/{MEETING_ID}/participants"",
        ""/v2/report/users/{userId}/meetings"",
        ""/v2/report/meetings/{meetingId}"",
        ""/v2/report/meetings/{meetingId}/participants""
      ],
      external_token_todo : <<EOT
## Zoom Setup
Zoom connector through Psoxy requires a custom managed app on the Zoom Marketplace (in development
mode, no need to publish).
1. Go to https://marketplace.zoom.us/develop/create and create an app of type ""Server to Server OAuth""
2. After creation it will show the App Credentials. Share them with the AWS/GCP administrator, the
following secret values must be filled in the Secret Manager for the Proxy with the appropriate values:
- `PSOXY_ZOOM_CLIENT_ID`
- `PSOXY_ZOOM_ACCOUNT_ID`
- `PSOXY_ZOOM_CLIENT_SECRET`
Anytime the *client secret* is regenerated it needs to be updated in the Proxy too.
3. Fill the information section
4. Fill the scopes section, enabling the following:
- Users / View all user information /user:read:admin
  - To be able to gather information about the zoom users
- Meetings / View all user meetings /meeting:read:admin
  - Allows us to list all user meeting
- Report / View report data /report:read:admin
  - Last 6 months view for user meetings
5. Activate the app
EOT
    },
    dropbox-business = {
      source_kind : ""dropbox-business""
      worklytics_connector_id : ""dropbox-business-log-psoxy""
      target_host: ""api.dropboxapi.com""
      source_auth_strategy: ""oauth2_refresh_token""
      display_name : ""Dropbox Business""
      identifier_scope_id : ""dropbox-business""
      worklytics_connector_name : ""Dropbox Business via Psoxy""
      secured_variables : [
        { name : ""REFRESH_TOKEN"", writable : false },
        { name : ""CLIENT_ID"", writable : false },
        { name : ""CLIENT_SECRET"", writable : false },
      ],
      environment_variables : {
        GRANT_TYPE : ""refresh_token""
        REFRESH_ENDPOINT : ""https://api.dropboxapi.com/oauth2/token""
      }
      reserved_concurrent_executions : null
      example_api_calls_user_to_impersonate : null
      example_api_calls : [
        ""/2/team/members/list_v2"",
        ""/2/team/groups/list"",
        ""/2/team_log/get_events"",
      ],
      external_token_todo : <<EOT
Dropbox connector through Psoxy requires a Dropbox Application created in Dropbox Console. The application
does not require to be public, and it needs to have the following scopes to support
all the operations for the connector:

- members.read: member listing
- events.read: event listing
- groups.read: group listing

1. Go to https://www.dropbox.com/apps and Build an App
2. Then go https://www.dropbox.com/developers to enter in `App Console` to configure your app
3. Now you are in the app, go to `Permissions` and mark all the scopes described before. NOTE: Probably in the UI will mark you more required permissions automatically (like *account_info_read*.) Just mark the ones
   described and the UI will ask you to include required.
4. On settings, you could access to `App key` and `App secret`. You can create an access token here, but with limited
   expiration. We need to create a long-lived token, so edit the following URL with your `App key` and paste it into the
   browser:

   `https://www.dropbox.com/oauth2/authorize?client_id=<APP_KEY>&token_access_type=offline&response_type=code`

   That will return an `Authorization Code` that you have to paste.
   **NOTE** This `Authorization Code` if for a one single use; if expired or used you will need to get it again pasting
   the
   URL in the browser.
5. Now, replace the values in following URL and run it from command line in your terminal. Replace `Authorization Code`
   , `App key`
   and `App secret` in the placeholders:

   `curl https://api.dropbox.com/oauth2/token -d code=<AUTHORIZATION_CODE> -d grant_type=authorization_code -u <APP_KEY>:<APP_SECRET>`
6. After running that command, if successful you will see a [JSON response](https://www.dropbox.com/developers/Documentation/http/Documentation#oauth2-authorize) like this:

```json
{
  ""access_token"": ""some short live access token"",
  ""token_type"": ""bearer"",
  ""expires_in"": 14399,
  ""refresh_token"": ""some long live token we are going to use"",
  ""scope"": ""account_info.read events.read files.metadata.read groups.read members.read team_data.governance.read team_data.governance.write team_data.member"",
  ""uid"": """",
  ""team_id"": ""some team id""
}
```
7. Finally set following variables in AWS System Manager parameters store / GCP Cloud Secrets (if default implementation):
  - `PSOXY_DROPBOX_BUSINESS_REFRESH_TOKEN` secret variable with value of `refresh_token` received in previous response
  - `PSOXY_DROPBOX_BUSINESS_CLIENT_ID` with `App key` value.
  - `PSOXY_DROPBOX_BUSINESS_CLIENT_SECRET` with `App secret` value.

EOT
    }
  }

  bulk_connectors = {
    ""badge"" = {
      source_kind               = ""badge""
      worklytics_connector_id   = ""bulk-import-psoxy"",
      worklytics_connector_name = ""Bulk Data Import via Psoxy""
      rules = {
        columnsToRedact = []
        columnsToPseudonymize = [
          ""EMPLOYEE_ID"", # primary key
          # ""employee_email"", # if exists
        ]
      }
      settings_to_provide = {
        ""Data Source Processing"" = ""badge""
      }
    }
    ""hris"" = {
      source_kind               = ""hris""
      worklytics_connector_id   = ""bulk-import-psoxy""
      worklytics_connector_name = ""HRIS Data Import via Psoxy""
      rules = {
        columnsToRedact = []
        columnsToPseudonymize = [
          ""EMPLOYEE_ID"",    # primary key
          ""EMPLOYEE_EMAIL"", # for matching
          ""MANAGER_ID"",     # should match to employee_id
          # ""MANAGER_EMAIL""      # if exists
        ]
      }
      settings_to_provide = {
        ""Parser"" = ""EMPLOYEE_SNAPSHOT""
      }
    }
    ""survey"" = {
      worklytics_connector_id   = ""survey-import-psoxy""
      source_kind               = ""survey""
      worklytics_connector_name = ""Survey Data Import via Psoxy""
      rules = {
        columnsToRedact = []
        columnsToPseudonymize = [
          ""EMPLOYEE_ID"", # primary key
          # ""EMPLOYEE_EMAIL"", # if exists
        ]
      }
    }
    ""qualtrics"" = {
      source_kind               = ""qualtrics""
      worklytics_connector_id   = ""survey-import-psoxy""
      worklytics_connector_name = ""Survey Data Import via Psoxy""
      rules = {
        columnsToRedact = []
        columnsToPseudonymize = [
          ""EMPLOYEE_ID"", # primary key
          # ""employee_email"", # if exists
        ]
      }
    }
  }
}
",locals,"resource ""time_static"" ""deployment"" {

}
",resource,2,2.0,312b106f15389e35c0bf553b361f4c21c20cd607,3687e70bceaa637086ceb913629b9462ec220d0c,https://github.com/Worklytics/psoxy/blob/312b106f15389e35c0bf553b361f4c21c20cd607/infra/modules/worklytics-connector-specs/main.tf#L2,https://github.com/Worklytics/psoxy/blob/3687e70bceaa637086ceb913629b9462ec220d0c/infra/modules/worklytics-connector-specs/main.tf#L2,2023-03-01 10:56:50-08:00,2024-05-21 13:49:30-07:00,70,0,0,1,0,0,0,0,0,0
https://github.com/kubernetes/k8s.io,22,infra/gcp/clusters/k8s-infra-prow-build-trusted/prow-build-trusted/main.tf,infra/gcp/clusters/projects/k8s-infra-prow-build-trusted/prow-build-trusted/main.tf,1,// todo,// TODO: I think more people than me should have owner/edit access to this project,// TODO: I think more people than me should have owner/edit access to this project,"module ""project"" {
  source = ""../../modules/k8s-infra-gke-project""
  project_id            = local.project_id
  project_name          = local.project_id
}
",module,"module ""project"" {
  source = ""../../../modules/gke-project""
  project_id            = local.project_id
  project_name          = local.project_id
}
",module,34,,f9cdbcda7e1c4ec32148ade91be7cbec6bf5f2fc,e8ea80d6c56a1b2bf601b509470e6834a837d646,https://github.com/kubernetes/k8s.io/blob/f9cdbcda7e1c4ec32148ade91be7cbec6bf5f2fc/infra/gcp/clusters/k8s-infra-prow-build-trusted/prow-build-trusted/main.tf#L34,https://github.com/kubernetes/k8s.io/blob/e8ea80d6c56a1b2bf601b509470e6834a837d646/infra/gcp/clusters/projects/k8s-infra-prow-build-trusted/prow-build-trusted/main.tf,2020-05-06 14:16:39-07:00,2020-05-15 18:23:44-07:00,4,1,0,1,0,1,0,0,0,0
https://github.com/cattle-ops/terraform-aws-gitlab-runner,112,main.tf,main.tf,0,todo,# TODO Please explain how `agent_enable_asg_recreation` works,# TODO Please explain how `agent_enable_asg_recreation` works,"resource ""aws_autoscaling_group"" ""gitlab_runner_instance"" {
  # TODO Please explain how `agent_enable_asg_recreation` works
  name                      = var.runner_enable_asg_recreation ? ""${aws_launch_template.gitlab_runner_instance.name}-asg"" : ""${var.environment}-as-group""
  vpc_zone_identifier       = length(var.runner_worker_docker_machine_instance.subnet_ids) > 0 ? var.runner_worker_docker_machine_instance.subnet_ids : [var.subnet_id]
  min_size                  = ""1""
  max_size                  = ""1""
  desired_capacity          = ""1""
  health_check_grace_period = 0
  max_instance_lifetime     = var.runner_instance.max_lifetime_seconds
  enabled_metrics           = var.runner_instance.collect_autoscaling_metrics

  dynamic ""tag"" {
    for_each = local.agent_tags

    content {
      key                 = tag.key
      value               = tag.value
      propagate_at_launch = true
    }
  }

  launch_template {
    id      = aws_launch_template.gitlab_runner_instance.id
    version = aws_launch_template.gitlab_runner_instance.latest_version
  }

  instance_refresh {
    strategy = ""Rolling""
    preferences {
      min_healthy_percentage = 0
    }
    triggers = [""tag""]
  }

  timeouts {
    delete = var.runner_terraform_timeout_delete_asg
  }
  lifecycle {
    ignore_changes = [min_size, max_size, desired_capacity]
  }
}
",resource,"resource ""aws_autoscaling_group"" ""gitlab_runner_instance"" {
  # TODO Please explain how `agent_enable_asg_recreation` works
  name                      = var.runner_enable_asg_recreation ? ""${aws_launch_template.gitlab_runner_instance.name}-asg"" : ""${var.environment}-as-group""
  vpc_zone_identifier       = length(var.runner_worker_docker_machine_instance.subnet_ids) > 0 ? var.runner_worker_docker_machine_instance.subnet_ids : [var.subnet_id]
  min_size                  = ""1""
  max_size                  = ""1""
  desired_capacity          = ""1""
  health_check_grace_period = 0
  max_instance_lifetime     = var.runner_instance.max_lifetime_seconds
  enabled_metrics           = var.runner_instance.collect_autoscaling_metrics

  dynamic ""tag"" {
    for_each = local.agent_tags

    content {
      key                 = tag.key
      value               = tag.value
      propagate_at_launch = true
    }
  }

  launch_template {
    id      = aws_launch_template.gitlab_runner_instance.id
    version = aws_launch_template.gitlab_runner_instance.latest_version
  }

  instance_refresh {
    strategy = ""Rolling""
    preferences {
      min_healthy_percentage = 0
    }
    triggers = [""tag""]
  }

  timeouts {
    delete = var.runner_terraform_timeout_delete_asg
  }
  lifecycle {
    ignore_changes = [min_size, max_size, desired_capacity]
  }
}
",resource,174,152.0,c8a3b89c46f749214461bade8e1e6d161d0ef860,fec8c8a8d729f8d6076a38d8b063f1e14f4aa518,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/c8a3b89c46f749214461bade8e1e6d161d0ef860/main.tf#L174,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/fec8c8a8d729f8d6076a38d8b063f1e14f4aa518/main.tf#L152,2023-09-07 17:14:21+02:00,2024-05-10 11:02:08+02:00,10,0,1,0,0,0,0,0,0,0
https://github.com/compiler-explorer/infra,224,terraform/cloudfront.tf,terraform/cloudfront.tf,0,todo,# TODO change,"  name  = ""RateLimitCompilerExplorer"" # TODO change","resource ""aws_wafv2_web_acl"" ""rate_limit"" {
  name  = ""RateLimitCompilerExplorer"" # TODO change
  scope = ""CLOUDFRONT""
  default_action {
    allow {}
  }

  rule {
    name     = ""deny-ipv4""
    priority = 0
    action {
      block {}
    }
    statement {
      ip_set_reference_statement {
        arn = aws_wafv2_ip_set.banned-ipv4.arn
        ip_set_forwarded_ip_config {
          fallback_behavior = ""MATCH""
          header_name       = ""X-Forwarded-For""
          position          = ""ANY""
        }
      }
    }
    visibility_config {
      cloudwatch_metrics_enabled = true
      metric_name                = ""deny-ipv4""
      sampled_requests_enabled   = true
    }
  }

  rule {
    name     = ""deny-ipv6""
    priority = 1
    action {
      block {}
    }
    statement {
      ip_set_reference_statement {
        arn = aws_wafv2_ip_set.banned-ipv6.arn
        ip_set_forwarded_ip_config {
          fallback_behavior = ""MATCH""
          header_name       = ""X-Forwarded-For""
          position          = ""ANY""
        }
      }
    }
    visibility_config {
      cloudwatch_metrics_enabled = true
      metric_name                = ""deny-ipv6""
      sampled_requests_enabled   = true
    }
  }

  rule {
    name     = ""RateLimitPost""
    priority = 2
    action {
      block {}
    }
    statement {
      rate_based_statement {
        // Limit to this many per 5 minutes (300 seconds)
        limit              = 300
        aggregate_key_type = ""IP""
        scope_down_statement {
          byte_match_statement {
            positional_constraint = ""EXACTLY""
            search_string         = ""POST""
            field_to_match {
              method {}
            }
            text_transformation {
              priority = 0
              type     = ""NONE""
            }
          }
        }
      }
    }
    visibility_config {
      cloudwatch_metrics_enabled = true
      metric_name                = ""ce_rate_limited_blocked""
      sampled_requests_enabled   = true
    }
  }

  visibility_config {
    cloudwatch_metrics_enabled = true
    metric_name                = ""request_ok""
    sampled_requests_enabled   = true
  }
}
",resource,the block associated got renamed or deleted,,502,,5984967d8fe56d0296be70c38a7c15dd8e16615d,d9f55d45f1463a6f60c5e16029a41c538165536d,https://github.com/compiler-explorer/infra/blob/5984967d8fe56d0296be70c38a7c15dd8e16615d/terraform/cloudfront.tf#L502,https://github.com/compiler-explorer/infra/blob/d9f55d45f1463a6f60c5e16029a41c538165536d/terraform/cloudfront.tf,2022-10-06 19:03:41-05:00,2022-10-06 19:56:07-05:00,3,1,0,1,0,0,0,0,0,0
https://github.com/kubernetes/k8s.io,119,infra/gcp/clusters/projects/k8s-infra-ii-sandbox/provider.tf,infra/gcp/terraform/k8s-infra-ii-sandbox/provider.tf,1,// todo,"// TODO(spiffxp): the names not matching weirds me out a bit, it would be","// TODO(spiffxp): the names not matching weirds me out a bit, it would be 
 //                nice to rename the project at some point","terraform {

  backend ""gcs"" {
    bucket = ""k8s-infra-tf-sandbox-ii""
    // TODO(spiffxp): the names not matching weirds me out a bit, it would be
    //                nice to rename the project at some point
    prefix = ""k8s-infra-ii-sandbox""
  }


  required_providers {
    google = {
      source  = ""hashicorp/google""
      version = ""~> 3.46.0""
    }
    google-beta = {
      source  = ""hashicorp/google-beta""
      version = ""~> 3.46.0""
    }
  }
}
",terraform,"terraform {

  backend ""gcs"" {
    bucket = ""k8s-infra-tf-sandbox-ii""
    // TODO(spiffxp): the names not matching weirds me out a bit, it would be
    //                nice to rename the project at some point
    prefix = ""k8s-infra-ii-sandbox""
  }


  required_providers {
    google = {
      source  = ""hashicorp/google""
      version = ""~> 4.73.2""
    }
    google-beta = {
      source  = ""hashicorp/google-beta""
      version = ""~> 4.73.2""
    }
  }
}
",terraform,11,27.0,54c58f67e60fd834cbd42c91d6347711a0be1def,e1d6f748424d52ef72ce49ad744f7c10854717db,https://github.com/kubernetes/k8s.io/blob/54c58f67e60fd834cbd42c91d6347711a0be1def/infra/gcp/clusters/projects/k8s-infra-ii-sandbox/provider.tf#L11,https://github.com/kubernetes/k8s.io/blob/e1d6f748424d52ef72ce49ad744f7c10854717db/infra/gcp/terraform/k8s-infra-ii-sandbox/provider.tf#L27,2021-05-05 01:50:41-04:00,2023-07-18 00:00:05+02:00,9,0,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,65,modules/gke-cluster/main.tf,modules/gke-cluster-standard/main.tf,1,// todo,// TODO: support GPUs too,// TODO: support GPUs too,"resource ""google_container_cluster"" ""cluster"" {
  provider                    = google-beta
  project                     = var.project_id
  name                        = var.name
  description                 = var.description
  location                    = var.location
  node_locations              = length(var.node_locations) == 0 ? null : var.node_locations
  min_master_version          = var.min_master_version
  network                     = var.network
  subnetwork                  = var.subnetwork
  logging_service             = var.logging_service
  monitoring_service          = var.monitoring_service
  resource_labels             = var.labels
  default_max_pods_per_node   = var.default_max_pods_per_node
  enable_binary_authorization = var.enable_binary_authorization
  enable_intranode_visibility = var.enable_intranode_visibility
  enable_shielded_nodes       = var.enable_shielded_nodes
  enable_tpu                  = var.enable_tpu
  initial_node_count          = 1
  remove_default_node_pool    = true

  # node_config {}
  # NOTE: Default node_pool is deleted, so node_config (here) is extranneous.
  # Specify that node_config as an parameter to gke-nodepool module instead.

  # TODO(ludomagno): compute addons map in locals and use a single dynamic block
  addons_config {
    dns_cache_config {
      enabled = var.addons.dns_cache_config
    }
    http_load_balancing {
      disabled = ! var.addons.http_load_balancing
    }
    horizontal_pod_autoscaling {
      disabled = ! var.addons.horizontal_pod_autoscaling
    }
    network_policy_config {
      disabled = ! var.addons.network_policy_config
    }
    # beta addons
    # cloudrun is dynamic as it tends to trigger cluster recreation on change
    dynamic cloudrun_config {
      for_each = var.addons.istio_config.enabled && var.addons.cloudrun_config ? [""""] : []
      content {
        disabled = false
      }
    }
    istio_config {
      disabled = ! var.addons.istio_config.enabled
      auth     = var.addons.istio_config.tls ? ""AUTH_MUTUAL_TLS"" : ""AUTH_NONE""
    }
  }

  # TODO(ludomagno): support setting address ranges instead of range names
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#cluster_ipv4_cidr_block
  ip_allocation_policy {
    cluster_secondary_range_name  = var.secondary_range_pods
    services_secondary_range_name = var.secondary_range_services
  }

  # TODO(ludomagno): make optional, and support beta feature
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#daily_maintenance_window
  maintenance_policy {
    daily_maintenance_window {
      start_time = var.maintenance_start_time
    }
  }

  master_auth {
    client_certificate_config {
      issue_client_certificate = false
    }
  }

  dynamic master_authorized_networks_config {
    for_each = length(var.master_authorized_ranges) == 0 ? [] : list(var.master_authorized_ranges)
    iterator = ranges
    content {
      dynamic cidr_blocks {
        for_each = ranges.value
        iterator = range
        content {
          cidr_block   = range.value
          display_name = range.key
        }
      }
    }
  }

  dynamic network_policy {
    for_each = var.addons.network_policy_config ? [""""] : []
    content {
      enabled  = true
      provider = ""CALICO""
    }
  }

  dynamic private_cluster_config {
    for_each = local.is_private ? [var.private_cluster_config] : []
    iterator = config
    content {
      enable_private_nodes    = config.value.enable_private_nodes
      enable_private_endpoint = config.value.enable_private_endpoint
      master_ipv4_cidr_block  = config.value.master_ipv4_cidr_block
    }
  }

  # beta features

  dynamic authenticator_groups_config {
    for_each = var.authenticator_security_group == null ? [] : [""""]
    content {
      security_group = var.authenticator_security_group
    }
  }

  dynamic cluster_autoscaling {
    for_each = var.cluster_autoscaling.enabled ? [var.cluster_autoscaling] : []
    iterator = config
    content {
      enabled = true
      resource_limits {
        resource_type = ""cpu""
        minimum       = config.value.cpu_min
        maximum       = config.value.cpu_max
      }
      resource_limits {
        resource_type = ""memory""
        minimum       = config.value.memory_min
        maximum       = config.value.memory_max
      }
      // TODO: support GPUs too
    }
  }

  dynamic database_encryption {
    for_each = var.database_encryption.enabled ? [var.database_encryption] : []
    iterator = config
    content {
      state    = config.value.state
      key_name = config.value.key_name
    }
  }

  dynamic pod_security_policy_config {
    for_each = var.pod_security_policy != null ? [""""] : []
    content {
      enabled = var.pod_security_policy
    }
  }

  dynamic release_channel {
    for_each = var.release_channel != null ? [""""] : []
    content {
      channel = var.release_channel
    }
  }

  dynamic resource_usage_export_config {
    for_each = (
      var.resource_usage_export_config.enabled != null
      &&
      var.resource_usage_export_config.dataset != null
      ? [""""] : []
    )
    content {
      enable_network_egress_metering = var.resource_usage_export_config.enabled
      bigquery_destination {
        dataset_id = var.resource_usage_export_config.dataset
      }
    }
  }

  dynamic vertical_pod_autoscaling {
    for_each = var.vertical_pod_autoscaling == null ? [] : [""""]
    content {
      enabled = var.vertical_pod_autoscaling
    }
  }

  dynamic workload_identity_config {
    for_each = var.workload_identity ? [""""] : []
    content {
      identity_namespace = ""${var.project_id}.svc.id.goog""
    }
  }

}
",resource,"resource ""google_container_cluster"" ""cluster"" {
  provider    = google-beta
  project     = var.project_id
  name        = var.name
  description = var.description
  location    = var.location
  node_locations = (
    length(var.node_locations) == 0 ? null : var.node_locations
  )
  min_master_version          = var.min_master_version
  network                     = var.vpc_config.network
  subnetwork                  = var.vpc_config.subnetwork
  resource_labels             = var.labels
  default_max_pods_per_node   = var.max_pods_per_node
  enable_intranode_visibility = var.enable_features.intranode_visibility
  enable_l4_ilb_subsetting    = var.enable_features.l4_ilb_subsetting
  enable_shielded_nodes       = var.enable_features.shielded_nodes
  enable_fqdn_network_policy  = var.enable_features.fqdn_network_policy
  enable_tpu                  = var.enable_features.tpu
  initial_node_count          = 1
  remove_default_node_pool    = true
  deletion_protection         = var.deletion_protection
  datapath_provider = (
    var.enable_features.dataplane_v2
    ? ""ADVANCED_DATAPATH""
    : ""DATAPATH_PROVIDER_UNSPECIFIED""
  )

  # the default node pool is deleted here, use the gke-nodepool module instead.
  # the default node pool configuration is based on a shielded_nodes variable.
  node_config {
    service_account = var.service_account
    dynamic ""shielded_instance_config"" {
      for_each = var.enable_features.shielded_nodes ? [""""] : []
      content {
        enable_secure_boot          = true
        enable_integrity_monitoring = true
      }
    }
    tags = var.tags
  }

  addons_config {
    dns_cache_config {
      enabled = var.enable_addons.dns_cache
    }
    http_load_balancing {
      disabled = !var.enable_addons.http_load_balancing
    }
    horizontal_pod_autoscaling {
      disabled = !var.enable_addons.horizontal_pod_autoscaling
    }
    network_policy_config {
      disabled = !var.enable_addons.network_policy
    }
    cloudrun_config {
      disabled = !var.enable_addons.cloudrun
    }
    istio_config {
      disabled = var.enable_addons.istio == null
      auth = (
        try(var.enable_addons.istio.enable_tls, false) ? ""AUTH_MUTUAL_TLS"" : ""AUTH_NONE""
      )
    }
    gce_persistent_disk_csi_driver_config {
      enabled = var.enable_addons.gce_persistent_disk_csi_driver
    }
    gcp_filestore_csi_driver_config {
      enabled = var.enable_addons.gcp_filestore_csi_driver
    }
    gcs_fuse_csi_driver_config {
      enabled = var.enable_addons.gcs_fuse_csi_driver
    }
    kalm_config {
      enabled = var.enable_addons.kalm
    }
    config_connector_config {
      enabled = var.enable_addons.config_connector
    }
    gke_backup_agent_config {
      enabled = var.backup_configs.enable_backup_agent
    }
  }

  dynamic ""authenticator_groups_config"" {
    for_each = var.enable_features.groups_for_rbac != null ? [""""] : []
    content {
      security_group = var.enable_features.groups_for_rbac
    }
  }

  dynamic ""binary_authorization"" {
    for_each = var.enable_features.binary_authorization ? [""""] : []
    content {
      evaluation_mode = ""PROJECT_SINGLETON_POLICY_ENFORCE""
    }
  }

  dynamic ""cost_management_config"" {
    for_each = var.enable_features.cost_management == true ? [""""] : []
    content {
      enabled = true
    }
  }

  dynamic ""cluster_autoscaling"" {
    for_each = var.cluster_autoscaling == null ? [] : [""""]
    content {
      enabled = true

      autoscaling_profile = var.cluster_autoscaling.autoscaling_profile

      dynamic ""auto_provisioning_defaults"" {
        for_each = var.cluster_autoscaling.auto_provisioning_defaults != null ? [""""] : []
        content {
          boot_disk_kms_key = var.cluster_autoscaling.auto_provisioning_defaults.boot_disk_kms_key
          disk_size         = var.cluster_autoscaling.auto_provisioning_defaults.disk_size
          disk_type         = var.cluster_autoscaling.auto_provisioning_defaults.disk_type
          image_type        = var.cluster_autoscaling.auto_provisioning_defaults.image_type
          oauth_scopes      = var.cluster_autoscaling.auto_provisioning_defaults.oauth_scopes
          service_account   = var.cluster_autoscaling.auto_provisioning_defaults.service_account
          dynamic ""management"" {
            for_each = var.cluster_autoscaling.auto_provisioning_defaults.management != null ? [""""] : []
            content {
              auto_repair  = var.cluster_autoscaling.auto_provisioning_defaults.management.auto_repair
              auto_upgrade = var.cluster_autoscaling.auto_provisioning_defaults.management.auto_upgrade
            }
          }
          dynamic ""shielded_instance_config"" {
            for_each = var.cluster_autoscaling.auto_provisioning_defaults.shielded_instance_config != null ? [""""] : []
            content {
              enable_integrity_monitoring = var.cluster_autoscaling.auto_provisioning_defaults.shielded_instance_config.integrity_monitoring
              enable_secure_boot          = var.cluster_autoscaling.auto_provisioning_defaults.shielded_instance_config.secure_boot
            }
          }
        }
      }
      dynamic ""resource_limits"" {
        for_each = var.cluster_autoscaling.cpu_limits != null ? [""""] : []
        content {
          resource_type = ""cpu""
          minimum       = var.cluster_autoscaling.cpu_limits.min
          maximum       = var.cluster_autoscaling.cpu_limits.max
        }
      }
      dynamic ""resource_limits"" {
        for_each = var.cluster_autoscaling.mem_limits != null ? [""""] : []
        content {
          resource_type = ""memory""
          minimum       = var.cluster_autoscaling.mem_limits.min
          maximum       = var.cluster_autoscaling.mem_limits.max
        }
      }
      dynamic ""resource_limits"" {
        for_each = (
          try(var.cluster_autoscaling.gpu_resources, null) == null
          ? []
          : var.cluster_autoscaling.gpu_resources
        )
        iterator = gpu_resources
        content {
          resource_type = gpu_resources.value.resource_type
          minimum       = gpu_resources.value.min
          maximum       = gpu_resources.value.max
        }
      }
    }
  }

  dynamic ""database_encryption"" {
    for_each = var.enable_features.database_encryption != null ? [""""] : []
    content {
      state    = var.enable_features.database_encryption.state
      key_name = var.enable_features.database_encryption.key_name
    }
  }

  dynamic ""dns_config"" {
    for_each = var.enable_features.dns != null ? [""""] : []
    content {
      cluster_dns        = var.enable_features.dns.provider
      cluster_dns_scope  = var.enable_features.dns.scope
      cluster_dns_domain = var.enable_features.dns.domain
    }
  }

  dynamic ""gateway_api_config"" {
    for_each = var.enable_features.gateway_api ? [""""] : []
    content {
      channel = ""CHANNEL_STANDARD""
    }
  }

  dynamic ""ip_allocation_policy"" {
    for_each = var.vpc_config.secondary_range_blocks != null ? [""""] : []
    content {
      cluster_ipv4_cidr_block  = var.vpc_config.secondary_range_blocks.pods
      services_ipv4_cidr_block = var.vpc_config.secondary_range_blocks.services
      stack_type               = var.vpc_config.stack_type
    }
  }
  dynamic ""ip_allocation_policy"" {
    for_each = var.vpc_config.secondary_range_names != null ? [""""] : []
    content {
      cluster_secondary_range_name  = var.vpc_config.secondary_range_names.pods
      services_secondary_range_name = var.vpc_config.secondary_range_names.services
      stack_type                    = var.vpc_config.stack_type
    }
  }

  # Send GKE cluster logs from chosen sources to Cloud Logging.
  # System logs must be enabled if any other source is enabled.
  # This is validated by input variable validation rules.
  dynamic ""logging_config"" {
    for_each = var.logging_config.enable_system_logs ? [""""] : []
    content {
      enable_components = toset(compact([
        var.logging_config.enable_api_server_logs ? ""APISERVER"" : null,
        var.logging_config.enable_controller_manager_logs ? ""CONTROLLER_MANAGER"" : null,
        var.logging_config.enable_scheduler_logs ? ""SCHEDULER"" : null,
        ""SYSTEM_COMPONENTS"",
        var.logging_config.enable_workloads_logs ? ""WORKLOADS"" : null,
      ]))
    }
  }
  # Don't send any GKE cluster logs to Cloud Logging. Input variable validation
  # makes sure every other log source is false when enable_system_logs is false.
  dynamic ""logging_config"" {
    for_each = var.logging_config.enable_system_logs == false ? [""""] : []
    content {
      enable_components = []
    }
  }

  maintenance_policy {
    dynamic ""daily_maintenance_window"" {
      for_each = (
        try(var.maintenance_config.daily_window_start_time, null) != null
        ? [""""]
        : []
      )
      content {
        start_time = var.maintenance_config.daily_window_start_time
      }
    }
    dynamic ""recurring_window"" {
      for_each = (
        try(var.maintenance_config.recurring_window, null) != null
        ? [""""]
        : []
      )
      content {
        start_time = var.maintenance_config.recurring_window.start_time
        end_time   = var.maintenance_config.recurring_window.end_time
        recurrence = var.maintenance_config.recurring_window.recurrence
      }
    }
    dynamic ""maintenance_exclusion"" {
      for_each = (
        try(var.maintenance_config.maintenance_exclusions, null) == null
        ? []
        : var.maintenance_config.maintenance_exclusions
      )
      iterator = exclusion
      content {
        exclusion_name = exclusion.value.name
        start_time     = exclusion.value.start_time
        end_time       = exclusion.value.end_time
      }
    }
  }

  master_auth {
    client_certificate_config {
      issue_client_certificate = var.issue_client_certificate
    }
  }

  dynamic ""master_authorized_networks_config"" {
    for_each = var.vpc_config.master_authorized_ranges != null ? [""""] : []
    content {
      dynamic ""cidr_blocks"" {
        for_each = var.vpc_config.master_authorized_ranges
        iterator = range
        content {
          cidr_block   = range.value
          display_name = range.key
        }
      }
    }
  }

  dynamic ""mesh_certificates"" {
    for_each = var.enable_features.mesh_certificates != null ? [""""] : []
    content {
      enable_certificates = var.enable_features.mesh_certificates
    }
  }

  monitoring_config {
    enable_components = toset(compact([
      # System metrics is the minimum requirement if any other metrics are enabled. This is checked by input var validation.
      var.monitoring_config.enable_system_metrics ? ""SYSTEM_COMPONENTS"" : null,
      # Control plane metrics
      var.monitoring_config.enable_api_server_metrics ? ""APISERVER"" : null,
      var.monitoring_config.enable_controller_manager_metrics ? ""CONTROLLER_MANAGER"" : null,
      var.monitoring_config.enable_scheduler_metrics ? ""SCHEDULER"" : null,
      # Kube state metrics
      var.monitoring_config.enable_daemonset_metrics ? ""DAEMONSET"" : null,
      var.monitoring_config.enable_deployment_metrics ? ""DEPLOYMENT"" : null,
      var.monitoring_config.enable_hpa_metrics ? ""HPA"" : null,
      var.monitoring_config.enable_pod_metrics ? ""POD"" : null,
      var.monitoring_config.enable_statefulset_metrics ? ""STATEFULSET"" : null,
      var.monitoring_config.enable_storage_metrics ? ""STORAGE"" : null,
    ]))
    managed_prometheus {
      enabled = var.monitoring_config.enable_managed_prometheus
    }
  }

  # Dataplane V2 has built-in network policies
  dynamic ""network_policy"" {
    for_each = (
      var.enable_addons.network_policy && !var.enable_features.dataplane_v2
      ? [""""]
      : []
    )
    content {
      enabled  = true
      provider = ""CALICO""
    }
  }

  dynamic ""notification_config"" {
    for_each = var.enable_features.upgrade_notifications != null ? [""""] : []
    content {
      pubsub {
        enabled = true
        topic = (
          try(var.enable_features.upgrade_notifications.topic_id, null) != null
          ? var.enable_features.upgrade_notifications.topic_id
          : google_pubsub_topic.notifications[0].id
        )
      }
    }
  }

  dynamic ""private_cluster_config"" {
    for_each = (
      var.private_cluster_config != null ? [""""] : []
    )
    content {
      enable_private_nodes    = true
      enable_private_endpoint = var.private_cluster_config.enable_private_endpoint
      master_ipv4_cidr_block  = try(var.vpc_config.master_ipv4_cidr_block, null)
      master_global_access_config {
        enabled = var.private_cluster_config.master_global_access
      }
    }
  }

  dynamic ""pod_security_policy_config"" {
    for_each = var.enable_features.pod_security_policy ? [""""] : []
    content {
      enabled = var.enable_features.pod_security_policy
    }
  }

  dynamic ""release_channel"" {
    for_each = var.release_channel != null ? [""""] : []
    content {
      channel = var.release_channel
    }
  }

  dynamic ""resource_usage_export_config"" {
    for_each = (
      try(var.enable_features.resource_usage_export.dataset, null) != null
      ? [""""]
      : []
    )
    content {
      enable_network_egress_metering = (
        var.enable_features.resource_usage_export.enable_network_egress_metering
      )
      enable_resource_consumption_metering = (
        var.enable_features.resource_usage_export.enable_resource_consumption_metering
      )
      bigquery_destination {
        dataset_id = var.enable_features.resource_usage_export.dataset
      }
    }
  }

  dynamic ""vertical_pod_autoscaling"" {
    for_each = var.enable_features.vertical_pod_autoscaling ? [""""] : []
    content {
      enabled = var.enable_features.vertical_pod_autoscaling
    }
  }

  dynamic ""workload_identity_config"" {
    for_each = var.enable_features.workload_identity ? [""""] : []
    content {
      workload_pool = ""${var.project_id}.svc.id.goog""
    }
  }
  lifecycle {
    ignore_changes = [node_config]
  }
}
",resource,158,,de9825310c11ec5d1bc3cc6e9736253e8ced1021,0f446e89d49a9a801dc5ff932295c6ba476119b3,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/de9825310c11ec5d1bc3cc6e9736253e8ced1021/modules/gke-cluster/main.tf#L158,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/0f446e89d49a9a801dc5ff932295c6ba476119b3/modules/gke-cluster-standard/main.tf,2020-05-12 18:46:50+02:00,2023-11-10 12:39:50+01:00,69,1,1,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,979,infra/modular-examples/gcp/main.tf,infra/modular-examples/gcp/main.tf,0,# todo,"# TODO: this would be cleaner as env var, but creates a cycle:","# TODO: this would be cleaner as env var, but creates a cycle: 
 # Error: Cycle: module.psoxy.module.psoxy-bulk.local_file.todo-gcp-psoxy-bulk-test, module.psoxy.module.lookup_output.var.function_service_account_email (expand), module.psoxy.module.lookup_output.google_storage_bucket_iam_member.write_to_output_bucket, module.psoxy.module.lookup_output.output.bucket_name (expand), module.psoxy.module.lookup_output.var.bucket_name_prefix (expand), module.psoxy.module.lookup_output.google_storage_bucket.bucket, module.psoxy.module.lookup_output.google_storage_bucket_iam_member.accessors, module.psoxy.module.lookup_output (close), module.psoxy.module.psoxy-bulk.var.environment_variables (expand), module.psoxy.module.psoxy-bulk.google_cloudfunctions_function.function, module.psoxy.module.psoxy-bulk (close)","resource ""google_secret_manager_secret"" ""additional_transforms"" {
  for_each = local.inputs_to_build_lookups_for

  project   = var.gcp_project_id
  secret_id = ""${local.config_parameter_prefix}${upper(replace(each.key, ""-"", ""_""))}_ADDITIONAL_TRANSFORMS""

  replication {
    automatic = true
  }
}
",resource,,,343,0.0,4f41d480721ffa94079bbfb0cdcc181e22423300,f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e,https://github.com/Worklytics/psoxy/blob/4f41d480721ffa94079bbfb0cdcc181e22423300/infra/modular-examples/gcp/main.tf#L343,https://github.com/Worklytics/psoxy/blob/f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e/infra/modular-examples/gcp/main.tf#L0,2023-04-18 16:24:15-07:00,2023-06-16 14:08:45-07:00,13,2,0,1,0,1,0,0,0,0
https://github.com/uyuni-project/sumaform,1180,modules/libvirt/suse_manager/main.tf,modules/libvirt/suse_manager/main.tf,0,hack,"# HACK: work around ""conditional operator cannot be used with list values""","# HACK: work around ""conditional operator cannot be used with list values""","module ""suse_manager"" {
  source = ""../host""

  base_configuration = ""${var.base_configuration}""
  name = ""${var.name}""
  count = 1
  use_os_released_updates = ""${var.use_os_released_updates}""
  use_os_unreleased_updates = ""${var.use_os_unreleased_updates}""
  additional_repos = ""${var.additional_repos}""
  additional_repos_only = ""${var.additional_repos_only}""
  additional_certs = ""${var.additional_certs}""
  additional_packages = ""${var.additional_packages}""
  swap_file_size = ""${var.swap_file_size}""
  ssh_key_path = ""${var.ssh_key_path}""
  gpg_keys = ""${var.gpg_keys}""
  ipv6 = ""${var.ipv6}""
  connect_to_base_network = true
  connect_to_additional_network = false
  # HACK: work around ""conditional operator cannot be used with list values""
  roles = ""${split("","", var.register_to_server == ""null"" ? ""suse_manager_server"" : ""suse_manager_server,minion"")}""
  grains = <<EOF

product_version: ${var.product_version}
cc_username: ${var.base_configuration[""cc_username""]}
cc_password: ${var.base_configuration[""cc_password""]}
channels: [${join("","", var.channels)}]
cloned_channels: ${var.cloned_channels}
mirror: ${var.base_configuration[""mirror""]}
iss_master: ${var.iss_master}
iss_slave: ${var.iss_slave}
server: ${var.register_to_server}
auto_connect_to_master: ${var.auto_register}
susemanager:
  activation_key: ${var.activation_key}
smt: ${var.smt}
server_username: ${var.server_username}
server_password: ${var.server_password}
disable_firewall: ${var.disable_firewall}
allow_postgres_connections: ${var.allow_postgres_connections}
unsafe_postgres: ${var.unsafe_postgres}
java_debugging: ${var.java_debugging}
skip_changelog_import: ${var.skip_changelog_import}
browser_side_less: ${var.browser_side_less}
create_first_user: ${var.create_first_user}
mgr_sync_autologin: ${var.mgr_sync_autologin}
create_sample_channel: ${var.create_sample_channel}
create_sample_activation_key: ${var.create_sample_activation_key}
create_sample_bootstrap_script: ${var.create_sample_bootstrap_script}
publish_private_ssl_key: ${var.publish_private_ssl_key}
auto_accept: ${var.auto_accept}
monitored: ${var.monitored}
pts: ${var.pts}
pts_minion: ${var.pts_minion}
pts_locust: ${var.pts_locust}
pts_system_count: ${var.pts_system_count}
pts_system_prefix: ${var.pts_system_prefix}
apparmor: ${var.apparmor}
from_email: ${var.from_email}
traceback_email: ${var.traceback_email}
saltapi_tcpdump: ${var.saltapi_tcpdump}

EOF

  // Provider-specific variables
  image = ""${var.image == ""default"" ? lookup(var.images, var.product_version) : var.image}""
  memory = ""${var.memory}""
  vcpu = ""${var.vcpu}""
  running = ""${var.running}""
  mac = ""${var.mac}""
  additional_disk = ""${var.additional_disk}""
}
",module,"module ""suse_manager"" {
  source = ""../host""

  base_configuration            = var.base_configuration
  name                          = var.name
  use_os_released_updates       = var.use_os_released_updates
  use_os_unreleased_updates     = var.use_os_unreleased_updates
  additional_repos              = var.additional_repos
  additional_repos_only         = var.additional_repos_only
  additional_certs              = var.additional_certs
  additional_packages           = var.additional_packages
  swap_file_size                = var.swap_file_size
  ssh_key_path                  = var.ssh_key_path
  gpg_keys                      = var.gpg_keys
  ipv6                          = var.ipv6
  connect_to_base_network       = true
  connect_to_additional_network = false
  roles = var.register_to_server == null ? [""suse_manager_server""] : [""suse_manager_server"", ""minion""]

  grains = {
    product_version        = var.product_version
    cc_username            = var.base_configuration[""cc_username""]
    cc_password            = var.base_configuration[""cc_password""]
    channels               = var.channels
    wait_for_reposync      = var.wait_for_reposync
    cloned_channels        = var.cloned_channels
    mirror                 = var.base_configuration[""mirror""]
    iss_master             = var.iss_master
    iss_slave              = var.iss_slave
    server                 = var.register_to_server
    auto_connect_to_master = var.auto_register
    susemanager = {
      activation_key = var.activation_key
    }
    smt                            = var.smt
    server_username                = var.server_username
    server_password                = var.server_password
    disable_firewall               = var.disable_firewall
    allow_postgres_connections     = var.allow_postgres_connections
    unsafe_postgres                = var.unsafe_postgres
    java_debugging                 = var.java_debugging
    skip_changelog_import          = var.skip_changelog_import
    browser_side_less              = var.browser_side_less
    create_first_user              = var.create_first_user
    mgr_sync_autologin             = var.mgr_sync_autologin
    create_sample_channel          = var.create_sample_channel
    create_sample_activation_key   = var.create_sample_activation_key
    create_sample_bootstrap_script = var.create_sample_bootstrap_script
    publish_private_ssl_key        = var.publish_private_ssl_key
    disable_download_tokens        = var.disable_download_tokens
    auto_accept                    = var.auto_accept
    monitored                      = var.monitored
    pts                            = var.pts
    pts_minion                     = var.pts_minion
    pts_locust                     = var.pts_locust
    pts_system_count               = var.pts_system_count
    pts_system_prefix              = var.pts_system_prefix
    apparmor                       = var.apparmor
    from_email                     = var.from_email
    traceback_email                = var.traceback_email
    saltapi_tcpdump                = var.saltapi_tcpdump
    repository_disk_size           = var.repository_disk_size
    repository_disk_device         = ""vdb""
  }


  // Provider-specific variables
  image           = var.image == ""default"" || var.product_version == ""head"" ? var.images[var.product_version] : var.image
  memory          = var.memory
  vcpu            = var.vcpu
  running         = var.running
  mac             = var.mac
  additional_disk = var.repository_disk_size > 0 ? [{ volume_id = libvirt_volume.server_data_disk[0].id }] : []
}
",module,32,,281c7982a20f0d4ae89b7de0744ec51f16e53203,ba7df33db5944093ff582de14f377399f43cd9b1,https://github.com/uyuni-project/sumaform/blob/281c7982a20f0d4ae89b7de0744ec51f16e53203/modules/libvirt/suse_manager/main.tf#L32,https://github.com/uyuni-project/sumaform/blob/ba7df33db5944093ff582de14f377399f43cd9b1/modules/libvirt/suse_manager/main.tf,2019-09-27 08:59:18+02:00,2019-12-03 17:04:10+01:00,14,1,0,1,0,0,0,0,0,0
