Repo URL,Satd Comment Id,File Path Of First Occurence,File Path Of Last Occurence,renamed,Keyword,SATD Comment,context,bloc of first occurrence,bloc type of first occurrence,bloc of last occurrence,bloc type of last occurrence,SATD Comment Line Of First Occurence,SATD Comment Line Of Last Occurence,first Commit Hash,last Commit Hash,Link To The File Of First Occurence,Link To The File Of Last Occurence/When Adressed,Introduction Time,Last Occurence (even solved or not),number of commits,adressed ?,Computing Management Debt,IaC Code Debt,Dependency Management,Security Debt,Networking Debt,Environment-Based Configuration Debt,Monitoring and Logging Debt,Test Debt
https://github.com/kubernetes/k8s.io,204,infra/gcp/terraform/k8s-infra-prow-build-trusted/main.tf,infra/gcp/terraform/k8s-infra-prow-build-trusted/main.tf,0,// todo,// TODO: consider moving the project role binding resources into the,"// TODO: consider moving the project role binding resources into the 
 //       workload-identity-service-account module 
 // 
 // Some of the roles are assigned in bash or other terraform modules, so as 
 // to keep the permissions necessary to run this terraform module scoped to 
 // ""roles/owner"" for local.project_id ","module ""prow_build_cluster_sa"" {
  source            = ""../modules/workload-identity-service-account""
  project_id        = local.project_id
  name              = local.cluster_sa_name
  description       = ""default service account for pods in ${local.cluster_name}""
  cluster_namespace = local.pod_namespace
}
",module,the block associated got renamed or deleted,,58,,e7225f5825a089b4cc3e27beb4f430306d09103d,ad8d03744a189aff28baee8dff6437940e7d6464,https://github.com/kubernetes/k8s.io/blob/e7225f5825a089b4cc3e27beb4f430306d09103d/infra/gcp/terraform/k8s-infra-prow-build-trusted/main.tf#L58,https://github.com/kubernetes/k8s.io/blob/ad8d03744a189aff28baee8dff6437940e7d6464/infra/gcp/terraform/k8s-infra-prow-build-trusted/main.tf,2021-08-30 11:00:11-04:00,2021-09-29 06:28:42-07:00,7,1,0,1,0,1,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,138,modules/oke/autoscaler.tf,modules/oke/autoscaler.tf,0,fix,"# image fixed to E4.flex, no need to lookup","# image fixed to E4.flex, no need to lookup","resource ""oci_containerengine_node_pool"" ""autoscaler_pool"" {
  for_each       = var.enable_cluster_autoscaler == true ? var.autoscaler_pools : {}
  cluster_id     = oci_containerengine_cluster.k8s_cluster.id
  compartment_id = var.compartment_id
  depends_on     = [oci_containerengine_cluster.k8s_cluster]

  kubernetes_version = var.cluster_kubernetes_version
  name               = var.label_prefix == ""none"" ? each.key : ""${var.label_prefix}-${each.key}""

  freeform_tags = merge(var.freeform_tags[""node_pool""], { app = ""cluster-autoscaler"", pool = ""autoscaler"" })
  defined_tags  = merge(var.defined_tags[""node_pool""], { ""oke.pool"" = ""autoscaler"" })

  node_config_details {

    is_pv_encryption_in_transit_enabled = var.enable_pv_encryption_in_transit

    kms_key_id = var.node_pool_volume_kms_key_id

    # iterating over ADs
    dynamic ""placement_configs"" {
      iterator = ad_iterator
      for_each = [for n in lookup(each.value, ""placement_ads"", local.ad_numbers) :
      local.ad_number_to_name[n]]
      content {
        availability_domain = ad_iterator.value
        subnet_id           = var.cluster_subnets[""workers""]
      }
    }

    nsg_ids = var.worker_nsgs

    # flannel requires cni type only
    dynamic ""node_pool_pod_network_option_details"" {
      for_each = var.cni_type == ""flannel"" ? [1] : []
      content {
        cni_type = ""FLANNEL_OVERLAY""
      }
    }

    # native requires max pods/node, nsg ids, subnet ids
    dynamic ""node_pool_pod_network_option_details"" {
      for_each = var.cni_type == ""npn"" ? [1] : []
      content {
        cni_type          = ""OCI_VCN_IP_NATIVE""
        max_pods_per_node = var.max_pods_per_node
        # pick the 1st pod nsg here until https://github.com/oracle/terraform-provider-oci/issues/1662 is clarified and resolved
        pod_nsg_ids    = element(var.pod_nsgs, 0)
        pod_subnet_ids = tolist([var.cluster_subnets[""pods""]])
      }
    }

    size = 1

    freeform_tags = merge(var.freeform_tags[""node""], { app = ""cluster-autoscaler"", pool = ""autoscaler"" })

    # hardcoded defined tags are used to determine dynamic group membership and permissions
    defined_tags = merge(var.defined_tags[""node""], { ""oke.pool"" = ""autoscaler"" })
  }

  # setting shape
  node_shape = ""VM.Standard.E4.Flex""

  node_shape_config {
    ocpus         = 2
    memory_in_gbs = 32
  }

  # cloud-init
  node_metadata = {
    user_data = var.cloudinit_nodepool_common == """" && lookup(var.cloudinit_nodepool, each.key, null) == null ? data.cloudinit_config.worker.rendered : lookup(var.cloudinit_nodepool, each.key, null) != null ? filebase64(lookup(var.cloudinit_nodepool, each.key, null)) : filebase64(var.cloudinit_nodepool_common)
  }

  # optimized OKE images
  dynamic ""node_source_details"" {
    for_each = var.node_pool_image_type == ""oke"" ? [1] : []
    content {
      boot_volume_size_in_gbs = lookup(each.value, ""boot_volume_size"", 50)

      # image fixed to E4.flex, no need to lookup
      image_id    = (element([for source in local.node_pool_image_ids : source.image_id if length(regexall(""Oracle-Linux-${var.node_pool_os_version}-20[0-9]*.*-OKE-${local.k8s_version_only}"", source.source_name)) > 0], 0))
      source_type = data.oci_containerengine_node_pool_option.node_pool_options.sources[0].source_type
    }
  }

  # OCI platform images
  dynamic ""node_source_details"" {
    for_each = var.node_pool_image_type == ""platform"" ? [1] : []
    content {
      boot_volume_size_in_gbs = lookup(each.value, ""boot_volume_size"", 50)
      # image fixed to E4.flex, no need to lookup
      image_id    = element([for source in local.node_pool_image_ids : source.image_id if length(regexall(""^(Oracle-Linux-${var.node_pool_os_version}-\\d{4}.\\d{2}.\\d{2}-[0-9]*)$"", source.source_name)) > 0], 0)
      source_type = data.oci_containerengine_node_pool_option.node_pool_options.sources[0].source_type
    }
  }

  # custom images 
  dynamic ""node_source_details"" {
    for_each = var.node_pool_image_type == ""custom"" ? [1] : []
    content {
      boot_volume_size_in_gbs = lookup(each.value, ""boot_volume_size"", 50)
      image_id                = var.node_pool_image_id
      source_type             = data.oci_containerengine_node_pool_option.node_pool_options.sources[0].source_type
    }
  }

  ssh_public_key = (var.ssh_public_key != """") ? var.ssh_public_key : (var.ssh_public_key_path != ""none"") ? file(var.ssh_public_key_path) : """"

  # do not destroy the node pool if the kubernetes version has changed as part of the upgrade
  lifecycle {
    ignore_changes = [kubernetes_version]
  }

  # initial node labels for the autoscaler pool
  initial_node_labels {
    key   = ""app""
    value = ""cluster-autoscaler""
  }
}
",resource,,,94,0.0,788f33fd036093de6591157ca5c9f96f3f990e41,9910c5654ac76c0235b0e7ff2e98137cc90226d3,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/788f33fd036093de6591157ca5c9f96f3f990e41/modules/oke/autoscaler.tf#L94,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/9910c5654ac76c0235b0e7ff2e98137cc90226d3/modules/oke/autoscaler.tf#L0,2023-02-16 14:25:03+11:00,2023-10-25 16:40:02+11:00,2,2,1,1,0,0,0,0,0,0
https://github.com/Azure/az-hop,185,tf/variables_local.tf,tf/variables_local.tf,0,implemented,# Lustre - AMLFS not implemented for TF,# Lustre - AMLFS not implemented for TF,"locals {
    # azure environment
    public_cloud_endpoints = {
        KeyVaultSuffix =  ""vault.azure.net""
        BlobStorageSuffix = ""blob.core.windows.net""
        FileStorageSuffix = ""file.core.windows.net""
        MariaDBPrivateLink = ""privatelink.mariadb.database.azure.com""
    }
    usgov_cloud_endpoints = {
        KeyVaultSuffix =  ""vault.usgovcloudapi.net""
        BlobStorageSuffix = ""blob.core.usgovcloudapi.net""
        FileStorageSuffix = ""file.core.usgovcloudapi.net""
        MariaDBPrivateLink = ""privatelink.mariadb.database.usgovcloudapi.net""
    }
    azure_endpoints = {
        AZUREPUBLICCLOUD = local.public_cloud_endpoints
        AZUREUSGOVERNMENTCLOUD = local.usgov_cloud_endpoints
    }
    azure_environment = var.AzureEnvironment
    key_vault_suffix = local.azure_endpoints[local.azure_environment].KeyVaultSuffix #var.KeyVaultSuffix
    blob_storage_suffix = local.azure_endpoints[local.azure_environment].BlobStorageSuffix #var.BlobStorageSuffix

    # azurerm_client_config contains empty values for Managed Identity so use variables instead
    tenant_id = var.tenant_id
    logged_user_objectId = var.logged_user_objectId

    # config files and directories
    packer_root_dir = ""${path.cwd}/packer""
    playbook_root_dir = ""${path.cwd}/playbooks""
    playbooks_template_dir = ""${path.root}/templates""
    configuration_file=""${path.cwd}/config.yml""
    configuration_yml=yamldecode(file(local.configuration_file))
    
    # Load parameters from the configuration file
    location = local.configuration_yml[""location""]
    resource_group = local.configuration_yml[""resource_group""]
    extra_tags = try(local.configuration_yml[""tags""], null)
    common_tags = {
        CreatedBy = var.CreatedBy
        CreatedOn = timestamp()
    }

    # the PUID for telemetry is meant to be unique and identifies azhop, so it should not be changed
    telem_azhop_puid  = ""58d16d1a-5b7c-11ed-8042-00155d5d7a47""

    # local to determine if the user chose to disable telemetry of azhop
    optout_telemetry = try(local.configuration_yml[""optout_telemetry""], false)

    telem_azhop_name = substr(
        format(
            ""pid-%s"",
            local.telem_azhop_puid
        ),
        0,
        64
    )

    # empty arm template to create the telemetry resource
    telem_arm_subscription_template_content = <<TEMPLATE
    {
        ""$schema"": ""https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json#"",
        ""contentVersion"": ""1.0.0.0"",
        ""parameters"": {},
        ""variables"": {},
        ""resources"": [],
        ""outputs"": {
            ""telemetry"": {
                ""type"": ""String"",
                ""value"": ""For more information, see https://azure.github.io/az-hop/deploy/telemetry.html""
            }
        }
    }
    TEMPLATE

    # Log Analytics
    create_log_analytics_workspace = try(local.configuration_yml[""log_analytics""][""create""], false)
    log_analytics_name = try(local.configuration_yml[""log_analytics""][""name""], null)
    log_analytics_resource_group = try(local.configuration_yml[""log_analytics""][""resource_group""], null)
    log_analytics_subscription_id = try(local.configuration_yml[""log_analytics""][""subscription_id""], data.azurerm_subscription.primary.subscription_id)
    log_analytics_workspace_id = try(""/subscriptions/${local.log_analytics_subscription_id}/resourceGroups/${local.log_analytics_resource_group}/providers/Microsoft.OperationalInsights/workspaces/${local.log_analytics_name}"", null)
    use_existing_ws = ( !local.create_log_analytics_workspace && local.log_analytics_workspace_id != null )  ? true : false
     
    monitor = ( local.create_log_analytics_workspace || local.use_existing_ws ) ? true : false
    ama_install = try(local.configuration_yml[""monitoring""][""azure_monitor_agent""], true) && local.monitor ? true : false
    create_grafana = try(local.configuration_yml[""monitoring""][""grafana""], true)

    alert_email = try(local.configuration_yml[""alerting""][""admin_email""], ""admin.mail@contoso.com"")

    #For alerting to be enabled - the analytics workspace needs to be created since log alerts are leveraged. 
    #We also need to ensure that we have an email to send alerts to.  
    create_alerts = local.monitor && local.alert_email != ""admin.mail@contoso.com"" && try(local.configuration_yml[""alerting""][""enabled""], false) ? true : false
    anf_vol_threshold = try(local.configuration_yml[""anf""][""alert_threshold""], 80)  # default to 80% if not specified 

    # will be used with a KQL query that checks the free space percentage of local volumes
    # if the user wants to create an alert when local volumes are 80% full, then the free space percentage should be 20%
    local_vol_threshold = 100 - try(local.configuration_yml[""alerting""][""local_volume_threshold""], 20) 

    mounts = try(local.configuration_yml[""mounts""], {})
    mountpoints =  [ for mount in local.mounts : mount.mountpoint ]
    mountpoints_str = ""[ ${join("","", [for mp in local.mountpoints : format(""%q"", mp)])} ]"" //necessary to build generic KQL query on local volumes

    # Active Directory values
    # Updates the assumptions to the possibility that DNS may not point to Active Directory when using the customer provided AD.
    create_ad             = !try(local.configuration_yml[""domain""].use_existing_dc, false) && (try(local.configuration_yml[""authentication""].user_auth, ""ad"") == ""ad"")
    use_existing_ad       = try(local.configuration_yml[""domain""].use_existing_dc, false)
    create_dns_records    = local.create_ad || local.use_existing_ad
    domain_name           = local.use_existing_ad ? local.configuration_yml[""domain""].name : ""hpc.azure""
    domain_join_user      = local.use_existing_ad ? local.configuration_yml[""domain""].domain_join_user.username : local.admin_username
    domain_join_password  = local.use_existing_ad ? data.azurerm_key_vault_secret.domain_join_password[0].value : random_password.password.result
    domain_join_ou        = local.use_existing_ad ? local.configuration_yml[""domain""].domain_join_ou : ""CN=Computers""
    ad_ha                 = try(local.configuration_yml[""ad""].high_availability, false)
    domain_controlers     = local.use_existing_ad ? zipmap(local.configuration_yml[""domain""].existing_dc_details.domain_controller_names, local.configuration_yml[""domain""].existing_dc_details.domain_controller_names) : (local.ad_ha ? {ad=local.ad_name, ad2=local.ad2_name} : {ad=local.ad_name})
    ldap_server           = local.use_existing_ad ? local.configuration_yml[""domain""].existing_dc_details.domain_controller_names[0]     : local.ad_name
    private_dns_servers   = local.use_existing_ad ? local.configuration_yml[""domain""].existing_dc_details.private_dns_servers            : (local.create_ad ? (local.ad_ha ? [azurerm_network_interface.ad-nic[0].private_ip_address, azurerm_network_interface.ad2-nic[0].private_ip_address] : [azurerm_network_interface.ad-nic[0].private_ip_address]) : [])
    domain_controller_ips = local.use_existing_ad ? local.configuration_yml[""domain""].existing_dc_details.domain_controller_ip_addresses : (local.create_ad ? (local.ad_ha ? [azurerm_network_interface.ad-nic[0].private_ip_address, azurerm_network_interface.ad2-nic[0].private_ip_address] : [azurerm_network_interface.ad-nic[0].private_ip_address]) : [])

    # Use a linux custom image reference if the linux_base_image is defined and contains "":""
    use_linux_image_reference = try(length(split("":"", local.configuration_yml[""linux_base_image""])[1])>0, false)
    # Use a linux custom image reference if the linux_base_image is defined and contains "":""
    use_windows_image_reference = try(length(split("":"", local.configuration_yml[""windows_base_image""])[1])>0, false)
    # Use a linux custom image reference if the linux_base_image is defined and contains "":""
    use_cyclecloud_image_reference = try(length(split("":"", local.configuration_yml[""cyclecloud""][""image""])[1])>0, false)

    linux_base_image_reference = {
        publisher = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[0] : ""OpenLogic""
        offer     = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[1] : ""CentOS""
        sku       = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[2] : ""7_9-gen2""
        version   = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[3] : ""latest""
    }
    windows_base_image_reference = {
        publisher = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[0] : ""MicrosoftWindowsServer""
        offer     = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[1] : ""WindowsServer""
        sku       = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[2] : ""2019-Datacenter-smalldisk""
        version   = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[3] : ""latest""
    }
    cyclecloud_image_reference = {
        publisher = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[0] : ""OpenLogic""
        offer     = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[1] : ""CentOS""
        sku       = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[2] : ""7_9-gen2""
        version   = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[3] : ""latest""
    }

    # Use a linux custom image id if the linux_base_image is defined and contains ""/""
    use_linux_image_id = try(length(split(""/"", local.configuration_yml[""linux_base_image""])[1])>0, false)
    linux_image_id = local.use_linux_image_id ? local.configuration_yml[""linux_base_image""] : null

    # Use a windows custom image id if the windows_base_image is defined and contains ""/""
    use_windows_image_id = try(length(split(""/"", local.configuration_yml[""windows_base_image""])[1])>0, false)
    windows_image_id = local.use_windows_image_id ? local.configuration_yml[""windows_base_image""] : null

    # Use a cyclecloud custom image id if the cyclecloud_base_image is defined and contains ""/""
    use_cyclecloud_image_id = try(length(split(""/"", local.configuration_yml[""cyclecloud""][""image""])[1])>0, false)
    cyclecloud_image_id = local.use_cyclecloud_image_id ? local.configuration_yml[""cyclecloud""][""image""] : null

    _empty_image_plan = {}
    _linux_base_image_plan = {
        publisher = try(split("":"", local.configuration_yml[""linux_base_plan""])[0], """")
        product   = try(split("":"", local.configuration_yml[""linux_base_plan""])[1], """")
        name      = try(split("":"", local.configuration_yml[""linux_base_plan""])[2], """")
    }
    linux_image_plan = try( length(local._linux_base_image_plan.publisher) > 0 ? local._linux_base_image_plan : local._empty_image_plan, local._empty_image_plan)

    _cyclecloud_image_plan = {
        publisher = try(split("":"", local.configuration_yml[""cyclecloud""][""plan""])[0], """")
        product   = try(split("":"", local.configuration_yml[""cyclecloud""][""plan""])[1], """")
        name      = try(split("":"", local.configuration_yml[""cyclecloud""][""plan""])[2], """")
    }
    cyclecloud_image_plan = try( length(local._cyclecloud_image_plan.publisher) > 0 ? local._cyclecloud_image_plan : local._empty_image_plan, local._empty_image_plan)


    # Create the RG if not using an existing RG and (creating a VNET or when reusing a VNET in another resource group)
    use_existing_rg = try(local.configuration_yml[""use_existing_rg""], false)
    create_rg = (!local.use_existing_rg) && (local.create_vnet || try(split(""/"", local.vnet_id)[4], local.resource_group) != local.resource_group)

    # ANF
    create_anf = try(local.configuration_yml[""anf""][""create""], false)
    anf_size=try(local.configuration_yml[""anf""][""homefs_size_tb""], 4)
    anf_service_level = try(local.configuration_yml[""anf""][""homefs_service_level""], ""Standard"")
    anf_dual_protocol = try(local.configuration_yml[""anf""][""dual_protocol""], false)

    #Azure Files
    create_nfsfiles = try(local.configuration_yml[""azurefiles""][""create""], false)
    azure_files_size= try(local.configuration_yml[""azurefiles""][""size_gb""], 1024)

    # Home Directory
    homedir_type = try(local.configuration_yml[""mounts""][""home""][""type""], ""existing"")
    config_nfs_home_ip = local.configuration_yml[""mounts""][""home""][""server""]
    config_nfs_home_path = local.configuration_yml[""mounts""][""home""][""export""]
    config_nfs_home_opts = local.configuration_yml[""mounts""][""home""][""options""]

    homedir_mountpoint = try(local.configuration_yml[""mounts""][""home""][""mountpoint""], ""/anfhome"")

    admin_username = local.configuration_yml[""admin_user""]
    key_vault_readers = try(local.configuration_yml[""key_vault_readers""], null)

    # Resource names
    scheduler_name = try(local.configuration_yml[""scheduler""][""name""], ""scheduler"")
    ccportal_name = try(local.configuration_yml[""cyclecloud""][""name""], ""ccportal"")
    ondemand_name = try(local.configuration_yml[""ondemand""][""name""], ""ondemand"")
    grafana_name = try(local.configuration_yml[""grafana""][""name""], ""grafana"")
    jumpbox_name = try(local.configuration_yml[""jumpbox""][""name""], ""jumpbox"")
    ad_name = try(local.configuration_yml[""ad""][""name""], ""ad"")
    ad2_name = try(local.configuration_yml[""ad""][""ha_name""], ""ad2"")

    key_vault_name = try(local.configuration_yml[""azure_key_vault""][""name""], format(""%s%s"", ""kv"", random_string.resource_postfix.result))
    storage_account_name = try(local.configuration_yml[""azure_storage_account""][""name""], ""azhop${random_string.resource_postfix.result}"")
    mariadb_name = try(local.configuration_yml[""database""][""name""], ""azhop-${random_string.resource_postfix.result}"")

    # Lustre - AMLFS not implemented for TF
    lustre_enabled = false

    # Use a jumpbox when defined
    jumpbox_enabled = try(length(local.configuration_yml[""jumpbox""]) > 0, false)

    # Queue manager
    queue_manager = try(local.configuration_yml[""queue_manager""], ""openpbs"")

    # Create Database
    create_database  = ( try(local.configuration_yml[""slurm""].accounting_enabled, false) ) && (! local.use_existing_database)
    use_existing_database = try(length(local.configuration_yml[""database""].fqdn) > 0 ? true : false, false)
    database_user = local.create_database ? ""sqladmin"" : (local.use_existing_database ? try(local.configuration_yml[""database""].user, """") : """")
    mariadb_private_dns_zone = local.azure_endpoints[local.azure_environment].MariaDBPrivateLink

    create_sig = try(local.configuration_yml[""image_gallery""][""create""], false)
    
    # VNET
    create_vnet = try(length(local.vnet_id) > 0 ? false : true, true)
    vnet_id = try(local.configuration_yml[""network""][""vnet""][""id""], null)

    # VNET Peering
    vnet_peering = try(tolist(local.configuration_yml[""network""][""peering""]), [])

    # Lockdown scenario
    locked_down_network = try(local.configuration_yml[""locked_down_network""][""enforce""], false)
    grant_access_from   = try(local.configuration_yml[""locked_down_network""][""grant_access_from""], [])
    allow_public_ip     = try(local.configuration_yml[""locked_down_network""][""public_ip""], true)
    jumpbox_ssh_port    = try(local.configuration_yml[""jumpbox""][""ssh_port""], ""22"")
    # subnets
    _subnets = {
        frontend = ""frontend"",
        admin = ""admin"",
        netapp = ""netapp"",
        compute = ""compute""
    }

    # Create subnet if required. If not specified create only if vnet is created
    create_frontend_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""frontend""][""create""], local.create_vnet )
    create_admin_subnet    = try(local.configuration_yml[""network""][""vnet""][""subnets""][""admin""][""create""], local.create_vnet )
    create_netapp_subnet   = try(local.configuration_yml[""network""][""vnet""][""subnets""][""netapp""][""create""], local.create_vnet )
    create_compute_subnet  = try(local.configuration_yml[""network""][""vnet""][""subnets""][""compute""][""create""], local.create_vnet )

    ad_subnet        = try(local.configuration_yml[""network""][""vnet""][""subnets""][""ad""], null)
    no_ad_subnet     = try(length(local.ad_subnet) > 0 ? false : true, true)
    create_ad_subnet = try(local.ad_subnet[""create""], (local.create_ad ? local.create_vnet : false))

    bastion_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""bastion""], null)
    no_bastion_subnet = try(length(local.bastion_subnet) > 0 ? false : true, true )
    create_bastion_subnet  = try(local.bastion_subnet[""create""], local.create_vnet )

    gateway_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""gateway""], null)
    no_gateway_subnet = try(length(local.gateway_subnet) > 0 ? false : true, true )
    create_gateway_subnet  = try(local.gateway_subnet[""create""], local.create_vnet )

    outbounddns_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""outbounddns""], null)
    no_outbounddns_subnet = try(length(local.outbounddns_subnet) > 0 ? false : true, true )
    create_outbounddns_subnet  = try(local.outbounddns_subnet[""create""], local.create_vnet ? (local.no_outbounddns_subnet ? false : true) : false )

    dns_forwarders = try(local.configuration_yml[""dns""][""forwarders""], [])
    create_dnsfw_rules = length(local.dns_forwarders) > 0 ? true : false

    subnets = merge(local._subnets, 
                    local.no_bastion_subnet ? {} : {bastion = ""AzureBastionSubnet""},
                    local.no_gateway_subnet ? {} : {gateway = ""GatewaySubnet""},
                    local.no_outbounddns_subnet ? {} : {outbounddns = ""outbounddns""}
                    )

    # Application Security Groups
    create_nsg = try(local.configuration_yml[""network""][""create_nsg""], local.create_vnet )
    # If create NSG then use the local resource group otherwise use the configured one. Default to local resource group
    asg_resource_group = local.create_nsg ? local.resource_group : try(length(local.configuration_yml[""network""][""asg""][""resource_group""]) > 0 ? local.configuration_yml[""network""][""asg""][""resource_group""] : local.resource_group, local.resource_group )

    _default_asgs = {
        asg-ssh = ""asg-ssh""
        asg-rdp = ""asg-rdp""
        asg-jumpbox = ""asg-jumpbox""
        asg-ad = ""asg-ad""
        asg-ad-client = ""asg-ad-client""
        asg-lustre-client = ""asg-lustre-client""
        asg-pbs = ""asg-pbs""
        asg-pbs-client = ""asg-pbs-client""
        asg-cyclecloud = ""asg-cyclecloud""
        asg-cyclecloud-client = ""asg-cyclecloud-client""
        asg-nfs-client = ""asg-nfs-client""
        asg-telegraf = ""asg-telegraf""
        asg-grafana = ""asg-grafana""
        asg-robinhood = ""asg-robinhood""
        asg-ondemand = ""asg-ondemand""
        asg-deployer = ""asg-deployer""
        asg-mariadb-client = ""asg-mariadb-client""
    }
    #asgs = local.create_nsg ? local._default_asgs :  try(local.configuration_yml[""network""][""asg""][""names""], local._default_asgs)
    asgs = try(local.configuration_yml[""network""][""asg""][""names""], local._default_asgs)
    #asgs = { for v in local.default_asgs : v => v }
    empty_array = []
    empty_map = { for v in local.empty_array : v => v }

    # VM name to list of ASGs associations
    # TODO : Add mapping for names
    asg_associations = {
        ad        = [""asg-ad"", ""asg-rdp"", ""asg-ad-client""] # asg-ad-client will allow the secondary DC scenario
        ccportal  = [""asg-ssh"", ""asg-cyclecloud"", ""asg-telegraf"", ""asg-ad-client""]
        grafana   = [""asg-ssh"", ""asg-grafana"", ""asg-ad-client"", ""asg-telegraf"", ""asg-nfs-client""]
        jumpbox   = [""asg-ssh"", ""asg-jumpbox"", ""asg-ad-client"", ""asg-telegraf"", ""asg-nfs-client""]
        ondemand  = [""asg-ssh"", ""asg-ondemand"", ""asg-ad-client"", ""asg-nfs-client"", ""asg-pbs-client"", ""asg-lustre-client"", ""asg-telegraf"", ""asg-cyclecloud-client"", ""asg-mariadb-client""]
        robinhood = [""asg-ssh"", ""asg-robinhood"", ""asg-lustre-client"", ""asg-telegraf""]
        scheduler = [""asg-ssh"", ""asg-pbs"", ""asg-ad-client"", ""asg-cyclecloud-client"", ""asg-nfs-client"", ""asg-telegraf"", ""asg-mariadb-client""]
    }

    # Open ports for NSG TCP rules
    # ANF and SMB https://docs.microsoft.com/en-us/azure/azure-netapp-files/create-active-directory-connections
    nsg_destination_ports = {
        All = [""0-65535""]
        Bastion = [""22"", ""3389""]
        Web = [""443"", ""80""]
        Ssh    = [""22""]
        Public_Ssh = [local.jumpbox_ssh_port]
        # DNS, Kerberos, RpcMapper, Ldap, Smb, KerberosPass, LdapSsl, LdapGc, LdapGcSsl, AD Web Services, RpcSam
        DomainControlerTcp = [""53"", ""88"", ""135"", ""389"", ""445"", ""464"", ""636"", ""3268"", ""3269"", ""9389"", ""49152-65535""]
        # DNS, Kerberos, W32Time, NetBIOS, Ldap, KerberosPass, LdapSsl
        DomainControlerUdp = [""53"", ""88"", ""123"", ""138"", ""389"", ""464"", ""636""]
        # Web, NoVNC, WebSockify
        NoVnc = [""80"", ""443"", ""5900-5910"", ""61001-61010""]
        Dns = [""53""]
        Rdp = [""3389""]
        Pbs = [""6200"", ""15001-15009"", ""17001"", ""32768-61000"", ""6817-6819""]
        Slurmd = [""6818""]
        Lustre = [""635"", ""988""]
        Nfs = [""111"", ""635"", ""2049"", ""4045"", ""4046""]
        SMB = [""445""]
        Telegraf = [""8086""]
        Grafana = [""3000""]
        # HTTPS, AMQP
        CycleCloud = [""9443"", ""5672""],
        # MariaDB
        MariaDB = [""3306"", ""33060""],
        # WinRM
        WinRM = [""5985"", ""5986""]
    }

    #Replace the AD ASG with domain controller IP addresses when customer is bringing their own AD
    #use an indexing concept since we can't substitute a list for a string
    ad_nsg_index = local.use_existing_ad ? ""ips/dc_ips"" : ""asg/asg-ad""
    ips = {
        dc_ips = local.domain_controller_ips
    }

    # Array of NSG rules to be applied on the common NSG
    # NsgRuleName = [priority, direction, access, protocol, destination_port_range, source, destination]
    #   - priority               : integer value from 100 to 4096
    #   - direction              : Inbound, Outbound
    #   - access                 : Allow, Deny
    #   - protocol               : Tcp, Udp, *
    #   - destination_port_range : name of one of the nsg_destination_ports defined above
    #   - source                 : asg/<asg-name>, subnet/<subnet-name>, tag/<tag-name>. tag-name = any Azure tags like Internet, VirtualNetwork, AzureLoadBalancer, ...
    #   - destination            : same as source
    _nsg_rules = {
        # ================================================================================================================================================================
        #                          ###
        #                           #     #    #  #####    ####   #    #  #    #  #####
        #                           #     ##   #  #    #  #    #  #    #  ##   #  #    #
        #                           #     # #  #  #####   #    #  #    #  # #  #  #    #
        #                           #     #  # #  #    #  #    #  #    #  #  # #  #    #
        #                           #     #   ##  #    #  #    #  #    #  #   ##  #    #
        #                          ###    #    #  #####    ####    ####   #    #  #####
        # ================================================================================================================================================================
        # AD communication
        AllowAdServerTcpIn        = [""220"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdServerUdpIn        = [""230"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdClientTcpIn        = [""240"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdClientUdpIn        = [""250"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdServerComputeTcpIn = [""260"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdServerComputeUdpIn = [""270"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdClientComputeTcpIn = [""280"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdClientComputeUdpIn = [""290"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdServerNetappTcpIn  = [""300"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""subnet/netapp"", local.ad_nsg_index],
        AllowAdServerNetappUdpIn  = [""310"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""subnet/netapp"", local.ad_nsg_index],

        # SSH internal rules
        AllowSshFromJumpboxIn       = [""320"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-jumpbox"",   ""asg/asg-ssh""],
        AllowSshFromComputeIn       = [""330"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""subnet/compute"",    ""asg/asg-ssh""],
        AllowSshFromDeployerIn      = [""340"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",  ""asg/asg-ssh""], # Only in a deployer VM scenario
        AllowDeployerToPackerSshIn  = [""350"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",  ""subnet/admin""], # Only in a deployer VM scenario
        AllowSshToComputeIn         = [""360"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-ssh"",       ""subnet/compute""],
        AllowSshComputeComputeIn    = [""365"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""subnet/compute"",    ""subnet/compute""],

        # PBS
        AllowPbsIn                  = [""369"", ""Inbound"", ""Allow"", ""*"",   ""Pbs"",                ""asg/asg-pbs"",        ""asg/asg-pbs-client""],
        AllowPbsClientIn            = [""370"", ""Inbound"", ""Allow"", ""*"",   ""Pbs"",                ""asg/asg-pbs-client"", ""asg/asg-pbs""],
        AllowPbsComputeIn           = [""380"", ""Inbound"", ""Allow"", ""*"",   ""Pbs"",                ""asg/asg-pbs"",        ""subnet/compute""],
        AllowComputePbsClientIn     = [""390"", ""Inbound"", ""Allow"", ""*"",   ""Pbs"",                ""subnet/compute"",     ""asg/asg-pbs-client""],
        AllowComputePbsIn           = [""400"", ""Inbound"", ""Allow"", ""*"",   ""Pbs"",                ""subnet/compute"",     ""asg/asg-pbs""],
        AllowComputeComputePbsIn    = [""401"", ""Inbound"", ""Allow"", ""*"",   ""Pbs"",                ""subnet/compute"",     ""subnet/compute""],

        # SLURM
        AllowComputeSlurmIn         = [""405"", ""Inbound"", ""Allow"", ""*"",   ""Slurmd"",             ""asg/asg-ondemand"",    ""subnet/compute""],

        # Lustre
        AllowLustreClientIn         = [""410"", ""Inbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""asg/asg-lustre-client"", ""subnet/admin""],
        AllowLustreClientComputeIn  = [""420"", ""Inbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""subnet/compute"",        ""subnet/admin""],

        # CycleCloud
        AllowCycleWebIn             = [""440"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                ""asg/asg-ondemand"",          ""asg/asg-cyclecloud""],
        AllowCycleClientIn          = [""450"", ""Inbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud-client"", ""asg/asg-cyclecloud""],
        AllowCycleClientComputeIn   = [""460"", ""Inbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""subnet/compute"",            ""asg/asg-cyclecloud""],
        AllowCycleServerIn          = [""465"", ""Inbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud"",        ""asg/asg-cyclecloud-client""],

        # OnDemand NoVNC
        AllowComputeNoVncIn         = [""470"", ""Inbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""subnet/compute"",            ""asg/asg-ondemand""],
        AllowNoVncComputeIn         = [""480"", ""Inbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""asg/asg-ondemand"",          ""subnet/compute""],

        # Admin and Deployment
        AllowWinRMIn                = [""520"", ""Inbound"", ""Allow"", ""Tcp"", ""WinRM"",              ""asg/asg-jumpbox"",          ""asg/asg-rdp""],
        AllowRdpIn                  = [""550"", ""Inbound"", ""Allow"", ""Tcp"", ""Rdp"",                ""asg/asg-jumpbox"",          ""asg/asg-rdp""],

        # MariaDB
        AllowMariaDBIn              = [""700"", ""Inbound"", ""Allow"", ""Tcp"", ""MariaDB"",             ""asg/asg-mariadb-client"",    ""subnet/admin""],

        # Deny all remaining traffic
        DenyVnetInbound             = [""3100"", ""Inbound"", ""Deny"", ""*"", ""All"",                  ""tag/VirtualNetwork"",       ""tag/VirtualNetwork""],

        # ================================================================================================================================================================
        #                            #######
        #                            #     #  #    #   #####  #####    ####   #    #  #    #  #####
        #                            #     #  #    #     #    #    #  #    #  #    #  ##   #  #    #
        #                            #     #  #    #     #    #####   #    #  #    #  # #  #  #    #
        #                            #     #  #    #     #    #    #  #    #  #    #  #  # #  #    #
        #                            #     #  #    #     #    #    #  #    #  #    #  #   ##  #    #
        #                            #######   ####      #    #####    ####    ####   #    #  #####
        # ================================================================================================================================================================
        # AD communication
        AllowAdClientTcpOut        = [""200"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdClientUdpOut        = [""210"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdClientComputeTcpOut = [""220"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdClientComputeUdpOut = [""230"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdServerTcpOut        = [""240"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdServerUdpOut        = [""250"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdServerComputeTcpOut = [""260"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdServerComputeUdpOut = [""270"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdServerNetappTcpOut  = [""280"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""subnet/netapp""],
        AllowAdServerNetappUdpOut  = [""290"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""subnet/netapp""],

        # CycleCloud
        AllowCycleServerOut         = [""300"", ""Outbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud"",        ""asg/asg-cyclecloud-client""],
        AllowCycleClientOut         = [""310"", ""Outbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud-client"", ""asg/asg-cyclecloud""],
        AllowComputeCycleClientIn   = [""320"", ""Outbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""subnet/compute"",            ""asg/asg-cyclecloud""],
        AllowCycleWebOut            = [""330"", ""Outbound"", ""Allow"", ""Tcp"", ""Web"",                ""asg/asg-ondemand"",          ""asg/asg-cyclecloud""],

        # PBS
        AllowPbsOut                 = [""340"", ""Outbound"", ""Allow"", ""*"",   ""Pbs"",                ""asg/asg-pbs"",        ""asg/asg-pbs-client""],
        AllowPbsClientOut           = [""350"", ""Outbound"", ""Allow"", ""*"",   ""Pbs"",                ""asg/asg-pbs-client"", ""asg/asg-pbs""],
        AllowPbsComputeOut          = [""360"", ""Outbound"", ""Allow"", ""*"",   ""Pbs"",                ""asg/asg-pbs"",        ""subnet/compute""],
        AllowPbsClientComputeOut    = [""370"", ""Outbound"", ""Allow"", ""*"",   ""Pbs"",                ""subnet/compute"",     ""asg/asg-pbs""],
        AllowComputePbsClientOut    = [""380"", ""Outbound"", ""Allow"", ""*"",   ""Pbs"",                ""subnet/compute"",     ""asg/asg-pbs-client""],
        AllowComputeComputePbsOut   = [""381"", ""Outbound"", ""Allow"", ""*"",   ""Pbs"",                ""subnet/compute"",     ""subnet/compute""],

        # SLURM
        AllowSlurmComputeOut        = [""385"", ""Outbound"", ""Allow"", ""*"",   ""Slurmd"",             ""asg/asg-ondemand"",        ""subnet/compute""],

        # Lustre
        AllowLustreClientOut        = [""400"", ""Outbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""asg/asg-lustre-client"",    ""subnet/admin""],
        AllowLustreClientComputeOut = [""420"", ""Outbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""subnet/compute"",           ""subnet/admin""],

        # NFS
        AllowNfsOut                 = [""440"", ""Outbound"", ""Allow"", ""*"",   ""Nfs"",                ""asg/asg-nfs-client"",       ""subnet/netapp""],
        AllowNfsComputeOut          = [""450"", ""Outbound"", ""Allow"", ""*"",   ""Nfs"",                ""subnet/compute"",           ""subnet/netapp""],

        # SMB
        AllowSMBComputeOut          = [""455"", ""Outbound"", ""Allow"", ""*"",   ""SMB"",                ""subnet/compute"",            ""subnet/netapp""],

        # SSH internal rules
        AllowSshFromJumpboxOut      = [""490"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-jumpbox"",          ""asg/asg-ssh""],
        AllowSshComputeOut          = [""500"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-ssh"",              ""subnet/compute""],
        AllowSshDeployerOut         = [""510"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",         ""asg/asg-ssh""],
        AllowSshDeployerPackerOut   = [""520"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",         ""subnet/admin""],
        AllowSshFromComputeOut      = [""530"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""subnet/compute"",           ""asg/asg-ssh""],
        AllowSshComputeComputeOut   = [""540"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""subnet/compute"",           ""subnet/compute""],

        # OnDemand NoVNC
        AllowComputeNoVncOut        = [""550"", ""Outbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""subnet/compute"",            ""asg/asg-ondemand""],
        AllowNoVncComputeOut        = [""560"", ""Outbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""asg/asg-ondemand"",          ""subnet/compute""],

        # Admin and Deployment
        AllowRdpOut                 = [""570"", ""Outbound"", ""Allow"", ""Tcp"", ""Rdp"",                ""asg/asg-jumpbox"",          ""asg/asg-rdp""],
        AllowWinRMOut               = [""580"", ""Outbound"", ""Allow"", ""Tcp"", ""WinRM"",              ""asg/asg-jumpbox"",          ""asg/asg-rdp""],
        AllowDnsOut                 = [""590"", ""Outbound"", ""Allow"", ""*"",   ""Dns"",                ""tag/VirtualNetwork"",       ""tag/VirtualNetwork""],

        # MariaDB
        AllowMariaDBOut             = [""700"", ""Outbound"", ""Allow"", ""Tcp"", ""MariaDB"",             ""asg/asg-mariadb-client"",    ""subnet/admin""],

        # Deny all remaining traffic and allow Internet access
        AllowInternetOutBound       = [""3000"", ""Outbound"", ""Allow"", ""Tcp"", ""All"",               ""tag/VirtualNetwork"",       ""tag/Internet""],
        DenyVnetOutbound            = [""3100"", ""Outbound"", ""Deny"",  ""*"",   ""All"",               ""tag/VirtualNetwork"",       ""tag/VirtualNetwork""],
    }

    internet_nsg_rules = {
        AllowInternetSshIn          = [""200"", ""Inbound"", ""Allow"", ""Tcp"", ""Public_Ssh"",         ""tag/Internet"", ""asg/asg-jumpbox""], # Only when using a PIP
        AllowInternetHttpIn         = [""210"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                ""tag/Internet"", ""asg/asg-ondemand""], # Only when using a PIP
    }

    hub_nsg_rules = {
        AllowHubSshIn          = [""200"", ""Inbound"", ""Allow"", ""Tcp"", ""Public_Ssh"",               ""tag/VirtualNetwork"", ""asg/asg-jumpbox""],
        AllowHubHttpIn         = [""210"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                      ""tag/VirtualNetwork"", ""asg/asg-ondemand""],
        AllowPackerWinRMIn     = [""560"", ""Inbound"", ""Allow"", ""Tcp"", ""WinRM"",                    ""tag/VirtualNetwork"", ""subnet/compute""],
    }

    bastion_nsg_rules = {
        AllowBastionIn              = [""530"", ""Inbound"", ""Allow"", ""Tcp"", ""Bastion"",            ""subnet/bastion"",           ""tag/VirtualNetwork""],
    }

    gateway_nsg_rules = {
        AllowInternalWebUsersIn     = [""540"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                ""subnet/gateway"",           ""asg/asg-ondemand""],
    }

    grafana_nsg_rules = {
        # Telegraf / Grafana
        AllowTelegrafIn             = [""490"", ""Inbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""asg/asg-telegraf"",          ""asg/asg-grafana""],
        AllowComputeTelegrafIn      = [""500"", ""Inbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""subnet/compute"",            ""asg/asg-grafana""],
        AllowGrafanaIn              = [""510"", ""Inbound"", ""Allow"", ""Tcp"", ""Grafana"",            ""asg/asg-ondemand"",          ""asg/asg-grafana""],

        # Telegraf / Grafana
        AllowTelegrafOut            = [""460"", ""Outbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""asg/asg-telegraf"",          ""asg/asg-grafana""],
        AllowComputeTelegrafOut     = [""470"", ""Outbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""subnet/compute"",            ""asg/asg-grafana""],
        AllowGrafanaOut             = [""480"", ""Outbound"", ""Allow"", ""Tcp"", ""Grafana"",            ""asg/asg-ondemand"",          ""asg/asg-grafana""],
    }

    nsg_rules = merge(  local._nsg_rules, 
                        local.no_bastion_subnet ? {} : local.bastion_nsg_rules, 
                        local.no_gateway_subnet ? {} : local.gateway_nsg_rules,
                        local.allow_public_ip ? local.internet_nsg_rules : local.hub_nsg_rules,
                        local.create_grafana ? local.grafana_nsg_rules : {})

}
",locals,"locals {
    # azure environment
    public_cloud_endpoints = {
        KeyVaultSuffix =  ""vault.azure.net""
        BlobStorageSuffix = ""blob.core.windows.net""
        FileStorageSuffix = ""file.core.windows.net""
        MariaDBPrivateLink = ""privatelink.mariadb.database.azure.com""
    }
    usgov_cloud_endpoints = {
        KeyVaultSuffix =  ""vault.usgovcloudapi.net""
        BlobStorageSuffix = ""blob.core.usgovcloudapi.net""
        FileStorageSuffix = ""file.core.usgovcloudapi.net""
        MariaDBPrivateLink = ""privatelink.mariadb.database.usgovcloudapi.net""
    }
    azure_endpoints = {
        AZUREPUBLICCLOUD = local.public_cloud_endpoints
        AZUREUSGOVERNMENTCLOUD = local.usgov_cloud_endpoints
    }
    azure_environment = var.AzureEnvironment
    key_vault_suffix = local.azure_endpoints[local.azure_environment].KeyVaultSuffix #var.KeyVaultSuffix
    blob_storage_suffix = local.azure_endpoints[local.azure_environment].BlobStorageSuffix #var.BlobStorageSuffix

    # azurerm_client_config contains empty values for Managed Identity so use variables instead
    tenant_id = var.tenant_id
    logged_user_objectId = var.logged_user_objectId

    # config files and directories
    packer_root_dir = ""${path.cwd}/packer""
    playbook_root_dir = ""${path.cwd}/playbooks""
    playbooks_template_dir = ""${path.root}/templates""
    configuration_file=""${path.cwd}/config.yml""
    configuration_yml=yamldecode(file(local.configuration_file))
    
    # Load parameters from the configuration file
    location = local.configuration_yml[""location""]
    resource_group = local.configuration_yml[""resource_group""]
    extra_tags = try(local.configuration_yml[""tags""], null)
    common_tags = {
        CreatedBy = var.CreatedBy
        CreatedOn = timestamp()
    }

    # the PUID for telemetry is meant to be unique and identifies azhop, so it should not be changed
    telem_azhop_puid  = ""58d16d1a-5b7c-11ed-8042-00155d5d7a47""

    # local to determine if the user chose to disable telemetry of azhop
    optout_telemetry = try(local.configuration_yml[""optout_telemetry""], false)

    telem_azhop_name = substr(
        format(
            ""pid-%s"",
            local.telem_azhop_puid
        ),
        0,
        64
    )

    # empty arm template to create the telemetry resource
    telem_arm_subscription_template_content = <<TEMPLATE
    {
        ""$schema"": ""https://schema.management.azure.com/schemas/2018-05-01/subscriptionDeploymentTemplate.json#"",
        ""contentVersion"": ""1.0.0.0"",
        ""parameters"": {},
        ""variables"": {},
        ""resources"": [],
        ""outputs"": {
            ""telemetry"": {
                ""type"": ""String"",
                ""value"": ""For more information, see https://azure.github.io/az-hop/deploy/telemetry.html""
            }
        }
    }
    TEMPLATE

    # Log Analytics
    create_log_analytics_workspace = try(local.configuration_yml[""log_analytics""][""create""], false)
    log_analytics_name = try(local.configuration_yml[""log_analytics""][""name""], null)
    log_analytics_resource_group = try(local.configuration_yml[""log_analytics""][""resource_group""], null)
    log_analytics_subscription_id = try(local.configuration_yml[""log_analytics""][""subscription_id""], data.azurerm_subscription.primary.subscription_id)
    log_analytics_workspace_id = try(""/subscriptions/${local.log_analytics_subscription_id}/resourceGroups/${local.log_analytics_resource_group}/providers/Microsoft.OperationalInsights/workspaces/${local.log_analytics_name}"", null)
    use_existing_ws = ( !local.create_log_analytics_workspace && local.log_analytics_workspace_id != null )  ? true : false
     
    monitor = ( local.create_log_analytics_workspace || local.use_existing_ws ) ? true : false
    ama_install = try(local.configuration_yml[""monitoring""][""azure_monitor_agent""], true) && local.monitor ? true : false
    create_grafana = try(local.configuration_yml[""monitoring""][""grafana""], true)
    create_ondemand = try(length(local.configuration_yml[""ondemand""]) > 0, false)

    alert_email = try(local.configuration_yml[""alerting""][""admin_email""], ""admin.mail@contoso.com"")

    #For alerting to be enabled - the analytics workspace needs to be created since log alerts are leveraged. 
    #We also need to ensure that we have an email to send alerts to.  
    create_alerts = local.monitor && local.alert_email != ""admin.mail@contoso.com"" && try(local.configuration_yml[""alerting""][""enabled""], false) ? true : false
    anf_vol_threshold = try(local.configuration_yml[""anf""][""alert_threshold""], 80)  # default to 80% if not specified 

    # will be used with a KQL query that checks the free space percentage of local volumes
    # if the user wants to create an alert when local volumes are 80% full, then the free space percentage should be 20%
    local_vol_threshold = 100 - try(local.configuration_yml[""alerting""][""local_volume_threshold""], 20) 

    mounts = try(local.configuration_yml[""mounts""], {})
    mountpoints =  [ for mount in local.mounts : mount.mountpoint ]
    mountpoints_str = ""[ ${join("","", [for mp in local.mountpoints : format(""%q"", mp)])} ]"" //necessary to build generic KQL query on local volumes

    # Active Directory values
    # Updates the assumptions to the possibility that DNS may not point to Active Directory when using the customer provided AD.
    create_ad             = !try(local.configuration_yml[""domain""].use_existing_dc, false) && (try(local.configuration_yml[""authentication""].user_auth, ""ad"") == ""ad"")
    use_existing_ad       = try(local.configuration_yml[""domain""].use_existing_dc, false)
    create_dns_records    = local.create_ad || local.use_existing_ad
    domain_name           = local.use_existing_ad ? local.configuration_yml[""domain""].name : ""hpc.azure""
    domain_join_user      = local.use_existing_ad ? local.configuration_yml[""domain""].domain_join_user.username : local.admin_username
    domain_join_password  = local.use_existing_ad ? data.azurerm_key_vault_secret.domain_join_password[0].value : random_password.password.result
    domain_join_ou        = local.use_existing_ad ? local.configuration_yml[""domain""].domain_join_ou : ""CN=Computers""
    ad_ha                 = try(local.configuration_yml[""ad""].high_availability, false)
    domain_controlers     = local.use_existing_ad ? zipmap(local.configuration_yml[""domain""].existing_dc_details.domain_controller_names, local.configuration_yml[""domain""].existing_dc_details.domain_controller_names) : (local.ad_ha ? {ad=local.ad_name, ad2=local.ad2_name} : {ad=local.ad_name})
    ldap_server           = local.use_existing_ad ? local.configuration_yml[""domain""].existing_dc_details.domain_controller_names[0]     : local.ad_name
    private_dns_servers   = local.use_existing_ad ? local.configuration_yml[""domain""].existing_dc_details.private_dns_servers            : (local.create_ad ? (local.ad_ha ? [azurerm_network_interface.ad-nic[0].private_ip_address, azurerm_network_interface.ad2-nic[0].private_ip_address] : [azurerm_network_interface.ad-nic[0].private_ip_address]) : [])
    domain_controller_ips = local.use_existing_ad ? local.configuration_yml[""domain""].existing_dc_details.domain_controller_ip_addresses : (local.create_ad ? (local.ad_ha ? [azurerm_network_interface.ad-nic[0].private_ip_address, azurerm_network_interface.ad2-nic[0].private_ip_address] : [azurerm_network_interface.ad-nic[0].private_ip_address]) : [])

    # private DNS 
    create_private_dns = try(local.configuration_yml[""private_dns""].create, false)
    private_dns_zone_name = try(local.configuration_yml[""private_dns""].name, ""hpc.azure"")
    private_dns_registration_enabled = try(local.configuration_yml[""private_dns""].registration_enabled, false)

    # Use a linux custom image reference if the linux_base_image is defined and contains "":""
    use_linux_image_reference = try(length(split("":"", local.configuration_yml[""linux_base_image""])[1])>0, false)
    # Use a linux custom image reference if the linux_base_image is defined and contains "":""
    use_windows_image_reference = try(length(split("":"", local.configuration_yml[""windows_base_image""])[1])>0, false)
    # Use a linux custom image reference if the linux_base_image is defined and contains "":""
    use_cyclecloud_image_reference = try(length(split("":"", local.configuration_yml[""cyclecloud""][""image""])[1])>0, false)

    linux_base_image_reference = {
        publisher = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[0] : ""OpenLogic""
        offer     = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[1] : ""CentOS""
        sku       = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[2] : ""7_9-gen2""
        version   = local.use_linux_image_reference ? split("":"", local.configuration_yml[""linux_base_image""])[3] : ""latest""
    }
    windows_base_image_reference = {
        publisher = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[0] : ""MicrosoftWindowsServer""
        offer     = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[1] : ""WindowsServer""
        sku       = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[2] : ""2019-Datacenter-smalldisk""
        version   = local.use_windows_image_reference ? split("":"", local.configuration_yml[""windows_base_image""])[3] : ""latest""
    }
    cyclecloud_image_reference = {
        publisher = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[0] : local.linux_base_image_reference.publisher
        offer     = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[1] : local.linux_base_image_reference.offer
        sku       = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[2] : local.linux_base_image_reference.sku
        version   = local.use_cyclecloud_image_reference ? split("":"", local.configuration_yml[""cyclecloud""][""image""])[3] : local.linux_base_image_reference.version
    }

    # Use a linux custom image id if the linux_base_image is defined and contains ""/""
    use_linux_image_id = try(length(split(""/"", local.configuration_yml[""linux_base_image""])[1])>0, false)
    linux_image_id = local.use_linux_image_id ? local.configuration_yml[""linux_base_image""] : null

    # Use a windows custom image id if the windows_base_image is defined and contains ""/""
    use_windows_image_id = try(length(split(""/"", local.configuration_yml[""windows_base_image""])[1])>0, false)
    windows_image_id = local.use_windows_image_id ? local.configuration_yml[""windows_base_image""] : null

    # Use a cyclecloud custom image id if the cyclecloud_base_image is defined and contains ""/""
    use_cyclecloud_image_id = try(length(split(""/"", local.configuration_yml[""cyclecloud""][""image""])[1])>0, false)
    cyclecloud_image_id = local.use_cyclecloud_image_id ? local.configuration_yml[""cyclecloud""][""image""] : null

    _empty_image_plan = {}
    _linux_base_image_plan = {
        publisher = try(split("":"", local.configuration_yml[""linux_base_plan""])[0], """")
        product   = try(split("":"", local.configuration_yml[""linux_base_plan""])[1], """")
        name      = try(split("":"", local.configuration_yml[""linux_base_plan""])[2], """")
    }
    linux_image_plan = try( length(local._linux_base_image_plan.publisher) > 0 ? local._linux_base_image_plan : local._empty_image_plan, local._empty_image_plan)

    _cyclecloud_image_plan = {
        publisher = try(split("":"", local.configuration_yml[""cyclecloud""][""plan""])[0], """")
        product   = try(split("":"", local.configuration_yml[""cyclecloud""][""plan""])[1], """")
        name      = try(split("":"", local.configuration_yml[""cyclecloud""][""plan""])[2], """")
    }
    cyclecloud_image_plan = try( length(local._cyclecloud_image_plan.publisher) > 0 ? local._cyclecloud_image_plan : local._empty_image_plan, local._empty_image_plan)


    # Create the RG if not using an existing RG and (creating a VNET or when reusing a VNET in another resource group)
    use_existing_rg = try(local.configuration_yml[""use_existing_rg""], false)
    create_rg = (!local.use_existing_rg) && (local.create_vnet || try(split(""/"", local.vnet_id)[4], local.resource_group) != local.resource_group)

    # ANF
    create_anf = try(local.configuration_yml[""anf""][""create""], false)
    anf_size=try(local.configuration_yml[""anf""][""homefs_size_tb""], 4)
    anf_service_level = try(local.configuration_yml[""anf""][""homefs_service_level""], ""Standard"")
    anf_dual_protocol = try(local.configuration_yml[""anf""][""dual_protocol""], false)

    #Azure Files
    create_nfsfiles = try(local.configuration_yml[""azurefiles""][""create""], false)
    azure_files_size= try(local.configuration_yml[""azurefiles""][""size_gb""], 1024)

    # Home Directory
    homedir_type = try(local.configuration_yml[""mounts""][""home""][""type""], ""existing"")
    config_nfs_home_ip = local.configuration_yml[""mounts""][""home""][""server""]
    config_nfs_home_path = local.configuration_yml[""mounts""][""home""][""export""]
    config_nfs_home_opts = local.configuration_yml[""mounts""][""home""][""options""]

    homedir_mountpoint = try(local.configuration_yml[""mounts""][""home""][""mountpoint""], ""/anfhome"")

    admin_username = local.configuration_yml[""admin_user""]
    key_vault_readers = try(local.configuration_yml[""key_vault_readers""], null)

    # Resource names
    scheduler_name = try(local.configuration_yml[""scheduler""][""name""], ""scheduler"")
    ccportal_name = try(local.configuration_yml[""cyclecloud""][""name""], ""ccportal"")
    ondemand_name = try(local.configuration_yml[""ondemand""][""name""], ""ondemand"")
    grafana_name = try(local.configuration_yml[""grafana""][""name""], ""grafana"")
    jumpbox_name = try(local.configuration_yml[""jumpbox""][""name""], ""jumpbox"")
    ad_name = try(local.configuration_yml[""ad""][""name""], ""ad"")
    ad2_name = try(local.configuration_yml[""ad""][""ha_name""], ""ad2"")

    key_vault_name = try(local.configuration_yml[""azure_key_vault""][""name""], format(""%s%s"", ""kv"", random_string.resource_postfix.result))
    storage_account_name = try(local.configuration_yml[""azure_storage_account""][""name""], ""azhop${random_string.resource_postfix.result}"")
    db_name = try(local.configuration_yml[""database""][""name""], ""mysql-${random_string.resource_postfix.result}"")

    # Lustre - AMLFS not implemented for TF
    lustre_enabled = false

    # Use a jumpbox when defined
    jumpbox_enabled = try(length(local.configuration_yml[""jumpbox""]) > 0, false)

    # Queue manager
    queue_manager = try(local.configuration_yml[""queue_manager""], ""openpbs"")

    # Create Database
    create_database  = ( try(local.configuration_yml[""slurm""].accounting_enabled, false) ) && (! local.use_existing_database)
    use_existing_database = try(length(local.configuration_yml[""database""].fqdn) > 0 ? true : false, false)
    database_user = local.create_database ? ""sqladmin"" : (local.use_existing_database ? try(local.configuration_yml[""database""].user, ""sqladmin"") : ""sqladmin"")
    #mariadb_private_dns_zone = local.azure_endpoints[local.azure_environment].MariaDBPrivateLink

    create_sig = try(local.configuration_yml[""image_gallery""][""create""], false)
    
    # VNET
    create_vnet = try(length(local.vnet_id) > 0 ? false : true, true)
    vnet_id = try(local.configuration_yml[""network""][""vnet""][""id""], null)

    # VNET Peering
    vnet_peering = try(tolist(local.configuration_yml[""network""][""peering""]), [])

    # Lockdown scenario
    locked_down_network = try(local.configuration_yml[""locked_down_network""][""enforce""], false)
    grant_access_from   = try(local.configuration_yml[""locked_down_network""][""grant_access_from""], [])
    allow_public_ip     = try(local.configuration_yml[""locked_down_network""][""public_ip""], true)
    jumpbox_ssh_port    = try(local.configuration_yml[""jumpbox""][""ssh_port""], ""22"")

    # NAT Gateway
    create_nat_gateway = try(local.configuration_yml[""nat_gateway""][""create""], false)
    nat_gateway_name = try(local.configuration_yml[""nat_gateway""][""name""], ""natgw-${random_string.resource_postfix.result}"")

    # subnets
    _subnets = {
        frontend = ""frontend"",
        admin = ""admin"",
        netapp = ""netapp"",
        compute = ""compute"",
        ad = ""ad"",
        database = ""database""
    }

    # Create subnet if required. If not specified create only if vnet is created
    create_frontend_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""frontend""][""create""], local.create_vnet )
    create_admin_subnet    = try(local.configuration_yml[""network""][""vnet""][""subnets""][""admin""][""create""], local.create_vnet )
    create_netapp_subnet   = try(local.configuration_yml[""network""][""vnet""][""subnets""][""netapp""][""create""], local.create_vnet )
    create_database_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""database""][""create""], local.create_vnet )
    create_compute_subnet  = try(local.configuration_yml[""network""][""vnet""][""subnets""][""compute""][""create""], local.create_vnet )

    ad_subnet        = try(local.configuration_yml[""network""][""vnet""][""subnets""][""ad""], null)
    no_ad_subnet     = try(length(local.ad_subnet) > 0 ? false : true, true)
    create_ad_subnet = try(local.ad_subnet[""create""], (local.create_ad ? local.create_vnet : false))

    bastion_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""bastion""], null)
    no_bastion_subnet = try(length(local.bastion_subnet) > 0 ? false : true, true )
    create_bastion_subnet  = try(local.bastion_subnet[""create""], local.create_vnet )

    gateway_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""gateway""], null)
    no_gateway_subnet = try(length(local.gateway_subnet) > 0 ? false : true, true )
    create_gateway_subnet  = try(local.gateway_subnet[""create""], local.create_vnet )

    outbounddns_subnet = try(local.configuration_yml[""network""][""vnet""][""subnets""][""outbounddns""], null)
    no_outbounddns_subnet = try(length(local.outbounddns_subnet) > 0 ? false : true, true )
    create_outbounddns_subnet  = try(local.outbounddns_subnet[""create""], local.create_vnet ? (local.no_outbounddns_subnet ? false : true) : false )

    dns_forwarders = try(local.configuration_yml[""dns""][""forwarders""], [])
    create_dnsfw_rules = length(local.dns_forwarders) > 0 ? true : false

    subnets = merge(local._subnets, 
                    local.no_bastion_subnet ? {} : {bastion = ""AzureBastionSubnet""},
                    local.no_gateway_subnet ? {} : {gateway = ""GatewaySubnet""},
                    local.no_outbounddns_subnet ? {} : {outbounddns = ""outbounddns""}
                    )

    # Application Security Groups
    create_nsg = try(local.configuration_yml[""network""][""create_nsg""], local.create_vnet )
    # If create NSG then use the local resource group otherwise use the configured one. Default to local resource group
    asg_resource_group = local.create_nsg ? local.resource_group : try(length(local.configuration_yml[""network""][""asg""][""resource_group""]) > 0 ? local.configuration_yml[""network""][""asg""][""resource_group""] : local.resource_group, local.resource_group )

    _asg_ad = {
        asg-ad = ""asg-ad""
        asg-ad-client = ""asg-ad-client""
        asg-rdp = ""asg-rdp""
     }

    _asg_grafana = {
        asg-telegraf = ""asg-telegraf""
        asg-grafana = ""asg-grafana""
    }

    _asg_ondemand = {
        asg-ondemand = ""asg-ondemand""
    }

    _asg_lustre = {
        asg-lustre-client = ""asg-lustre-client""
    }

    _asg_mysql = {
        asg-mysql-client = ""asg-mysql-client""
    }

    _default_asgs = merge ({
            asg-ssh = ""asg-ssh""
            asg-jumpbox = ""asg-jumpbox""
            asg-sched = ""asg-sched""
            asg-cyclecloud = ""asg-cyclecloud""
            asg-cyclecloud-client = ""asg-cyclecloud-client""
            asg-nfs-client = ""asg-nfs-client""
            asg-deployer = ""asg-deployer""
        },
        local.create_ad || local.use_existing_ad ? local._asg_ad : {},
        local.create_grafana ? local._asg_grafana : {},
        local.create_ondemand ? local._asg_ondemand : {},
        local.lustre_enabled ? local._asg_lustre : {},
        local.create_database || local.use_existing_database ? local._asg_mysql : {}
    )

    #asgs = local.create_nsg ? local._default_asgs :  try(local.configuration_yml[""network""][""asg""][""names""], local._default_asgs)
    asgs = try(local.configuration_yml[""network""][""asg""][""names""], local._default_asgs)
    #asgs = { for v in local.default_asgs : v => v }
    empty_array = []
    empty_map = { for v in local.empty_array : v => v }

    # VM name to list of ASGs associations
    asg_asso_ad = [""asg-ad"", ""asg-rdp"", ""asg-ad-client""] # asg-ad-client will allow the secondary DC scenario
    asg_asso_ccportal = concat([""asg-ssh"", ""asg-cyclecloud""], 
                            local.create_grafana ? [""asg-telegraf""] : [], 
                            local.create_ad || local.use_existing_ad ? [""asg-ad-client""] : [])
    asg_asso_grafana = concat([""asg-ssh"", ""asg-grafana"", ""asg-telegraf"", ""asg-nfs-client""], 
                            local.create_ad || local.use_existing_ad ? [""asg-ad-client""] : [])
    asg_asso_jumpbox = concat([""asg-ssh"", ""asg-jumpbox"" ],
                                local.create_grafana ? [""asg-telegraf""] : [])
    asg_asso_ondemand = concat([""asg-ssh"", ""asg-ondemand"", ""asg-nfs-client"", ""asg-sched"", ""asg-cyclecloud-client"" ],
                            local.create_grafana ? [""asg-telegraf""] : [],
                            local.lustre_enabled ? [""asg-lustre-client""] : [],
                            local.create_ad || local.use_existing_ad ? [""asg-ad-client""] : [])
    asg_asso_scheduler = concat([""asg-ssh"", ""asg-sched"", ""asg-cyclecloud-client"", ""asg-nfs-client""],
                            local.create_grafana ? [""asg-telegraf""] : [], 
                            local.create_ad || local.use_existing_ad ? [""asg-ad-client""] : [],
                            local.create_database || local.use_existing_database ? [""asg-mysql-client""] : [])

    asg_associations = {
        ad        = local.asg_asso_ad 
        ccportal  = local.asg_asso_ccportal
        grafana   = local.asg_asso_grafana
        jumpbox   = local.asg_asso_jumpbox
        ondemand  = local.asg_asso_ondemand
        scheduler = local.asg_asso_scheduler
    }

    # Open ports for NSG TCP rules
    # ANF and SMB https://docs.microsoft.com/en-us/azure/azure-netapp-files/create-active-directory-connections
    nsg_destination_ports = {
        All = [""0-65535""]
        Bastion = [""22"", ""3389""]
        Web = [""443"", ""80""]
        Ssh    = [""22""]
        Public_Ssh = [local.jumpbox_ssh_port]
        # DNS, Kerberos, RpcMapper, Ldap, Smb, KerberosPass, LdapSsl, LdapGc, LdapGcSsl, AD Web Services, RpcSam
        DomainControlerTcp = [""53"", ""88"", ""135"", ""389"", ""445"", ""464"", ""636"", ""3268"", ""3269"", ""9389"", ""49152-65535""]
        # DNS, Kerberos, W32Time, NetBIOS, Ldap, KerberosPass, LdapSsl
        DomainControlerUdp = [""53"", ""88"", ""123"", ""138"", ""389"", ""464"", ""636""]
        # Web, NoVNC, WebSockify
        NoVnc = [""80"", ""443"", ""5900-5910"", ""61001-61010""]
        Dns = [""53""]
        Rdp = [""3389""]
        #Pbs = [""6200"", ""15001-15009"", ""17001"", ""32768-61000""]
        #Slurmd = [""6817-6819""]
        Sched = (local.queue_manager == ""slurm"") ? [""6817-6819"", ""59000-61000""] : [""6200"", ""15001-15009"", ""17001"", ""32768-61000""]
        Lustre = [""635"", ""988""]
        Nfs = [""111"", ""635"", ""2049"", ""4045"", ""4046""]
        SMB = [""445""]
        Telegraf = [""8086""]
        Grafana = [""3000""]
        # HTTPS, AMQP
        CycleCloud = [""9443"", ""5672""],
        # MySQL
        MySQL = [""3306"", ""33060""],
        # WinRM
        WinRM = [""5985"", ""5986""]
    }

    #Replace the AD ASG with domain controller IP addresses when customer is bringing their own AD
    #use an indexing concept since we can't substitute a list for a string
    ad_nsg_index = local.use_existing_ad ? ""ips/dc_ips"" : ""asg/asg-ad""
    ips = {
        dc_ips = local.domain_controller_ips
    }

    # Array of NSG rules to be applied on the common NSG
    # NsgRuleName = [priority, direction, access, protocol, destination_port_range, source, destination]
    #   - priority               : integer value from 100 to 4096
    #   - direction              : Inbound, Outbound
    #   - access                 : Allow, Deny
    #   - protocol               : Tcp, Udp, *
    #   - destination_port_range : name of one of the nsg_destination_ports defined above
    #   - source                 : asg/<asg-name>, subnet/<subnet-name>, tag/<tag-name>. tag-name = any Azure tags like Internet, VirtualNetwork, AzureLoadBalancer, ...
    #   - destination            : same as source
    _nsg_rules = {
        # ================================================================================================================================================================
        #                          ###
        #                           #     #    #  #####    ####   #    #  #    #  #####
        #                           #     ##   #  #    #  #    #  #    #  ##   #  #    #
        #                           #     # #  #  #####   #    #  #    #  # #  #  #    #
        #                           #     #  # #  #    #  #    #  #    #  #  # #  #    #
        #                           #     #   ##  #    #  #    #  #    #  #   ##  #    #
        #                          ###    #    #  #####    ####    ####   #    #  #####
        # ================================================================================================================================================================

        # SSH internal rules
        AllowSshFromJumpboxIn       = [""320"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-jumpbox"",   ""asg/asg-ssh""],
        AllowSshFromComputeIn       = [""330"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""subnet/compute"",    ""asg/asg-ssh""],
        AllowSshFromDeployerIn      = [""340"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",  ""asg/asg-ssh""], # Only in a deployer VM scenario
        AllowDeployerToPackerSshIn  = [""350"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",  ""subnet/admin""], # Only in a deployer VM scenario
        AllowSshToComputeIn         = [""360"", ""Inbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-ssh"",       ""subnet/compute""],
        AllowAllComputeComputeIn    = [""365"", ""Inbound"", ""Allow"", ""Tcp"", ""All"",                ""subnet/compute"",    ""subnet/compute""],

        # Scheduler
        AllowSchedIn                = [""369"", ""Inbound"", ""Allow"", ""*"",   ""Sched"",                ""asg/asg-sched"",      ""asg/asg-sched""],
        #AllowPbsClientIn            = [""370"", ""Inbound"", ""Allow"", ""*"",   ""Sched"",                ""asg/asg-pbs-client"", ""asg/asg-pbs""],
        AllowSchedComputeIn         = [""380"", ""Inbound"", ""Allow"", ""*"",   ""Sched"",                ""asg/asg-sched"",      ""subnet/compute""],
        #AllowComputePbsClientIn     = [""390"", ""Inbound"", ""Allow"", ""*"",   ""Sched"",                ""subnet/compute"",     ""asg/asg-pbs-client""],
        AllowComputeSchedIn         = [""400"", ""Inbound"", ""Allow"", ""*"",   ""Sched"",                ""subnet/compute"",     ""asg/asg-sched""],
        #AllowComputeComputeSchedIn  = [""401"", ""Inbound"", ""Allow"", ""*"",   ""Sched"",                ""subnet/compute"",     ""subnet/compute""],

        # CycleCloud
        AllowCycleClientIn          = [""450"", ""Inbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud-client"", ""asg/asg-cyclecloud""],
        AllowCycleClientComputeIn   = [""460"", ""Inbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""subnet/compute"",            ""asg/asg-cyclecloud""],
        AllowCycleServerIn          = [""465"", ""Inbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud"",        ""asg/asg-cyclecloud-client""],

        # Deny all remaining traffic
        DenyVnetInbound             = [""3100"", ""Inbound"", ""Deny"", ""*"", ""All"",                  ""tag/VirtualNetwork"",       ""tag/VirtualNetwork""],

        # ================================================================================================================================================================
        #                            #######
        #                            #     #  #    #   #####  #####    ####   #    #  #    #  #####
        #                            #     #  #    #     #    #    #  #    #  #    #  ##   #  #    #
        #                            #     #  #    #     #    #####   #    #  #    #  # #  #  #    #
        #                            #     #  #    #     #    #    #  #    #  #    #  #  # #  #    #
        #                            #     #  #    #     #    #    #  #    #  #    #  #   ##  #    #
        #                            #######   ####      #    #####    ####    ####   #    #  #####
        # ================================================================================================================================================================

        # CycleCloud
        AllowCycleServerOut         = [""300"", ""Outbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud"",        ""asg/asg-cyclecloud-client""],
        AllowCycleClientOut         = [""310"", ""Outbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""asg/asg-cyclecloud-client"", ""asg/asg-cyclecloud""],
        AllowComputeCycleClientIn   = [""320"", ""Outbound"", ""Allow"", ""Tcp"", ""CycleCloud"",         ""subnet/compute"",            ""asg/asg-cyclecloud""],

        # Scheduler
        AllowSchedOut               = [""340"", ""Outbound"", ""Allow"", ""*"",   ""Sched"",                ""asg/asg-sched"",      ""asg/asg-sched""],
        #AllowPbsClientOut           = [""350"", ""Outbound"", ""Allow"", ""*"",   ""Sched"",                ""asg/asg-pbs-client"", ""asg/asg-pbs""],
        AllowSchedComputeOut        = [""360"", ""Outbound"", ""Allow"", ""*"",   ""Sched"",                ""asg/asg-sched"",      ""subnet/compute""],
        AllowComputeSchedOut        = [""370"", ""Outbound"", ""Allow"", ""*"",   ""Sched"",                ""subnet/compute"",     ""asg/asg-sched""],
        #AllowComputePbsClientOut    = [""380"", ""Outbound"", ""Allow"", ""*"",   ""Sched"",                ""subnet/compute"",     ""asg/asg-pbs-client""],
        #AllowComputeComputeSchedOut = [""381"", ""Outbound"", ""Allow"", ""*"",   ""Sched"",                ""subnet/compute"",     ""subnet/compute""],

        # SSH internal rules
        AllowSshFromJumpboxOut      = [""490"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-jumpbox"",          ""asg/asg-ssh""],
        AllowSshComputeOut          = [""500"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-ssh"",              ""subnet/compute""],
        AllowSshDeployerOut         = [""510"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",         ""asg/asg-ssh""],
        AllowSshDeployerPackerOut   = [""520"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""asg/asg-deployer"",         ""subnet/admin""],
        AllowSshFromComputeOut      = [""530"", ""Outbound"", ""Allow"", ""Tcp"", ""Ssh"",                ""subnet/compute"",           ""asg/asg-ssh""],
        AllowAllComputeComputeOut   = [""540"", ""Outbound"", ""Allow"", ""Tcp"", ""All"",                ""subnet/compute"",           ""subnet/compute""],

        # Admin and Deployment
        AllowDnsOut                 = [""590"", ""Outbound"", ""Allow"", ""*"",   ""Dns"",                ""tag/VirtualNetwork"",       ""tag/VirtualNetwork""],

        # Deny all remaining traffic and allow Internet access
        AllowInternetOutBound       = [""3000"", ""Outbound"", ""Allow"", ""Tcp"", ""All"",               ""tag/VirtualNetwork"",       ""tag/Internet""],
        DenyVnetOutbound            = [""3100"", ""Outbound"", ""Deny"",  ""*"",   ""All"",               ""tag/VirtualNetwork"",       ""tag/VirtualNetwork""],
    }

    internet_nsg_rules = {
        AllowInternetSshIn          = [""200"", ""Inbound"", ""Allow"", ""Tcp"", ""Public_Ssh"",         ""tag/Internet"", ""asg/asg-jumpbox""], # Only when using a PIP
        AllowInternetHttpIn         = [""210"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                ""tag/Internet"", ""subnet/frontend""], # Only when using a PIP
    }

    hub_nsg_rules = {
        AllowHubSshIn          = [""200"", ""Inbound"", ""Allow"", ""Tcp"", ""Public_Ssh"",               ""tag/VirtualNetwork"", ""tag/VirtualNetwork""],
        AllowHubHttpIn         = [""210"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                      ""tag/VirtualNetwork"", ""tag/VirtualNetwork""],
    }

    ad_nsg_rules = {
        # Inbound
        AllowAdServerTcpIn        = [""220"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdServerUdpIn        = [""230"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdClientTcpIn        = [""240"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdClientUdpIn        = [""250"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdServerComputeTcpIn = [""260"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdServerComputeUdpIn = [""270"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdClientComputeTcpIn = [""280"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdClientComputeUdpIn = [""290"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdServerNetappTcpIn  = [""300"", ""Inbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""subnet/netapp"", local.ad_nsg_index],
        AllowAdServerNetappUdpIn  = [""310"", ""Inbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""subnet/netapp"", local.ad_nsg_index],
        AllowWinRMIn              = [""520"", ""Inbound"", ""Allow"", ""Tcp"", ""WinRM"",              ""asg/asg-jumpbox"", ""asg/asg-rdp""],
        AllowRdpIn                = [""550"", ""Inbound"", ""Allow"", ""Tcp"", ""Rdp"",                ""asg/asg-jumpbox"", ""asg/asg-rdp""],
        AllowPackerWinRMIn        = [""560"", ""Inbound"", ""Allow"", ""Tcp"", ""WinRM"",              ""tag/VirtualNetwork"", ""subnet/compute""],

        # Outbound
        AllowAdClientTcpOut        = [""200"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdClientUdpOut        = [""210"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""asg/asg-ad-client"", local.ad_nsg_index],
        AllowAdClientComputeTcpOut = [""220"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdClientComputeUdpOut = [""230"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", ""subnet/compute"", local.ad_nsg_index],
        AllowAdServerTcpOut        = [""240"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdServerUdpOut        = [""250"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""asg/asg-ad-client""],
        AllowAdServerComputeTcpOut = [""260"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdServerComputeUdpOut = [""270"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""subnet/compute""],
        AllowAdServerNetappTcpOut  = [""280"", ""Outbound"", ""Allow"", ""Tcp"", ""DomainControlerTcp"", local.ad_nsg_index, ""subnet/netapp""],
        AllowAdServerNetappUdpOut  = [""290"", ""Outbound"", ""Allow"", ""Udp"", ""DomainControlerUdp"", local.ad_nsg_index, ""subnet/netapp""],
        AllowRdpOut                = [""570"", ""Outbound"", ""Allow"", ""Tcp"", ""Rdp"", ""asg/asg-jumpbox"", ""asg/asg-rdp""],
        AllowWinRMOut              = [""580"", ""Outbound"", ""Allow"", ""Tcp"", ""WinRM"", ""asg/asg-jumpbox"", ""asg/asg-rdp""],
    }

    bastion_nsg_rules = {
        AllowBastionIn              = [""530"", ""Inbound"" , ""Allow"", ""Tcp"", ""Bastion"",            ""subnet/bastion"",           ""tag/VirtualNetwork""],
        AllowBastionOut             = [""531"", ""Outbound"", ""Allow"", ""Tcp"", ""Bastion"",            ""subnet/bastion"",           ""tag/VirtualNetwork""],
    }

    gateway_nsg_rules = {
        AllowInternalWebUsersIn     = [""540"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                ""subnet/gateway"",           ""asg/asg-ondemand""],
    }

    ondemand_nsg_rules = {
        # Inbound
#        AllowComputeSlurmIn         = [""405"", ""Inbound"", ""Allow"", ""*"",   ""Slurmd"",             ""asg/asg-ondemand"",    ""subnet/compute""],
        AllowCycleWebIn             = [""440"", ""Inbound"", ""Allow"", ""Tcp"", ""Web"",                ""asg/asg-ondemand"",          ""asg/asg-cyclecloud""],
        AllowComputeNoVncIn         = [""470"", ""Inbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""subnet/compute"",            ""asg/asg-ondemand""],
        AllowNoVncComputeIn         = [""480"", ""Inbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""asg/asg-ondemand"",          ""subnet/compute""],
        # Outbound
        AllowCycleWebOut            = [""330"", ""Outbound"", ""Allow"", ""Tcp"", ""Web"",                ""asg/asg-ondemand"",          ""asg/asg-cyclecloud""],
#        AllowSlurmComputeOut        = [""385"", ""Outbound"", ""Allow"", ""*"",   ""Slurmd"",             ""asg/asg-ondemand"",        ""subnet/compute""],
        AllowComputeNoVncOut        = [""550"", ""Outbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""subnet/compute"",            ""asg/asg-ondemand""],
        AllowNoVncComputeOut        = [""560"", ""Outbound"", ""Allow"", ""Tcp"", ""NoVnc"",              ""asg/asg-ondemand"",          ""subnet/compute""],

    }
    grafana_nsg_rules = {
        # Telegraf / Grafana
        AllowTelegrafIn             = [""490"", ""Inbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""asg/asg-telegraf"",          ""asg/asg-grafana""],
        AllowComputeTelegrafIn      = [""500"", ""Inbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""subnet/compute"",            ""asg/asg-grafana""],
        AllowGrafanaIn              = [""510"", ""Inbound"", ""Allow"", ""Tcp"", ""Grafana"",            ""asg/asg-ondemand"",          ""asg/asg-grafana""],

        # Telegraf / Grafana
        AllowTelegrafOut            = [""460"", ""Outbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""asg/asg-telegraf"",          ""asg/asg-grafana""],
        AllowComputeTelegrafOut     = [""470"", ""Outbound"", ""Allow"", ""Tcp"", ""Telegraf"",           ""subnet/compute"",            ""asg/asg-grafana""],
        AllowGrafanaOut             = [""480"", ""Outbound"", ""Allow"", ""Tcp"", ""Grafana"",            ""asg/asg-ondemand"",          ""asg/asg-grafana""],
    }

    mysql_nsg_rules = {
        # Inbound
        AllowMySQLIn              = [""700"", ""Inbound"", ""Allow"", ""Tcp"", ""MySQL"",             ""asg/asg-mysql-client"",    ""subnet/database""],
        # Outbound
        AllowMySQLOut             = [""700"", ""Outbound"", ""Allow"", ""Tcp"", ""MySQL"",             ""asg/asg-mysql-client"",    ""subnet/database""],
    }

    anf_nsg_rules = {
        # Inbound
        AllowNfsIn                  = [""430"", ""Inbound"", ""Allow"", ""*"",   ""Nfs"",                ""asg/asg-nfs-client"",       ""subnet/netapp""],
        AllowNfsComputeIn           = [""435"", ""Inbound"", ""Allow"", ""*"",   ""Nfs"",                ""subnet/compute"",           ""subnet/netapp""],
        # Outbound
        AllowNfsOut                 = [""440"", ""Outbound"", ""Allow"", ""*"",   ""Nfs"",                ""asg/asg-nfs-client"",       ""subnet/netapp""],
        AllowNfsComputeOut          = [""450"", ""Outbound"", ""Allow"", ""*"",   ""Nfs"",                ""subnet/compute"",           ""subnet/netapp""],
        AllowSMBComputeOut          = [""455"", ""Outbound"", ""Allow"", ""*"",   ""SMB"",                ""subnet/compute"",            ""subnet/netapp""],
    }

    lustre_nsg_rules = {
        # Inbound
        AllowLustreClientIn         = [""410"", ""Inbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""asg/asg-lustre-client"", ""subnet/admin""],
        AllowLustreClientComputeIn  = [""420"", ""Inbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""subnet/compute"",        ""subnet/admin""],
        # Outbound
        AllowLustreClientOut        = [""400"", ""Outbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""asg/asg-lustre-client"",    ""subnet/admin""],
        AllowLustreClientComputeOut = [""420"", ""Outbound"", ""Allow"", ""Tcp"", ""Lustre"",             ""subnet/compute"",           ""subnet/admin""],

    }
    
    nsg_rules = merge(  local._nsg_rules, 
                        local.create_ad || local.use_existing_ad ? local.ad_nsg_rules : {},
                        local.no_bastion_subnet ? {} : local.bastion_nsg_rules, 
                        local.no_gateway_subnet ? {} : local.gateway_nsg_rules,
                        local.allow_public_ip ? local.internet_nsg_rules : local.hub_nsg_rules,
                        local.create_grafana ? local.grafana_nsg_rules : {},
                        local.create_database || local.use_existing_database ? local.mysql_nsg_rules : {},
                        local.create_ondemand ? local.ondemand_nsg_rules : {},
                        local.lustre_enabled ? local.lustre_nsg_rules : {},
                        local.anf_nsg_rules
                    )

}
",locals,209,215.0,1fa901c8a5ca9d56bac65d1f0bd0bf273fd0a509,de6daf4da6feefb4fcbc0cef1f20dac41d83d28e,https://github.com/Azure/az-hop/blob/1fa901c8a5ca9d56bac65d1f0bd0bf273fd0a509/tf/variables_local.tf#L209,https://github.com/Azure/az-hop/blob/de6daf4da6feefb4fcbc0cef1f20dac41d83d28e/tf/variables_local.tf#L215,2023-10-10 12:12:10+02:00,2024-03-29 16:03:05+01:00,12,0,1,0,1,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,171,workergroups.tf,workerpools.tf,1,implementation,# Default workergroup sub-module implementation for OKE cluster,"# Copyright (c) 2022, 2023 Oracle Corporation and/or its affiliates. 
 # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl  
 # Default workergroup sub-module implementation for OKE cluster","module ""workergroup"" {
  source                          = ""./modules/workergroup""
  config_file_profile             = var.config_file_profile
  worker_groups                   = var.worker_groups
  tenancy_id                      = local.tenancy_id
  compartment_id                  = local.worker_compartment_id
  region                          = var.region
  cluster_id                      = coalesce(var.cluster_id, module.oke.cluster_id)
  apiserver_private_host          = try(split("":"", module.oke.endpoints[0].private_endpoint)[0], """")
  apiserver_public_host           = try(split("":"", module.oke.endpoints[0].public_endpoint)[0], """")
  image_id                        = local.worker_image_id
  image_type                      = local.worker_image_type
  os                              = var.node_pool_os
  os_version                      = var.node_pool_os_version
  enabled                         = var.worker_group_enabled
  mode                            = var.worker_group_mode
  boot_volume_size                = var.worker_group_boot_volume_size
  memory                          = var.worker_group_memory
  ocpus                           = var.worker_group_ocpus
  shape                           = var.worker_group_shape
  size                            = var.worker_group_size
  cloudinit                       = var.cloudinit_nodepool_common
  cluster_ca_cert                 = var.cluster_ca_cert
  kubernetes_version              = var.kubernetes_version
  pod_nsg_ids                     = try(split("","", lookup(module.network.nsg_ids, ""pods"", """")), [])
  worker_nsg_ids                  = coalescelist(var.worker_nsgs, try(split("","", lookup(module.network.nsg_ids, ""workers"", """")), []))
  assign_public_ip                = var.worker_type == ""public""
  subnet_id                       = coalesce(var.worker_group_primary_subnet_id, lookup(module.network.subnet_ids, ""workers"", """"))
  enable_pv_encryption_in_transit = var.enable_pv_encryption_in_transit
  sriov_num_vfs                   = var.sriov_num_vfs
  ssh_public_key                  = var.ssh_public_key
  ssh_public_key_path             = var.ssh_public_key_path
  timezone                        = var.node_pool_timezone
  volume_kms_key_id               = var.node_pool_volume_kms_key_id
  defined_tags                    = lookup(lookup(var.defined_tags, ""oke"", {}), ""node"", {})
  freeform_tags                   = lookup(lookup(var.freeform_tags, ""oke"", {}), ""node"", {})
  providers = {
    oci.home = oci.home
  }
}
",module,the block associated got renamed or deleted,,4,,4d2b3f3d672a8f41655da3a7c58fded42c6858f3,897bae1fd6cdbd22478066e6f93643a8f7482757,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/4d2b3f3d672a8f41655da3a7c58fded42c6858f3/workergroups.tf#L4,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/897bae1fd6cdbd22478066e6f93643a8f7482757/workerpools.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,2,1,1,0,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,90,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,0,# todo,# LOGIN # TODO,"##################### 
 # CONTROLLER: CLOUD # See variables_controller_instance.tf for the controller instance variables. 
 #####################  
 ######### 
 # LOGIN # TODO 
 ######### ","variable ""enable_login"" {
  description = <<EOD
Enables the creation of login nodes and instance templates.
EOD
  type        = bool
  default     = true
}
",variable,"variable ""enable_login"" {
  description = <<EOD
Enables the creation of login nodes and instance templates.
EOD
  type        = bool
  default     = true
}
",variable,91,,33bf402eaa82607a027754c6048fb0dce6d7668c,c6cada9ba77bff2158a2c1c7ef8e8c76f7978f20,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/33bf402eaa82607a027754c6048fb0dce6d7668c/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf#L91,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/c6cada9ba77bff2158a2c1c7ef8e8c76f7978f20/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,2023-10-26 09:59:26-07:00,2023-10-27 11:08:29-07:00,2,1,0,0,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,483,tests/fast/stages/s03_project_factory/fixture/main.tf,tests/fast/stages/s03_project_factory/fixture/main.tf,0,#todo,#TODO(sruffilli): Pin to release,#TODO(sruffilli): Pin to release,"module ""projects"" {
  #TODO(sruffilli): Pin to release
  source             = ""github.com/terraform-google-modules/cloud-foundation-fabric/examples/factories/project-factory""
  for_each           = local.projects
  defaults           = local.defaults
  project_id         = each.key
  billing_account_id = try(each.value.billing_account_id, null)
  billing_alert      = try(each.value.billing_alert, null)
  dns_zones          = try(each.value.dns_zones, [])
  essential_contacts = try(each.value.essential_contacts, [])
  folder_id          = each.value.folder_id
  group_iam          = try(each.value.group_iam, {})
  iam                = try(each.value.iam, {})
  kms_service_agents = try(each.value.kms, {})
  labels             = try(each.value.labels, {})
  org_policies       = try(each.value.org_policies, null)
  service_accounts   = try(each.value.service_accounts, {})
  services           = try(each.value.services, [])
  services_iam       = try(each.value.services_iam, {})
  vpc                = try(each.value.vpc, null)
}
",module,"module ""projects"" {
  source               = ""../../../../../fast/stages/03-project-factory/dev""
  data_dir             = ""./data/projects/""
  defaults_file        = ""./data/defaults.yaml""
  prefix               = ""test""
  billing_account_id   = ""12345-67890A-BCDEF0""
  environment_dns_zone = ""dev""
  shared_vpc_self_link = ""fake_link""
  vpc_host_project     = ""host_project""
}
",module,36,,cee207b4544cfe2bc2eb517fd91c79952e3052b3,1d187ddd236a0f522528139204f0b64d71d74d9e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/cee207b4544cfe2bc2eb517fd91c79952e3052b3/tests/fast/stages/s03_project_factory/fixture/main.tf#L36,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/1d187ddd236a0f522528139204f0b64d71d74d9e/tests/fast/stages/s03_project_factory/fixture/main.tf,2022-01-17 10:36:38+01:00,2022-02-15 12:22:08+01:00,5,1,0,1,0,0,0,0,0,0
https://github.com/alphagov/govuk-aws,322,terraform/projects/infra-security-groups/publishing-api.tf,terraform/projects/infra-security-groups/publishing-api.tf,0,# todo,# TODO: test whether egress rules are needed on elbs,# TODO: test whether egress rules are needed on elbs,"resource ""aws_security_group_rule"" ""allow_publishing-api_elb_external_egress"" {
  type              = ""egress""
  from_port         = 0
  to_port           = 0
  protocol          = ""-1""
  cidr_blocks       = [""0.0.0.0/0""]
  security_group_id = ""${aws_security_group.publishing-api_elb_external.id}""
}
",resource,the block associated got renamed or deleted,,99,,b212a5508ed19a405a50c07afee2d3d66c55a60b,7fd13330f8d6238e108f76ce76a76a28a99caaaf,https://github.com/alphagov/govuk-aws/blob/b212a5508ed19a405a50c07afee2d3d66c55a60b/terraform/projects/infra-security-groups/publishing-api.tf#L99,https://github.com/alphagov/govuk-aws/blob/7fd13330f8d6238e108f76ce76a76a28a99caaaf/terraform/projects/infra-security-groups/publishing-api.tf,2017-09-15 17:07:50+01:00,2018-01-02 17:41:32+00:00,4,1,0,0,0,1,1,0,0,1
https://github.com/ManagedKube/kubernetes-ops,6,terraform-modules/aws/opensearch/main.tf,terraform-modules/aws/opensearch/main.tf,0,ineffici,"# you can identify slow search operations and investigate potential causes, such as inefficient queries, resource constraints, or heavy search loads.","  # These logs record search and query operations that exceed the specified threshold, usually in terms of time taken to process the operation.
  # Search operations involve running queries against the OpenSearch index to retrieve documents. By analyzing search slow logs, 
  # you can identify slow search operations and investigate potential causes, such as inefficient queries, resource constraints, or heavy search loads.","resource ""aws_opensearch_domain"" ""this"" {
  domain_name    = var.domain_name
  engine_version = ""OpenSearch_2.5""

  cluster_config {
    instance_type          = var.instance_type
    zone_awareness_enabled = var.zone_awareness_enabled
    instance_count         = var.instance_count
  }

  ebs_options {
    ebs_enabled = var.ebs_enabled
    volume_size = var.volume_size
  }

  encrypt_at_rest {
    enabled = true
  }

  node_to_node_encryption {
    enabled = true
  }

  domain_endpoint_options {
    enforce_https       = var.enforce_https
    tls_security_policy = var.tls_security_policy
  }

  # the dynamic block creates a vpc_options block with the specified security group and subnet IDs.
  # If the variable vpc_enabled is set to false, the dynamic block is not created, 
  # and the aws_opensearch_domain resource will not include a vpc_options block, 
  # creating the OpenSearch domain publicly.
  dynamic ""vpc_options"" {
    for_each = var.vpc_enabled ? [1] : []
    content {
      security_group_ids = concat([aws_security_group.opensearch_sg.id], var.additional_security_group_ids)
      subnet_ids         = var.subnet_ids
    }
  }

  # The current configuration with Principal = { AWS = ""*"" } allows any authenticated AWS user or role to access the OpenSearch domain but the policy enforces that the access must be over a secure transport (HTTPS),
  # as specified in the Condition block.
  # Plese refer the Documentation for access policies https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/opensearch_domain#access-policy
  # use variable allowed_roles to update the policy with a user role who can access this
  access_policies = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""es:*""
        Effect = ""Allow""
        Principal = {
          AWS = var.allowed_roles
        }
        Resource = ""arn:aws:es:${var.aws_region}:${var.account_id}:domain/${var.domain_name}/*""
        Condition = var.vpc_enabled ? {
          Bool = {
            ""aws:SecureTransport"" = ""true""
          }
        } : {}
      }
    ]
  })

  # Index and search slow logs are logs generated by OpenSearch (formerly Elasticsearch) to record operations that take longer than a specified threshold. 
  # These logs help identify performance issues and bottlenecks within the OpenSearch cluster

  # Index Slow Logs: These logs record indexing operations that exceed the specified threshold, usually in terms of time taken to process the operation. 
  # Indexing operations involve adding, updating, or deleting documents in the OpenSearch index. By analyzing index slow logs, 
  # you can identify slow indexing operations and investigate potential causes, such as complex mappings, resource constraints, or heavy indexing loads.

  log_publishing_options {
    cloudwatch_log_group_arn = aws_cloudwatch_log_group.opensearch_slow_logs.arn
    log_type                 = ""INDEX_SLOW_LOGS""
  }

  # These logs record search and query operations that exceed the specified threshold, usually in terms of time taken to process the operation.
  # Search operations involve running queries against the OpenSearch index to retrieve documents. By analyzing search slow logs, 
  # you can identify slow search operations and investigate potential causes, such as inefficient queries, resource constraints, or heavy search loads.
  log_publishing_options {
    cloudwatch_log_group_arn = aws_cloudwatch_log_group.opensearch_slow_logs.arn
    log_type                 = ""SEARCH_SLOW_LOGS""
  }

  depends_on = [aws_security_group.opensearch_sg]
}
",resource,"resource ""aws_opensearch_domain"" ""this"" {
  domain_name    = var.domain_name
  engine_version = ""OpenSearch_2.5""

  cluster_config {
    instance_type          = var.instance_type
    zone_awareness_enabled = var.zone_awareness_enabled
    instance_count         = var.instance_count
  }

  ebs_options {
    ebs_enabled = var.ebs_enabled
    volume_size = var.volume_size
  }

  encrypt_at_rest {
    enabled = true
  }

  node_to_node_encryption {
    enabled = true
  }

  domain_endpoint_options {
    enforce_https       = var.enforce_https
    tls_security_policy = var.tls_security_policy
  }

  # the dynamic block creates a vpc_options block with the specified security group and subnet IDs.
  # If the variable vpc_enabled is set to false, the dynamic block is not created, 
  # and the aws_opensearch_domain resource will not include a vpc_options block, 
  # creating the OpenSearch domain publicly.
  dynamic ""vpc_options"" {
    for_each = var.vpc_enabled ? [1] : []
    content {
      security_group_ids = concat([aws_security_group.opensearch_sg.id], var.additional_security_group_ids)
      subnet_ids         = var.subnet_ids
    }
  }

  # The current configuration with Principal = { AWS = ""*"" } allows any authenticated AWS user or role to access the OpenSearch domain but the policy enforces that the access must be over a secure transport (HTTPS),
  # as specified in the Condition block.
  # Plese refer the Documentation for access policies https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/opensearch_domain#access-policy
  # use variable allowed_roles to update the policy with a user role who can access this
  access_policies = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""es:*""
        Effect = ""Allow""
        Principal = {
          AWS = var.allowed_roles
        }
        Resource = ""arn:aws:es:${var.aws_region}:${var.account_id}:domain/${var.domain_name}/*""
        Condition = var.vpc_enabled ? {
          Bool = {
            ""aws:SecureTransport"" = ""true""
          }
        } : {}
      }
    ]
  })

  # Index and search slow logs are logs generated by OpenSearch (formerly Elasticsearch) to record operations that take longer than a specified threshold. 
  # These logs help identify performance issues and bottlenecks within the OpenSearch cluster

  # Index Slow Logs: These logs record indexing operations that exceed the specified threshold, usually in terms of time taken to process the operation. 
  # Indexing operations involve adding, updating, or deleting documents in the OpenSearch index. By analyzing index slow logs, 
  # you can identify slow indexing operations and investigate potential causes, such as complex mappings, resource constraints, or heavy indexing loads.

  log_publishing_options {
    cloudwatch_log_group_arn = aws_cloudwatch_log_group.opensearch_slow_logs.arn
    log_type                 = ""INDEX_SLOW_LOGS""
  }

  # These logs record search and query operations that exceed the specified threshold, usually in terms of time taken to process the operation.
  # Search operations involve running queries against the OpenSearch index to retrieve documents. By analyzing search slow logs, 
  # you can identify slow search operations and investigate potential causes, such as inefficient queries, resource constraints, or heavy search loads.
  log_publishing_options {
    cloudwatch_log_group_arn = aws_cloudwatch_log_group.opensearch_slow_logs.arn
    log_type                 = ""SEARCH_SLOW_LOGS""
  }

  depends_on = [aws_security_group.opensearch_sg]
}
",resource,104,104.0,7c2c662a68a16d435ce56177a7a5ae659f94618d,7c2c662a68a16d435ce56177a7a5ae659f94618d,https://github.com/ManagedKube/kubernetes-ops/blob/7c2c662a68a16d435ce56177a7a5ae659f94618d/terraform-modules/aws/opensearch/main.tf#L104,https://github.com/ManagedKube/kubernetes-ops/blob/7c2c662a68a16d435ce56177a7a5ae659f94618d/terraform-modules/aws/opensearch/main.tf#L104,2023-03-21 13:14:52-07:00,2023-03-21 13:14:52-07:00,1,0,1,0,0,0,0,0,1,0
https://github.com/uyuni-project/sumaform,1408,modules/virthost/variables.tf,modules/virthost/variables.tf,0,fix,# WORKAROUND: temporary fix Error: HTTP 404: Not Found reading,"# default     = ""https://download.opensuse.org/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2"" 
 # WORKAROUND: temporary fix Error: HTTP 404: Not Found reading","variable ""hvm_disk_image"" {
  description = ""URL to the disk image to use for KVM guests""
  # default     = ""https://download.opensuse.org/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2""
  # WORKAROUND: temporary fix Error: HTTP 404: Not Found reading
  default = ""https://www.mirrorservice.org/sites/download.opensuse.org/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2""
}
",variable,"variable ""hvm_disk_image"" {
  description = ""URL to the disk image to use for KVM guests""
  default     = ""http://download.opensuse.org/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2""
}
",variable,75,,57e0ee45d4ed104becc937fc5eecbc9e4c2b8361,5d343d967b407b5f1645e34090646066c7fe2fed,https://github.com/uyuni-project/sumaform/blob/57e0ee45d4ed104becc937fc5eecbc9e4c2b8361/modules/virthost/variables.tf#L75,https://github.com/uyuni-project/sumaform/blob/5d343d967b407b5f1645e34090646066c7fe2fed/modules/virthost/variables.tf,2021-07-14 17:32:06+02:00,2021-08-10 12:11:54+02:00,4,1,1,1,0,0,0,0,0,0
https://github.com/alphagov/govuk-aws,1446,terraform/projects/infra-assets/main.tf,terraform/projects/infra-assets/main.tf,0,# todo,"# TODO: check if still used; clean up and remove (app_user, s3_writer).","# TODO: check if still used; clean up and remove (app_user, s3_writer).","resource ""aws_iam_user"" ""app_user"" {
  name = ""govuk-assets-${var.aws_environment}-user""
}
",resource,"resource ""aws_iam_user"" ""app_user"" {
  name = ""govuk-assets-${var.aws_environment}-user""
}
",resource,51,51.0,c1d29a6caabcd4f041d87d76a10f2deef06685ef,c1d29a6caabcd4f041d87d76a10f2deef06685ef,https://github.com/alphagov/govuk-aws/blob/c1d29a6caabcd4f041d87d76a10f2deef06685ef/terraform/projects/infra-assets/main.tf#L51,https://github.com/alphagov/govuk-aws/blob/c1d29a6caabcd4f041d87d76a10f2deef06685ef/terraform/projects/infra-assets/main.tf#L51,2023-06-02 10:34:23+01:00,2023-06-02 10:34:23+01:00,1,0,0,1,0,0,0,0,0,1
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,26,modules/gke-cluster/main.tf,modules/gke-cluster/main.tf,0,# todo,# TODO(ludomagno): compute addons map in locals and use a single dynamic block,"# node_config 
 # TODO(ludomagno): compute addons map in locals and use a single dynamic block ","resource ""google_container_cluster"" ""cluster"" {
  provider                    = google-beta
  project                     = var.project_id
  name                        = var.name
  description                 = var.description
  location                    = var.location
  node_locations              = length(var.node_locations) == 0 ? null : var.node_locations
  min_master_version          = var.min_master_version
  network                     = var.network
  subnetwork                  = var.subnetwork
  logging_service             = var.logging_service
  monitoring_service          = var.monitoring_service
  resource_labels             = var.labels
  default_max_pods_per_node   = var.default_max_pods_per_node
  enable_binary_authorization = var.enable_binary_authorization
  enable_intranode_visibility = var.enable_intranode_visibility
  enable_shielded_nodes       = var.enable_shielded_nodes
  enable_tpu                  = var.enable_tpu
  initial_node_count          = 1
  remove_default_node_pool    = true

  # node_config
  # TODO(ludomagno): compute addons map in locals and use a single dynamic block

  addons_config {
    dns_cache_config {
      enabled = var.addons.dns_cache_config
    }
    http_load_balancing {
      disabled = ! var.addons.http_load_balancing
    }
    horizontal_pod_autoscaling {
      disabled = ! var.addons.horizontal_pod_autoscaling
    }
    network_policy_config {
      disabled = ! var.addons.network_policy_config
    }
    # beta addons
    # cloudrun is dynamic as it tends to trigger cluster recreation on change
    dynamic cloudrun_config {
      for_each = var.addons.istio_config.enabled && var.addons.cloudrun_config ? [""""] : []
      content {
        disabled = false
      }
    }
    istio_config {
      disabled = ! var.addons.istio_config.enabled
      auth     = var.addons.istio_config.tls ? ""AUTH_MUTUAL_TLS"" : ""AUTH_NONE""
    }
  }

  # TODO(ludomagno): support setting address ranges instead of range names
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#cluster_ipv4_cidr_block
  ip_allocation_policy {
    cluster_secondary_range_name  = var.secondary_range_pods
    services_secondary_range_name = var.secondary_range_services
  }

  # TODO(ludomagno): make optional, and support beta feature
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#daily_maintenance_window
  maintenance_policy {
    daily_maintenance_window {
      start_time = var.maintenance_start_time
    }
  }

  master_auth {
    client_certificate_config {
      issue_client_certificate = false
    }
  }

  dynamic master_authorized_networks_config {
    for_each = length(var.master_authorized_ranges) == 0 ? [] : list(var.master_authorized_ranges)
    iterator = ranges
    content {
      dynamic cidr_blocks {
        for_each = ranges.value
        iterator = range
        content {
          cidr_block   = range.value
          display_name = range.key
        }
      }
    }
  }

  dynamic network_policy {
    for_each = var.addons.network_policy_config ? [""""] : []
    content {
      enabled  = true
      provider = ""CALICO""
    }
  }

  dynamic private_cluster_config {
    for_each = local.is_private ? [var.private_cluster_config] : []
    iterator = config
    content {
      enable_private_nodes    = config.value.enable_private_nodes
      enable_private_endpoint = config.value.enable_private_endpoint
      master_ipv4_cidr_block  = config.value.master_ipv4_cidr_block
    }
  }

  # beta features

  dynamic authenticator_groups_config {
    for_each = var.authenticator_security_group == null ? [] : [""""]
    content {
      security_group = var.authenticator_security_group
    }
  }

  dynamic cluster_autoscaling {
    for_each = var.cluster_autoscaling.enabled ? [var.cluster_autoscaling] : []
    iterator = config
    content {
      enabled = true
      resource_limits {
        resource_type = ""cpu""
        minimum       = config.cpu_min
        maximum       = config.cpu_max
      }
      resource_limits {
        resource_type = ""memory""
        minimum       = config.memory_min
        maximum       = config.memory_max
      }
    }
  }

  dynamic database_encryption {
    for_each = var.database_encryption.enabled ? [var.database_encryption] : []
    iterator = config
    content {
      state    = config.value.state
      key_name = config.value.key_name
    }
  }

  dynamic pod_security_policy_config {
    for_each = var.pod_security_policy != null ? [""""] : []
    content {
      enabled = var.pod_security_policy
    }
  }

  dynamic release_channel {
    for_each = var.release_channel != null ? [""""] : []
    content {
      channel = var.release_channel
    }
  }

  dynamic resource_usage_export_config {
    for_each = (
      var.resource_usage_export_config.enabled != null
      &&
      var.resource_usage_export_config.dataset != null
      ? [""""] : []
    )
    content {
      enable_network_egress_metering = var.resource_usage_export_config.enabled
      bigquery_destination {
        dataset_id = var.resource_usage_export_config.dataset
      }
    }
  }

  dynamic vertical_pod_autoscaling {
    for_each = var.vertical_pod_autoscaling == null ? [] : [""""]
    content {
      enabled = var.vertical_pod_autoscaling
    }
  }

  dynamic workload_identity_config {
    for_each = var.workload_identity ? [""""] : []
    content {
      identity_namespace = ""${var.project_id}.svc.id.goog""
    }
  }

}
",resource,"resource ""google_container_cluster"" ""cluster"" {
  provider    = google-beta
  project     = var.project_id
  name        = var.name
  description = var.description
  location    = var.location
  node_locations = (
    length(var.node_locations) == 0 ? null : var.node_locations
  )
  min_master_version = var.min_master_version
  network            = var.vpc_config.network
  subnetwork         = var.vpc_config.subnetwork
  resource_labels    = var.labels
  default_max_pods_per_node = (
    var.enable_features.autopilot ? null : var.max_pods_per_node
  )
  enable_intranode_visibility = (
    var.enable_features.autopilot ? null : var.enable_features.intranode_visibility
  )
  enable_l4_ilb_subsetting = var.enable_features.l4_ilb_subsetting
  enable_shielded_nodes = (
    var.enable_features.autopilot ? null : var.enable_features.shielded_nodes
  )
  enable_tpu               = var.enable_features.tpu
  initial_node_count       = 1
  remove_default_node_pool = var.enable_features.autopilot ? null : true
  datapath_provider = (
    var.enable_features.dataplane_v2
    ? ""ADVANCED_DATAPATH""
    : ""DATAPATH_PROVIDER_UNSPECIFIED""
  )
  enable_autopilot = var.enable_features.autopilot ? true : null

  # the default nodepool is deleted here, use the gke-nodepool module instead
  # node_config {}

  addons_config {
    dynamic ""dns_cache_config"" {
      for_each = !var.enable_features.autopilot ? [""""] : []
      content {
        enabled = var.enable_addons.dns_cache
      }
    }
    http_load_balancing {
      disabled = !var.enable_addons.http_load_balancing
    }
    horizontal_pod_autoscaling {
      disabled = !var.enable_addons.horizontal_pod_autoscaling
    }
    dynamic ""network_policy_config"" {
      for_each = !var.enable_features.autopilot ? [""""] : []
      content {
        disabled = !var.enable_addons.network_policy
      }
    }
    cloudrun_config {
      disabled = !var.enable_addons.cloudrun
    }
    istio_config {
      disabled = var.enable_addons.istio == null
      auth = (
        try(var.enable_addons.istio.enable_tls, false) ? ""AUTH_MUTUAL_TLS"" : ""AUTH_NONE""
      )
    }
    gce_persistent_disk_csi_driver_config {
      enabled = var.enable_addons.gce_persistent_disk_csi_driver
    }
    dynamic ""gcp_filestore_csi_driver_config"" {
      for_each = !var.enable_features.autopilot ? [""""] : []
      content {
        enabled = var.enable_addons.gcp_filestore_csi_driver
      }
    }
    kalm_config {
      enabled = var.enable_addons.kalm
    }
    config_connector_config {
      enabled = var.enable_addons.config_connector
    }
    gke_backup_agent_config {
      enabled = var.enable_addons.gke_backup_agent
    }
  }

  dynamic ""authenticator_groups_config"" {
    for_each = var.enable_features.groups_for_rbac != null ? [""""] : []
    content {
      security_group = var.enable_features.groups_for_rbac
    }
  }

  dynamic ""binary_authorization"" {
    for_each = var.enable_features.binary_authorization ? [""""] : []
    content {
      evaluation_mode = ""PROJECT_SINGLETON_POLICY_ENFORCE""
    }
  }

  dynamic ""cluster_autoscaling"" {
    for_each = var.cluster_autoscaling == null ? [] : [""""]
    content {
      enabled = true
      dynamic ""resource_limits"" {
        for_each = var.cluster_autoscaling.cpu_limits != null ? [""""] : []
        content {
          resource_type = ""cpu""
          minimum       = var.cluster_autoscaling.cpu_limits.min
          maximum       = var.cluster_autoscaling.cpu_limits.max
        }
      }
      dynamic ""resource_limits"" {
        for_each = var.cluster_autoscaling.mem_limits != null ? [""""] : []
        content {
          resource_type = ""cpu""
          minimum       = var.cluster_autoscaling.mem_limits.min
          maximum       = var.cluster_autoscaling.mem_limits.max
        }
      }
      // TODO: support GPUs too
    }
  }

  dynamic ""database_encryption"" {
    for_each = var.enable_features.database_encryption != null ? [""""] : []
    content {
      state    = var.enable_features.database_encryption.state
      key_name = var.enable_features.database_encryption.key_name
    }
  }

  dynamic ""dns_config"" {
    for_each = var.enable_features.cloud_dns != null ? [""""] : []
    content {
      cluster_dns        = enable_features.cloud_dns.cluster_dns
      cluster_dns_scope  = enable_features.cloud_dns.cluster_dns_scope
      cluster_dns_domain = enable_features.cloud_dns.cluster_dns_domain
    }
  }

  dynamic ""ip_allocation_policy"" {
    for_each = var.vpc_config.secondary_range_blocks != null ? [""""] : []
    content {
      cluster_ipv4_cidr_block  = var.vpc_config.secondary_range_blocks.pods
      services_ipv4_cidr_block = var.vpc_config.secondary_range_blocks.services
    }
  }
  dynamic ""ip_allocation_policy"" {
    for_each = var.vpc_config.secondary_range_names != null ? [""""] : []
    content {
      cluster_secondary_range_name  = var.vpc_config.secondary_range_names.pods
      services_secondary_range_name = var.vpc_config.secondary_range_names.services
    }
  }

  dynamic ""logging_config"" {
    for_each = var.logging_config != null ? [""""] : []
    content {
      enable_components = var.logging_config
    }
  }

  maintenance_policy {
    dynamic ""daily_maintenance_window"" {
      for_each = (
        try(var.maintenance_config.daily_window_start_time, null) != null
        ? [""""]
        : []
      )
      content {
        start_time = var.maintenance_config.daily_window_start_time
      }
    }
    dynamic ""recurring_window"" {
      for_each = (
        try(var.maintenance_config.recurring_window, null) != null
        ? [""""]
        : []
      )
      content {
        start_time = var.maintenance_config.recurring_window.start_time
        end_time   = var.maintenance_config.recurring_window.end_time
        recurrence = var.maintenance_config.recurring_window.recurrence
      }
    }
    dynamic ""maintenance_exclusion"" {
      for_each = (
        try(var.maintenance_config.maintenance_exclusions, null) == null
        ? []
        : var.maintenance_config.maintenance_exclusions
      )
      iterator = exclusion
      content {
        exclusion_name = exclusion.value.name
        start_time     = exclusion.value.start_time
        end_time       = exclusion.value.end_time
      }
    }
  }

  master_auth {
    client_certificate_config {
      issue_client_certificate = var.issue_client_certificate
    }
  }

  dynamic ""master_authorized_networks_config"" {
    for_each = var.vpc_config.master_authorized_ranges != null ? [""""] : []
    content {
      dynamic ""cidr_blocks"" {
        for_each = var.vpc_config.master_authorized_ranges
        iterator = range
        content {
          cidr_block   = range.value
          display_name = range.key
        }
      }
    }
  }

  dynamic ""monitoring_config"" {
    for_each = var.monitoring_config != null ? [""""] : []
    content {
      enable_components = var.monitoring_config
    }
  }

  # dataplane v2 has bult-in network policies
  dynamic ""network_policy"" {
    for_each = (
      var.enable_addons.network_policy && !var.enable_features.dataplane_v2
      ? [""""]
      : []
    )
    content {
      enabled  = true
      provider = ""CALICO""
    }
  }

  dynamic ""notification_config"" {
    for_each = var.enable_features.upgrade_notifications != null ? [""""] : []
    content {
      pubsub {
        enabled = true
        topic = (
          try(var.enable_features.upgrade_notifications.topic_id, null) != null
          ? var.enable_features.upgrade_notifications.topic_id
          : google_pubsub_topic.notifications[0].id
        )
      }
    }
  }

  dynamic ""private_cluster_config"" {
    for_each = (
      var.private_cluster_config != null ? [""""] : []
    )
    content {
      enable_private_nodes    = true
      enable_private_endpoint = var.private_cluster_config.enable_private_endpoint
      master_ipv4_cidr_block  = var.private_cluster_config.master_ipv4_cidr_block
      master_global_access_config {
        enabled = var.private_cluster_config.master_global_access
      }
    }
  }

  dynamic ""pod_security_policy_config"" {
    for_each = var.enable_features.pod_security_policy ? [""""] : []
    content {
      enabled = var.enable_features.pod_security_policy
    }
  }

  dynamic ""release_channel"" {
    for_each = var.release_channel != null ? [""""] : []
    content {
      channel = var.release_channel
    }
  }

  dynamic ""resource_usage_export_config"" {
    for_each = (
      try(var.enable_features.resource_usage_export.dataset, null) != null
      ? [""""]
      : []
    )
    content {
      enable_network_egress_metering = (
        var.enable_features.resource_usage_export.enable_network_egress_metering
      )
      enable_resource_consumption_metering = (
        var.enable_features.resource_usage_export.enable_resource_consumption_metering
      )
      bigquery_destination {
        dataset_id = var.enable_features.resource_usage_export.dataset
      }
    }
  }

  dynamic ""vertical_pod_autoscaling"" {
    for_each = var.enable_features.vertical_pod_autoscaling ? [""""] : []
    content {
      enabled = var.enable_features.vertical_pod_autoscaling
    }
  }

  dynamic ""workload_identity_config"" {
    for_each = var.enable_features.workload_identity ? [""""] : []
    content {
      workload_pool = ""${var.project_id}.svc.id.goog""
    }
  }
}
",resource,49,,587f6113b214d466f45a5ad7eff73262426b7397,16822e94ab70d75099214b9db786affcb231fbf6,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/587f6113b214d466f45a5ad7eff73262426b7397/modules/gke-cluster/main.tf#L49,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/16822e94ab70d75099214b9db786affcb231fbf6/modules/gke-cluster/main.tf,2020-04-23 09:54:04+02:00,2022-10-10 09:38:21+02:00,34,1,1,1,0,0,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,306,modules/kubernetes-addons/external-dns/data.tf,modules/kubernetes-addons/external-dns/data.tf,0,todo,# TODO - remove at next breaking change,# TODO - remove at next breaking change,"data ""aws_route53_zone"" ""selected"" {
  name         = var.domain_name
  private_zone = var.private_zone
}
",data,,,1,0.0,e6c597ecd70be756fd5be142a46519555e448066,85a0fca86ea422a2273a5e2845ce4e4a30413f37,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/e6c597ecd70be756fd5be142a46519555e448066/modules/kubernetes-addons/external-dns/data.tf#L1,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/85a0fca86ea422a2273a5e2845ce4e4a30413f37/modules/kubernetes-addons/external-dns/data.tf#L0,2022-08-10 11:46:16-04:00,2022-10-28 18:55:20-04:00,2,2,0,1,0,0,1,0,0,0
https://github.com/ManagedKube/kubernetes-ops,2,terraform-modules/aws/cloudposse/aws-cloudtrail-cloudwatch-alarms/main.tf,terraform-modules/aws/cloudposse/aws-cloudtrail-cloudwatch-alarms/main.tf,0,/*todo,"/*ToDo: We are collaborating with cloudposse to bring this solution to your project, we have the task of following up this pr to integrate it            and return to the direct version of cloudposse.                      Cloudposse' issue: New input variable s3_object_ownership cloudposse/terraform-aws-cloudtrail-s3-bucket#62           Cloudposse' pr: add input var s3_object_ownership cloudposse/terraform-aws-cloudtrail-s3-bucket#63*/","/*ToDo: We are collaborating with cloudposse to bring this solution to your project, we have the task of following up this pr to integrate it 
 and return to the direct version of cloudposse. 
  
 Cloudposse' issue: New input variable s3_object_ownership cloudposse/terraform-aws-cloudtrail-s3-bucket#62 
 Cloudposse' pr: add input var s3_object_ownership cloudposse/terraform-aws-cloudtrail-s3-bucket#63 
 */","module ""cloudtrail_s3_bucket"" {
  source  = ""github.com/ManagedKube/terraform-aws-cloudtrail-s3-bucket.git//?ref=0.24.0""
  #version = ""master""
  force_destroy          = var.force_destroy
  versioning_enabled     = var.versioning_enabled
  access_log_bucket_name = var.access_log_bucket_name
  allow_ssl_requests_only= var.allow_ssl_requests_only
  acl                    = var.acl
  s3_object_ownership    = var.s3_object_ownership
  sse_algorithm          = ""aws:kms""
  context = module.this.context
}
",module,"module ""cloudtrail_s3_bucket"" {
  source  = ""github.com/ManagedKube/terraform-aws-cloudtrail-s3-bucket.git//?ref=0.24.0""
  #version = ""master""
  force_destroy          = var.force_destroy
  versioning_enabled     = var.versioning_enabled
  access_log_bucket_name = var.access_log_bucket_name
  allow_ssl_requests_only= var.allow_ssl_requests_only
  acl                    = var.acl
  s3_object_ownership    = var.s3_object_ownership
  sse_algorithm          = ""aws:kms""
  context = module.this.context
}
",module,4,,9370126da6d0e1ff99a7a6cedbdd8e1a696f8a65,666deaf5b451460ad4b7253ba6b82bf6a64a018a,https://github.com/ManagedKube/kubernetes-ops/blob/9370126da6d0e1ff99a7a6cedbdd8e1a696f8a65/terraform-modules/aws/cloudposse/aws-cloudtrail-cloudwatch-alarms/main.tf#L4,https://github.com/ManagedKube/kubernetes-ops/blob/666deaf5b451460ad4b7253ba6b82bf6a64a018a/terraform-modules/aws/cloudposse/aws-cloudtrail-cloudwatch-alarms/main.tf,2022-06-24 10:30:24-07:00,2022-07-18 14:43:06-07:00,3,1,1,0,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,404,modules/vpc-sc/access_levels.tf,modules/vpc-sc/access-levels.tf,1,# todo,# TODO(ludomagno): add a second variable and resource for custom access levels,"/** 
 * Copyright 2021 Google LLC 
 * 
 * Licensed under the Apache License, Version 2.0 (the ""License""); 
 * you may not use this file except in compliance with the License. 
 * You may obtain a copy of the License at 
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, software 
 * distributed under the License is distributed on an ""AS IS"" BASIS, 
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 * See the License for the specific language governing permissions and 
 * limitations under the License. 
 */  
 # TODO(ludomagno): add a second variable and resource for custom access levels  
 # this code implements ""additive"" access levels, if ""authoritative"" 
 # access levels are needed, switch to the 
 # google_access_context_manager_access_levels resource ","resource ""google_access_context_manager_access_level"" ""basic"" {
  for_each = var.access_levels
  parent   = ""accessPolicies/${local.access_policy}""
  name     = ""accessPolicies/${local.access_policy}/accessLevels/${each.key}""
  title    = each.key
  basic {
    combining_function = each.value.combining_function
    dynamic ""conditions"" {
      for_each = toset(
        each.value.conditions == null ? [] : each.value.conditions
      )
      iterator = condition
      content {
        dynamic ""device_policy"" {
          for_each = toset(
            condition.key.device_policy == null ? [] : [condition.key.device_policy]
          )
          iterator = device_policy
          content {
            dynamic ""os_constraints"" {
              for_each = toset(
                device_policy.key.os_constraints == null ? [] : device_policy.key.os_constraints
              )
              iterator = os_constraint
              content {
                minimum_version            = os_constraint.key.minimum_version
                os_type                    = os_constraint.key.os_type
                require_verified_chrome_os = os_constraint.key.require_verified_chrome_os
              }
            }
            allowed_encryption_statuses      = device_policy.key.allowed_encryption_statuses
            allowed_device_management_levels = device_policy.key.allowed_device_management_levels
            require_admin_approval           = device_policy.key.require_admin_approval
            require_corp_owned               = device_policy.key.require_corp_owned
            require_screen_lock              = device_policy.key.require_screen_lock
          }
        }
        ip_subnetworks = (
          condition.key.ip_subnetworks == null ? [] : condition.key.ip_subnetworks
        )
        members = (
          condition.key.members == null ? [] : condition.key.members
        )
        negate = condition.key.negate
        regions = (
          condition.key.regions == null ? [] : condition.key.regions
        )
        required_access_levels = (
          condition.key.required_access_levels == null
          ? []
          : condition.key.required_access_levels
        )
      }
    }
  }
}
",resource,"resource ""google_access_context_manager_access_level"" ""basic"" {
  for_each    = var.access_levels
  parent      = ""accessPolicies/${local.access_policy}""
  name        = ""accessPolicies/${local.access_policy}/accessLevels/${each.key}""
  title       = each.key
  description = each.value.description

  basic {
    combining_function = each.value.combining_function

    dynamic ""conditions"" {
      for_each = toset(each.value.conditions)
      iterator = c
      content {
        ip_subnetworks         = c.value.ip_subnetworks
        members                = c.value.members
        negate                 = c.value.negate
        regions                = c.value.regions
        required_access_levels = coalesce(c.value.required_access_levels, [])

        dynamic ""device_policy"" {
          for_each = c.value.device_policy == null ? [] : [c.value.device_policy]
          iterator = dp
          content {

            allowed_device_management_levels = (
              dp.value.allowed_device_management_levels
            )
            allowed_encryption_statuses = (
              dp.value.allowed_encryption_statuses
            )
            require_admin_approval = dp.value.key.require_admin_approval
            require_corp_owned     = dp.value.require_corp_owned
            require_screen_lock    = dp.value.require_screen_lock

            dynamic ""os_constraints"" {
              for_each = toset(
                dp.value.os_constraints == null
                ? []
                : dp.value.os_constraints
              )
              iterator = oc
              content {
                minimum_version            = oc.value.minimum_version
                os_type                    = oc.value.os_type
                require_verified_chrome_os = oc.value.require_verified_chrome_os
              }
            }

          }
        }

      }
    }

  }
}
",resource,17,,2c7dab3bb2b572c4d60bc91ba48eb9e4ffbb8913,a9c47681d8c6a8e4ed7ff4f3ccfbdcf09fc575a4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2c7dab3bb2b572c4d60bc91ba48eb9e4ffbb8913/modules/vpc-sc/access_levels.tf#L17,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/a9c47681d8c6a8e4ed7ff4f3ccfbdcf09fc575a4/modules/vpc-sc/access-levels.tf,2021-12-31 13:29:22+01:00,2022-11-10 19:34:45+01:00,5,1,0,1,0,1,0,0,0,0
https://github.com/ministryofjustice/modernisation-platform,61,terraform/modernisation-platform-account/secrets.tf,terraform/modernisation-platform-account/secrets.tf,0,nuke,# Account IDs to be excluded from auto-nuke,"# Account IDs to be excluded from auto-nuke 
 # Tfsec ignore 
 # - AWS095: No requirement currently to encrypt this secret with customer-managed KMS key 
 #tfsec:ignore:AWS095","resource ""aws_secretsmanager_secret"" ""nuke_account_blocklist"" {
  # checkov:skip=CKV_AWS_149:No requirement currently to encrypt this secret with customer-managed KMS key
  name        = ""nuke_account_blocklist""
  description = ""Account IDs to be excluded from auto-nuke. AWS-Nuke (https://github.com/rebuy-de/aws-nuke) requires at least one Account ID to be present in this blocklist, while it is recommended to add every production account to this blocklist.""
  tags        = local.tags
}
",resource,"resource ""aws_secretsmanager_secret"" ""nuke_account_blocklist"" {
  # checkov:skip=CKV2_AWS_57:Auto rotation not possible
  name        = ""nuke_account_blocklist""
  description = ""Account IDs to be excluded from auto-nuke. AWS-Nuke (https://github.com/rebuy-de/aws-nuke) requires at least one Account ID to be present in this blocklist, while it is recommended to add every production account to this blocklist.""
  kms_key_id  = aws_kms_key.secrets_key.id
  tags        = local.tags
  replica {
    region = local.replica_region
  }
}
",resource,62,115.0,193e4b1f222be19253dcc26fced826c50f64bd4c,f5df80ab6ca1c06741f9457065b2bed2f0e04125,https://github.com/ministryofjustice/modernisation-platform/blob/193e4b1f222be19253dcc26fced826c50f64bd4c/terraform/modernisation-platform-account/secrets.tf#L62,https://github.com/ministryofjustice/modernisation-platform/blob/f5df80ab6ca1c06741f9457065b2bed2f0e04125/terraform/modernisation-platform-account/secrets.tf#L115,2022-05-03 14:13:24+01:00,2024-03-22 04:50:35+00:00,22,0,0,0,0,1,0,0,0,1
https://github.com/terraform-aws-modules/terraform-aws-eks,649,modules/karpenter/variables.tf,modules/karpenter/variables.tf,0,todo,# TODO - Change default to `true` at next breaking change,"################################################################################ 
 # Pod Identity Association 
 ################################################################################ 
 # TODO - Change default to `true` at next breaking change","variable ""create_pod_identity_association"" {
  description = ""Determines whether to create pod identity association""
  type        = bool
  default     = false
}
",variable,"variable ""create_pod_identity_association"" {
  description = ""Determines whether to create pod identity association""
  type        = bool
  default     = false
}
",variable,144,144.0,cfcaf27ac78278916ebf3d51dc64a20fe0d7bf01,cfcaf27ac78278916ebf3d51dc64a20fe0d7bf01,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/cfcaf27ac78278916ebf3d51dc64a20fe0d7bf01/modules/karpenter/variables.tf#L144,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/cfcaf27ac78278916ebf3d51dc64a20fe0d7bf01/modules/karpenter/variables.tf#L144,2024-05-09 07:57:57-04:00,2024-05-09 07:57:57-04:00,1,0,1,1,0,0,0,0,0,0
https://github.com/alphagov/govuk-aws,1488,terraform/projects/app-publishing-amazonmq/main.tf,terraform/projects/app-publishing-amazonmq/main.tf,0,# todo,# TODO: remove redundant FQDN.,# TODO: remove redundant FQDN.,"resource ""aws_route53_record"" ""publishing_amazonmq_internal_root_domain_name"" {
  zone_id = data.terraform_remote_state.infra_root_dns_zones.outputs.internal_root_zone_id
  # TODO: remove redundant FQDN.
  name = ""${lower(aws_mq_broker.publishing_amazonmq.broker_name)}.${data.terraform_remote_state.infra_root_dns_zones.outputs.internal_root_domain_name}""
  type = ""A""

  alias {
    name                   = aws_lb.publishingmq_lb_internal.dns_name
    zone_id                = aws_lb.publishingmq_lb_internal.zone_id
    evaluate_target_health = true
  }
}
",resource,"resource ""aws_route53_record"" ""publishing_amazonmq_internal_root_domain_name"" {
  zone_id = data.terraform_remote_state.infra_root_dns_zones.outputs.internal_root_zone_id
  # TODO: remove redundant FQDN.
  name = ""${lower(aws_mq_broker.publishing_amazonmq.broker_name)}.${data.terraform_remote_state.infra_root_dns_zones.outputs.internal_root_domain_name}""
  type = ""A""

  alias {
    name                   = aws_lb.publishingmq_lb_internal.dns_name
    zone_id                = aws_lb.publishingmq_lb_internal.zone_id
    evaluate_target_health = true
  }
}
",resource,279,246.0,60917714286eda71ec88d3bc3966268d8b2e29d9,03b331be65675f8da737783528a48c59db3b9c7f,https://github.com/alphagov/govuk-aws/blob/60917714286eda71ec88d3bc3966268d8b2e29d9/terraform/projects/app-publishing-amazonmq/main.tf#L279,https://github.com/alphagov/govuk-aws/blob/03b331be65675f8da737783528a48c59db3b9c7f/terraform/projects/app-publishing-amazonmq/main.tf#L246,2024-04-19 15:38:13+01:00,2024-05-02 14:08:35+01:00,10,0,0,1,0,0,1,0,0,0
https://github.com/CDCgov/prime-simplereport,94,ops/services/virtual_network/main.tf,ops/services/virtual_network/main.tf,0,# todo,# TODO: Import the existing links for each standing environment.,"# DNS/VNet linkage for Flexible DB functionality 
 # TODO: Import the existing links for each standing environment.","resource ""azurerm_private_dns_zone_virtual_network_link"" ""vnet_link"" {
  name                  = ""${var.env}-vnet-dns-link""
  resource_group_name   = var.resource_group_name
  private_dns_zone_name = azurerm_private_dns_zone.default.name
  virtual_network_id    = azurerm_virtual_network.vn.id
}
",resource,"resource ""azurerm_private_dns_zone_virtual_network_link"" ""vnet_link"" {
  name                  = ""${var.env}-vnet-dns-link""
  resource_group_name   = var.resource_group_name
  private_dns_zone_name = azurerm_private_dns_zone.default.name
  virtual_network_id    = azurerm_virtual_network.vn.id
}
",resource,66,67.0,c617c1ab4838016aa686ecff84fde0e6be45b8a5,7bfb9a5275daaed20cb854ea8390e176b3cd8536,https://github.com/CDCgov/prime-simplereport/blob/c617c1ab4838016aa686ecff84fde0e6be45b8a5/ops/services/virtual_network/main.tf#L66,https://github.com/CDCgov/prime-simplereport/blob/7bfb9a5275daaed20cb854ea8390e176b3cd8536/ops/services/virtual_network/main.tf#L67,2022-05-05 00:48:15-05:00,2023-05-30 21:03:57-05:00,4,0,0,1,0,0,1,1,0,0
https://github.com/kbst/terraform-kubestack,27,aws/cluster-local/configuration.tf,aws/cluster-local/configuration.tf,0,implementation,# in the local implementation we don't have access to that,"# on AWS the region is determined by the provider configuration 
 # in the local implementation we don't have access to that 
 # to still support multi-region setups locally, we hash the availability zones 
 # and use that as the region part of the cluster name prefixed with eks-","locals {
  # current workspace config
  cfg = module.configuration.merged[terraform.workspace]

  name_prefix = local.cfg[""name_prefix""]

  base_domain = local.cfg[""base_domain""]

  http_port_default = terraform.workspace == ""apps"" ? 80 : 8080
  http_port         = lookup(local.cfg, ""http_port"", local.http_port_default)

  https_port_default = terraform.workspace == ""apps"" ? 443 : 8443
  https_port         = lookup(local.cfg, ""https_port"", local.https_port_default)

  manifest_path_default = ""manifests/overlays/${terraform.workspace}""
  manifest_path         = var.manifest_path != null ? var.manifest_path : local.manifest_path_default

  disable_default_ingress = lookup(local.cfg, ""disable_default_ingress"", false)

  node_image = lookup(local.cfg, ""node_image"", ""kindest/node:v1.18.0"")

  node_count = lookup(local.cfg, ""cluster_min_size"", 1)
  nodes = [
    for node, _ in range(local.node_count) :
    ""worker""
  ]
  extra_nodes = join("","", local.nodes)

  # on AWS the region is determined by the provider configuration
  # in the local implementation we don't have access to that
  # to still support multi-region setups locally, we hash the availability zones
  # and use that as the region part of the cluster name prefixed with eks-
  cluster_availability_zones_lookup = lookup(local.cfg, ""cluster_availability_zones"", """")
  fake_region_hash                  = substr(sha256(local.cluster_availability_zones_lookup), 0, 7)
  fake_region                       = ""eks-${local.fake_region_hash}""
}
",locals,"locals {
  # current workspace config
  cfg = module.configuration.merged[terraform.workspace]

  name_prefix = local.cfg[""name_prefix""]

  base_domain = local.cfg[""base_domain""]

  http_port_default = terraform.workspace == ""apps"" ? 80 : 8080
  http_port         = lookup(local.cfg, ""http_port"", local.http_port_default)

  https_port_default = terraform.workspace == ""apps"" ? 443 : 8443
  https_port         = lookup(local.cfg, ""https_port"", local.https_port_default)

  disable_default_ingress = lookup(local.cfg, ""disable_default_ingress"", false)

  node_image = lookup(local.cfg, ""node_image"", null)

  node_count = lookup(local.cfg, ""cluster_min_size"", 1)
  nodes = [
    for node, _ in range(local.node_count) :
    ""worker""
  ]
  extra_nodes = join("","", local.nodes)

  # on AWS the region is determined by the provider configuration
  # in the local implementation we don't have access to that
  # to still support multi-region setups locally, we hash the availability zones
  # and use that as the region part of the cluster name prefixed with eks-
  cluster_availability_zones_lookup = lookup(local.cfg, ""cluster_availability_zones"", """")
  fake_region_hash                  = substr(sha256(local.cluster_availability_zones_lookup), 0, 7)
  fake_region                       = ""eks-${local.fake_region_hash}""
}
",locals,37,34.0,40af925aa6e6543373a298870aadf27d1672f58d,67b7dfce00f2dc67a9293d74ef5ac80879de5a2b,https://github.com/kbst/terraform-kubestack/blob/40af925aa6e6543373a298870aadf27d1672f58d/aws/cluster-local/configuration.tf#L37,https://github.com/kbst/terraform-kubestack/blob/67b7dfce00f2dc67a9293d74ef5ac80879de5a2b/aws/cluster-local/configuration.tf#L34,2020-09-29 15:18:43+02:00,2021-05-25 21:22:58+02:00,3,0,0,1,1,1,0,0,0,0
https://github.com/uyuni-project/sumaform,20,openstack_host/variables.tf,openstack_host/variables.tf,0,# todo,# TODO: instead of root/vagrant,"# TODO: instead of root/vagrant 
 #variable ""openstack_keypair"" { 
 #    description = ""The keypair to be used."" 
 #    default  = """" 
 #} ","variable ""avahi-domain"" {
  default = ""vagrant.local""
}
",variable,"variable ""avahi-domain"" {
  default = ""vagrant.local""
}
",variable,16,,89f41bf3e6d1f35e418daeb0663dfb81d05b1eb4,f16a1b7a10364b97d987a840f038f1bd89573ff8,https://github.com/uyuni-project/sumaform/blob/89f41bf3e6d1f35e418daeb0663dfb81d05b1eb4/openstack_host/variables.tf#L16,https://github.com/uyuni-project/sumaform/blob/f16a1b7a10364b97d987a840f038f1bd89573ff8/openstack_host/variables.tf,2016-06-30 17:49:17+02:00,2016-07-01 17:14:45+02:00,3,1,0,1,0,1,0,0,0,0
https://github.com/alphagov/govuk-aws,1274,terraform/projects/app-amazonmq/main.tf,terraform/projects/app-publishing-amazonmq/main.tf,1,# todo,"# TODO: this version will only work with a single instance, as on integration. ","# TODO: this version will only work with a single instance, as on integration. 
 # For staging/production, we'll have a highly-available cluster, at which point 
 # we'll need to repoint this Route53 record at a Network Load Balancer that balances 
 # between the instances. See Amazon's article about how to do that here: 
 # https://aws.amazon.com/blogs/compute/creating-static-custom-domain-endpoints-with-amazon-mq-for-rabbitmq/","resource ""aws_route53_record"" ""amazonmq_internal_root_domain_name"" {
  zone_id = data.terraform_remote_state.infra_root_dns_zones.outputs.internal_root_zone_id
  name    = ""${aws_mq_broker.publishing_amazonmq.broker_name}.${data.terraform_remote_state.infra_root_dns_zones.outputs.internal_root_domain_name}""
  type    = ""CNAME""
  ttl     = 300
  # TODO: this version will only work with a single instance, as on integration. 
  # For staging/production, we'll have a highly-available cluster, at which point
  # we'll need to repoint this Route53 record at a Network Load Balancer that balances
  # between the instances. See Amazon's article about how to do that here:
  # https://aws.amazon.com/blogs/compute/creating-static-custom-domain-endpoints-with-amazon-mq-for-rabbitmq/
  records = [regex(""://([^/:]+)"", aws_mq_broker.publishing_amazonmq.instances.0.console_url)[0]]

}
",resource,the block associated got renamed or deleted,,126,,7bc6821c6e0ca7490ebc9a6b9388a1e3565d7cdf,7562838d07e1e9808599970c19e0c5589a58c83c,https://github.com/alphagov/govuk-aws/blob/7bc6821c6e0ca7490ebc9a6b9388a1e3565d7cdf/terraform/projects/app-amazonmq/main.tf#L126,https://github.com/alphagov/govuk-aws/blob/7562838d07e1e9808599970c19e0c5589a58c83c/terraform/projects/app-publishing-amazonmq/main.tf,2022-11-29 10:57:17+00:00,2023-01-20 10:56:58+00:00,18,1,0,1,0,0,1,1,0,0
https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules,18,modules/vm-series/main.tf,modules/vm-series/main.tf,0,fix,# FIXME move it,"variable ""lb_backend_pool_ids"" { # FIXME move it","variable ""lb_backend_pool_ids"" { # FIXME move it 
}
",variable,the block associated got renamed or deleted,,47,,8d5eba64b06fb6b8420aa918a46de617228b3358,aa456ab5fd8767a2a3e40959d3a2376b24ceb368,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/8d5eba64b06fb6b8420aa918a46de617228b3358/modules/vm-series/main.tf#L47,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/aa456ab5fd8767a2a3e40959d3a2376b24ceb368/modules/vm-series/main.tf,2021-02-17 13:27:46+01:00,2021-02-17 13:27:46+01:00,3,1,0,1,0,0,1,0,0,0
https://github.com/wireapp/wire-server-deploy,19,terraform/modules/aws-vpc-security-groups/main.tf,terraform/modules/aws-vpc-security-groups/main.tf,0,fix,"# we go with the CIDR for now, which is hard-coded and needs fixing","# NOTE: NLBs dont allow security groups to be set on them, which is why 
 # we go with the CIDR for now, which is hard-coded and needs fixing","resource ""aws_security_group"" ""k8s_node"" {
  name        = ""k8s_node""
  description = ""hosts that have kubernetes.""
  vpc_id      = var.vpc_id

  # incoming from the admin node (kubectl)
  ingress {
    from_port       = 6443
    to_port         = 6443
    protocol        = ""tcp""
    security_groups = [""${aws_security_group.talk_to_k8s.id}""]
  }

  # FIXME: tighten this up.
  ingress {
    from_port       = 0
    to_port         = 65535
    protocol        = ""tcp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # FIXME: tighten this up. need UDP for flannel.
  ingress {
    from_port       = 0
    to_port         = 65535
    protocol        = ""udp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # incoming traffic to the application.
  ingress {
    from_port   = 31772
    to_port     = 31773
    protocol    = ""tcp""
    # NOTE: NLBs dont allow security groups to be set on them, which is why
    # we go with the CIDR for now, which is hard-coded and needs fixing
    cidr_blocks = [""172.17.0.0/20""]
  }

  tags = {
    Name = ""k8s_node""
  }
}
",resource,"resource ""aws_security_group"" ""k8s_node"" {
  name        = ""k8s_node""
  description = ""hosts that have kubernetes.""
  vpc_id      = var.vpc_id

  # incoming from the admin node (kubectl)
  ingress {
    description     = """"
    from_port       = 6443
    to_port         = 6443
    protocol        = ""tcp""
    security_groups = [""${aws_security_group.talk_to_k8s.id}""]
  }

  # FIXME: tighten this up.
  ingress {
    description     = """"
    from_port       = 0
    to_port         = 65535
    protocol        = ""tcp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # FIXME: tighten this up. need UDP for flannel.
  ingress {
    description     = """"
    from_port       = 0
    to_port         = 65535
    protocol        = ""udp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # incoming traffic to the application.
  ingress {
    from_port   = 31772
    to_port     = 31773
    protocol    = ""tcp""
    # NOTE: NLBs dont allow security groups to be set on them, which is why
    # we go with the CIDR for now, which is hard-coded and needs fixing
    cidr_blocks = [""172.17.0.0/20""]
  }

  tags = {
    Name = ""k8s_node""
  }
}
",resource,244,249.0,1e8874480a2745e81a973d5e2eee84c58baa983e,e7f081ca744de60bf9d10cb037828b995883d2aa,https://github.com/wireapp/wire-server-deploy/blob/1e8874480a2745e81a973d5e2eee84c58baa983e/terraform/modules/aws-vpc-security-groups/main.tf#L244,https://github.com/wireapp/wire-server-deploy/blob/e7f081ca744de60bf9d10cb037828b995883d2aa/terraform/modules/aws-vpc-security-groups/main.tf#L249,2020-06-24 14:14:54+01:00,2020-08-26 16:29:39+02:00,3,0,0,1,0,0,1,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,154,libvirt/terraform/main.tf,libvirt/terraform/main.tf,0,// todo,// todo: verify this,// todo: verify this,"module ""monitoring"" {
  source             = ""./modules/monitoring""
  base_configuration = ""${module.base.configuration}""

  name                   = ""monitoring""
  count                  = 1
  vcpu                   = 4
  memory                 = 4095
  // todo: verify this
  host_ips               = ""${var.host_ips}""
  
  reg_code               = ""${var.reg_code}""
  reg_email              = ""${var.reg_email}""
  reg_additional_modules = ""${var.reg_additional_modules}""
  additional_repos       = ""${var.additional_repos}""
  ha_sap_deployment_repo = ""${var.ha_sap_deployment_repo}""
  provisioner            = ""${var.provisioner}""
  background             = ""${var.background}""
}
",module,"module ""monitoring"" {
  source             = ""./modules/monitoring""
  base_configuration = ""${module.base.configuration}""

  name                   = ""monitoring""
  count                  = 1
  vcpu                   = 4
  memory                 = 4095
  host_ips               = ""${var.host_ips}""
  reg_code               = ""${var.reg_code}""
  reg_email              = ""${var.reg_email}""
  reg_additional_modules = ""${var.reg_additional_modules}""
  additional_repos       = ""${var.additional_repos}""
  ha_sap_deployment_repo = ""${var.ha_sap_deployment_repo}""
  provisioner            = ""${var.provisioner}""
  background             = ""${var.background}""
}
",module,27,,ded573f03083c78a15ca8c85606954edfc0c5ad5,d32b0c93370b96b65c4de6378081d2af5a1b56bf,https://github.com/SUSE/ha-sap-terraform-deployments/blob/ded573f03083c78a15ca8c85606954edfc0c5ad5/libvirt/terraform/main.tf#L27,https://github.com/SUSE/ha-sap-terraform-deployments/blob/d32b0c93370b96b65c4de6378081d2af5a1b56bf/libvirt/terraform/main.tf,2019-07-22 11:53:34+02:00,2019-07-23 12:37:23+02:00,3,1,0,1,0,0,0,0,1,0
https://github.com/CDCgov/prime-simplereport,43,ops/stg/persistent/main.tf,ops/stg/persistent/main.tf,0,// todo,// TODO: remove this when removing old DB config,// TODO: remove this when removing old DB config,"module ""db"" {
  source      = ""../../services/postgres_db""
  env         = local.env
  rg_location = local.rg_location
  rg_name     = local.rg_name

  global_vault_id = data.azurerm_key_vault.global.id
  db_vault_id     = data.azurerm_key_vault.db_keys.id
  // TODO: delete old_subnet_id when removing the old DB configuration
  old_subnet_id = module.vnet.subnet_vm_id
  subnet_id     = module.vnet.subnet_db_id
  // TODO: remove this when removing old DB config
  dns_zone_id = module.vnet.private_dns_zone_id
  // TODO: remove this when removing old DB config
  administrator_login = ""simplereport""
  log_workspace_id    = module.monitoring.log_analytics_workspace_id
  // TODO: remove this when removing old DB config
  nophi_user_password = random_password.random_nophi_password.result

  tags = local.management_tags
}
",module,"module ""db"" {
  source      = ""../../services/postgres_db""
  env         = local.env
  rg_location = local.rg_location
  rg_name     = local.rg_name

  global_vault_id  = data.azurerm_key_vault.global.id
  db_vault_id      = data.azurerm_key_vault.db_keys.id
  subnet_id        = module.vnet.subnet_db_id
  log_workspace_id = module.monitoring.log_analytics_workspace_id

  nophi_user_password = random_password.random_nophi_password.result

  tags = local.management_tags
}
",module,58,,3e4185fa549937cff9213c325bc0fee780e41eb0,fb9a18eb71f31044ca7a58958a25dea489263ca7,https://github.com/CDCgov/prime-simplereport/blob/3e4185fa549937cff9213c325bc0fee780e41eb0/ops/stg/persistent/main.tf#L58,https://github.com/CDCgov/prime-simplereport/blob/fb9a18eb71f31044ca7a58958a25dea489263ca7/ops/stg/persistent/main.tf,2022-02-16 12:59:39-05:00,2022-04-28 23:27:57-04:00,2,1,1,1,0,0,1,0,0,0
https://github.com/kubernetes/k8s.io,430,infra/aws/terraform/kops-infra-ci/vpc.tf,infra/aws/terraform/kops-infra-ci/vpc.tf,0,// todo,// TODO(ameukam): Remove this after https://github.com/kubernetes/k8s.io/issues/5127 is closed,// TODO(ameukam): Remove this after https://github.com/kubernetes/k8s.io/issues/5127 is closed,"module ""vpc"" {
  providers = {
    aws = aws.kops-infra-ci
  }

  source  = ""terraform-aws-modules/vpc/aws""
  version = ""~> 5.0""

  name = ""${local.prefix}-vpc""
  cidr = aws_vpc_ipam_preview_next_cidr.main.cidr

  ipv4_ipam_pool_id = aws_vpc_ipam_pool.main.id

  azs             = local.azs
  private_subnets = local.private_subnets
  public_subnets  = local.public_subnets

  enable_nat_gateway     = true
  single_nat_gateway     = false
  one_nat_gateway_per_az = true

  // TODO(ameukam): Remove this after https://github.com/kubernetes/k8s.io/issues/5127 is closed
  enable_flow_log                                 = true
  create_flow_log_cloudwatch_iam_role             = true
  create_flow_log_cloudwatch_log_group            = true
  flow_log_cloudwatch_log_group_retention_in_days = 30

  enable_dns_hostnames = true

  public_subnet_tags = {
    ""kubernetes.io/role/elb"" = 1
  }

  private_subnet_tags = {
    ""kubernetes.io/role/internal-elb"" = 1
  }

  tags = merge(var.tags, {
    ""region"" = ""${data.aws_region.current.name}""
  })
}
",module,"module ""vpc"" {
  providers = {
    aws = aws.kops-infra-ci
  }

  source  = ""terraform-aws-modules/vpc/aws""
  version = ""~> 5.0""

  name = ""${local.prefix}-vpc""
  cidr = aws_vpc_ipam_preview_next_cidr.main.cidr

  ipv4_ipam_pool_id = aws_vpc_ipam_pool.main.id

  azs             = local.azs
  private_subnets = local.private_subnets
  public_subnets  = local.public_subnets

  enable_nat_gateway     = true
  single_nat_gateway     = false
  one_nat_gateway_per_az = true

  // TODO(ameukam): Remove this after https://github.com/kubernetes/k8s.io/issues/5127 is closed
  enable_flow_log                                 = true
  create_flow_log_cloudwatch_iam_role             = true
  create_flow_log_cloudwatch_log_group            = true
  flow_log_cloudwatch_log_group_retention_in_days = 30

  enable_dns_hostnames = true
  enable_dns_support   = true

  public_subnet_tags = {
    ""kubernetes.io/role/elb"" = 1
  }

  private_subnet_tags = {
    ""kubernetes.io/role/internal-elb"" = 1
  }

  tags = merge(var.tags, {
    ""region"" = ""${data.aws_region.current.name}""
  })
}
",module,84,84.0,7bb0b42e29e4e4069cc3fbe8e40963c3ec76895b,8110ebffa612964181d3b647db6e884d74f4c0d4,https://github.com/kubernetes/k8s.io/blob/7bb0b42e29e4e4069cc3fbe8e40963c3ec76895b/infra/aws/terraform/kops-infra-ci/vpc.tf#L84,https://github.com/kubernetes/k8s.io/blob/8110ebffa612964181d3b647db6e884d74f4c0d4/infra/aws/terraform/kops-infra-ci/vpc.tf#L84,2023-08-08 19:18:57+02:00,2023-12-08 14:54:27+01:00,4,0,0,1,1,0,1,0,1,0
https://github.com/terraform-google-modules/terraform-google-slo,7,modules/slo/main.tf,modules/slo/main.tf,0,fix,# is fixed.,"# Temporary code to replace module invocation (see bottom of this file) until 
 # https://github.com/terraform-google-modules/terraform-google-event-function/issues/37 
 # is fixed.","resource ""google_cloud_scheduler_job"" ""job"" {
  name        = local.full_name
  project     = var.project_id
  region      = var.region
  description = var.config.slo_description
  schedule    = var.schedule
  time_zone   = var.time_zone

  pubsub_target {
    topic_name = ""projects/${var.project_id}/topics/${module.pubsub_topic.topic}""
    data       = var.message_data
  }
}
",resource,the block associated got renamed or deleted,,44,,825194fc70002c5b310442faceccf313321768d0,7b2bb290a79dab58f9a7232441909c6949bfcab1,https://github.com/terraform-google-modules/terraform-google-slo/blob/825194fc70002c5b310442faceccf313321768d0/modules/slo/main.tf#L44,https://github.com/terraform-google-modules/terraform-google-slo/blob/7b2bb290a79dab58f9a7232441909c6949bfcab1/modules/slo/main.tf,2019-12-07 23:32:23+01:00,2019-12-13 16:05:15-08:00,2,1,1,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1402,fast/stages-multitenant/1-resman-tenant/branch-teams.tf,fast/stages-multitenant/1-resman-tenant/branch-teams.tf,0,# todo,# TODO(ludo): add support for CI/CD,"/** 
 * Copyright 2023 Google LLC 
 * 
 * Licensed under the Apache License, Version 2.0 (the ""License""); 
 * you may not use this file except in compliance with the License. 
 * You may obtain a copy of the License at 
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, software 
 * distributed under the License is distributed on an ""AS IS"" BASIS, 
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 * See the License for the specific language governing permissions and 
 * limitations under the License. 
 */  
 # tfdoc:file:description Team stage resources.  
 # TODO(ludo): add support for CI/CD  
 ############### top-level Teams branch and automation resources ############### ","module ""branch-teams-folder"" {
  source = ""../../../modules/folder""
  count  = var.fast_features.teams ? 1 : 0
  parent = module.root-folder.id
  name   = ""Teams""
  iam = {
    ""roles/logging.admin""                  = [local.automation_sas_iam.teams]
    ""roles/owner""                          = [local.automation_sas_iam.teams]
    ""roles/resourcemanager.folderAdmin""    = [local.automation_sas_iam.teams]
    ""roles/resourcemanager.projectCreator"" = [local.automation_sas_iam.teams]
    ""roles/compute.xpnAdmin""               = [local.automation_sas_iam.teams]
  }
  tag_bindings = {
    context = var.tags.values[""${var.tags.names.context}/teams""]
  }
}
",module,,,19,0.0,5453c585e0086227b9eda70f62cc11cf989bf14f,7a5dd4e6db197daa52da8a8d877ce86b5c93182e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/5453c585e0086227b9eda70f62cc11cf989bf14f/fast/stages-multitenant/1-resman-tenant/branch-teams.tf#L19,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/7a5dd4e6db197daa52da8a8d877ce86b5c93182e/fast/stages-multitenant/1-resman-tenant/branch-teams.tf#L0,2023-02-04 15:00:45+01:00,2024-05-15 09:17:13+00:00,4,2,0,1,0,0,0,1,0,0
https://github.com/terraform-google-modules/terraform-google-project-factory,126,examples/shared_vpc/main.tf,examples/shared_vpc/main.tf,0,fix,# is merged and released.  This is here to fix the `Error: Unsupported block,"# TODO: Switch to released version once 
 # https://github.com/terraform-google-modules/terraform-google-network/pull/47 
 # is merged and released.  This is here to fix the `Error: Unsupported block 
 # type` on the `triggers` block in network's main.tf file. 
 # 
 # source  = ""terraform-google-modules/network/google"" 
 # version = ""0.8.0""","module ""vpc"" {
  # TODO: Switch to released version once
  # https://github.com/terraform-google-modules/terraform-google-network/pull/47
  # is merged and released.  This is here to fix the `Error: Unsupported block
  # type` on the `triggers` block in network's main.tf file.
  #
  # source  = ""terraform-google-modules/network/google""
  # version = ""0.8.0""
  source = ""git::https://github.com/terraform-google-modules/terraform-google-network.git?ref=aaron-lane-0.12""

  project_id   = module.host-project.project_id
  network_name = var.network_name

  delete_default_internet_gateway_routes = ""true""
  shared_vpc_host                        = ""true""

  subnets = [
    {
      subnet_name   = local.subnet_01
      subnet_ip     = ""10.10.10.0/24""
      subnet_region = ""us-west1""
    },
    {
      subnet_name           = local.subnet_02
      subnet_ip             = ""10.10.20.0/24""
      subnet_region         = ""us-west1""
      subnet_private_access = ""true""
      subnet_flow_logs      = ""true""
    },
  ]

  secondary_ranges = {
    ""${local.subnet_01}"" = [
      {
        range_name    = ""${local.subnet_01}-01""
        ip_cidr_range = ""192.168.64.0/24""
      },
      {
        range_name    = ""${local.subnet_01}-02""
        ip_cidr_range = ""192.168.65.0/24""
      },
    ]

    ""${local.subnet_02}"" = [
      {
        range_name    = ""${local.subnet_02}-01""
        ip_cidr_range = ""192.168.66.0/24""
      },
    ]
  }
}
",module,"module ""vpc"" {
  # source  = ""terraform-google-modules/network/google""
  # version = ""~> 1.4.0""

  project_id   = module.host-project.project_id
  network_name = var.network_name

  delete_default_internet_gateway_routes = true
  shared_vpc_host                        = true

  subnets = [
    {
      subnet_name   = local.subnet_01
      subnet_ip     = ""10.10.10.0/24""
      subnet_region = ""us-west1""
    },
    {
      subnet_name           = local.subnet_02
      subnet_ip             = ""10.10.20.0/24""
      subnet_region         = ""us-west1""
      subnet_private_access = true
      subnet_flow_logs      = true
    },
  ]

  secondary_ranges = {
    ""${local.subnet_01}"" = [
      {
        range_name    = ""${local.subnet_01}-01""
        ip_cidr_range = ""192.168.64.0/24""
      },
      {
        range_name    = ""${local.subnet_01}-02""
        ip_cidr_range = ""192.168.65.0/24""
      },
    ]

    ""${local.subnet_02}"" = [
      {
        range_name    = ""${local.subnet_02}-01""
        ip_cidr_range = ""192.168.66.0/24""
      },
    ]
  }
}
",module,54,,c40a3d3c37d2cf4d9376d08a31b8ac05af21360e,6557d7a7d4dacc0ddcfcb9c89e61627d4dfe1a90,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/c40a3d3c37d2cf4d9376d08a31b8ac05af21360e/examples/shared_vpc/main.tf#L54,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/6557d7a7d4dacc0ddcfcb9c89e61627d4dfe1a90/examples/shared_vpc/main.tf,2019-07-12 13:49:19-04:00,2019-10-19 11:56:49-07:00,4,1,0,1,1,0,1,0,0,0
https://github.com/kubernetes/k8s.io,414,infra/aws/terraform/modules/eks-prow-iam/eks_infra_admin_boundary.tf,infra/aws/terraform/modules/eks-prow-iam/boundary_eks_infra_admin.tf,1,# todo,# TODO(pkprzekwas): remove after replacing boundary name in EKS cluster roles and applying changes.,"/* 
 Copyright 2023 The Kubernetes Authors. 
  
 Licensed under the Apache License, Version 2.0 (the ""License""); 
 you may not use this file except in compliance with the License. 
 You may obtain a copy of the License at 
  
 http://www.apache.org/licenses/LICENSE-2.0 
  
 Unless required by applicable law or agreed to in writing, software 
 distributed under the License is distributed on an ""AS IS"" BASIS, 
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 See the License for the specific language governing permissions and 
 limitations under the License. 
 */  
 # TODO(pkprzekwas): remove after replacing boundary name in EKS cluster roles and applying changes.","resource ""aws_iam_policy"" ""provisioner_permission_boundary"" {
  name        = ""ProvisionerPermissionBoundary""
  description = ""Permission boundary for terraform operator roles.""
  policy      = data.aws_iam_policy_document.eks_resources_permission_boundary_doc.json
  tags        = var.tags
}
",resource,,,17,0.0,e071c9f3f230a0512fc9b671682d5890bb6cce2d,1751c18e379b789c52b5380be377ca1493c94c97,https://github.com/kubernetes/k8s.io/blob/e071c9f3f230a0512fc9b671682d5890bb6cce2d/infra/aws/terraform/modules/eks-prow-iam/eks_infra_admin_boundary.tf#L17,https://github.com/kubernetes/k8s.io/blob/1751c18e379b789c52b5380be377ca1493c94c97/infra/aws/terraform/modules/eks-prow-iam/boundary_eks_infra_admin.tf#L0,2023-05-23 10:35:27+02:00,2023-05-23 16:27:33+02:00,6,2,0,1,0,1,0,0,0,0
https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules,26,examples/standalone-vm-series/main.tf,examples/transit_vnet_common/main.tf,1,fix,# FIXME untested,# FIXME untested,"module ""common_vmseries"" {
  source   = ""../../modules/vmseries""
  for_each = var.instances

  resource_group_name       = local.resource_group_name
  location                  = var.location
  name                      = ""${var.name_prefix}-${each.key}""
  avset_id                  = azurerm_availability_set.this[0].id
  username                  = var.username
  password                  = coalesce(var.password, random_password.password.result)
  vm_series_version         = ""9.1.3""
  vm_series_sku             = ""byol""
  bootstrap_storage_account = module.bootstrap.storage_account
  bootstrap_share_name      = module.bootstrap.storage_share_name
  data_nics = [
    {
      name   = ""${each.key}-mgmt""
      subnet = module.networks.subnet_mgmt
      # FIXME untested
      public_ip_address_id = azurerm_public_ip.mgmt[each.key].id
      enable_backend_pool  = false
    },
    {
      name                 = ""${each.key}-public""
      subnet               = module.networks.subnet_public
      public_ip_address_id = azurerm_public_ip.public[each.key].id
      lb_backend_pool_id   = module.inbound-lb.backend-pool-id
      enable_backend_pool  = true
    },
    {
      name                = ""${each.key}-private""
      subnet              = module.networks.subnet_private
      enable_backend_pool = false
    },
  ]

  depends_on = [module.bootstrap]
}
",module,"module ""common_vmseries"" {
  source   = ""../../modules/vmseries""
  for_each = var.vmseries

  resource_group_name       = local.resource_group_name
  location                  = var.location
  name                      = ""${var.name_prefix}${each.key}""
  avset_id                  = try(azurerm_availability_set.this[0].id, null)
  avzone                    = try(each.value.avzone, null)
  username                  = var.username
  password                  = coalesce(var.password, random_password.password.result)
  img_version               = var.common_vmseries_version
  img_sku                   = var.common_vmseries_sku
  vm_size                   = var.common_vmseries_vm_size
  bootstrap_storage_account = module.bootstrap.storage_account
  bootstrap_share_name      = module.bootstrap.storage_share_name
  interfaces = [
    {
      name                 = ""${each.key}-mgmt""
      subnet               = module.networks.subnet_mgmt
      public_ip_address_id = azurerm_public_ip.mgmt[each.key].id
      enable_backend_pool  = false
    },
    {
      name                 = ""${each.key}-public""
      subnet               = module.networks.subnet_public
      public_ip_address_id = azurerm_public_ip.public[each.key].id
      lb_backend_pool_id   = module.inbound-lb.backend-pool-id
      enable_backend_pool  = true
    },
    {
      name                = ""${each.key}-private""
      subnet              = module.networks.subnet_private
      enable_backend_pool = false
    },
  ]

  depends_on = [module.bootstrap]
}
",module,128,,3eaaae43a57b27b4aafe29795d03f1c7136c5b70,a4d6cf8665b63f9733d9d536d18e71f0a50c98a5,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/3eaaae43a57b27b4aafe29795d03f1c7136c5b70/examples/standalone-vm-series/main.tf#L128,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/a4d6cf8665b63f9733d9d536d18e71f0a50c98a5/examples/transit_vnet_common/main.tf,2021-03-22 12:11:14+01:00,2021-03-22 12:11:14+01:00,6,1,1,0,0,0,1,0,0,1
https://github.com/alphagov/govuk-aws,1091,terraform/projects/infra-database-backups-bucket/main.tf,terraform/projects/infra-database-backups-bucket/main.tf,0,# todo,# TODO: these are all set to the same var.expiration_time so just replace,"# Integration-specific lifecycle rules. These rules are created in all 
 # environments but are only enabled in Integration. 
 # 
 # TODO: create these only in environments where they're needed, instead of 
 # creating them everywhere and leaving them disabled. 
 # 
 # TODO: these are all set to the same var.expiration_time so just replace 
 # them with one rule. Similarly for the prod ones above. ","resource ""aws_s3_bucket"" ""database_backups"" {
  bucket = ""govuk-${var.aws_environment}-database-backups""
  region = ""${var.aws_region}""

  tags {
    Name            = ""govuk-${var.aws_environment}-database-backups""
    aws_environment = ""${var.aws_environment}""
  }

  logging {
    target_bucket = ""${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}""
    target_prefix = ""s3/govuk-${var.aws_environment}-database-backups/""
  }

  # Production/Staging lifecycle rules.
  #
  # TODO: make staging use the same rules as integration. We don't need to
  # retain backups of staging for very long.

  lifecycle_rule {
    id      = ""mysql_lifecycle_rule""
    prefix  = ""mysql/""
    enabled = ""${var.aws_environment != ""integration""}""

    transition {
      storage_class = ""STANDARD_IA""
      days          = ""${var.standard_s3_storage_time}""
    }

    transition {
      storage_class = ""GLACIER""
      days          = ""${var.glacier_storage_time}""
    }

    expiration {
      days = 120
    }
  }
  lifecycle_rule {
    id      = ""postgres_lifecycle_rule""
    prefix  = ""postgres/""
    enabled = ""${var.aws_environment != ""integration""}""

    transition {
      storage_class = ""STANDARD_IA""
      days          = ""${var.standard_s3_storage_time}""
    }

    transition {
      storage_class = ""GLACIER""
      days          = ""${var.glacier_storage_time}""
    }

    expiration {
      days = 120
    }
  }
  lifecycle_rule {
    id      = ""mongo_daily_lifecycle_rule""
    prefix  = ""mongodb/daily""
    enabled = ""${var.aws_environment != ""integration""}""

    transition {
      storage_class = ""STANDARD_IA""
      days          = ""${var.standard_s3_storage_time}""
    }

    transition {
      storage_class = ""GLACIER""
      days          = ""${var.glacier_storage_time}""
    }

    expiration {
      days = 120
    }
  }
  lifecycle_rule {
    id      = ""mongo_regular_lifecycle_rule""
    prefix  = ""mongodb/regular""
    enabled = true

    expiration {
      days = ""${var.expiration_time_whisper_mongo}""
    }
  }
  lifecycle_rule {
    id      = ""whisper_lifecycle_rule""
    prefix  = ""whisper/""
    enabled = true

    expiration {
      days = ""${var.expiration_time_whisper_mongo}""
    }
  }

  # Integration-specific lifecycle rules. These rules are created in all
  # environments but are only enabled in Integration.
  #
  # TODO: create these only in environments where they're needed, instead of
  # creating them everywhere and leaving them disabled.
  #
  # TODO: these are all set to the same var.expiration_time so just replace
  # them with one rule. Similarly for the prod ones above.

  lifecycle_rule {
    id      = ""mysql_lifecycle_rule_integration""
    prefix  = ""mysql/""
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }
  }
  lifecycle_rule {
    id      = ""postgres_lifecycle_rule_integration""
    prefix  = ""postgres/""
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }
  }
  lifecycle_rule {
    id      = ""mongo_daily_lifecycle_rule_integration""
    prefix  = ""mongodb/daily""
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }
  }
  lifecycle_rule {
    id      = ""whole_bucket_lifecycle_rule_integration""
    prefix  = """"
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }

    noncurrent_version_expiration {
      days = ""1""
    }
  }

  # End of Integration-specific lifecycle rules.


  # Lifecycle rule for coronavirus find support backup

  lifecycle_rule {
    id      = ""coronavirus_find_support_lifecycle_rule""
    prefix  = ""coronavirus-find-support/production.sql.gzip""
    enabled = true

    expiration {
      days = 365
    }
  }

  # Lifecycle rule for coronavirus business volunteer form backup

  lifecycle_rule {
    id      = ""coronavirus_business_volunteer_form_lifecycle_rule""
    prefix  = ""coronavirus-business-volunteer-form/production.sql.gzip""
    enabled = true

    expiration {
      days = 365
    }
  }
  versioning {
    enabled = true
  }
  replication_configuration {
    role = ""${aws_iam_role.backup_replication_role.arn}""

    rules {
      id     = ""main_replication_rule""
      prefix = """"
      status = ""${var.replication_setting}""

      destination {
        bucket        = ""${aws_s3_bucket.database_backups_replica.arn}""
        storage_class = ""STANDARD""
      }
    }
  }
}
",resource,"resource ""aws_s3_bucket"" ""database_backups"" {
  bucket = ""govuk-${var.aws_environment}-database-backups""
  region = ""${var.aws_region}""

  tags {
    Name            = ""govuk-${var.aws_environment}-database-backups""
    aws_environment = ""${var.aws_environment}""
  }

  logging {
    target_bucket = ""${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}""
    target_prefix = ""s3/govuk-${var.aws_environment}-database-backups/""
  }

  versioning {
    # It's not entirely clear if versioning is useful on this bucket  but it was previously configured this way,
    # so we've decided not to change it. Whilst it helps protect against accidental deletion, it doesn't protect
    # against malicious actors, so shouldn't be considered a security feature.
    enabled = true
  }

  lifecycle_rule {
    # Use a long retention period in production
    id      = ""long_retention_period""
    enabled = ""${var.aws_environment == ""production""}""

    # Ideally everything would go in the Standard (Infrequent Access) storage class when created.
    # But newly created objects always go into Standard, and can only move into IA after at least 30 days.
    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html
    transition {
      storage_class = ""STANDARD_IA""
      days          = 30
    }

    # Likewise, we have to wait at least another 30 days before we can move objects into Glacier storage.
    transition {
      storage_class = ""GLACIER""
      days          = 60
    }

    # Versioning is enabled on this bucket, so this rule will 'soft delete' objects.
    # In AWS lingo, this means a 'delete marker' will be set on the current version of the object.
    # More info on how expiration rules apply to versioned buckets here:
    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#intro-lifecycle-rules-actions
    expiration {
      days = 120
    }

    # This rule will 'hard delete' objects 1 day after they were 'soft deleted'.
    # In other words: old database backups will be permanently deleted 1 day after they've expired.
    noncurrent_version_expiration {
      days = ""1""
    }
  }

  lifecycle_rule {
    # Use a short retention period in integration and staging
    id      = ""short_retention_period""
    enabled = ""${var.aws_environment != ""production""}""

    expiration {
      days = ""3""
    }

    noncurrent_version_expiration {
      days = ""1""
    }
  }

  replication_configuration {
    role = ""${aws_iam_role.backup_replication_role.arn}""

    rules {
      id     = ""main_replication_rule""
      status = ""Enabled""

      destination {
        bucket        = ""${aws_s3_bucket.database_backups_replica.arn}""
        storage_class = ""STANDARD_IA""
      }
    }
  }
}
",resource,201,,d1fcb45657475a7de489503eae548845ec8e4296,78cc3f5ce73fb5a8f6d5126ff694ac87e5a0eb79,https://github.com/alphagov/govuk-aws/blob/d1fcb45657475a7de489503eae548845ec8e4296/terraform/projects/infra-database-backups-bucket/main.tf#L201,https://github.com/alphagov/govuk-aws/blob/78cc3f5ce73fb5a8f6d5126ff694ac87e5a0eb79/terraform/projects/infra-database-backups-bucket/main.tf,2020-11-24 17:53:04+00:00,2022-01-31 17:30:52+00:00,4,1,1,1,0,0,0,1,0,0
https://github.com/alphagov/govuk-aws,514,terraform/projects/infra-public-services/main.tf,terraform/projects/infra-public-services/main.tf,0,todo,# Apt: TODO EXTERNAL,"# 
 # Apt: TODO EXTERNAL 
 # ","resource ""aws_route53_record"" ""apt_internal_service_names"" {
  count   = ""${length(var.apt_internal_service_names)}""
  zone_id = ""${data.terraform_remote_state.infra_root_dns_zones.internal_root_zone_id}""
  name    = ""${element(var.apt_internal_service_names, count.index)}.${data.terraform_remote_state.infra_root_dns_zones.internal_root_domain_name}""
  type    = ""CNAME""
  records = [""${element(var.apt_internal_service_names, count.index)}.blue.${data.terraform_remote_state.infra_root_dns_zones.internal_root_domain_name}""]
  ttl     = ""300""
}
",resource,,,303,0.0,051e76ef4b68a6c03269aedaa2f32422853a8c22,54e05ce9e2e6d77fb915c0334170cc72826463bf,https://github.com/alphagov/govuk-aws/blob/051e76ef4b68a6c03269aedaa2f32422853a8c22/terraform/projects/infra-public-services/main.tf#L303,https://github.com/alphagov/govuk-aws/blob/54e05ce9e2e6d77fb915c0334170cc72826463bf/terraform/projects/infra-public-services/main.tf#L0,2017-12-14 16:17:17+00:00,2024-03-11 15:00:01+00:00,143,2,0,0,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,207,modules/organization/variables.tf,modules/organization/variables.tf,0,todo,# TODO exclusions also support description and disabled,# TODO exclusions also support description and disabled,"variable ""logging_sinks"" {
  description = ""Logging sinks to create for this organization.""
  type = map(object({
    destination      = string
    type             = string
    filter           = string
    iam              = bool
    include_children = bool
    # TODO exclusions also support description and disabled
    exclusions = map(string)
  }))
  default = {}
}
",variable,"variable ""logging_sinks"" {
  description = ""Logging sinks to create for the organization.""
  type = map(object({
    bigquery_use_partitioned_table = optional(bool)
    description                    = optional(string)
    destination = object({
      type   = string
      target = string
    })
    disabled         = optional(bool, false)
    exclusions       = optional(map(string), {})
    filter           = string
    include_children = optional(bool, true)
  }))
  default  = {}
  nullable = false
  validation {
    condition = alltrue([
      for k, v in var.logging_sinks :
      contains([""bigquery"", ""logging"", ""pubsub"", ""storage""], v.destination.type)
    ])
    error_message = ""Destination type must be one of 'bigquery', 'logging', 'pubsub', 'storage'.""
  }
  validation {
    condition = alltrue([
      for k, v in var.logging_sinks :
      v.bigquery_use_partitioned_table != true || v.destination.type == ""bigquery""
    ])
    error_message = ""Can only set bigquery_use_partitioned_table when destination type is `bigquery`.""
  }
}
",variable,127,,ad68fc4dfa576624a7e2caa1b96499161d8b0937,486d398c7d68ce1b0784a540feb7be5fb2c680c1,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/ad68fc4dfa576624a7e2caa1b96499161d8b0937/modules/organization/variables.tf#L127,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/486d398c7d68ce1b0784a540feb7be5fb2c680c1/modules/organization/variables.tf,2021-03-03 14:23:59+01:00,2022-11-11 19:22:05+01:00,17,1,0,1,0,0,0,0,1,0
https://github.com/Worklytics/psoxy,395,infra/modules/aws-psoxy-lambda/main.tf,infra/modules/aws-psoxy-lambda/main.tf,0,todo,# TODO : revisit this is exploiting convention,# TODO : revisit this is exploiting convention,"locals {
  # TODO : revisit; this is exploiting convention
  prefix = ""${upper(replace(var.function_name, ""-"", ""_""))}_""

  # Read grant to any param that belongs to the function (identified by param prefix)
  # Can't use same approach as for write, because some params are not defined in connector specs,
  # but later in certain modules, f.e. SERVICE_ACCOUNT_KEY or HRIS_RULES
  filtered_function_read_arns = [
    for arn in data.aws_ssm_parameters_by_path.psoxy_parameters.arns : arn if length(regexall(local.prefix, arn)) > 0
  ]

  # Write grant to any writeable param specified in the connector definition
  filtered_function_write_arns = distinct(flatten([
    for p in var.function_parameters : [
      for arn in data.aws_ssm_parameters_by_path.psoxy_parameters.arns :
      arn if endswith(arn, join("""", [local.prefix, p.name])) && p.writable
    ]
  ]))

  function_write_arns = local.filtered_function_write_arns
  function_read_arns  = concat(local.filtered_function_read_arns, var.global_parameter_arns)
}
",locals,"locals {
  prefix = ""PSOXY_${upper(replace(var.source_kind, ""-"", ""_""))}_""

  param_arn_prefix = ""arn:aws:ssm:${data.aws_arn.lambda.region}:${data.aws_arn.lambda.account}:parameter/${local.prefix}""


  # Write grant to any writeable param specified in the connector definition
  filtered_function_write_arns = distinct(flatten([
    for p in var.function_parameters : [
      for arn in data.aws_ssm_parameters_by_path.psoxy_parameters.arns :
      arn if endswith(arn, join("""", [local.prefix, p.name])) && p.writable
    ]
  ]))

  function_write_arns = local.filtered_function_write_arns
  function_read_arns  = concat(
    [""${local.param_arn_prefix}*""], # wildcard to match all params corresponding to this function
    var.global_parameter_arns
  )

  write_statements =  length(local.function_write_arns) < 1 ? [] : [{
    Action   = [
      ""ssm:PutParameter""
    ]
    Effect   = ""Allow""
    Resource = local.function_write_arns
  }]

  read_statements = [{
    Action = [
      ""ssm:GetParameter*""
    ]
    Effect   = ""Allow""
    Resource =  local.function_read_arns
  }]

  policy_statements = concat(
    local.read_statements,
    local.write_statements
  )
}
",locals,80,,2507bc18439124435a38c08eacf36f6e6002b20e,5d8f4928978c96878368db88bc908047217bcda8,https://github.com/Worklytics/psoxy/blob/2507bc18439124435a38c08eacf36f6e6002b20e/infra/modules/aws-psoxy-lambda/main.tf#L80,https://github.com/Worklytics/psoxy/blob/5d8f4928978c96878368db88bc908047217bcda8/infra/modules/aws-psoxy-lambda/main.tf,2022-10-04 13:38:52-07:00,2022-10-06 13:05:00-07:00,2,1,0,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,1511,infra/modules/aws/variables.tf,infra/modules/aws/variables.tf,0,todo,"# TODO : change default in v0.5, or remove; should be based on deployment_id","# TODO : change default in v0.5, or remove; should be based on deployment_id","variable ""api_function_name_prefix"" {
  type        = string
  description = ""prefix for API function names""
  default     = ""psoxy-""
}
",variable,"variable ""api_function_name_prefix"" {
  type        = string
  description = ""prefix for API function names""
  default     = ""psoxy-""
}
",variable,85,96.0,f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e,5ed7bc9ef2e9108a02f4d42e7d0d156eb5c7184d,https://github.com/Worklytics/psoxy/blob/f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e/infra/modules/aws/variables.tf#L85,https://github.com/Worklytics/psoxy/blob/5ed7bc9ef2e9108a02f4d42e7d0d156eb5c7184d/infra/modules/aws/variables.tf#L96,2023-06-16 14:08:45-07:00,2024-01-31 10:34:59-08:00,4,0,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1591,blueprints/data-solutions/data-platform-foundations/locals-03-orchestration.tf,blueprints/data-solutions/data-platform-foundations/locals-03-orchestration.tf,0,# todo,# TODO: use new artifact registry module output,# TODO: use new artifact registry module output,"locals {
  _orch_iam = flatten([
    for principal, roles in local.orch_iam : [
      for role in roles : {
        key       = ""${principal}-${role}""
        principal = principal
        role      = role
      }
    ]
  ])
  orch_iam_additive = {
    for binding in local._orch_iam : binding.key => {
      role   = binding.role
      member = local.iam_principals[binding.principal]
    }
  }
  orch_iam_auth = {
    for binding in local._orch_iam :
    binding.role => local.iam_principals[binding.principal]...
  }
  orch_subnet = (
    local.use_shared_vpc
    ? var.network_config.subnet_self_links.orchestration
    : values(module.orch-vpc.0.subnet_self_links)[0]
  )
  orch_vpc = (
    local.use_shared_vpc
    ? var.network_config.network_self_link
    : module.orch-vpc.0.self_link
  )
  # TODO: use new artifact registry module output
  orch_docker_path = format(""%s-docker.pkg.dev/%s/%s"",
  var.region, module.orch-project.project_id, module.orch-artifact-reg.name)
}
",locals,"locals {
  _orch_iam = flatten([
    for principal, roles in local.orch_iam : [
      for role in roles : {
        key       = ""${principal}-${role}""
        principal = principal
        role      = role
      }
    ]
  ])
  orch_iam_additive = {
    for binding in local._orch_iam : binding.key => {
      role   = binding.role
      member = local.iam_principals[binding.principal]
    }
  }
  orch_iam_auth = {
    for binding in local._orch_iam :
    binding.role => local.iam_principals[binding.principal]...
  }
  orch_subnet = (
    local.use_shared_vpc
    ? var.network_config.subnet_self_links.orchestration
    : values(module.orch-vpc[0].subnet_self_links)[0]
  )
  orch_vpc = (
    local.use_shared_vpc
    ? var.network_config.network_self_link
    : module.orch-vpc[0].self_link
  )
  # TODO: use new artifact registry module output
  orch_docker_path = format(""%s-docker.pkg.dev/%s/%s"",
  var.region, module.orch-project.project_id, module.orch-artifact-reg.name)
}
",locals,47,47.0,819894d2bab4b440f1b52b1ac8035912fb107004,3af7e257d21f889ffaf7b32a3bab974fdbfda6e4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/819894d2bab4b440f1b52b1ac8035912fb107004/blueprints/data-solutions/data-platform-foundations/locals-03-orchestration.tf#L47,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3af7e257d21f889ffaf7b32a3bab974fdbfda6e4/blueprints/data-solutions/data-platform-foundations/locals-03-orchestration.tf#L47,2023-08-20 09:44:20+02:00,2024-04-17 10:23:48+02:00,2,0,0,1,0,0,0,0,0,0
https://github.com/uyuni-project/sumaform,1362,backend_modules/aws/host/main.tf,backend_modules/aws/host/main.tf,0,hack,# HACK,"# HACK 
 # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner""). 
 # After the first `apply`, terraform removes those tags. The following block avoids this behavior. 
 # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider 
 # https://github.com/terraform-providers/terraform-provider-aws/issues/10689","resource ""aws_ebs_volume"" ""data_disk"" {
  count = var.additional_disk_size == null ? 0 : var.additional_disk_size > 0 ? var.quantity : 0

  availability_zone = local.availability_zone
  size              = var.additional_disk_size == null ? 0 : var.additional_disk_size
  type              = lookup(var.volume_provider_settings, ""type"", ""sc1"")
  snapshot_id       = lookup(var.volume_provider_settings, ""volume_snapshot_id"", null)
  tags = {
    Name = ""${local.resource_name_prefix}-data-volume${var.quantity > 1 ? ""-${count.index + 1}"" : """"}""
  }
  # HACK
  # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner"").
  # After the first `apply`, terraform removes those tags. The following block avoids this behavior.
  # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider
  # https://github.com/terraform-providers/terraform-provider-aws/issues/10689
  lifecycle {
    ignore_changes = [tags]
  }
}
",resource,"resource ""aws_ebs_volume"" ""data_disk"" {
  count = var.additional_disk_size == null ? 0 : var.additional_disk_size > 0 ? var.quantity : 0

  availability_zone = local.availability_zone
  size              = var.additional_disk_size == null ? 0 : var.additional_disk_size
  type              = lookup(var.volume_provider_settings, ""type"", ""sc1"")
  snapshot_id       = lookup(var.volume_provider_settings, ""volume_snapshot_id"", null)
  tags = {
    Name = ""${local.resource_name_prefix}-data-volume${var.quantity > 1 ? ""-${count.index + 1}"" : """"}""
  }
  # WORKAROUND
  # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner"").
  # After the first `apply`, terraform removes those tags. The following block avoids this behavior.
  # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider
  # See github:terraform-providers/terraform-provider-aws#10689
  lifecycle {
    ignore_changes = [tags]
  }
}
",resource,104,,15bd77941377320f15ab95c290aff17bfe80d0e7,12fc857978857ebd94f9b0906480004ca9b88c22,https://github.com/uyuni-project/sumaform/blob/15bd77941377320f15ab95c290aff17bfe80d0e7/backend_modules/aws/host/main.tf#L104,https://github.com/uyuni-project/sumaform/blob/12fc857978857ebd94f9b0906480004ca9b88c22/backend_modules/aws/host/main.tf,2020-05-04 19:34:32+01:00,2021-01-26 15:58:29+01:00,5,1,1,1,1,1,0,0,0,0
https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd,123,modules/gke-cloudbuild-private-pool/main.tf,modules/gke-cloudbuild-private-pool/main.tf,0,# todo,# TODO: dynamic block --> 2 tunnels per destination GKE VPC,# TODO: dynamic block --> 2 tunnels per destination GKE VPC,"module ""vpn_ha-1"" {
  source     = ""terraform-google-modules/vpn/google//modules/vpn_ha""
  version    = ""~> 1.3.0""
  project_id = var.project_id
  region     = var.location
  network    = google_compute_network.private_pool_vpc.self_link
  name       = ""cloudbuild-to-gke""
  peer_gcp_gateway = module.vpn_ha-2.self_link
  router_asn = 64514
  # TODO: dynamic block --> 2 tunnels per destination GKE VPC
  tunnels = {
    remote-0 = {
      bgp_peer = {
        address = ""169.254.1.1""
        asn     = 64513
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.1.2/30""
      ike_version       = 2
      vpn_gateway_interface = 0
      peer_external_gateway_interface = null
      shared_secret     = """"
    }
    remote-1 = {
      bgp_peer = {
        address = ""169.254.2.1""
        asn     = 64513
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.2.2/30""
      ike_version       = 2
      vpn_gateway_interface = 1
      peer_external_gateway_interface = null
      shared_secret     = """"
    }
  }
}
",module,"module ""vpn_ha-1"" {
  count = length(local.gke_networks)

  source     = ""terraform-google-modules/vpn/google//modules/vpn_ha""
  version    = ""~> 1.3.0""
  project_id = var.project_id
  region     = var.location
  network    = google_compute_network.private_pool_vpc.self_link
  name       = ""cloudbuild-to-${local.gke_networks[count.index].network}""
  peer_gcp_gateway = module.vpn_ha-2[count.index].self_link
  router_asn = 65001+(count.index*2)
  tunnels = {
    remote-0 = {
      bgp_peer = {
        address = ""169.254.${1+(count.index*2)}.2""
        asn     = 65002+(count.index*2)
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.${1+(count.index*2)}.1/30""
      ike_version       = 2
      vpn_gateway_interface = 0
      peer_external_gateway_interface = null
      shared_secret     = """"
    }
    remote-1 = {
      bgp_peer = {
        address = ""169.254.${2+(count.index*2)}.2""
        asn     = 65002+(count.index*2)
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.${2+(count.index*2)}.1/30""
      ike_version       = 2
      vpn_gateway_interface = 1
      peer_external_gateway_interface = null
      shared_secret     = """"
    }
  }
}
",module,73,,a384bc29c9bcb80dd1b1f60ece9dee723b4bf378,e29ac91f2eedf8a48e82065434b81010d298a423,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/a384bc29c9bcb80dd1b1f60ece9dee723b4bf378/modules/gke-cloudbuild-private-pool/main.tf#L73,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/e29ac91f2eedf8a48e82065434b81010d298a423/modules/gke-cloudbuild-private-pool/main.tf,2021-11-30 12:07:36-06:00,2021-12-06 17:46:36-06:00,2,1,0,1,0,0,1,0,0,0
https://github.com/terraform-google-modules/terraform-google-iam,10,modules/helper/main.tf,modules/helper/main.tf,0,workaround,# expression is resolved synchonously. And we have to workaround,"# It is important to provide a set for the `for_each` instead of 
 # the map, since we have to guarantee that the `for_each` 
 # expression is resolved synchonously. And we have to workaround 
 # the potential dependency on dynamic resource values by polyfilling 
 # the `for_each` with `count`-like list of indexes.","locals {
  authoritative = var.mode == ""authoritative""
  additive      = var.mode == ""additive""

  # When there are *_num specified, consider the module configuration
  # dynamic. In this case the `for_each` will basically work as
  # a polyfill for `count`
  #
  # The downside of the dynamic mode is that we can't guarantee the
  # resources being reused whenever the configuration changes.
  # Which leads to unnecessary resource recreations.
  dynamic = var.entities_num > 0 || var.bindings_num > 0

  calculated_entities_num = (
    var.entities_num > 0
    ? var.entities_num
    : length(var.entities)
  )

  bindings_by_role = distinct(flatten([
    for name in var.entities
    : [
      for role, members in var.bindings
      : { name = name, role = role, members = members }
    ]
  ]))

  bindings_by_member = distinct(flatten([
    for binding in local.bindings_by_role
    : [
      for member in binding[""members""]
      : { name = binding[""name""], role = binding[""role""], member = member }
    ]
  ]))

  total_roles = (
    var.bindings_num > 0
    ? var.bindings_num * local.calculated_entities_num
    : length(local.bindings_by_role)
  )

  total_members = (
    var.bindings_num > 0
    ? var.bindings_num * local.calculated_entities_num
    : length(local.bindings_by_member)
  )

  keys_authoritative = (
    local.dynamic
    # [dynamic] fallback for_each to a simple list of indexes
    ? [for i in range(local.total_roles) : tostring(i)]
    # [static] generate unique ids which are resilient to updates
    : [
      for binding in local.bindings_by_role
      : ""${binding[""name""]}--${binding[""role""]}""
    ]
  )

  keys_additive = (
    local.dynamic
    # [dynamic] fallback for_each to a simple list of indexes
    ? [for i in range(local.total_members) : tostring(i)]
    # [static] generate unique ids which are resilient to updates
    : [
      for binding in local.bindings_by_member
      : ""${binding[""name""]}--${binding[""role""]}--${binding[""member""]}""
    ]
  )

  bindings_authoritative = (
    local.authoritative
    ? zipmap(local.keys_authoritative, local.bindings_by_role)
    : {}
  )

  bindings_additive = (
    local.additive
    ? zipmap(local.keys_additive, local.bindings_by_member)
    : {}
  )

  # It is important to provide a set for the `for_each` instead of
  # the map, since we have to guarantee that the `for_each`
  # expression is resolved synchonously. And we have to workaround
  # the potential dependency on dynamic resource values by polyfilling
  # the `for_each` with `count`-like list of indexes.
  set_authoritative = (
    local.authoritative
    ? toset(local.keys_authoritative)
    : []
  )

  set_additive = (
    local.additive
    ? toset(local.keys_additive)
    : []
  )
}
",locals,"locals {
  authoritative = var.mode == ""authoritative""
  additive      = var.mode == ""additive""

  # When there is only one entity, consider that the entity passed
  # might be dynamic. In this case the `for_each` will not use
  # entity name when constructing the unique ID.
  #
  # Other rules regrading the dynamic nature of resources:
  # 1. The roles might never be dynamic.
  # 2. Members might only be dynamic in `authoritative` mode.
  singular = length(var.entities) == 1

  # In singular mode, replace entity name with a constant ""default"". This
  # will prevent the potentially dynamic resource name usage in the `for_each`
  aliased_entities = local.singular ? [""default""] : var.entities

  bindings_by_role = distinct(flatten([
    for name in var.entities
    : [
      for role, members in var.bindings
      : { name = name, role = role, members = members }
    ]
  ]))

  bindings_by_member = distinct(flatten([
    for binding in local.bindings_by_role
    : [
      for member in binding[""members""]
      : { name = binding[""name""], role = binding[""role""], member = member }
    ]
  ]))

  keys_authoritative = distinct(flatten([
    for alias in local.aliased_entities
    : [
      for role in keys(var.bindings)
      : ""${alias}--${role}""
    ]
  ]))

  keys_additive = distinct(flatten([
    for alias in local.aliased_entities
    : [
      for role, members in var.bindings
      : [
        for member in members
        : ""${alias}--${role}--${member}""
      ]
    ]
  ]))

  # TODO: Refactor this to force the order somehow.
  #       If you are to change the algo of generating `keys_authoritative`
  #       or `bindings_by_role`, you have to make sure that the order
  #       of the elements inside them matches.
  bindings_authoritative = (
    local.authoritative
    ? zipmap(local.keys_authoritative, local.bindings_by_role)
    : {}
  )

  # TODO: Refactor this to force the order somehow.
  #       If you are to change the algo of generating `keys_additive`
  #       or `bindings_by_member`, you have to make sure that the order
  #       of the elements inside them matches.
  bindings_additive = (
    local.additive
    ? zipmap(local.keys_additive, local.bindings_by_member)
    : {}
  )

  # It is important to provide a set for the `for_each` instead of
  # the map, since we have to guarantee that the `for_each`
  # expression is resolved synchonously.
  set_authoritative = (
    local.authoritative
    ? toset(local.keys_authoritative)
    : []
  )

  set_additive = (
    local.additive
    ? toset(local.keys_additive)
    : []
  )
}
",locals,100,,5afe9a37ed825d0e9ef6c69920df2a6f6cd4fe2b,665a160dd97a99c80a325250d7fea70fa9f4fac5,https://github.com/terraform-google-modules/terraform-google-iam/blob/5afe9a37ed825d0e9ef6c69920df2a6f6cd4fe2b/modules/helper/main.tf#L100,https://github.com/terraform-google-modules/terraform-google-iam/blob/665a160dd97a99c80a325250d7fea70fa9f4fac5/modules/helper/main.tf,2019-10-11 19:00:07+03:00,2019-10-14 19:29:14+03:00,2,1,0,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,414,infra/modules/aws-psoxy-lambda/main.tf,infra/modules/aws-psoxy-lambda/main.tf,0,# todo,# TODO: param_arn_prefix without relying something that itself is provisioned by terraform ...,"# q: makes policy dynamic, so actual statements don't appear in `terraform plan`? 
 # TODO: param_arn_prefix without relying something that itself is provisioned by terraform ...","data ""aws_arn"" ""lambda"" {
  arn = aws_lambda_function.psoxy-instance.arn
}
",data,the block associated got renamed or deleted,,74,,5d8f4928978c96878368db88bc908047217bcda8,8608c0c31d22d62e86546a8c14c6a97ef0c50f39,https://github.com/Worklytics/psoxy/blob/5d8f4928978c96878368db88bc908047217bcda8/infra/modules/aws-psoxy-lambda/main.tf#L74,https://github.com/Worklytics/psoxy/blob/8608c0c31d22d62e86546a8c14c6a97ef0c50f39/infra/modules/aws-psoxy-lambda/main.tf,2022-10-06 13:05:00-07:00,2022-10-06 13:13:36-07:00,3,1,1,1,0,0,0,0,0,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,1,master.tf,init.tf,1,hack,"# This ugly hack is here, because terraform serializes the","# This ugly hack is here, because terraform serializes the 
 # embedded yaml files with ""- |2"", when there is more than 
 # one yamldocument in the embedded file. Kustomize does not understand 
 # that syntax and tries to parse the blocks content as a file, resulting 
 # in weird errors. so gnu sed with funny escaping is used to 
 # replace lines like ""- |3"" by ""- |"" (yaml block syntax). 
 # due to indendation this should not changes the embedded 
 # manifests themselves","resource ""hcloud_server"" ""first_control_plane"" {
  name = ""k3s-control-plane-0""

  image              = data.hcloud_image.linux.name
  rescue             = ""linux64""
  server_type        = var.control_plane_server_type
  location           = var.location
  ssh_keys           = [hcloud_ssh_key.k3s.id]
  firewall_ids       = [hcloud_firewall.k3s.id]
  placement_group_id = hcloud_placement_group.k3s.id

  labels = {
    ""provisioner"" = ""terraform"",
    ""engine""      = ""k3s""
  }

  connection {
    user           = ""root""
    private_key    = local.ssh_private_key
    agent_identity = local.ssh_identity
    host           = self.ipv4_address
  }

  provisioner ""file"" {
    content = templatefile(""${path.module}/templates/config.ign.tpl"", {
      name           = self.name
      ssh_public_key = local.ssh_public_key
    })
    destination = ""/root/config.ign""
  }

  # Install MicroOS
  provisioner ""remote-exec"" {
    inline = local.MicroOS_install_commands
  }

  # Issue a reboot command
  provisioner ""local-exec"" {
    command = ""ssh ${local.ssh_args} root@${self.ipv4_address} '(sleep 2; reboot)&'; sleep 3""
  }

  # Wait for MicroOS to reboot and be ready
  provisioner ""local-exec"" {
    command = <<-EOT
      until ssh ${local.ssh_args} -o ConnectTimeout=2 root@${self.ipv4_address} true 2> /dev/null
      do
        echo ""Waiting for MicroOS to reboot and become available...""
        sleep 2
      done
    EOT
  }

  # Generating k3s master config file
  provisioner ""file"" {
    content = yamlencode({
      node-name                = self.name
      cluster-init             = true
      disable-cloud-controller = true
      disable                  = [""servicelb"", ""local-storage""]
      flannel-iface            = ""eth1""
      kubelet-arg              = ""cloud-provider=external""
      node-ip                  = local.first_control_plane_network_ip
      advertise-address        = local.first_control_plane_network_ip
      token                    = random_password.k3s_token.result
      node-taint               = var.allow_scheduling_on_control_plane ? [] : [""node-role.kubernetes.io/master:NoSchedule""]
    })
    destination = ""/etc/rancher/k3s/config.yaml""
  }

  # Run the first control plane
  provisioner ""remote-exec"" {
    inline = [
      # set the hostname in a persistent fashion
      ""hostnamectl set-hostname ${self.name}"",
      # first we disable automatic reboot (after transactional updates), and configure the reboot method as kured
      ""rebootmgrctl set-strategy off && echo 'REBOOT_METHOD=kured' > /etc/transactional-update.conf"",
      # prepare a directory for our post-installation kustomizations
      ""mkdir -p /tmp/post_install"",
      # then we initiate the cluster
      ""systemctl enable k3s-server"",
      # start k3s
      ""systemctl start k3s-server"",
      # wait for k3s to get ready
      <<-EOT
      timeout 120 bash <<EOF
        until systemctl status k3s-server > /dev/null; do
          systemctl start k3s-server
          echo ""Initiating the cluster...""
          sleep 1
        done
        until [ -e /etc/rancher/k3s/k3s.yaml ]; do
          echo ""Waiting for kubectl config...""
          sleep 1
        done
        until [[ ""\$(kubectl get --raw='/readyz')"" == ""ok"" ]]; do
          echo ""Waiting for cluster to become ready...""
          sleep 1
        done
      EOF
      EOT
    ]
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content = yamlencode({
      apiVersion = ""kustomize.config.k8s.io/v1beta1""
      kind       = ""Kustomization""
      resources = [
        ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",
        ""https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml"",
        ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",
        ""./traefik.yaml""
      ]
      patchesStrategicMerge = [
        file(""${path.module}/patches/kured.yaml""),
        local.ccm_latest ? file(""${path.module}/patches/ccm_latest.yaml"") : file(""${path.module}/patches/ccm.yaml""),
        local.csi_latest ? file(""${path.module}/patches/csi_latest.yaml"") : null,
      ]
    })
    destination = ""/tmp/post_install/kustomization.yaml""
  }

  # Upload traefik config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/traefik_config.yaml.tpl"",
      {
        lb_disable_ipv6    = var.lb_disable_ipv6
        lb_server_type     = var.lb_server_type
        location           = var.location
        traefik_acme_tls   = var.traefik_acme_tls
        traefik_acme_email = var.traefik_acme_email
    })
    destination = ""/tmp/post_install/traefik.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name}"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token}"",
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = [
      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /tmp/post_install/kustomization.yaml"",
      ""kubectl apply -k /tmp/post_install"",
    ]
  }

  network {
    network_id = hcloud_network.k3s.id
    ip         = local.first_control_plane_network_ip
  }

  depends_on = [
    hcloud_network_subnet.k3s,
    hcloud_firewall.k3s
  ]
}
",resource,"resource ""null_resource"" ""kustomization"" {
  triggers = {
    # Redeploy helm charts when the underlying values change
    helm_values_yaml = join(""---\n"", [
      local.traefik_values,
      local.nginx_values,
      local.calico_values,
      local.cilium_values,
      local.longhorn_values,
      local.csi_driver_smb_values,
      local.cert_manager_values,
      local.rancher_values
    ])
    # Redeploy when versions of addons need to be updated
    versions = join(""\n"", [
      coalesce(var.initial_k3s_channel, ""N/A""),
      coalesce(var.cluster_autoscaler_version, ""N/A""),
      coalesce(var.hetzner_ccm_version, ""N/A""),
      coalesce(var.hetzner_csi_version, ""N/A""),
      coalesce(var.kured_version, ""N/A""),
      coalesce(var.calico_version, ""N/A""),
      coalesce(var.cilium_version, ""N/A""),
      coalesce(var.traefik_version, ""N/A""),
      coalesce(var.nginx_version, ""N/A""),
    ])
    options = join(""\n"", [
      for option, value in local.kured_options : ""${option}=${value}""
    ])
  }

  connection {
    user           = ""root""
    private_key    = var.ssh_private_key
    agent_identity = local.ssh_agent_identity
    host           = module.control_planes[keys(module.control_planes)[0]].ipv4_address
    port           = var.ssh_port
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content     = local.kustomization_backup_yaml
    destination = ""/var/post_install/kustomization.yaml""
  }

  # Upload traefik ingress controller config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/traefik_ingress.yaml.tpl"",
      {
        version          = var.traefik_version
        values           = indent(4, trimspace(local.traefik_values))
        target_namespace = local.ingress_controller_namespace
    })
    destination = ""/var/post_install/traefik_ingress.yaml""
  }

  # Upload nginx ingress controller config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/nginx_ingress.yaml.tpl"",
      {
        version          = var.nginx_version
        values           = indent(4, trimspace(local.nginx_values))
        target_namespace = local.ingress_controller_namespace
    })
    destination = ""/var/post_install/nginx_ingress.yaml""
  }

  # Upload the CCM patch config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/ccm.yaml.tpl"",
      {
        cluster_cidr_ipv4   = var.cluster_ipv4_cidr
        default_lb_location = var.load_balancer_location
        using_klipper_lb    = local.using_klipper_lb
    })
    destination = ""/var/post_install/ccm.yaml""
  }

  # Upload the calico patch config, for the kustomization of the calico manifest
  # This method is a stub which could be replaced by a more practical helm implementation
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/calico.yaml.tpl"",
      {
        values = trimspace(local.calico_values)
    })
    destination = ""/var/post_install/calico.yaml""
  }

  # Upload the cilium install file
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cilium.yaml.tpl"",
      {
        values  = indent(4, trimspace(local.cilium_values))
        version = var.cilium_version
    })
    destination = ""/var/post_install/cilium.yaml""
  }

  # Upload the system upgrade controller plans config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/plans.yaml.tpl"",
      {
        channel          = var.initial_k3s_channel
        disable_eviction = !var.system_upgrade_enable_eviction
    })
    destination = ""/var/post_install/plans.yaml""
  }

  # Upload the Longhorn config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/longhorn.yaml.tpl"",
      {
        longhorn_namespace  = var.longhorn_namespace
        longhorn_repository = var.longhorn_repository
        values              = indent(4, trimspace(local.longhorn_values))
    })
    destination = ""/var/post_install/longhorn.yaml""
  }

  # Upload the csi-driver-smb config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/csi-driver-smb.yaml.tpl"",
      {
        values = indent(4, trimspace(local.csi_driver_smb_values))
    })
    destination = ""/var/post_install/csi-driver-smb.yaml""
  }

  # Upload the cert-manager config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cert_manager.yaml.tpl"",
      {
        values = indent(4, trimspace(local.cert_manager_values))
    })
    destination = ""/var/post_install/cert_manager.yaml""
  }

  # Upload the Rancher config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/rancher.yaml.tpl"",
      {
        rancher_install_channel = var.rancher_install_channel
        values                  = indent(4, trimspace(local.rancher_values))
    })
    destination = ""/var/post_install/rancher.yaml""
  }

  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/kured.yaml.tpl"",
      {
        options = local.kured_options
      }
    )
    destination = ""/var/post_install/kured.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""set -ex"",
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${data.hcloud_network.k3s.name} --dry-run=client -o yaml | kubectl apply -f -"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token} --dry-run=client -o yaml | kubectl apply -f -"",
      local.csi_version != null ? ""curl https://raw.githubusercontent.com/hetznercloud/csi-driver/${coalesce(local.csi_version, ""v2.4.0"")}/deploy/kubernetes/hcloud-csi.yml -o /var/post_install/hcloud-csi.yml"" : ""echo 'Skipping hetzner csi.'""
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = concat([
      ""set -ex"",

      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /var/post_install/kustomization.yaml"",

      # Wait for k3s to become ready (we check one more time) because in some edge cases,
      # the cluster had become unvailable for a few seconds, at this very instant.
      <<-EOT
      timeout 360 bash <<EOF
        until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do
          echo ""Waiting for the cluster to become ready...""
          sleep 2
        done
      EOF
      EOT
      ]
      ,

      [
        # Ready, set, go for the kustomization
        ""kubectl apply -k /var/post_install"",
        ""echo 'Waiting for the system-upgrade-controller deployment to become available...'"",
        ""kubectl -n system-upgrade wait --for=condition=available --timeout=360s deployment/system-upgrade-controller"",
        ""sleep 7"", # important as the system upgrade controller CRDs sometimes don't get ready right away, especially with Cilium.
        ""kubectl -n system-upgrade apply -f /var/post_install/plans.yaml""
      ],
      local.has_external_load_balancer ? [] : [
        <<-EOT
      timeout 360 bash <<EOF
      until [ -n ""\$(kubectl get -n ${local.ingress_controller_namespace} service/${lookup(local.ingress_controller_service_names, var.ingress_controller)} --output=jsonpath='{.status.loadBalancer.ingress[0].${var.lb_hostname != """" ? ""hostname"" : ""ip""}}' 2> /dev/null)"" ]; do
          echo ""Waiting for load-balancer to get an IP...""
          sleep 2
      done
      EOF
      EOT
    ])
  }

  depends_on = [
    hcloud_load_balancer.cluster,
    null_resource.control_planes,
    random_password.rancher_bootstrap,
    hcloud_volume.longhorn_volume
  ]
}
",resource,149,295.0,8ba33a12c807193bc1d50076ec7bd3b5d681255a,7b82c726a6d9bc3da643eefbee1b587e0126d889,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/8ba33a12c807193bc1d50076ec7bd3b5d681255a/master.tf#L149,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/7b82c726a6d9bc3da643eefbee1b587e0126d889/init.tf#L295,2022-02-12 00:52:13+01:00,2024-05-06 13:56:24+02:00,187,0,1,0,0,0,0,1,0,0
https://github.com/jenkins-x/terraform-google-jx,2,main.tf,main.tf,0,// todo,"// TODO: remove parent_domain when its deprecations is complete: domain_enabled = var.apex_domain != """" ? true : false","// DNS 
 // TODO: remove parent_domain when its deprecations is complete: domain_enabled = var.apex_domain != """" ? true : false","locals {
  interpolated_content = templatefile(""${path.module}/modules/jx-requirements.yml.tpl"", {
    gcp_project                 = var.gcp_project
    zone                        = var.cluster_location
    cluster_name                = local.cluster_name
    git_owner_requirement_repos = var.git_owner_requirement_repos
    dev_env_approvers           = var.dev_env_approvers
    lets_encrypt_production     = var.lets_encrypt_production
    // Storage buckets
    log_storage_url        = module.cluster.log_storage_url
    report_storage_url     = module.cluster.report_storage_url
    repository_storage_url = module.cluster.repository_storage_url
    backup_bucket_url      = module.backup.backup_bucket_url
    // Vault
    external_vault = local.external_vault
    vault_bucket   = length(module.vault) > 0 ? module.vault[0].vault_bucket_name : """"
    vault_key      = length(module.vault) > 0 ? module.vault[0].vault_key : """"
    vault_keyring  = length(module.vault) > 0 ? module.vault[0].vault_keyring : """"
    vault_name     = length(module.vault) > 0 ? module.vault[0].vault_name : """"
    vault_sa       = length(module.vault) > 0 ? module.vault[0].vault_sa : """"
    vault_url      = var.vault_url
    // Velero
    enable_backup    = var.enable_backup
    velero_sa        = module.backup.velero_sa
    velero_namespace = module.backup.backup_bucket_url != """" ? var.velero_namespace : """"
    velero_schedule  = var.velero_schedule
    velero_ttl       = var.velero_ttl
    // DNS
    // TODO: remove parent_domain when its deprecations is complete: domain_enabled = var.apex_domain != """" ? true : false
    domain_enabled = var.apex_domain != """" ? true : (var.parent_domain != """" ? true : false)
    // TODO: replace with the following when parent_domain deprecations is complete: apex_domain  = var.apex_domain
    apex_domain    = var.apex_domain != """" ? var.apex_domain : var.parent_domain
    subdomain      = var.subdomain
    tls_email      = var.tls_email

    version_stream_ref = var.version_stream_ref
    version_stream_url = var.version_stream_url
    webhook            = var.webhook
  })

  split_content   = split(""\n"", local.interpolated_content)
  compact_content = compact(local.split_content)
  content         = join(""\n"", local.compact_content)
}
",locals,"locals {
  requirements_file = var.jx2 ? ""${path.module}/modules/jx-requirements.yml.tpl"" : ""${path.module}/modules/jx-requirements-v3.yml.tpl""
  interpolated_content = templatefile(local.requirements_file, {
    gcp_project                 = var.gcp_project
    zone                        = var.cluster_location
    cluster_name                = local.cluster_name
    git_owner_requirement_repos = var.git_owner_requirement_repos
    dev_env_approvers           = var.dev_env_approvers
    lets_encrypt_production     = var.lets_encrypt_production
    // GCP Artifact
    enable_artifact        = var.artifact_enable
    registry               = module.cluster.artifact_registry_repository
    docker_registry_org    = module.cluster.artifact_registry_repository_name
    // Storage buckets
    log_storage_url        = module.cluster.log_storage_url
    report_storage_url     = module.cluster.report_storage_url
    repository_storage_url = module.cluster.repository_storage_url
    backup_bucket_url      = module.backup.backup_bucket_url
    // Vault
    external_vault  = local.external_vault
    vault_bucket    = length(module.vault) > 0 ? module.vault[0].vault_bucket_name : """"
    vault_key       = length(module.vault) > 0 ? module.vault[0].vault_key : """"
    vault_keyring   = length(module.vault) > 0 ? module.vault[0].vault_keyring : """"
    vault_name      = length(module.vault) > 0 ? module.vault[0].vault_name : """"
    vault_sa        = length(module.vault) > 0 ? module.vault[0].vault_sa : """"
    vault_url       = var.vault_url
    vault_installed = !var.gsm ? true : false
    // Velero
    enable_backup    = var.enable_backup
    velero_sa        = module.backup.velero_sa
    velero_namespace = module.backup.backup_bucket_url != """" ? var.velero_namespace : """"
    velero_schedule  = var.velero_schedule
    velero_ttl       = var.velero_ttl
    // DNS
    // TODO: remove parent_domain when its deprecations is complete: domain_enabled = var.apex_domain != """" ? true : false
    domain_enabled = var.apex_domain != """" ? true : (var.parent_domain != """" ? true : false)
    // TODO: replace with the following when parent_domain deprecations is complete: apex_domain  = var.apex_domain
    apex_domain = var.apex_domain != """" ? var.apex_domain : var.parent_domain
    subdomain   = var.subdomain
    tls_email   = var.tls_email
    // Kuberhealthy
    kuberhealthy = var.kuberhealthy

    version_stream_ref = var.version_stream_ref
    version_stream_url = var.version_stream_url
    webhook            = var.webhook
  })

  split_content   = split(""\n"", local.interpolated_content)
  compact_content = compact(local.split_content)
  content         = join(""\n"", local.compact_content)
}
",locals,279,304.0,ffc0b68673f1e9341ef89e88f6af157631a9086d,6d61937b9d1a81a879246040a6f6e11ed5626a4e,https://github.com/jenkins-x/terraform-google-jx/blob/ffc0b68673f1e9341ef89e88f6af157631a9086d/main.tf#L279,https://github.com/jenkins-x/terraform-google-jx/blob/6d61937b9d1a81a879246040a6f6e11ed5626a4e/main.tf#L304,2020-12-09 11:35:14+10:00,2024-04-25 13:02:59+02:00,30,0,0,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,588,examples/data-solutions/dp-foundation/07-exposure.tf,examples/data-solutions/data-platform-foundations/07-exposure.tf,1,#todo,#TODO add group => role mapping to asign on exposure project,#TODO add group => role mapping to asign on exposure project,"locals {
  group_iam_exp = {
    #TODO add group => role mapping to asign on exposure project
  }
  iam_exp = {
    #TODO add role => service account mapping to assign roles on exposure project
  }
  prefix_exp = ""${var.prefix}-exp""
}
",locals,the block associated got renamed or deleted,,19,,3c99074b3ff652827d277217e4f84b48a713b224,4f4a9cd7ac2cee3a1b2e413cd19f6b6b07c35622,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3c99074b3ff652827d277217e4f84b48a713b224/examples/data-solutions/dp-foundation/07-exposure.tf#L19,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/4f4a9cd7ac2cee3a1b2e413cd19f6b6b07c35622/examples/data-solutions/data-platform-foundations/07-exposure.tf,2022-02-02 15:31:54+01:00,2022-02-09 17:01:25+01:00,3,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,484,tests/fast/stages/s03_project_factory/fixture/variables.tf,tests/fast/stages/s03_project_factory/fixture/variables.tf,0,#todo,#TODO: tfdoc annotations,"/** 
 * Copyright 2022 Google LLC 
 * 
 * Licensed under the Apache License, Version 2.0 (the ""License""); 
 * you may not use this file except in compliance with the License. 
 * You may obtain a copy of the License at 
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, software 
 * distributed under the License is distributed on an ""AS IS"" BASIS, 
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 * See the License for the specific language governing permissions and 
 * limitations under the License. 
 */  
 #TODO: tfdoc annotations ","variable ""billing_account_id"" {
  # tfdoc:variable:source 00-bootstrap
  description = ""Billing account id.""
  type        = string
}
",variable,,,17,0.0,cee207b4544cfe2bc2eb517fd91c79952e3052b3,dc3a2ad7be032c093ae4d82d3abfeb628c16dfe6,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/cee207b4544cfe2bc2eb517fd91c79952e3052b3/tests/fast/stages/s03_project_factory/fixture/variables.tf#L17,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/dc3a2ad7be032c093ae4d82d3abfeb628c16dfe6/tests/fast/stages/s03_project_factory/fixture/variables.tf#L0,2022-01-17 10:36:38+01:00,2022-02-24 15:05:18+01:00,5,2,0,0,0,1,0,0,0,0
https://github.com/Azure/sap-automation,9,deploy/terraform/terraform-units/modules/sap_deployer/infrastructure.tf,deploy/terraform/terraform-units/modules/sap_deployer/infrastructure.tf,0,implemented,//        Management lock should be implemented id a seperate Terraform workspace,"// TODO: Add management lock when this issue is addressed https://github.com/terraform-providers/terraform-provider-azurerm/issues/5473 
 //        Management lock should be implemented id a seperate Terraform workspace   
 // Create/Import management vnet","resource ""azurerm_virtual_network"" ""vnet_mgmt"" {
  count               = (local.enable_deployers && !local.vnet_mgmt_exists) ? 1 : 0
  name                = local.vnet_mgmt_name
  resource_group_name = local.rg_exists ? data.azurerm_resource_group.deployer[0].name : azurerm_resource_group.deployer[0].name
  location            = local.rg_exists ? data.azurerm_resource_group.deployer[0].location : azurerm_resource_group.deployer[0].location
  address_space       = [local.vnet_mgmt_addr]
}
",resource,"resource ""azurerm_virtual_network"" ""vnet_mgmt"" {
  count                                = (!local.vnet_mgmt_exists) ? 1 : 0
  name                                 = local.vnet_mgmt_name
  resource_group_name                  = local.resource_group_exists ? data.azurerm_resource_group.deployer[0].name : azurerm_resource_group.deployer[0].name
  location                             = local.resource_group_exists ? data.azurerm_resource_group.deployer[0].location : azurerm_resource_group.deployer[0].location
  address_space                        = [local.vnet_mgmt_addr]
}
",resource,94,35.0,6ff0b891114c36d3aeccb850d830b698cd1fe52a,db20ac2a47d9d00329385330cb4af6b3c726c400,https://github.com/Azure/sap-automation/blob/6ff0b891114c36d3aeccb850d830b698cd1fe52a/deploy/terraform/terraform-units/modules/sap_deployer/infrastructure.tf#L94,https://github.com/Azure/sap-automation/blob/db20ac2a47d9d00329385330cb4af6b3c726c400/deploy/terraform/terraform-units/modules/sap_deployer/infrastructure.tf#L35,2021-11-17 19:29:07+02:00,2024-03-11 23:15:11+05:30,24,0,0,1,1,0,1,0,0,0
https://github.com/kubernetes/k8s.io,436,infra/aws/terraform/prow-build-cluster/node_group_blue.tf,infra/aws/terraform/prow-build-cluster/node_group_blue.tf,0,# todo,# TODO(xmudrii-ubuntu): Temporarily disabled because it's not supported by Bottlerocket Linux,"# TODO(xmudrii-ubuntu): Temporarily disabled because it's not supported by Bottlerocket Linux 
 # ami_id                     = var.node_ami_blue 
 # enable_bootstrap_user_data = true ","locals {
  node_group_build_blue = {
    name            = ""build-managed-blue""
    description     = ""EKS managed node group called blue used to facilitate node rotations/rollouts""
    use_name_prefix = true

    cluster_version = var.node_group_version_blue

    taints = var.node_taints_blue
    labels = var.node_labels_blue

    subnet_ids = module.vpc.public_subnets

    min_size     = var.node_min_size_blue
    max_size     = var.node_max_size_blue
    desired_size = var.node_desired_size_blue

    iam_role_permissions_boundary = data.aws_iam_policy.eks_resources_permission_boundary.arn

    # TODO(xmudrii-ubuntu): Temporarily disabled because it's not supported by Bottlerocket Linux
    # ami_id                     = var.node_ami_blue
    # enable_bootstrap_user_data = true

    ami_type             = ""BOTTLEROCKET_x86_64""
    platform             = ""bottlerocket""
    bootstrap_extra_args = <<-EOT
      # Bottlerocket instances don't have SSH installed by default, but
      # there's the admin container that can be enabled and that comes
      # with SSH installed and enabled
      [settings.host-containers.admin]
      enabled = true

      # Bootstrap the instance using our bootstrap script embeded in a Docker image
      [settings.bootstrap-containers.bootstrap]
      source = ""public.ecr.aws/q4o2z4d8/k8s-prow-bottlerocket:v0.0.1""
      mode = ""always""
      essential = true
    EOT

    force_update_version = false
    update_config = {
      max_unavailable_percentage = var.node_max_unavailable_percentage
    }

    pre_bootstrap_user_data = file(""${path.module}/bootstrap/node_bootstrap.sh"")

    capacity_type  = ""ON_DEMAND""
    instance_types = var.node_instance_types_blue

    ebs_optimized     = true
    enable_monitoring = true

    key_name = aws_key_pair.eks_nodes.key_name

    block_device_mappings = {
      # This must be sda1 in order to match the root volume,
      # otherwise a new volume is created.
      sda1 = {
        device_name = ""/dev/sda1""
        ebs = {
          volume_size           = var.node_volume_size
          volume_type           = ""gp3""
          iops                  = 16000 # Maximum for gp3 volume.
          throughput            = 1000  # Maximum for gp3 volume.
          encrypted             = false
          delete_on_termination = true
        }
      }
    }

    enclave_options = {
      enabled = true
    }

    timeouts = {
      update = ""180m""
    }

    tags = merge(
      local.tags,
      local.auto_scaling_tags,
      var.additional_node_group_tags_blue
    )
  }
}
",locals,"locals {
  node_group_build_blue = {
    name            = ""build-managed-blue""
    description     = ""EKS managed node group called blue used to facilitate node rotations/rollouts""
    use_name_prefix = true

    cluster_version = var.node_group_version_blue

    taints = var.node_taints_blue
    labels = var.node_labels_blue

    subnet_ids = module.vpc.public_subnets

    min_size     = var.node_min_size_blue
    max_size     = var.node_max_size_blue
    desired_size = var.node_desired_size_blue

    iam_role_permissions_boundary = data.aws_iam_policy.eks_resources_permission_boundary.arn

    ami_type             = ""BOTTLEROCKET_x86_64""
    platform             = ""bottlerocket""
    bootstrap_extra_args = <<-EOT
      # Bottlerocket instances don't have SSH installed by default, but
      # there's the admin container that can be enabled and that comes
      # with SSH installed and enabled
      [settings.host-containers.admin]
      enabled = true

      # Bootstrap the instance using our bootstrap script embeded in a Docker image
      [settings.bootstrap-containers.bootstrap]
      source = ""public.ecr.aws/q4o2z4d8/k8s-prow-bottlerocket:v0.0.2""
      mode = ""always""
      essential = true

      [settings.kernel.sysctl]
      ""fs.inotify.max_user_watches"" = ""1048576""
      ""fs.inotify.max_user_instances"" = ""8192""
      ""vm.min_free_kbytes"" = ""540672""
    EOT

    force_update_version = false
    update_config = {
      max_unavailable_percentage = var.node_max_unavailable_percentage
    }

    pre_bootstrap_user_data = file(""${path.module}/bootstrap/node_bootstrap.sh"")

    capacity_type  = ""ON_DEMAND""
    instance_types = var.node_instance_types_blue

    ebs_optimized     = true
    enable_monitoring = true

    key_name = aws_key_pair.eks_nodes.key_name

    block_device_mappings = {
      # This must be sda1 in order to match the root volume,
      # otherwise a new volume is created.
      sda1 = {
        device_name = ""/dev/sda1""
        ebs = {
          volume_size           = var.node_volume_size
          volume_type           = ""gp3""
          iops                  = 16000 # Maximum for gp3 volume.
          throughput            = 1000  # Maximum for gp3 volume.
          encrypted             = false
          delete_on_termination = true
        }
      }
    }

    enclave_options = {
      enabled = true
    }

    timeouts = {
      update = ""180m""
    }

    tags = merge(
      local.tags,
      local.auto_scaling_tags,
      var.additional_node_group_tags_blue
    )
  }
}
",locals,36,,445da81d263c7f1f6dae465f8047c5aa6b9a2934,f8cb33e74bada53999da408888d92a324eec52b2,https://github.com/kubernetes/k8s.io/blob/445da81d263c7f1f6dae465f8047c5aa6b9a2934/infra/aws/terraform/prow-build-cluster/node_group_blue.tf#L36,https://github.com/kubernetes/k8s.io/blob/f8cb33e74bada53999da408888d92a324eec52b2/infra/aws/terraform/prow-build-cluster/node_group_blue.tf,2023-10-16 10:30:36+02:00,2024-01-09 11:56:09+01:00,3,1,0,1,1,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-kubernetes-engine,8,modules/safer-cluster/main.tf,modules/safer-cluster/main.tf,0,// todo,"// TODO(mmontan): define a registry_project parameter in the private_beta_cluster,","// TODO(mmontan): define a registry_project parameter in the private_beta_cluster, 
 // so that we can give GCS permissions to the service account on a project 
 // that hosts only container-images and not data.","module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // Disable the dashboard. It creates risk by running as a very sensitive user.
  kubernetes_dashboard = false

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy          = true
  network_policy_provider = ""CALICO""

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  disable_legacy_metadata_endpoints = true

  node_pools        = var.node_pools
  node_pools_labels = var.node_pools_labels

  // TODO(mmontan): check whether we need to restrict these
  // settings.
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  // TODO(mmontan): we generally considered applying
  // just the cloud-platofrm scope and use Cloud IAM
  // If we have Workload Identity, are there advantages
  // in restricting scopes even more?
  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  // We should use IP Alias.
  configure_ip_masq = false

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = length(var.compute_engine_service_account) > 0 ? false : true
  service_account        = var.compute_engine_service_account

  // TODO(mmontan): define a registry_project parameter in the private_beta_cluster,
  // so that we can give GCS permissions to the service account on a project
  // that hosts only container-images and not data.
  grant_registry_access = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // TODO(mmontan): link to a couple of policies.
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id
  node_metadata                    = ""SECURE""

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // TODO(mmontan): investigate whether this should be a recommended setting
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group
}
",module,"module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy = true

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  node_pools        = var.node_pools
  node_pools_labels = var.node_pools_labels

  // TODO(mmontan): check whether we need to restrict these
  // settings.
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = var.compute_engine_service_account == """" ? true : false
  service_account        = var.compute_engine_service_account
  registry_project_id    = var.registry_project_id
  grant_registry_access  = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // Example: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#example
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // Intranode Visibility enables you to capture flow logs for traffic between pods and create FW rules that apply to traffic between pods.
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group

  enable_shielded_nodes = var.enable_shielded_nodes
}
",module,107,,e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5,5a194719faa144ad0a7ee578663d336358f5073c,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5/modules/safer-cluster/main.tf#L107,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/5a194719faa144ad0a7ee578663d336358f5073c/modules/safer-cluster/main.tf,2019-10-04 15:21:33-07:00,2019-11-13 15:10:12-06:00,6,1,1,1,0,1,0,0,0,0
https://github.com/alphagov/govuk-aws,348,terraform/projects/infra-security-groups/draft-frontend.tf,terraform/projects/infra-security-groups/draft-frontend.tf,0,# todo,# TODO: most application instances need to talk to draft-frontend - we could,"# TODO: most application instances need to talk to draft-frontend - we could 
 # split out some security for application and service instances?","resource ""aws_security_group_rule"" ""allow_management_to_draft-frontend_elb_https"" {
  type      = ""ingress""
  from_port = 443
  to_port   = 443
  protocol  = ""tcp""

  security_group_id        = ""${aws_security_group.draft-frontend_elb.id}""
  source_security_group_id = ""${aws_security_group.management.id}""
}
",resource,the block associated got renamed or deleted,,47,,3067a0d38467044b34ab13c78f254c3be734c10f,7fd13330f8d6238e108f76ce76a76a28a99caaaf,https://github.com/alphagov/govuk-aws/blob/3067a0d38467044b34ab13c78f254c3be734c10f/terraform/projects/infra-security-groups/draft-frontend.tf#L47,https://github.com/alphagov/govuk-aws/blob/7fd13330f8d6238e108f76ce76a76a28a99caaaf/terraform/projects/infra-security-groups/draft-frontend.tf,2017-09-22 10:05:04+01:00,2018-01-02 17:41:32+00:00,2,1,0,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-iam,14,test/fixtures/update/main.tf,test/fixtures/update/main.tf,0,# todo,# TODO: Temporary. Have to change it manually after each destroy,"# TODO: Temporary. Have to change it manually after each destroy 
 #       since project names stay reserved even after deletion. 
 #       Used to test static IAM. We can get rid of it if 
 #       we make a 2 step converge process with the var.random_hexes 
 #       generation being the first step.","locals {
  static_n  = 2
  dynamic_n = 1 # Dynamic mode supports only 1 entity atm.
  mode      = ""authoritative""
  prefix    = ""test-iam""

  # TODO: Temporary. Have to change it manually after each destroy
  #       since project names stay reserved even after deletion.
  #       Used to test static IAM. We can get rid of it if
  #       we make a 2 step converge process with the var.random_hexes
  #       generation being the first step.
  uniqSuffix = ""prj6""

  member_group_0 = [
    ""serviceAccount:${var.member1}"",
  ]

  member_group_1 = [
    ""serviceAccount:${var.member1}"",
    ""serviceAccount:${var.member2}""
  ]

  project_bindings = {
    ""roles/iam.roleViewer"" = local.member_group_0
    ""roles/logging.viewer"" = local.member_group_1

    # Uncomment the following role and re`converge` to test
    # whether some resources were recreated.
    # Expectation is that no resource are gonna be recreated, but only
    # new ones added.
    # ""roles/iam.securityReviewer"" = local.member_group_0
  }
}
",locals,"locals {
  static_n  = 2
  dynamic_n = 1 # Dynamic mode supports only 1 entity atm.
  mode      = ""authoritative""
  prefix    = ""test-iam""

  member_group_0 = [
    ""serviceAccount:${var.member1}"",
  ]

  member_group_1 = [
    ""serviceAccount:${var.member1}"",
    ""serviceAccount:${var.member2}""
  ]

  project_bindings = zipmap(
    slice([
      ""roles/iam.roleViewer"",
      ""roles/logging.viewer"",
      ""roles/iam.securityReviewer""
    ], 0, var.roles),
    slice([
      local.member_group_0,
      local.member_group_1,
      local.member_group_0
    ], 0, var.roles)
  )
}
",locals,23,,665a160dd97a99c80a325250d7fea70fa9f4fac5,0de33e0943c7eabd31dc618990477d11614de007,https://github.com/terraform-google-modules/terraform-google-iam/blob/665a160dd97a99c80a325250d7fea70fa9f4fac5/test/fixtures/update/main.tf#L23,https://github.com/terraform-google-modules/terraform-google-iam/blob/0de33e0943c7eabd31dc618990477d11614de007/test/fixtures/update/main.tf,2019-10-14 19:29:14+03:00,2019-10-17 13:08:12+03:00,2,1,0,1,0,0,0,0,0,0
https://github.com/cattle-ops/terraform-aws-gitlab-runner,88,modules/cache/main.tf,modules/cache/main.tf,0,fix,# checkov:skip=CKV_AWS_300:False positive. Can be removed when https://github.com/bridgecrewio/checkov/issues/4733 is fixed.,# checkov:skip=CKV_AWS_300:False positive. Can be removed when https://github.com/bridgecrewio/checkov/issues/4733 is fixed.,"resource ""aws_s3_bucket_lifecycle_configuration"" ""build_cache_versioning"" {
  # checkov:skip=CKV_AWS_300:False positive. Can be removed when https://github.com/bridgecrewio/checkov/issues/4733 is fixed.
  bucket = aws_s3_bucket.build_cache.id

  rule {
    id     = ""AbortIncompleteMultipartUploads""
    status = ""Enabled""

    abort_incomplete_multipart_upload {
      days_after_initiation = 1
    }
  }

  rule {
    id     = ""clear""
    status = var.cache_lifecycle_clear ? ""Enabled"" : ""Disabled""

    filter {
      prefix = var.cache_lifecycle_prefix
    }

    expiration {
      days = var.cache_expiration_days
    }
  }
}
",resource,"resource ""aws_s3_bucket_lifecycle_configuration"" ""build_cache_versioning"" {
  # checkov:skip=CKV_AWS_300:False positive. Can be removed when https://github.com/bridgecrewio/checkov/issues/4733 is fixed.
  bucket = aws_s3_bucket.build_cache.id

  rule {
    id     = ""AbortIncompleteMultipartUploads""
    status = ""Enabled""

    abort_incomplete_multipart_upload {
      days_after_initiation = 1
    }
  }

  rule {
    id     = ""clear""
    status = var.cache_lifecycle_clear ? ""Enabled"" : ""Disabled""

    filter {
      prefix = var.cache_lifecycle_prefix
    }

    expiration {
      days = var.cache_expiration_days
    }
  }
}
",resource,59,53.0,5f8d24e835462b1f08f2b7889c015255961ebbbd,aa93e768a1e2d414197feea1330ff3290f65dbb2,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/5f8d24e835462b1f08f2b7889c015255961ebbbd/modules/cache/main.tf#L59,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/aa93e768a1e2d414197feea1330ff3290f65dbb2/modules/cache/main.tf#L53,2023-03-29 07:27:10+02:00,2024-04-11 09:52:40+02:00,3,0,1,1,0,0,0,1,0,1
https://github.com/aws-observability/terraform-aws-observability-accelerator,30,examples/existing-cluster-with-base-and-infra/main.tf,examples/existing-cluster-with-base-and-infra/main.tf,0,todo,# create a new Grafana workspace - TODO review design,# create a new Grafana workspace - TODO review design,"module ""eks_observability_accelerator"" {
  # source = ""aws-ia/terrarom-aws-observability-accelerator?ref=dev""
  source = ""../../""

  aws_region     = var.aws_region
  eks_cluster_id = var.eks_cluster_id

  # deploys AWS Distro for OpenTelemetry operator into the cluster
  enable_amazon_eks_adot = true

  # reusing existing certificate manager? defaults to true
  enable_cert_manager = true

  # # -- or enable opentelemetry operator
  enable_opentelemetry_operator = false
  #open_telemetry_operator_config = map() // custom config

  # creates a new AMP workspace, defaults to true
  enable_managed_prometheus = false

  # reusing existing AMP -- needs data source for alerting rules
  managed_prometheus_id     = var.managed_prometheus_workspace_id
  managed_prometheus_region = null # defaults to the current region, useful for cross region scenarios (same account)

  # sets up the AMP alert manager at the workspace level
  enable_alertmanager = true

  # create a new Grafana workspace - TODO review design
  enable_managed_grafana       = false
  managed_grafana_workspace_id = var.managed_grafana_workspace_id
  grafana_api_key              = var.grafana_api_key

  tags = local.tags
}
",module,"module ""eks_observability_accelerator"" {
  # source = ""aws-ia/terrarom-aws-observability-accelerator""
  source = ""../../""

  aws_region     = var.aws_region
  eks_cluster_id = var.eks_cluster_id

  # deploys AWS Distro for OpenTelemetry operator into the cluster
  enable_amazon_eks_adot = true

  # reusing existing certificate manager? defaults to true
  enable_cert_manager = true

  # creates a new AMP workspace, defaults to true
  enable_managed_prometheus = false

  # reusing existing AMP -- needs data source for alerting rules
  managed_prometheus_workspace_id     = var.managed_prometheus_workspace_id
  managed_prometheus_workspace_region = null # defaults to the current region, useful for cross region scenarios (same account)

  # sets up the AMP alert manager at the workspace level
  enable_alertmanager = true

  # reusing existing Amazon Managed Grafana workspace
  enable_managed_grafana       = false
  managed_grafana_workspace_id = var.managed_grafana_workspace_id
  grafana_api_key              = var.grafana_api_key

  tags = local.tags
}
",module,77,,5b5d5e7dd17a83d36e2a857da5feda77a8b95776,6dca51b135de9dae7aa62a61e7eebc9895c27412,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/5b5d5e7dd17a83d36e2a857da5feda77a8b95776/examples/existing-cluster-with-base-and-infra/main.tf#L77,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/6dca51b135de9dae7aa62a61e7eebc9895c27412/examples/existing-cluster-with-base-and-infra/main.tf,2022-08-26 17:30:03+02:00,2022-08-26 17:30:03+02:00,2,1,1,1,0,0,0,0,1,0
https://github.com/kubernetes/k8s.io,12,infra/gcp/clusters/kubernetes-public/prow-build-test/k8s-infra-gke-nodepool/main.tf,infra/gcp/terraform/modules/gke-nodepool/main.tf,1,todo,// TODO ??,"// TODO ?? 
 // name = var.name","resource ""google_container_node_pool"" ""node_pool"" {
  provider = google-beta

  // TODO ??
  // name = var.name
  name_prefix = ""${var.name}-""

  project     = var.project_name
  location    = var.location
  cluster     = var.cluster_name

  // Auto repair, and auto upgrade nodes to match the master version
  management {
    auto_repair  = true
    auto_upgrade = true
  }

  // Autoscale the cluster as needed. Note if using a regional cluster these values will be multiplied by 3
  initial_node_count = var.min_count
  autoscaling {
    min_node_count = var.min_count
    max_node_count = var.max_count
  }

  // Set machine type, and enable all oauth scopes tied to the service account
  node_config {
    machine_type = var.machine_type
    disk_size_gb = var.disk_size_gb
    disk_type    = var.disk_type

    service_account = var.service_account
    oauth_scopes    = [""https://www.googleapis.com/auth/cloud-platform""]

    // Needed for workload identity
    workload_metadata_config {
      node_metadata = ""GKE_METADATA_SERVER""
    }
    metadata = {
      disable-legacy-endpoints = ""true""
    }
  }

  // If we need to destroy the node pool, create the new one before destroying
  // the old one
  lifecycle {
    create_before_destroy = true
  }
}
",resource,"resource ""google_container_node_pool"" ""node_pool"" {
  provider = google-beta

  // TODO ??
  // name = var.name
  name_prefix = ""${var.name}-""

  project     = var.project_name
  location    = var.location
  cluster     = var.cluster_name

  // Auto repair, and auto upgrade nodes to match the master version
  management {
    auto_repair  = true
    auto_upgrade = true
  }

  // Autoscale the cluster as needed. Note if using a regional cluster these values will be multiplied by 3
  initial_node_count = var.initial_count
  autoscaling {
    min_node_count = var.min_count
    max_node_count = var.max_count
  }

  // Set machine type, and enable all oauth scopes tied to the service account
  node_config {
    image_type   = var.image_type
    machine_type = var.machine_type
    disk_size_gb = var.disk_size_gb
    disk_type    = var.disk_type
    labels       = var.labels
    taint        = var.taints

    service_account = var.service_account
    oauth_scopes    = [""https://www.googleapis.com/auth/cloud-platform""]

    dynamic ""ephemeral_storage_config"" {
      for_each = var.ephemeral_local_ssd_count > 0 ? [var.ephemeral_local_ssd_count] : [] 
      content {
        local_ssd_count = ephemeral_storage_config.value
      }
    }

    // Needed for workload identity
    workload_metadata_config {
      mode = ""GKE_METADATA""
    }
    metadata = {
      disable-legacy-endpoints = ""true""
    }
  }

  // If we need to destroy the node pool, create the new one before destroying
  // the old one
  lifecycle {
    create_before_destroy = true
    # https://www.terraform.io/docs/providers/google/r/container_cluster.html#taint
    ignore_changes = [
      node_config[0].taint,
    ]
  }
}
",resource,20,20.0,c21384be39d30f4b0b0d7ecbc52a8d1109bc36c9,8e2d286dbeb61acbe598ae083b9e246518c0b78e,https://github.com/kubernetes/k8s.io/blob/c21384be39d30f4b0b0d7ecbc52a8d1109bc36c9/infra/gcp/clusters/kubernetes-public/prow-build-test/k8s-infra-gke-nodepool/main.tf#L20,https://github.com/kubernetes/k8s.io/blob/8e2d286dbeb61acbe598ae083b9e246518c0b78e/infra/gcp/terraform/modules/gke-nodepool/main.tf#L20,2020-04-29 16:44:08-07:00,2021-11-01 15:53:37-07:00,8,0,1,1,0,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,254,modules/utilities/drain.tf,modules/utilities/drain.tf,0,todo,# TODO Implement,count = false && var.expected_node_count > 0 ? 1 : 0 # TODO Implement,"resource ""null_resource"" ""drain_workers"" {
  count = false && var.expected_node_count > 0 ? 1 : 0 # TODO Implement
  triggers = {
    drain_count = jsonencode(keys(local.worker_pools_draining))
  }

  connection {
    bastion_host        = var.bastion_host
    bastion_user        = var.bastion_user
    bastion_private_key = var.ssh_private_key
    host                = var.operator_host
    user                = var.operator_user
    private_key         = var.ssh_private_key
    timeout             = ""40m""
    type                = ""ssh""
  }

  provisioner ""remote-exec"" {
    inline = [
      ""echo kubectl get nodes ..."",             # TODO List nodes by label for draining pools
      ""echo kubectl drain --ignore-daemonsets"", # TODO Drain nodes for draining pools
    ]
  }
}
",resource,"resource ""null_resource"" ""drain_workers"" {
  count = local.drain_enabled ? 1 : 0
  triggers = {
    drain_workers = jsonencode(sort(keys(local.worker_pools_draining)))
  }

  connection {
    bastion_host        = var.bastion_host
    bastion_user        = var.bastion_user
    bastion_private_key = var.ssh_private_key
    host                = var.operator_host
    user                = var.operator_user
    private_key         = var.ssh_private_key
    timeout             = ""40m""
    type                = ""ssh""
  }

  provisioner ""remote-exec"" {
    inline = [
      ""echo kubectl get nodes ..."",             # TODO List nodes by label for draining pools
      ""echo kubectl drain --ignore-daemonsets"", # TODO Drain nodes for draining pools
    ]
  }
}
",resource,9,,663103be472bc1a358aa48b1bd6f619aa4de7bb0,e711d80a08eed0078476fecb9e0665a40bada4ed,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/663103be472bc1a358aa48b1bd6f619aa4de7bb0/modules/utilities/drain.tf#L9,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/e711d80a08eed0078476fecb9e0665a40bada4ed/modules/utilities/drain.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,2,1,0,1,0,0,0,1,0,0
https://github.com/nasa/cumulus,20,tf-modules/archive/iam.tf,tf-modules/archive/iam.tf,0,todo,# TODO Refactor so this doesn't make assumptions about table name prefixes,# TODO Refactor so this doesn't make assumptions about table name prefixes,"data ""aws_iam_policy_document"" ""lambda_api_gateway_policy"" {
  statement {
    actions   = [""ecs:RunTask""]
    resources = [aws_ecs_task_definition.async_operation.arn]
  }

  statement {
    actions = [
      ""logs:DescribeLogStreams"",
      ""logs:CreateLogGroup"",
      ""logs:CreateLogStream"",
      ""logs:PutLogEvents"",
      ""lambda:GetFunction"",
      ""lambda:invokeFunction"",
      ""lambda:CreateEventSourceMapping"",
      ""lambda:UpdateEventSourceMapping"",
      ""lambda:DeleteEventSourceMapping"",
      ""lambda:GetEventSourceMapping"",
      ""lambda:ListEventSourceMappings"",
      ""lambda:AddPermission"",
      ""lambda:RemovePermission""
    ]
    resources = [""*""]
  }

  statement {
    actions = [
      ""dynamodb:DeleteItem"",
      ""dynamodb:GetItem"",
      ""dynamodb:PutItem"",
      ""dynamodb:Query"",
      ""dynamodb:Scan"",
      ""dynamodb:UpdateItem""
    ]
    # TODO Refactor so this doesn't make assumptions about table name prefixes
    resources = [""arn:aws:dynamodb:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:table/${var.prefix}-*""]
  }

  statement {
    actions = [
      ""dynamodb:GetRecords"",
      ""dynamodb:GetShardIterator"",
      ""dynamodb:DescribeStream"",
      ""dynamodb:ListStreams""
    ]
    # TODO Refactor so this doesn't make assumptions about table name prefixes
    resources = [""arn:aws:dynamodb:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:table/${var.prefix}-*/stream/*""]
  }

  statement {
    actions   = [""dynamodb:ListTables""]
    resources = [""*""]
  }

  statement {
    actions = [
      ""s3:GetAccelerateConfiguration"",
      ""s3:GetLifecycleConfiguration"",
      ""s3:GetReplicationConfiguration"",
      ""s3:GetBucket*"",
      ""s3:PutAccelerateConfiguration"",
      ""s3:PutLifecycleConfiguration"",
      ""s3:PutReplicationConfiguration"",
      ""s3:PutBucket*"",
      ""s3:ListBucket*""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}""]
  }

  statement {
    actions = [
      ""s3:GetObject*"",
      ""s3:PutObject*"",
      ""s3:ListMultipartUploadParts"",
      ""s3:DeleteObject"",
      ""s3:DeleteObjectVersion""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}/*""]
  }

  statement {
    actions   = [""s3:ListAllMyBuckets""]
    resources = [""*""]
  }

  statement {
    actions = [
      ""sns:publish"",
      ""sns:Subscribe"",
      ""sns:Unsubscribe"",
      ""sns:List*"",
    ]
    resources = [""*""]
  }

  statement {
    actions = [
      ""sqs:GetQueueUrl"",
      ""sqs:GetQueueAttributes"",
      ""sqs:SendMessage"",
    ]
    resources = [""arn:aws:sqs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:${var.prefix}-*""]
  }

  statement {
    actions = [
      ""cloudwatch:List*"",
      ""cloudwatch:Get*"",
      ""cloudwatch:Describe*"",
    ]
    resources = [""*""]
  }

  statement {
    actions   = [""apigateway:GET""]
    resources = [""arn:aws:apigateway:${data.aws_region.current.name}::/restapis/*/stages""]
  }

  statement {
    actions = [
      ""events:DisableRule"",
      ""events:DeleteRule"",
      ""events:EnableRule"",
      ""events:ListRules"",
      ""events:PutRule"",
      ""events:DescribeRule"",
      ""events:PutTargets"",
      ""events:RemoveTargets"",
    ]
    resources = [""arn:aws:events:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:rule/${var.prefix}-*""]
  }

  statement {
    actions = [
      ""states:DescribeExecution"",
      ""states:DescribeStateMachine"",
      ""states:GetExecutionHistory"",
    ]
    resources = [""*""]
  }

  statement {
    actions = [
      ""ec2:CreateNetworkInterface"",
      ""ec2:DescribeNetworkInterfaces"",
      ""ec2:DeleteNetworkInterface"",
    ]
    resources = [""*""]
  }
}
",data,"data ""aws_iam_policy_document"" ""lambda_api_gateway_policy"" {
  statement {
    actions   = [""ecs:RunTask""]
    resources = [aws_ecs_task_definition.async_operation.arn]
  }

  statement {
    actions = [
      ""logs:DescribeLogStreams"",
      ""logs:CreateLogGroup"",
      ""logs:CreateLogStream"",
      ""logs:PutLogEvents"",
      ""lambda:GetFunction"",
      ""lambda:invokeFunction"",
      ""lambda:CreateEventSourceMapping"",
      ""lambda:UpdateEventSourceMapping"",
      ""lambda:DeleteEventSourceMapping"",
      ""lambda:GetEventSourceMapping"",
      ""lambda:ListEventSourceMappings"",
      ""lambda:AddPermission"",
      ""lambda:RemovePermission""
    ]
    resources = [""*""]
  }

  statement {
    actions = [
      ""dynamodb:DeleteItem"",
      ""dynamodb:GetItem"",
      ""dynamodb:PutItem"",
      ""dynamodb:Query"",
      ""dynamodb:Scan"",
      ""dynamodb:UpdateItem""
    ]
    resources = [for k, v in var.dynamo_tables : v.arn]
  }

  statement {
    actions = [
      ""dynamodb:GetRecords"",
      ""dynamodb:GetShardIterator"",
      ""dynamodb:DescribeStream"",
      ""dynamodb:ListStreams""
    ]
    resources = [for k, v in var.dynamo_tables : ""${v.arn}/stream/*""]
  }

  statement {
    actions   = [""dynamodb:ListTables""]
    resources = [""*""]
  }

  statement {
    actions = [
      ""s3:GetAccelerateConfiguration"",
      ""s3:GetLifecycleConfiguration"",
      ""s3:GetReplicationConfiguration"",
      ""s3:GetBucket*"",
      ""s3:PutAccelerateConfiguration"",
      ""s3:PutLifecycleConfiguration"",
      ""s3:PutReplicationConfiguration"",
      ""s3:PutBucket*"",
      ""s3:ListBucket*""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}""]
  }

  statement {
    actions = [
      ""s3:GetObject*"",
      ""s3:PutObject*"",
      ""s3:ListMultipartUploadParts"",
      ""s3:DeleteObject"",
      ""s3:DeleteObjectVersion""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}/*""]
  }

  statement {
    actions   = [""s3:ListAllMyBuckets""]
    resources = [""*""]
  }

  statement {
    actions = [
      ""sns:publish"",
      ""sns:Subscribe"",
      ""sns:Unsubscribe"",
      ""sns:List*"",
    ]
    resources = [""*""]
  }

  statement {
    actions = [
      ""sqs:GetQueueUrl"",
      ""sqs:GetQueueAttributes"",
      ""sqs:SendMessage"",
    ]
    resources = [""arn:aws:sqs:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:${var.prefix}-*""]
  }

  statement {
    actions = [
      ""cloudwatch:List*"",
      ""cloudwatch:Get*"",
      ""cloudwatch:Describe*"",
    ]
    resources = [""*""]
  }

  statement {
    actions   = [""apigateway:GET""]
    resources = [""arn:aws:apigateway:${data.aws_region.current.name}::/restapis/*/stages""]
  }

  statement {
    actions = [
      ""events:DisableRule"",
      ""events:DeleteRule"",
      ""events:EnableRule"",
      ""events:ListRules"",
      ""events:PutRule"",
      ""events:DescribeRule"",
      ""events:PutTargets"",
      ""events:RemoveTargets"",
    ]
    resources = [""arn:aws:events:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:rule/${var.prefix}-*""]
  }

  statement {
    actions = [
      ""states:DescribeExecution"",
      ""states:DescribeStateMachine"",
      ""states:GetExecutionHistory"",
    ]
    resources = [""*""]
  }

  statement {
    actions = [
      ""ec2:CreateNetworkInterface"",
      ""ec2:DescribeNetworkInterfaces"",
      ""ec2:DeleteNetworkInterface"",
    ]
    resources = [""*""]
  }
}
",data,64,,1da53282470313085da6e713a94458500df71f6c,275b9a88869f413f231765ff1784ed9b3c6e042b,https://github.com/nasa/cumulus/blob/1da53282470313085da6e713a94458500df71f6c/tf-modules/archive/iam.tf#L64,https://github.com/nasa/cumulus/blob/275b9a88869f413f231765ff1784ed9b3c6e042b/tf-modules/archive/iam.tf,2019-08-02 16:32:51-04:00,2019-08-26 10:31:45-04:00,3,1,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,540,modules/project/logging.tf,modules/project/logging.tf,0,# todo,# TODO(jccb): use a condition to limit writer-identity only to this,"# TODO(jccb): use a condition to limit writer-identity only to this 
 # bucket","resource ""google_project_iam_member"" ""bucket-sinks-binding"" {
  for_each = local.sink_bindings[""logging""]
  project  = split(""/"", each.value.destination)[1]
  role     = ""roles/logging.bucketWriter""
  member   = google_logging_project_sink.sink[each.key].writer_identity
  # TODO(jccb): use a condition to limit writer-identity only to this
  # bucket
}
",resource,"resource ""google_project_iam_member"" ""bucket-sinks-binding"" {
  for_each = local.sink_bindings[""logging""]
  project  = split(""/"", each.value.destination.target)[1]
  role     = ""roles/logging.bucketWriter""
  member   = google_logging_project_sink.sink[each.key].writer_identity

  condition {
    title       = ""${each.key} bucket writer""
    description = ""Grants bucketWriter to ${google_logging_project_sink.sink[each.key].writer_identity} used by log sink ${each.key} on ${local.project.project_id}""
    expression  = ""resource.name.endsWith('${each.value.destination.target}')""
  }
}
",resource,88,,9a533180a0cfd87493bce91f9c0211987382eeab,486d398c7d68ce1b0784a540feb7be5fb2c680c1,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9a533180a0cfd87493bce91f9c0211987382eeab/modules/project/logging.tf#L88,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/486d398c7d68ce1b0784a540feb7be5fb2c680c1/modules/project/logging.tf,2022-01-22 11:34:18+01:00,2022-11-11 19:22:05+01:00,4,1,0,1,0,1,0,0,0,0
https://github.com/compiler-explorer/infra,267,terraform/cloudfront_gpu.tf,terraform/cloudfront_gpu.tf,0,todo,// TODO delete me,// TODO delete me,"resource ""aws_cloudfront_distribution"" ""gpu-compiler-explorer-com"" {
  origin {
    domain_name = ""compiler-explorer.s3.amazonaws.com""
    origin_id   = ""S3-compiler-explorer""
  }
  origin {
    domain_name = aws_alb.GccExplorerApp.dns_name
    origin_id   = ""GccExplorerApp""
    custom_origin_config {
      http_port                = 1081
      https_port               = 1444
      origin_read_timeout      = 60
      origin_keepalive_timeout = 60
      origin_protocol_policy   = ""https-only""
      origin_ssl_protocols     = [
        ""TLSv1"",
        ""TLSv1.2"",
        ""TLSv1.1""
      ]
    }
  }

  enabled          = true
  is_ipv6_enabled  = true
  retain_on_delete = true
  aliases          = [
    ""gpu.compiler-explorer.com""
  ]

  viewer_certificate {
    acm_certificate_arn      = data.aws_acm_certificate.godbolt-org-et-al.arn
    ssl_support_method       = ""sni-only""
    minimum_protocol_version = ""TLSv1.1_2016""
  }

  logging_config {
    include_cookies = false
    bucket          = ""compiler-explorer-logs.s3.amazonaws.com""
    prefix          = ""cloudfront/""
  }

  http_version = ""http2""

  restrictions {
    geo_restriction {
      restriction_type = ""blacklist""
      locations        = [
        ""CU"",
        ""IR"",
        ""KP"",
        ""SD"",
        ""SY""
      ]
    }
  }

  default_cache_behavior {
    allowed_methods = [
      ""HEAD"",
      ""DELETE"",
      ""POST"",
      ""GET"",
      ""OPTIONS"",
      ""PUT"",
      ""PATCH""
    ]
    cached_methods = [
      ""HEAD"",
      ""GET""
    ]
    forwarded_values {
      cookies {
        forward = ""all""
      }
      query_string = true
      headers      = [
        ""Accept"",
        ""Host"",
        ""Authorization""
      ]
    }
    target_origin_id       = ""GccExplorerApp""
    viewer_protocol_policy = ""redirect-to-https""
    compress               = true
  }
}
",resource,,,1,0.0,85295c876b56c7417ea7917c51c0a20ddb9b0a07,7e0840cb5b4fdc5b0c79ab82b68ed66fcd522e01,https://github.com/compiler-explorer/infra/blob/85295c876b56c7417ea7917c51c0a20ddb9b0a07/terraform/cloudfront_gpu.tf#L1,https://github.com/compiler-explorer/infra/blob/7e0840cb5b4fdc5b0c79ab82b68ed66fcd522e01/terraform/cloudfront_gpu.tf#L0,2022-11-14 21:02:28-06:00,2022-11-15 07:39:11-06:00,2,2,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1838,modules/cloud-run-v2/variables-vpcconnector.tf,modules/cloud-run-v2/variables-vpcconnector.tf,0,workaround,# workaround for a wrong default in provider,"max = optional(number, 1000) # workaround for a wrong default in provider","variable ""vpc_connector_create"" {
  description = ""Populate this to create a Serverless VPC Access connector.""
  type = object({
    ip_cidr_range = optional(string)
    machine_type  = optional(string)
    name          = optional(string)
    network       = optional(string)
    instances = optional(object({
      max = optional(number)
      min = optional(number)
    }), {})
    throughput = optional(object({
      max = optional(number, 1000) # workaround for a wrong default in provider
      min = optional(number)
    }), {})
    subnet = optional(object({
      name       = optional(string)
      project_id = optional(string)
    }), {})
  })
  default = null
}
",variable,"variable ""vpc_connector_create"" {
  description = ""Populate this to create a Serverless VPC Access connector.""
  type = object({
    ip_cidr_range = optional(string)
    machine_type  = optional(string)
    name          = optional(string)
    network       = optional(string)
    instances = optional(object({
      max = optional(number)
      min = optional(number)
    }), {})
    throughput = optional(object({
      max = optional(number, 1000) # workaround for a wrong default in provider
      min = optional(number)
    }), {})
    subnet = optional(object({
      name       = optional(string)
      project_id = optional(string)
    }), {})
  })
  default = null
}
",variable,29,29.0,bee307256877ca5f44ba0829ee160f6341fb3ca4,bee307256877ca5f44ba0829ee160f6341fb3ca4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/bee307256877ca5f44ba0829ee160f6341fb3ca4/modules/cloud-run-v2/variables-vpcconnector.tf#L29,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/bee307256877ca5f44ba0829ee160f6341fb3ca4/modules/cloud-run-v2/variables-vpcconnector.tf#L29,2024-02-18 14:57:34+01:00,2024-02-18 14:57:34+01:00,1,0,0,0,1,0,1,0,0,0
https://github.com/rancherfederal/rke2-aws-tf,38,modules/nlb/main.tf,modules/nlb/main.tf,0,ill,# Handle case where target group/load balancer name exceeds 32 character limit without creating illegal names,# Handle case where target group/load balancer name exceeds 32 character limit without creating illegal names,"locals {
  # Handle case where target group/load balancer name exceeds 32 character limit without creating illegal names
  controlplane_name = ""${substr(var.name, 0, 23)}-rke2-cp""
  server_name       = ""${substr(var.name, 0, 18)}-rke2-server""
  supervisor_name   = ""${substr(var.name, 0, 15)}-rke2-supervisor""
}
",locals,"locals {
  # Handle case where target group/load balancer name exceeds 32 character limit without creating illegal names
  controlplane_name = ""${substr(var.name, 0, 23)}-rke2-cp""
  server_name       = ""${substr(var.name, 0, 18)}-rke2-server""
  supervisor_name   = ""${substr(var.name, 0, 15)}-rke2-supervisor""
}
",locals,2,2.0,feee0b4e4d0f1f0c368eef2e591dae2d5cf8b8fa,9849d3f890c60c9c88a29b31272a14c464c084ef,https://github.com/rancherfederal/rke2-aws-tf/blob/feee0b4e4d0f1f0c368eef2e591dae2d5cf8b8fa/modules/nlb/main.tf#L2,https://github.com/rancherfederal/rke2-aws-tf/blob/9849d3f890c60c9c88a29b31272a14c464c084ef/modules/nlb/main.tf#L2,2022-12-15 07:04:45-05:00,2023-12-18 13:59:14-06:00,3,0,0,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1984,modules/net-vpc/subnets.tf,modules/net-vpc/subnets.tf,0,# todo,# TODO(sruffilli): Provider 5.29.1 disabled reserved_internal_range because of a bug.,"# TODO(sruffilli): Provider 5.29.1 disabled reserved_internal_range because of a bug. 
 #                  Revert to the following once fixed. 
 # ip_cidr_range = ( 
 #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"") 
 #   ? null 
 #   : secondary_ip_range.value 
 # ) 
 # reserved_internal_range = ( 
 #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"") 
 #   ? secondary_ip_range.value 
 #   : null 
 # )","resource ""google_compute_subnetwork"" ""subnetwork"" {
  for_each      = local.subnets
  project       = var.project_id
  network       = local.network.name
  name          = each.value.name
  region        = each.value.region
  ip_cidr_range = each.value.ip_cidr_range
  description = (
    each.value.description == null
    ? ""Terraform-managed.""
    : each.value.description
  )
  private_ip_google_access = each.value.enable_private_access
  stack_type = (
    try(each.value.ipv6, null) != null ? ""IPV4_IPV6"" : null
  )
  ipv6_access_type = (
    try(each.value.ipv6, null) != null ? each.value.ipv6.access_type : null
  )
  # private_ipv6_google_access = try(each.value.ipv6.enable_private_access, null)
  dynamic ""secondary_ip_range"" {
    for_each = each.value.secondary_ip_ranges == null ? {} : each.value.secondary_ip_ranges
    content {
      range_name    = secondary_ip_range.key
      ip_cidr_range = secondary_ip_range.value
      # TODO(sruffilli): Provider 5.29.1 disabled reserved_internal_range because of a bug.
      #                  Revert to the following once fixed.
      # ip_cidr_range = (
      #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"")
      #   ? null
      #   : secondary_ip_range.value
      # )    
      # reserved_internal_range = (
      #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"")
      #   ? secondary_ip_range.value
      #   : null
      # )
    }
  }
  dynamic ""log_config"" {
    for_each = each.value.flow_logs_config != null ? [""""] : []
    content {
      aggregation_interval = each.value.flow_logs_config.aggregation_interval
      filter_expr          = each.value.flow_logs_config.filter_expression
      flow_sampling        = each.value.flow_logs_config.flow_sampling
      metadata             = each.value.flow_logs_config.metadata
      metadata_fields = (
        each.value.flow_logs_config.metadata == ""CUSTOM_METADATA""
        ? each.value.flow_logs_config.metadata_fields
        : null
      )
    }
  }
}
",resource,"resource ""google_compute_subnetwork"" ""subnetwork"" {
  for_each      = local.subnets
  project       = var.project_id
  network       = local.network.name
  name          = each.value.name
  region        = each.value.region
  ip_cidr_range = each.value.ip_cidr_range
  description = (
    each.value.description == null
    ? ""Terraform-managed.""
    : each.value.description
  )
  private_ip_google_access = each.value.enable_private_access
  stack_type = (
    try(each.value.ipv6, null) != null ? ""IPV4_IPV6"" : null
  )
  ipv6_access_type = (
    try(each.value.ipv6, null) != null ? each.value.ipv6.access_type : null
  )
  # private_ipv6_google_access = try(each.value.ipv6.enable_private_access, null)
  dynamic ""secondary_ip_range"" {
    for_each = each.value.secondary_ip_ranges == null ? {} : each.value.secondary_ip_ranges
    content {
      range_name    = secondary_ip_range.key
      ip_cidr_range = secondary_ip_range.value
      # TODO(sruffilli): Provider 5.29.1 disabled reserved_internal_range because of a bug.
      #                  Revert to the following once fixed.
      # ip_cidr_range = (
      #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"")
      #   ? null
      #   : secondary_ip_range.value
      # )    
      # reserved_internal_range = (
      #   startswith(secondary_ip_range.value, ""networkconnectivity.googleapis.com"")
      #   ? secondary_ip_range.value
      #   : null
      # )
    }
  }
  dynamic ""log_config"" {
    for_each = each.value.flow_logs_config != null ? [""""] : []
    content {
      aggregation_interval = each.value.flow_logs_config.aggregation_interval
      filter_expr          = each.value.flow_logs_config.filter_expression
      flow_sampling        = each.value.flow_logs_config.flow_sampling
      metadata             = each.value.flow_logs_config.metadata
      metadata_fields = (
        each.value.flow_logs_config.metadata == ""CUSTOM_METADATA""
        ? each.value.flow_logs_config.metadata_fields
        : null
      )
    }
  }
}
",resource,162,162.0,d3ffcc2b1cf2f2ffc3fc480b90719116dee59563,d3ffcc2b1cf2f2ffc3fc480b90719116dee59563,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d3ffcc2b1cf2f2ffc3fc480b90719116dee59563/modules/net-vpc/subnets.tf#L162,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d3ffcc2b1cf2f2ffc3fc480b90719116dee59563/modules/net-vpc/subnets.tf#L162,2024-05-15 05:46:18+00:00,2024-05-15 05:46:18+00:00,1,0,0,1,1,0,1,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,167,modules/workergroup/instancepools.tf,modules/workers/instancepools.tf,1,todo,# TODO Support w/ named secondary VNICs,"      vnic_selection = lookup(lb, ""vnic_selection"", ""PrimaryVnic"") # TODO Support w/ named secondary VNICs","resource ""oci_core_instance_pool"" ""instance_pools"" {
  # Create an OCI Instance Pool resource for each enabled entry of the worker_groups map with that mode.
  for_each                  = local.enabled_instance_pools
  compartment_id            = each.value.compartment_id
  display_name              = ""${each.value.label_prefix}-${each.key}""
  size                      = each.value.size
  instance_configuration_id = oci_core_instance_configuration.instance_configuration[each.key].id
  defined_tags              = merge(local.defined_tags, contains(keys(each.value), ""defined_tags"") ? each.value.defined_tags : {})
  freeform_tags             = merge(local.freeform_tags, contains(keys(each.value), ""freeform_tags"") ? each.value.freeform_tags : { worker_group = each.key })

  dynamic ""placement_configurations"" {
    # Define each configured availability domain for placement, with bounds on # available
    # Configured AD numbers e.g. [1,2,3] are converted into tenancy/compartment-specific names
    iterator = ad_number
    for_each = (contains(keys(each.value), ""placement_ads"")
      ? tolist(setintersection(each.value.placement_ads, local.ad_numbers))
      : local.ad_numbers
    )

    content {
      availability_domain = lookup(local.ad_number_to_name, ad_number.value, local.first_ad_name)
      primary_subnet_id   = each.value.subnet_id
    }
  }

  lifecycle {
    ignore_changes = [
      display_name, defined_tags, freeform_tags,
      placement_configurations,
    ]
  }

  dynamic ""load_balancers"" {
    # Associate the instance pool with 0+ load balancers for ingress traffic
    # TODO Accept full definition to create
    for_each = contains(keys(each.value), ""load_balancers"") ? each.value.load_balancers : {}

    content {
      # TODO From dynamic creation when no lb_id provided; introspected fields when present
      backend_set_name = lookup(lb, ""backend_set_name"", display_name)
      load_balancer_id = lookup(lb, ""lb_id"", lb_id)
      port             = lookup(lb, ""port"", 8080)

      // Possible values are ""PrimaryVnic"" or the displayName of
      // one of the secondary VNICs on the instance configuration
      // that is associated with the instance pool.
      vnic_selection = lookup(lb, ""vnic_selection"", ""PrimaryVnic"") # TODO Support w/ named secondary VNICs
    }
  }

  depends_on = [
    oci_core_instance_configuration.instance_configuration,
  ]
}",resource,the block associated got renamed or deleted,,51,,4d2b3f3d672a8f41655da3a7c58fded42c6858f3,f49f1da39d79cf260d80dcb10ee8e399828e6e1c,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/4d2b3f3d672a8f41655da3a7c58fded42c6858f3/modules/workergroup/instancepools.tf#L51,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/f49f1da39d79cf260d80dcb10ee8e399828e6e1c/modules/workers/instancepools.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,4,1,1,1,0,0,1,0,0,0
https://github.com/terraform-google-modules/terraform-google-kubernetes-engine,10,modules/safer-cluster/main.tf,modules/safer-cluster/main.tf,0,// todo,// TODO(mmontan): investigate whether this should be a recommended setting,// TODO(mmontan): investigate whether this should be a recommended setting,"module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // Disable the dashboard. It creates risk by running as a very sensitive user.
  kubernetes_dashboard = false

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy          = true
  network_policy_provider = ""CALICO""

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  disable_legacy_metadata_endpoints = true

  node_pools        = var.node_pools
  node_pools_labels = var.node_pools_labels

  // TODO(mmontan): check whether we need to restrict these
  // settings.
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  // TODO(mmontan): we generally considered applying
  // just the cloud-platofrm scope and use Cloud IAM
  // If we have Workload Identity, are there advantages
  // in restricting scopes even more?
  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  // We should use IP Alias.
  configure_ip_masq = false

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = length(var.compute_engine_service_account) > 0 ? false : true
  service_account        = var.compute_engine_service_account

  // TODO(mmontan): define a registry_project parameter in the private_beta_cluster,
  // so that we can give GCS permissions to the service account on a project
  // that hosts only container-images and not data.
  grant_registry_access = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // TODO(mmontan): link to a couple of policies.
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id
  node_metadata                    = ""SECURE""

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // TODO(mmontan): investigate whether this should be a recommended setting
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group
}
",module,"module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy = true

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  node_pools        = var.node_pools
  node_pools_labels = var.node_pools_labels

  // TODO(mmontan): check whether we need to restrict these
  // settings.
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = var.compute_engine_service_account == """" ? true : false
  service_account        = var.compute_engine_service_account
  registry_project_id    = var.registry_project_id
  grant_registry_access  = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // Example: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#example
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // Intranode Visibility enables you to capture flow logs for traffic between pods and create FW rules that apply to traffic between pods.
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group

  enable_shielded_nodes = var.enable_shielded_nodes
}
",module,157,,e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5,5a194719faa144ad0a7ee578663d336358f5073c,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5/modules/safer-cluster/main.tf#L157,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/5a194719faa144ad0a7ee578663d336358f5073c/modules/safer-cluster/main.tf,2019-10-04 15:21:33-07:00,2019-11-13 15:10:12-06:00,6,1,1,1,0,0,0,0,0,1
https://github.com/Azure/sap-automation,1117,deploy/terraform/run/sap_system/output.tf,deploy/terraform/run/sap_system/output.tf,0,#todo,#TODO Change to use Admin IP,) #TODO Change to use Admin IP,"output ""db_vm_ips"" {
  description = ""Database Virtual Machine IPs""
  value       =   upper(try(local.database.platform, ""HANA"")) == ""HANA"" ? (
    module.hdb_node.db_ip) : (
    module.anydb_node.anydb_db_ip
  ) #TODO Change to use Admin IP

}
",output,"output ""db_vm_ips""                     {
                                         description = ""Database Virtual Machine IPs""
                                         value = upper(try(local.database.platform, ""HANA"")) == ""HANA"" ? (
                                                   module.hdb_node.database_server_ips) : (
                                                   module.anydb_node.database_server_ips
                                                 ) #TODO Change to use Admin IP
                                       }
",output,148,163.0,ee9ec96d7068dfd9bb390253f31394ff30652eb8,db20ac2a47d9d00329385330cb4af6b3c726c400,https://github.com/Azure/sap-automation/blob/ee9ec96d7068dfd9bb390253f31394ff30652eb8/deploy/terraform/run/sap_system/output.tf#L148,https://github.com/Azure/sap-automation/blob/db20ac2a47d9d00329385330cb4af6b3c726c400/deploy/terraform/run/sap_system/output.tf#L163,2023-02-15 12:47:00+02:00,2024-03-11 23:15:11+05:30,24,0,1,1,0,0,1,0,0,0
https://github.com/terraform-google-modules/terraform-google-iam,19,test/fixtures/additive/outputs.tf,test/fixtures/additive/outputs.tf,0,# todo,"# TODO: This has to be pure integer, but InSpec attributes don't seem","# TODO: This has to be pure integer, but InSpec attributes don't seem 
 #       to support neither of the types number, integer, int, float, double","output ""roles"" {
  # TODO: This has to be pure integer, but InSpec attributes don't seem
  #       to support neither of the types number, integer, int, float, double
  value       = tostring(var.roles)
  description = ""Amount of roles assigned. Useful for testing how the module behaves on updates.""
}
",output,"output ""roles"" {
  # workaround InSpec lack of support for integer
  value       = tostring(var.roles)
  description = ""Amount of roles assigned. Useful for testing how the module behaves on updates.""
}
",output,119,,40c4014c903788118f39c6decda3f02858721890,2529fe6173f7540994a346066d5ab141144861e9,https://github.com/terraform-google-modules/terraform-google-iam/blob/40c4014c903788118f39c6decda3f02858721890/test/fixtures/additive/outputs.tf#L119,https://github.com/terraform-google-modules/terraform-google-iam/blob/2529fe6173f7540994a346066d5ab141144861e9/test/fixtures/additive/outputs.tf,2019-10-21 15:23:47+03:00,2019-10-23 19:31:30+03:00,2,1,0,1,0,0,0,0,0,1
https://github.com/SUSE/ha-sap-terraform-deployments,155,libvirt/terraform/modules/monitoring/variables.tf,libvirt/terraform/modules/monitoring/variables.tf,0,// todo,// TODO: verify if it is needed,"// TODO: verify if it is needed 
 // variable ""server_configuration"" { 
 //   description = ""use ${module.<SERVER_NAME>.configuration}, see the main.tf example file"" 
 //  type = ""map"" 
 //} ","variable ""public_key_location"" {
  description = ""path of additional pub ssh key you want to use to access VMs""
  default     = ""/dev/null""

  # HACK: """" cannot be used as a default because of https://github.com/hashicorp/hil/issues/50
}
",variable,"variable ""public_key_location"" {
  description = ""path of additional pub ssh key you want to use to access VMs""
  default     = ""/dev/null""

  # HACK: """" cannot be used as a default because of https://github.com/hashicorp/hil/issues/50
}
",variable,52,,ded573f03083c78a15ca8c85606954edfc0c5ad5,d32b0c93370b96b65c4de6378081d2af5a1b56bf,https://github.com/SUSE/ha-sap-terraform-deployments/blob/ded573f03083c78a15ca8c85606954edfc0c5ad5/libvirt/terraform/modules/monitoring/variables.tf#L52,https://github.com/SUSE/ha-sap-terraform-deployments/blob/d32b0c93370b96b65c4de6378081d2af5a1b56bf/libvirt/terraform/modules/monitoring/variables.tf,2019-07-22 11:53:34+02:00,2019-07-23 12:37:23+02:00,3,1,1,1,0,0,0,0,0,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,11,master.tf,master.tf,0,hack,"/*# Run the first control plane   provisioner ""remote-exec"" {     inline = [       # set the hostname in a persistent fashion       ""hostnamectl set-hostname ${self.name}"",       # first we disable automatic reboot (after transactional updates), and configure the reboot method as kured       ""rebootmgrctl set-strategy off && echo 'REBOOT_METHOD=kured' > /etc/transactional-update.conf"",       # prepare a directory for our post-installation kustomizations       ""mkdir -p /tmp/post_install"",       # then we initiate the cluster       ""systemctl enable k3s-server"",       # wait for k3s to get ready       <<-EOT       timeout 120 bash <<EOF         until systemctl status k3s-server > /dev/null; do           systemctl start k3s-server           echo ""Initiating the cluster...""           sleep 1         done         until [ -e /etc/rancher/k3s/k3s.yaml ]; do           echo ""Waiting for kubectl config...""           sleep 1         done         until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do           echo ""Waiting for the cluster to become ready...""           sleep 1         done       EOF       EOT     ]   }    # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.   provisioner ""file"" {     content = yamlencode({       apiVersion = ""kustomize.config.k8s.io/v1beta1""       kind       = ""Kustomization""       resources = [         ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",         ""https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml"",         ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",         ""./traefik.yaml""       ]       patchesStrategicMerge = [         file(""${path.module}/patches/kured.yaml""),         file(""${path.module}/patches/ccm.yaml"")       ]     })     destination = ""/tmp/post_install/kustomization.yaml""   }    # Upload traefik config   provisioner ""file"" {     content = templatefile(       ""${path.module}/templates/traefik_config.yaml.tpl"",       {         lb_disable_ipv6    = var.lb_disable_ipv6         lb_server_type     = var.lb_server_type         location           = var.location         traefik_acme_tls   = var.traefik_acme_tls         traefik_acme_email = var.traefik_acme_email     })     destination = ""/tmp/post_install/traefik.yaml""   }    # Deploy secrets, logging is automatically disabled due to sensitive variables   provisioner ""remote-exec"" {     inline = [       ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name}"",       ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token}"",     ]   }    # Deploy our post-installation kustomization   provisioner ""remote-exec"" {     inline = [       # This ugly hack is here, because terraform serializes the       # embedded yaml files with ""- |2"", when there is more than       # one yamldocument in the embedded file. Kustomize does not understand       # that syntax and tries to parse the blocks content as a file, resulting       # in weird errors. so gnu sed with funny escaping is used to       # replace lines like ""- |3"" by ""- |"" (yaml block syntax).       # due to indendation this should not changes the embedded       # manifests themselves       ""sed -i 's/^- |[0-9]\\+$/- |/g' /tmp/post_install/kustomization.yaml"",       ""kubectl apply -k /tmp/post_install"",     ]   }*/","/*# Run the first control plane   provisioner ""remote-exec"" {     inline = [       # set the hostname in a persistent fashion       ""hostnamectl set-hostname ${self.name}"",       # first we disable automatic reboot (after transactional updates), and configure the reboot method as kured       ""rebootmgrctl set-strategy off && echo 'REBOOT_METHOD=kured' > /etc/transactional-update.conf"",       # prepare a directory for our post-installation kustomizations       ""mkdir -p /tmp/post_install"",       # then we initiate the cluster       ""systemctl enable k3s-server"",       # wait for k3s to get ready       <<-EOT       timeout 120 bash <<EOF         until systemctl status k3s-server > /dev/null; do           systemctl start k3s-server           echo ""Initiating the cluster...""           sleep 1         done         until [ -e /etc/rancher/k3s/k3s.yaml ]; do           echo ""Waiting for kubectl config...""           sleep 1         done         until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do           echo ""Waiting for the cluster to become ready...""           sleep 1         done       EOF       EOT     ]   }    # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.   provisioner ""file"" {     content = yamlencode({       apiVersion = ""kustomize.config.k8s.io/v1beta1""       kind       = ""Kustomization""       resources = [         ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",         ""https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml"",         ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",         ""./traefik.yaml""       ]       patchesStrategicMerge = [         file(""${path.module}/patches/kured.yaml""),         file(""${path.module}/patches/ccm.yaml"")       ]     })     destination = ""/tmp/post_install/kustomization.yaml""   }    # Upload traefik config   provisioner ""file"" {     content = templatefile(       ""${path.module}/templates/traefik_config.yaml.tpl"",       {         lb_disable_ipv6    = var.lb_disable_ipv6         lb_server_type     = var.lb_server_type         location           = var.location         traefik_acme_tls   = var.traefik_acme_tls         traefik_acme_email = var.traefik_acme_email     })     destination = ""/tmp/post_install/traefik.yaml""   }    # Deploy secrets, logging is automatically disabled due to sensitive variables   provisioner ""remote-exec"" {     inline = [       ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name}"",       ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token}"",     ]   }    # Deploy our post-installation kustomization   provisioner ""remote-exec"" {     inline = [       # This ugly hack is here, because terraform serializes the       # embedded yaml files with ""- |2"", when there is more than       # one yamldocument in the embedded file. Kustomize does not understand       # that syntax and tries to parse the blocks content as a file, resulting       # in weird errors. so gnu sed with funny escaping is used to       # replace lines like ""- |3"" by ""- |"" (yaml block syntax).       # due to indendation this should not changes the embedded       # manifests themselves       ""sed -i 's/^- |[0-9]\\+$/- |/g' /tmp/post_install/kustomization.yaml"",       ""kubectl apply -k /tmp/post_install"",     ]   }*/","resource ""hcloud_server"" ""first_control_plane"" {
  name = ""k3s-control-plane-0""

  image              = data.hcloud_image.linux.name
  rescue             = ""linux64""
  server_type        = var.control_plane_server_type
  location           = var.location
  ssh_keys           = [hcloud_ssh_key.k3s.id]
  firewall_ids       = [hcloud_firewall.k3s.id]
  placement_group_id = hcloud_placement_group.k3s.id

  labels = {
    ""provisioner"" = ""terraform"",
    ""engine""      = ""k3s""
  }

  connection {
    user           = ""root""
    private_key    = local.ssh_private_key
    agent_identity = local.ssh_identity
    host           = self.ipv4_address
  }

  provisioner ""file"" {
    content = templatefile(""${path.module}/templates/config.ign.tpl"", {
      name           = self.name
      ssh_public_key = local.ssh_public_key
    })
    destination = ""/root/config.ign""
  }

  # Install MicroOS
  provisioner ""remote-exec"" {
    inline = local.MicroOS_install_commands
  }

  # Issue a reboot command
  provisioner ""local-exec"" {
    command = ""ssh ${local.ssh_args} root@${self.ipv4_address} '(sleep 2; reboot)&'; sleep 3""
  }

  # Wait for MicroOS to reboot and be ready
  provisioner ""local-exec"" {
    command = <<-EOT
      until ssh ${local.ssh_args} -o ConnectTimeout=2 root@${self.ipv4_address} true 2> /dev/null
      do
        echo ""Waiting for MicroOS to reboot and become available...""
        sleep 2
      done
    EOT
  }

  # Generating k3s master config file
  provisioner ""file"" {
    content = yamlencode({
      node-name                = self.name
      cluster-init             = true
      disable-cloud-controller = true
      disable                  = [""servicelb"", ""local-storage""]
      flannel-iface            = ""eth1""
      kubelet-arg              = ""cloud-provider=external""
      node-ip                  = local.first_control_plane_network_ip
      advertise-address        = local.first_control_plane_network_ip
      token                    = random_password.k3s_token.result
      node-taint               = var.allow_scheduling_on_control_plane ? [] : [""node-role.kubernetes.io/master:NoSchedule""]
    })
    destination = ""/etc/rancher/k3s/config.yaml""
  }

  /*  # Run the first control plane
  provisioner ""remote-exec"" {
    inline = [
      # set the hostname in a persistent fashion
      ""hostnamectl set-hostname ${self.name}"",
      # first we disable automatic reboot (after transactional updates), and configure the reboot method as kured
      ""rebootmgrctl set-strategy off && echo 'REBOOT_METHOD=kured' > /etc/transactional-update.conf"",
      # prepare a directory for our post-installation kustomizations
      ""mkdir -p /tmp/post_install"",
      # then we initiate the cluster
      ""systemctl enable k3s-server"",
      # wait for k3s to get ready
      <<-EOT
      timeout 120 bash <<EOF
        until systemctl status k3s-server > /dev/null; do
          systemctl start k3s-server
          echo ""Initiating the cluster...""
          sleep 1
        done
        until [ -e /etc/rancher/k3s/k3s.yaml ]; do
          echo ""Waiting for kubectl config...""
          sleep 1
        done
        until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do
          echo ""Waiting for the cluster to become ready...""
          sleep 1
        done
      EOF
      EOT
    ]
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content = yamlencode({
      apiVersion = ""kustomize.config.k8s.io/v1beta1""
      kind       = ""Kustomization""
      resources = [
        ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",
        ""https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml"",
        ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",
        ""./traefik.yaml""
      ]
      patchesStrategicMerge = [
        file(""${path.module}/patches/kured.yaml""),
        file(""${path.module}/patches/ccm.yaml"")
      ]
    })
    destination = ""/tmp/post_install/kustomization.yaml""
  }

  # Upload traefik config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/traefik_config.yaml.tpl"",
      {
        lb_disable_ipv6    = var.lb_disable_ipv6
        lb_server_type     = var.lb_server_type
        location           = var.location
        traefik_acme_tls   = var.traefik_acme_tls
        traefik_acme_email = var.traefik_acme_email
    })
    destination = ""/tmp/post_install/traefik.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name}"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token}"",
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = [
      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /tmp/post_install/kustomization.yaml"",
      ""kubectl apply -k /tmp/post_install"",
    ]
  } */

  network {
    network_id = hcloud_network.k3s.id
    ip         = local.first_control_plane_network_ip
  }

  depends_on = [
    hcloud_network_subnet.k3s,
    hcloud_firewall.k3s
  ]
}
",resource,"resource ""hcloud_server"" ""first_control_plane"" {
  name = ""k3s-control-plane-0""

  image              = data.hcloud_image.linux.name
  rescue             = ""linux64""
  server_type        = var.control_plane_server_type
  location           = var.location
  ssh_keys           = [hcloud_ssh_key.k3s.id]
  firewall_ids       = [hcloud_firewall.k3s.id]
  placement_group_id = hcloud_placement_group.k3s.id

  labels = {
    ""provisioner"" = ""terraform"",
    ""engine""      = ""k3s""
  }

  connection {
    user           = ""root""
    private_key    = local.ssh_private_key
    agent_identity = local.ssh_identity
    host           = self.ipv4_address
  }

  provisioner ""file"" {
    content = templatefile(""${path.module}/templates/config.ign.tpl"", {
      name           = self.name
      ssh_public_key = local.ssh_public_key
    })
    destination = ""/root/config.ign""
  }

  # Install MicroOS
  provisioner ""remote-exec"" {
    inline = local.microOS_install_commands
  }

  # Issue a reboot command and wait for the node to reboot
  provisioner ""local-exec"" {
    command = ""ssh ${local.ssh_args} root@${self.ipv4_address} '(sleep 2; reboot)&'; sleep 3""
  }
  provisioner ""local-exec"" {
    command = <<-EOT
      until ssh ${local.ssh_args} -o ConnectTimeout=2 root@${self.ipv4_address} true 2> /dev/null
      do
        echo ""Waiting for MicroOS to reboot and become available...""
        sleep 2
      done
    EOT
  }

  # Generating k3s master config file
  provisioner ""file"" {
    content = yamlencode({
      node-name                = self.name
      cluster-init             = true
      disable-cloud-controller = true
      disable                  = [""servicelb"", ""local-storage""]
      flannel-iface            = ""eth1""
      kubelet-arg              = ""cloud-provider=external""
      node-ip                  = local.first_control_plane_network_ip
      advertise-address        = local.first_control_plane_network_ip
      token                    = random_password.k3s_token.result
      node-taint               = var.allow_scheduling_on_control_plane ? [] : [""node-role.kubernetes.io/master:NoSchedule""]
      node-label               = var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : []
    })
    destination = ""/tmp/config.yaml""
  }



  # Install k3s server
  provisioner ""remote-exec"" {
    inline = local.install_k3s_server
  }

  # Issue a reboot command and wait for the node to reboot
  provisioner ""local-exec"" {
    command = ""ssh ${local.ssh_args} root@${self.ipv4_address} '(sleep 2; reboot)&'; sleep 3""
  }
  provisioner ""local-exec"" {
    command = <<-EOT
      until ssh ${local.ssh_args} -o ConnectTimeout=2 root@${self.ipv4_address} true 2> /dev/null
      do
        echo ""Waiting for MicroOS to reboot and become available...""
        sleep 2
      done
    EOT
  }

  # Upon reboot verify that the k3s server is starts, and wait for k3s to be ready to receive commands
  provisioner ""remote-exec"" {
    inline = [
      # prepare the post_install directory
      ""mkdir -p /tmp/post_install"",
      # wait for k3s to become ready
      <<-EOT
      timeout 120 bash <<EOF
        until systemctl status k3s > /dev/null; do
          echo ""Waiting for the k3s server to start...""
          sleep 1
        done
        until [ -e /etc/rancher/k3s/k3s.yaml ]; do
          echo ""Waiting for kubectl config...""
          sleep 1
        done
        until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do
          echo ""Waiting for the cluster to become ready...""
          sleep 1
        done
      EOF
      EOT
    ]
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content = yamlencode({
      apiVersion = ""kustomize.config.k8s.io/v1beta1""
      kind       = ""Kustomization""
      resources = [
        ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",
        ""https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml"",
        ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",
        ""https://raw.githubusercontent.com/rancher/system-upgrade-controller/master/manifests/system-upgrade-controller.yaml"",
        ""./traefik.yaml"",
      ]
      patchesStrategicMerge = [
        file(""${path.module}/patches/kured.yaml""),
        file(""${path.module}/patches/ccm.yaml"")
      ]
    })
    destination = ""/tmp/post_install/kustomization.yaml""
  }

  # Upload traefik config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/traefik_config.yaml.tpl"",
      {
        lb_disable_ipv6    = var.lb_disable_ipv6
        lb_server_type     = var.lb_server_type
        location           = var.location
        traefik_acme_tls   = var.traefik_acme_tls
        traefik_acme_email = var.traefik_acme_email
    })
    destination = ""/tmp/post_install/traefik.yaml""
  }

  # Upload the system upgrade controller plans config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/plans.yaml.tpl"",
      {
        channel = var.k3s_upgrade_channel
    })
    destination = ""/tmp/post_install/plans.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""set -ex"",
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name}"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token}"",
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = [
      ""set -ex"",
      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /tmp/post_install/kustomization.yaml"",
      ""kubectl apply -k /tmp/post_install"",
      ""echo 'Waiting for the system-upgrade-controller deployment to become available...' && kubectl -n system-upgrade wait --for=condition=available --timeout=300s deployment/system-upgrade-controller"",
      ""kubectl apply -f /tmp/post_install/plans.yaml""
    ]
  }

  network {
    network_id = hcloud_network.k3s.id
    ip         = local.first_control_plane_network_ip
  }

  depends_on = [
    hcloud_network_subnet.k3s,
    hcloud_firewall.k3s
  ]
}
",resource,70,,1f0c825b234b5cf5a8b2a081c1b1cc9b7ebedd3d,fec695086ad9edc4d9bf53d7c1d06b27945f6962,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/1f0c825b234b5cf5a8b2a081c1b1cc9b7ebedd3d/master.tf#L70,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/fec695086ad9edc4d9bf53d7c1d06b27945f6962/master.tf,2022-02-16 00:13:02+01:00,2022-02-16 03:18:40+01:00,2,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,474,tests/modules/net_xlb/fixture/variables.tf,tests/modules/net_glb/fixture/variables.tf,1,hack,"# Will lookup for ids in health_chacks_config first,","# Optional health check ids for backend service groups. 
 # Will lookup for ids in health_chacks_config first, 
 # then will use the id as is. If no ids are defined 
 # at all (null, []) health_checks_config_defaults is used","variable ""backend_services_config"" {
  description = ""The backends services configuration.""
  type = map(object({
    enable_cdn = bool

    cdn_config = object({
      cache_mode                   = string
      client_ttl                   = number
      default_ttl                  = number
      max_ttl                      = number
      negative_caching             = bool
      negative_caching_policy      = map(number)
      serve_while_stale            = bool
      signed_url_cache_max_age_sec = string
    })

    bucket_config = object({
      bucket_name = string
      options = object({
        custom_response_headers = list(string)
      })
    })

    group_config = object({
      backends = list(object({
        group = string # IG or NEG FQDN address
        options = object({
          balancing_mode               = string # Can be UTILIZATION, RATE, CONNECTION
          capacity_scaler              = number # Valid range is [0.0,1.0]
          max_connections              = number
          max_connections_per_instance = number
          max_connections_per_endpoint = number
          max_rate                     = number
          max_rate_per_instance        = number
          max_rate_per_endpoint        = number
          max_utilization              = number
        })
      }))

      # Optional health check ids for backend service groups.
      # Will lookup for ids in health_chacks_config first,
      # then will use the id as is. If no ids are defined
      # at all (null, []) health_checks_config_defaults is used
      health_checks = list(string)

      log_config = object({
        enable      = bool
        sample_rate = number # must be in [0, 1]
      })

      options = object({
        affinity_cookie_ttl_sec         = number
        custom_request_headers          = list(string)
        custom_response_headers         = list(string)
        connection_draining_timeout_sec = number
        load_balancing_scheme           = string # only EXTERNAL (default) makes sense here
        locality_lb_policy              = string
        port_name                       = string
        protocol                        = string
        security_policy                 = string
        session_affinity                = string
        timeout_sec                     = number

        circuits_breakers = object({
          max_requests_per_connection = number # Set to 1 to disable keep-alive
          max_connections             = number # Defaults to 1024
          max_pending_requests        = number # Defaults to 1024
          max_requests                = number # Defaults to 1024
          max_retries                 = number # Defaults to 3
        })

        consistent_hash = object({
          http_header_name  = string
          minimum_ring_size = string
          http_cookie = object({
            name = string
            path = string
            ttl = object({
              seconds = number
              nanos   = number
            })
          })
        })

        iap = object({
          oauth2_client_id            = string
          oauth2_client_secret        = string
          oauth2_client_secret_sha256 = string
        })
      })
    })
  }))
  default = {}
}
",variable,,,86,0.0,ca82d5157a8c51690567ad2740cc84092f385764,46f694be082aa5c42e39094143acadbc1f2181be,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/ca82d5157a8c51690567ad2740cc84092f385764/tests/modules/net_xlb/fixture/variables.tf#L86,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/46f694be082aa5c42e39094143acadbc1f2181be/tests/modules/net_glb/fixture/variables.tf#L0,2022-01-14 16:05:10+01:00,2022-12-08 17:35:44+01:00,3,2,0,1,0,0,0,0,0,0
https://github.com/nebari-dev/nebari,12,qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf,qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf,0,fix,# FIXME: Make this an actual schema instead of this dummy schema that,"# FIXME: Make this an actual schema instead of this dummy schema that 
 #        is a workaround to meet the requirement of having a schema.","resource ""kubernetes_manifest"" ""main"" {
  manifest = {
    apiVersion = ""apiextensions.k8s.io/v1""
    kind       = ""CustomResourceDefinition""
    metadata = {
      name = ""daskclusters.gateway.dask.org""
    }
    spec = {
      group   = ""gateway.dask.org""
      names = {
        kind     = ""DaskCluster""
        listKind = ""DaskClusterList""
        plural   = ""daskclusters""
        singular = ""daskcluster""
      }
      scope = ""Namespaced""
      versions = [{
        name = ""v1alpha1""
        served = true
        storage = true
        subresources = {
          status = {}
        }

        # NOTE: While we define a schema, it is a dummy schema that doesn't
        #       validate anything. We just have it to comply with the schema of
        #       a CustomResourceDefinition that requires it.
        #
        #       A decision has been made to not implement an actual schema at
        #       this point in time due to the additional maintenance work it
        #       would require.
        #
        #       Reference: https://github.com/dask/dask-gateway/issues/434
        #
        schema = {
          openAPIV3Schema = {
            type = ""object""
            # FIXME: Make this an actual schema instead of this dummy schema that
            #        is a workaround to meet the requirement of having a schema.
            x-kubernetes-preserve-unknown-fields = true
          }
        }
      }]
    }
  }
}
",resource,"resource ""kubernetes_manifest"" ""main"" {
  manifest = {
    apiVersion = ""apiextensions.k8s.io/v1""
    kind       = ""CustomResourceDefinition""
    metadata = {
      name = ""daskclusters.gateway.dask.org""
    }
    spec = {
      group = ""gateway.dask.org""
      names = {
        kind     = ""DaskCluster""
        listKind = ""DaskClusterList""
        plural   = ""daskclusters""
        singular = ""daskcluster""
      }
      scope = ""Namespaced""
      versions = [{
        name    = ""v1alpha1""
        served  = true
        storage = true
        subresources = {
          status = {}
        }

        # NOTE: While we define a schema, it is a dummy schema that doesn't
        #       validate anything. We just have it to comply with the schema of
        #       a CustomResourceDefinition that requires it.
        #
        #       A decision has been made to not implement an actual schema at
        #       this point in time due to the additional maintenance work it
        #       would require.
        #
        #       Reference: https://github.com/dask/dask-gateway/issues/434
        #
        schema = {
          openAPIV3Schema = {
            type = ""object""
            # FIXME: Make this an actual schema instead of this dummy schema that
            #        is a workaround to meet the requirement of having a schema.
            x-kubernetes-preserve-unknown-fields = true
          }
        }
      }]
    }
  }
}
",resource,38,38.0,e65621ed9fc3e374626cc3929742df6ba94fc8d7,d0cc26638fbc9e69aa736105ffd61cfb50d561d6,https://github.com/nebari-dev/nebari/blob/e65621ed9fc3e374626cc3929742df6ba94fc8d7/qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf#L38,https://github.com/nebari-dev/nebari/blob/d0cc26638fbc9e69aa736105ffd61cfb50d561d6/qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf#L38,2022-02-03 11:12:40-05:00,2022-05-26 14:22:33-07:00,2,0,1,1,0,0,0,0,0,0
https://github.com/nasa/cumulus,268,lambdas/data-migration/main.tf,lambdas/data-migration1/main.tf,1,// todo,// TODO: should we be creating our own security group here?,// TODO: should we be creating our own security group here?,"resource ""aws_lambda_function"" ""data_migration"" {
  function_name    = ""${var.prefix}-data-migration""
  filename         = local.lambda_path
  source_code_hash = filebase64sha256(local.lambda_path)
  handler          = ""index.handler""
  role             = aws_iam_role.data_migration.arn
  runtime          = ""nodejs12.x""
  timeout          = 60
  memory_size      = 128

  environment {
    variables = {
      PG_HOST          = var.pg_host
      PG_USER          = var.pg_user
      PG_PASSWORD      = var.pg_password
      PG_DATABASE      = var.pg_database
      CollectionsTable = var.dynamo_tables.collections.name
    }
  }

  dynamic ""vpc_config"" {
    for_each = length(var.subnet_ids) == 0 ? [] : [1]
    content {
      subnet_ids = var.subnet_ids
      // TODO: should we be creating our own security group here?
      security_group_ids = [
        aws_security_group.data_migration[0].id
      ]
    }
  }

  tags = var.tags
}
",resource,the block associated got renamed or deleted,,93,,8480e4969011afe1186f85d493806bcf48ea9698,6b78c090ce1b3288d967bd8f225319b8b5d37dc2,https://github.com/nasa/cumulus/blob/8480e4969011afe1186f85d493806bcf48ea9698/lambdas/data-migration/main.tf#L93,https://github.com/nasa/cumulus/blob/6b78c090ce1b3288d967bd8f225319b8b5d37dc2/lambdas/data-migration1/main.tf,2020-08-21 17:47:34-04:00,2020-08-28 17:34:02-04:00,4,1,1,1,0,1,1,0,0,0
https://github.com/Worklytics/psoxy,640,infra/examples-dev/aws-google-workspace/main.tf,infra/examples-dev/aws-google-workspace/main.tf,0,# todo,"## TODO: requires targeted apply to create key first, bc value of key_id determines map content","## TODO: requires targeted apply to create key first, bc value of key_id determines map content 
 ## in example 
 #resource ""aws_kms_key"" ""key"" { 
 #  description             = ""KMS key for Psoxy"" 
 #  enable_key_rotation     = true 
 #  is_enabled              = true 
 #}  
 # if you generated these, you may want them to import back into your data warehouse","output ""lookup_tables"" {
  value = module.psoxy-aws-google-workspace.lookup_tables
}
",output,"output ""lookup_tables"" {
  value = module.psoxy.lookup_tables
}
",output,87,,3c69b9a75c1ba840ddd15b603df9817b1c425e5b,de496ca8bf20117dbb31db7021b07d22cb4579c3,https://github.com/Worklytics/psoxy/blob/3c69b9a75c1ba840ddd15b603df9817b1c425e5b/infra/examples-dev/aws-google-workspace/main.tf#L87,https://github.com/Worklytics/psoxy/blob/de496ca8bf20117dbb31db7021b07d22cb4579c3/infra/examples-dev/aws-google-workspace/main.tf,2023-01-23 09:56:38-08:00,2023-06-28 10:02:05-07:00,31,1,1,1,0,1,0,0,0,0
https://github.com/pingcap/tidb-operator,5,deploy/alicloud/ack/main.tf,deploy/aliyun/ack/main.tf,1,fix,"# FIXME: currently update vswitch_ids will force will recreate, allow updating when upstream support in-place","# FIXME: currently update vswitch_ids will force will recreate, allow updating when upstream support in-place 
 # vswitch id update","resource ""alicloud_ess_scaling_group"" ""workers"" {
  count              = ""${length(var.worker_groups)}""
  scaling_group_name = ""${alicloud_cs_managed_kubernetes.k8s.name}-${lookup(var.worker_groups[count.index], ""name"", count.index)}""
  vswitch_ids        = [""${split("","", var.vpc_id != """" ? join("","", data.template_file.vswitch_id.*.rendered) : join("","", alicloud_vswitch.all.*.id))}""]
  min_size           = ""${lookup(var.worker_groups[count.index], ""min_size"", var.group_default[""min_size""])}""
  max_size           = ""${lookup(var.worker_groups[count.index], ""max_size"", var.group_default[""max_size""])}""
  default_cooldown   = ""${lookup(var.worker_groups[count.index], ""default_cooldown"", var.group_default[""default_cooldown""])}""
  multi_az_policy    = ""${lookup(var.worker_groups[count.index], ""multi_az_policy"", var.group_default[""multi_az_policy""])}""

  # Remove the newest instance in the oldest scaling configuration
  removal_policies = [
    ""OldestScalingConfiguration"",
    ""NewestInstance""
  ]

  lifecycle {
    # FIXME: currently update vswitch_ids will force will recreate, allow updating when upstream support in-place
    # vswitch id update
    ignore_changes = [""vswitch_ids""]

    create_before_destroy = true
  }
}
",resource,,,104,0.0,eebd686956c0b2adf67a10e1a376659c95c571a3,042b1a97fbbdf342297002990564828e6644a3f0,https://github.com/pingcap/tidb-operator/blob/eebd686956c0b2adf67a10e1a376659c95c571a3/deploy/alicloud/ack/main.tf#L104,https://github.com/pingcap/tidb-operator/blob/042b1a97fbbdf342297002990564828e6644a3f0/deploy/aliyun/ack/main.tf#L0,2019-05-06 19:59:43+08:00,2019-07-23 19:44:58+08:00,3,2,1,1,0,0,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,204,outputs.tf,outputs.tf,0,todo,"//"", module.aws_eks.cluster_oidc_issuer_url)[1], ""EKS Cluster not enabled"") # TODO - remove `split()` since `oidc_provider` coverss https:// removal","value       = try(split(""//"", module.aws_eks.cluster_oidc_issuer_url)[1], ""EKS Cluster not enabled"") # TODO - remove `split()` since `oidc_provider` coverss https:// removal","output ""eks_oidc_issuer_url"" {
  description = ""The URL on the EKS cluster OIDC Issuer""
  value       = try(split(""//"", module.aws_eks.cluster_oidc_issuer_url)[1], ""EKS Cluster not enabled"") # TODO - remove `split()` since `oidc_provider` coverss https:// removal
}
",output,,,21,0.0,deec7d5caea47c06dea48fa616ad2c56e52c3cce,19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/deec7d5caea47c06dea48fa616ad2c56e52c3cce/outputs.tf#L21,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1/outputs.tf#L0,2022-04-29 14:37:13-07:00,2023-06-05 10:07:47-04:00,11,2,1,1,0,0,0,0,0,0
https://github.com/aws-observability/terraform-aws-observability-accelerator,1,examples/workloads.tf,examples/workloads.tf,0,fix,"#-- true doesn't work for me, needs fix","  enable_opentelemetry_operator = false #-- true doesn't work for me, needs fix","module ""eks_observability_accelerator"" {
  #source = ""aws-ia/terrarom-aws-observability-accelerator""
  source = ""../""

  aws_region     = var.aws_region
  eks_cluster_id = var.eks_cluster_id

  # deploys AWS Distro for OpenTelemetry operator into the cluster
  enable_amazon_eks_adot = false

  # reusing existing certificate manager? defaults to true
  enable_cert_manager = false

  # # -- or enable opentelemetry operator
  enable_opentelemetry_operator = false #-- true doesn't work for me, needs fix
  # open_telemetry_operator_config = map() // custom config

  # creates a new AMP workspace, defaults to true
  create_managed_prometheus_workspace = false

  # reusing existing AMP -- needs data source for alerting rules
  managed_prometheus_id       = var.managed_prometheus_id
  managed_prometheus_endpoint = var.managed_prometheus_endpoint
  managed_prometheus_region   = var.managed_prometheus_region


  enable_java = true

  # enable_haproxy = true
  # haproxy_config = {
  #   amp_endpoint     = module / amp.endpoint
  #   grafana_endpoint = module.grafana.endpoint
  # }


  # java_config = {
  #   amp_endpoint     = """"
  #   grafana_endpoint = """"
  # }




  # # -- or use an existing one
  # # seems like https://github.com/terraform-aws-modules/terraform-aws-managed-service-prometheus
  # # supports importing
  # amp_workspace_alias = var.amp_alias

  # # enable rules and alerts
  # enable_alert_manager = true

  # # -- or provide custom alerts definition
  # prometheus_custom_alert_rule = var.prometheus_custom_alert_rule


  # # create grafana workspace, and customer to deal with authentication later
  # create_managed_grafana_workspace = true
  # grafana_auth_provider            = var.grafana_auth_provider       //SAML or AWS_SSO
  # grafana_account_access_type      = var.grafana_account_access_type // CURRENT_ACCOUNT or ORGANIZATION
  # grafana_permission_type          = var.grafana_permission_type     // SERVICE_MANAGED or CUSTOMER_MANAGED
  # grafana_permission_role_arn      = var.grafana_permission_role_arn // if CUSTOMER_MANAGED

  # # -- or using existing amg workspace. so we can use API for keys
  # managed_grafana_workspace_id = var.managed_grafana_workspace_id

  tags = local.tags

}
",module,"module ""eks_observability_accelerator"" {
  #source = ""aws-ia/terrarom-aws-observability-accelerator""
  source = ""../""

  aws_region     = var.aws_region
  eks_cluster_id = var.eks_cluster_id

  # deploys AWS Distro for OpenTelemetry operator into the cluster
  enable_amazon_eks_adot = true

  # reusing existing certificate manager? defaults to true
  enable_cert_manager = true

  # # -- or enable opentelemetry operator
  enable_opentelemetry_operator = false
  #open_telemetry_operator_config = map() // custom config

  # creates a new AMP workspace, defaults to true
  enable_managed_prometheus = false

  # reusing existing AMP -- needs data source for alerting rules
  managed_prometheus_id     = var.managed_prometheus_workspace_id
  managed_prometheus_region = null # defaults to the current region, useful for cross region scenarios (same account)

  # sets up the AMP alert manager at the workspace level
  enable_alertmanager = true

  # create a new Grafana workspace - TODO review design
  enable_managed_grafana       = false
  managed_grafana_workspace_id = var.managed_grafana_workspace_id
  grafana_api_key              = var.grafana_api_key

  # enable workload-specific collector, metrics, alerts and dashboards
  enable_java                 = false
  enable_java_recording_rules = false

  # enable_haproxy = true
  # haproxy_config = {
  #   amp_endpoint     = module / amp.endpoint
  #   grafana_endpoint = module.grafana.endpoint
  # }

  enable_infra_metrics = true
  #infra_metrics_config = {}

  tags = local.tags
}
",module,17,,b7e909c92ddaff2b4ee2f3e1be30e99c0b1c6de1,a8fc12f7b7d7f4ed814d48061cf3e0eb4a646c9e,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/b7e909c92ddaff2b4ee2f3e1be30e99c0b1c6de1/examples/workloads.tf#L17,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/a8fc12f7b7d7f4ed814d48061cf3e0eb4a646c9e/examples/workloads.tf,2022-08-26 17:30:03+02:00,2022-08-26 17:30:03+02:00,5,1,1,1,0,0,0,0,1,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,848,fast/stages/01-resman/branch-gke.tf,fast/stages/01-resman/branch-gke.tf,0,fix,# FIXME(jccb): who should we use here?,# FIXME(jccb): who should we use here?,"module ""branch-gke-multitenant-prod-sa"" {
  source      = ""../../../modules/iam-service-account""
  project_id  = var.automation_project_id
  name        = ""gke-prod-0""
  description = ""Terraform gke multitenant prod service account.""
  prefix      = var.prefix
  iam = {
    # FIXME(jccb): who should we use here?
    ""roles/iam.serviceAccountTokenCreator"" = [""group:${local.groups.gcp-devops}""]
  }
}
",module,the block associated got renamed or deleted,,56,,f3f9a4a88cedd64f6fc91b64666046aa6726a2f3,7b5ced7e15a537b1760bfe822780d9429e94f182,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/f3f9a4a88cedd64f6fc91b64666046aa6726a2f3/fast/stages/01-resman/branch-gke.tf#L56,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/7b5ced7e15a537b1760bfe822780d9429e94f182/fast/stages/01-resman/branch-gke.tf,2022-06-08 11:41:50+02:00,2022-06-30 18:22:57+02:00,6,1,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,8,community/modules/scheduler/cloud-batch-login-node/main.tf,community/modules/scheduler/cloud-batch-login-node/main.tf,0,# todo,# TODO: This workaround should be removed once startup-script supports Bash syntax,{ # TODO: This workaround should be removed once startup-script supports Bash syntax,"module ""login_startup_script"" {
  source          = ""github.com/GoogleCloudPlatform/hpc-toolkit//modules/scripts/startup-script?ref=v1.0.0""
  labels          = var.labels
  project_id      = var.project_id
  deployment_name = var.deployment_name
  region          = var.region
  runners = [
    {
      content     = local.batch_startup_script
      destination = ""/tmp/startup-scripts/batch_startup_script.sh""
      type        = ""data""
    },
    { # TODO: This workaround should be removed once startup-script supports Bash syntax
      content     = ""bash /tmp/startup-scripts/batch_startup_script.sh""
      destination = ""/tmp/startup-scripts/invoke_batch_startup_script.sh""
      type        = ""shell""
    },
    {
      content     = var.job_template_contents
      destination = ""${var.batch_job_directory}/${var.job_filename}""
      type        = ""data""
    }
  ]
}
",module,"module ""login_startup_script"" {
  source          = ""github.com/GoogleCloudPlatform/hpc-toolkit//modules/scripts/startup-script?ref=v1.6.0""
  labels          = var.labels
  project_id      = var.project_id
  deployment_name = var.deployment_name
  region          = var.region
  runners = [
    {
      content     = local.batch_startup_script
      destination = ""batch_startup_script.sh""
      type        = ""shell""
    },
    {
      content     = var.job_template_contents
      destination = ""${var.batch_job_directory}/${var.job_filename}""
      type        = ""data""
    }
  ]
}
",module,39,,9af8e32cdaabe4e30b03f245061414230e11d930,061ab8ccf926ba56d1fb97e2fd97cfaa4fdcaf88,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/9af8e32cdaabe4e30b03f245061414230e11d930/community/modules/scheduler/cloud-batch-login-node/main.tf#L39,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/061ab8ccf926ba56d1fb97e2fd97cfaa4fdcaf88/community/modules/scheduler/cloud-batch-login-node/main.tf,2022-07-08 10:56:43-07:00,2022-10-10 15:37:34-07:00,3,1,0,1,0,0,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,646,modules/self-managed-node-group/main.tf,modules/self-managed-node-group/main.tf,0,todo,# TODO - Temporary stopgap for backwards compatibility until v21.0,# TODO - Temporary stopgap for backwards compatibility until v21.0,"locals {
  # Just to ensure templating doesn't fail when values are not provided
  ssm_cluster_version = var.cluster_version != null ? var.cluster_version : """"

  # TODO - Temporary stopgap for backwards compatibility until v21.0
  ami_type_to_user_data_type = {
    AL2_x86_64                 = ""linux""
    AL2_x86_64_GPU             = ""linux""
    AL2_ARM_64                 = ""linux""
    BOTTLEROCKET_ARM_64        = ""bottlerocket""
    BOTTLEROCKET_x86_64        = ""bottlerocket""
    BOTTLEROCKET_ARM_64_NVIDIA = ""bottlerocket""
    BOTTLEROCKET_x86_64_NVIDIA = ""bottlerocket""
    WINDOWS_CORE_2019_x86_64   = ""windows""
    WINDOWS_FULL_2019_x86_64   = ""windows""
    WINDOWS_CORE_2022_x86_64   = ""windows""
    WINDOWS_FULL_2022_x86_64   = ""windows""
    AL2023_x86_64_STANDARD     = ""al2023""
    AL2023_ARM_64_STANDARD     = ""al2023""
  }
  # Try to use `ami_type` first, but fall back to current, default behavior
  # TODO - will be removed in v21.0
  user_data_type = try(local.ami_type_to_user_data_type[var.ami_type], var.platform)

  # Map the AMI type to the respective SSM param path
  ami_type_to_ssm_param = {
    AL2_x86_64                 = ""/aws/service/eks/optimized-ami/${local.ssm_cluster_version}/amazon-linux-2/recommended/image_id""
    AL2_x86_64_GPU             = ""/aws/service/eks/optimized-ami/${local.ssm_cluster_version}/amazon-linux-2-gpu/recommended/image_id""
    AL2_ARM_64                 = ""/aws/service/eks/optimized-ami/${local.ssm_cluster_version}/amazon-linux-2-arm64/recommended/image_id""
    BOTTLEROCKET_ARM_64        = ""/aws/service/bottlerocket/aws-k8s-${local.ssm_cluster_version}/arm64/latest/image_id""
    BOTTLEROCKET_x86_64        = ""/aws/service/bottlerocket/aws-k8s-${local.ssm_cluster_version}/x86_64/latest/image_id""
    BOTTLEROCKET_ARM_64_NVIDIA = ""/aws/service/bottlerocket/aws-k8s-${local.ssm_cluster_version}-nvidia/arm64/latest/image_id""
    BOTTLEROCKET_x86_64_NVIDIA = ""/aws/service/bottlerocket/aws-k8s-${local.ssm_cluster_version}-nvidia/x86_64/latest/image_id""
    WINDOWS_CORE_2019_x86_64   = ""/aws/service/ami-windows-latest/Windows_Server-2019-English-Full-EKS_Optimized-${local.ssm_cluster_version}/image_id""
    WINDOWS_FULL_2019_x86_64   = ""/aws/service/ami-windows-latest/Windows_Server-2019-English-Core-EKS_Optimized-${local.ssm_cluster_version}/image_id""
    WINDOWS_CORE_2022_x86_64   = ""/aws/service/ami-windows-latest/Windows_Server-2022-English-Full-EKS_Optimized-${local.ssm_cluster_version}/image_id""
    WINDOWS_FULL_2022_x86_64   = ""/aws/service/ami-windows-latest/Windows_Server-2022-English-Core-EKS_Optimized-${local.ssm_cluster_version}/image_id""
    AL2023_x86_64_STANDARD     = ""/aws/service/eks/optimized-ami/${local.ssm_cluster_version}/amazon-linux-2023/x86_64/standard/recommended/image_id""
    AL2023_ARM_64_STANDARD     = ""/aws/service/eks/optimized-ami/${local.ssm_cluster_version}/amazon-linux-2023/arm64/standard/recommended/image_id""
  }
}
",locals,"locals {
  # Just to ensure templating doesn't fail when values are not provided
  ssm_cluster_version = var.cluster_version != null ? var.cluster_version : """"

  # TODO - Temporary stopgap for backwards compatibility until v21.0
  ami_type_to_user_data_type = {
    AL2_x86_64                 = ""linux""
    AL2_x86_64_GPU             = ""linux""
    AL2_ARM_64                 = ""linux""
    BOTTLEROCKET_ARM_64        = ""bottlerocket""
    BOTTLEROCKET_x86_64        = ""bottlerocket""
    BOTTLEROCKET_ARM_64_NVIDIA = ""bottlerocket""
    BOTTLEROCKET_x86_64_NVIDIA = ""bottlerocket""
    WINDOWS_CORE_2019_x86_64   = ""windows""
    WINDOWS_FULL_2019_x86_64   = ""windows""
    WINDOWS_CORE_2022_x86_64   = ""windows""
    WINDOWS_FULL_2022_x86_64   = ""windows""
    AL2023_x86_64_STANDARD     = ""al2023""
    AL2023_ARM_64_STANDARD     = ""al2023""
  }
  # Try to use `ami_type` first, but fall back to current, default behavior
  # TODO - will be removed in v21.0
  user_data_type = try(local.ami_type_to_user_data_type[var.ami_type], var.platform)

  # Map the AMI type to the respective SSM param path
  ami_type_to_ssm_param = {
    AL2_x86_64                 = ""/aws/service/eks/optimized-ami/${local.ssm_cluster_version}/amazon-linux-2/recommended/image_id""
    AL2_x86_64_GPU             = ""/aws/service/eks/optimized-ami/${local.ssm_cluster_version}/amazon-linux-2-gpu/recommended/image_id""
    AL2_ARM_64                 = ""/aws/service/eks/optimized-ami/${local.ssm_cluster_version}/amazon-linux-2-arm64/recommended/image_id""
    BOTTLEROCKET_ARM_64        = ""/aws/service/bottlerocket/aws-k8s-${local.ssm_cluster_version}/arm64/latest/image_id""
    BOTTLEROCKET_x86_64        = ""/aws/service/bottlerocket/aws-k8s-${local.ssm_cluster_version}/x86_64/latest/image_id""
    BOTTLEROCKET_ARM_64_NVIDIA = ""/aws/service/bottlerocket/aws-k8s-${local.ssm_cluster_version}-nvidia/arm64/latest/image_id""
    BOTTLEROCKET_x86_64_NVIDIA = ""/aws/service/bottlerocket/aws-k8s-${local.ssm_cluster_version}-nvidia/x86_64/latest/image_id""
    WINDOWS_CORE_2019_x86_64   = ""/aws/service/ami-windows-latest/Windows_Server-2019-English-Full-EKS_Optimized-${local.ssm_cluster_version}/image_id""
    WINDOWS_FULL_2019_x86_64   = ""/aws/service/ami-windows-latest/Windows_Server-2019-English-Core-EKS_Optimized-${local.ssm_cluster_version}/image_id""
    WINDOWS_CORE_2022_x86_64   = ""/aws/service/ami-windows-latest/Windows_Server-2022-English-Full-EKS_Optimized-${local.ssm_cluster_version}/image_id""
    WINDOWS_FULL_2022_x86_64   = ""/aws/service/ami-windows-latest/Windows_Server-2022-English-Core-EKS_Optimized-${local.ssm_cluster_version}/image_id""
    AL2023_x86_64_STANDARD     = ""/aws/service/eks/optimized-ami/${local.ssm_cluster_version}/amazon-linux-2023/x86_64/standard/recommended/image_id""
    AL2023_ARM_64_STANDARD     = ""/aws/service/eks/optimized-ami/${local.ssm_cluster_version}/amazon-linux-2023/arm64/standard/recommended/image_id""
  }
}
",locals,12,12.0,74d39187d855932dd976da6180eda42dcfe09873,74d39187d855932dd976da6180eda42dcfe09873,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/74d39187d855932dd976da6180eda42dcfe09873/modules/self-managed-node-group/main.tf#L12,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/74d39187d855932dd976da6180eda42dcfe09873/modules/self-managed-node-group/main.tf#L12,2024-05-08 08:04:19-04:00,2024-05-08 08:04:19-04:00,1,0,0,1,0,0,0,0,0,0
https://github.com/compiler-explorer/infra,221,terraform/cloudfront.tf,terraform/cloudfront.tf,0,todo,# todo change,"    metric_name                = ""deny-banned-ips"" # todo change","resource ""aws_wafv2_web_acl"" ""banned-ips"" {
  name  = ""deny-banned-ips""
  scope = ""REGIONAL"" # TODO all these?
  default_action {
    allow {}
  }

  rule {
    name     = ""deny-ipv4""
    priority = 0
    action {
      block {}
    }
    statement {
      ip_set_reference_statement {
        arn = aws_wafv2_ip_set.banned-ipv4.arn
        ip_set_forwarded_ip_config {
          fallback_behavior = ""MATCH""
          header_name       = ""X-Forwarded-For""
          position          = ""ANY""
        }
      }
    }
    visibility_config {
      cloudwatch_metrics_enabled = true
      metric_name                = ""deny-ipv4""
      sampled_requests_enabled   = true
    }
  }
  rule {
    name     = ""deny-ipv6""
    priority = 1
    action {
      block {}
    }
    statement {
      ip_set_reference_statement {
        arn = aws_wafv2_ip_set.banned-ipv6.arn
        ip_set_forwarded_ip_config {
          fallback_behavior = ""MATCH""
          header_name       = ""X-Forwarded-For""
          position          = ""ANY""
        }
      }
    }
    visibility_config {
      cloudwatch_metrics_enabled = true
      metric_name                = ""deny-ipv6""
      sampled_requests_enabled   = true
    }
  }
  visibility_config {
    cloudwatch_metrics_enabled = true
    metric_name                = ""deny-banned-ips"" # todo change
    sampled_requests_enabled   = true
  }
}
",resource,the block associated got renamed or deleted,,617,,bae9867971b4d1a63f896071dd267f6a8bbe9637,5984967d8fe56d0296be70c38a7c15dd8e16615d,https://github.com/compiler-explorer/infra/blob/bae9867971b4d1a63f896071dd267f6a8bbe9637/terraform/cloudfront.tf#L617,https://github.com/compiler-explorer/infra/blob/5984967d8fe56d0296be70c38a7c15dd8e16615d/terraform/cloudfront.tf,2022-10-06 18:18:22-05:00,2022-10-06 19:03:41-05:00,2,1,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1910,fast/stages/2-security/vpc-sc.tf,fast/stages/2-security/vpc-sc.tf,0,# todo,# TODO(ludomagno): allow passing in restricted services via variable and factory file,"# TODO(ludomagno): allow passing in restricted services via variable and factory file 
 # TODO(ludomagno): implement vpc accessible services via variable or factory file ","module ""vpc-sc"" {
  source = ""../../../modules/vpc-sc""
  # only enable if the default perimeter is defined
  count         = var.vpc_sc.perimeter_default == null ? 0 : 1
  access_policy = null
  access_policy_create = {
    parent = ""organizations/${var.organization.id}""
    title  = ""default""
  }
  access_levels   = var.vpc_sc.access_levels
  egress_policies = var.vpc_sc.egress_policies
  ingress_policies = merge(
    var.vpc_sc.ingress_policies,
    local.vpc_sc_ingress_policies
  )
  factories_config = var.factories_config.vpc_sc
  service_perimeters_regular = {
    default = {
      spec = (
        var.vpc_sc.perimeter_default.dry_run ? local.vpc_sc_perimeter : null
      )
      status = (
        !var.vpc_sc.perimeter_default.dry_run ? local.vpc_sc_perimeter : null
      )
      use_explicit_dry_run_spec = var.vpc_sc.perimeter_default.dry_run
    }
  }
}
",module,"module ""vpc-sc"" {
  source = ""../../../modules/vpc-sc""
  # only enable if the default perimeter is defined
  count         = var.vpc_sc.perimeter_default == null ? 0 : 1
  access_policy = var.access_policy
  access_policy_create = var.access_policy != null ? null : {
    parent = ""organizations/${var.organization.id}""
    title  = ""default""
  }
  access_levels   = var.vpc_sc.access_levels
  egress_policies = var.vpc_sc.egress_policies
  ingress_policies = merge(
    var.vpc_sc.ingress_policies,
    local.vpc_sc_ingress_policies
  )
  factories_config = var.factories_config.vpc_sc
  service_perimeters_regular = {
    default = {
      spec = (
        var.vpc_sc.perimeter_default.dry_run ? local.vpc_sc_perimeter : null
      )
      status = (
        !var.vpc_sc.perimeter_default.dry_run ? local.vpc_sc_perimeter : null
      )
      use_explicit_dry_run_spec = var.vpc_sc.perimeter_default.dry_run
    }
  }
}
",module,61,61.0,8511170412355efd6c5995431f2e5617d28d604e,7a5dd4e6db197daa52da8a8d877ce86b5c93182e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/8511170412355efd6c5995431f2e5617d28d604e/fast/stages/2-security/vpc-sc.tf#L61,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/7a5dd4e6db197daa52da8a8d877ce86b5c93182e/fast/stages/2-security/vpc-sc.tf#L61,2024-04-07 20:14:39-07:00,2024-05-15 09:17:13+00:00,3,0,0,1,0,1,1,0,0,0
https://github.com/Worklytics/psoxy,2895,infra/modules/gcp-secrets/main.tf,infra/modules/gcp-secrets/main.tf,0,# todo,# TODO: put each.value.description somewhere shouldn't be a 'label' annotations not yet supprted,"# TODO: put each.value.description somewhere; shouldn't be a 'label'; annotations not yet supprted 
 # by google terraform provider ","resource ""google_secret_manager_secret"" ""secret"" {
  for_each = var.secrets

  project   = var.secret_project
  secret_id = ""${var.path_prefix}${each.key}""
  labels    = merge(
    var.default_labels,
    {
      terraform_managed_value = each.value.value_managed_by_tf
    }
  )

  # TODO: put each.value.description somewhere; shouldn't be a 'label'; annotations not yet supprted
  # by google terraform provider

  replication {
    user_managed {
      dynamic ""replicas"" {
        for_each = local.replica_locations
        content {
          location = replicas.value
        }
      }
    }
  }

  lifecycle {
    ignore_changes = [
      replication, # for backwards compatibility; replication can't be changed after secrets created
      labels
    ]
  }
}
",resource,"resource ""google_secret_manager_secret"" ""secret"" {
  for_each = var.secrets

  project   = var.secret_project
  secret_id = ""${var.path_prefix}${each.key}""
  labels = merge(
    var.default_labels,
    {
      terraform_managed_value = each.value.value_managed_by_tf
    }
  )

  # TODO: put each.value.description somewhere; shouldn't be a 'label'; annotations not yet supprted
  # by google terraform provider

  replication {
    user_managed {
      dynamic ""replicas"" {
        for_each = local.replica_locations
        content {
          location = replicas.value
        }
      }
    }
  }

  lifecycle {
    ignore_changes = [
      replication, # for backwards compatibility; replication can't be changed after secrets created
      labels
    ]
  }
}
",resource,25,25.0,96eefe71e3fe43be1be749d78545c2c25a17ac83,5e25bc21633cc46c7ba0066959fc9268dedfe92f,https://github.com/Worklytics/psoxy/blob/96eefe71e3fe43be1be749d78545c2c25a17ac83/infra/modules/gcp-secrets/main.tf#L25,https://github.com/Worklytics/psoxy/blob/5e25bc21633cc46c7ba0066959fc9268dedfe92f/infra/modules/gcp-secrets/main.tf#L25,2023-08-11 06:21:43-07:00,2023-08-21 11:08:12-07:00,2,0,0,1,1,1,0,0,0,0
https://github.com/Azure/Avere,15,src/terraform/modules/vmss_mountable/variables.tf,src/terraform/modules/vmss_mountable/variables.tf,0,implement,// depends on technique described here: https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2,// depends on technique described here: https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2,"variable ""vmss_depends_on"" {
  description = ""used to establish dependency between objects""
  type = any
  default = null
}
",variable,the block associated got renamed or deleted,,82,,3a982a8a0eab051c6b1bc553c5d9406b8a3d53c6,feb88e973389482312720d4be85a13cac2973006,https://github.com/Azure/Avere/blob/3a982a8a0eab051c6b1bc553c5d9406b8a3d53c6/src/terraform/modules/vmss_mountable/variables.tf#L82,https://github.com/Azure/Avere/blob/feb88e973389482312720d4be85a13cac2973006/src/terraform/modules/vmss_mountable/variables.tf,2020-06-25 12:01:00+01:00,2021-04-23 18:58:31-04:00,4,1,0,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-kubernetes-engine,1,examples/simple_regional/main.tf,examples/simple_regional/main.tf,0,todo,# TODO remove,"region = ""us-east4"" # TODO remove","provider ""google"" {
  credentials = ""${file(local.credentials_file_path)}""
  region = ""us-east4"" # TODO remove
}
",provider,"provider ""google"" {
  credentials = ""${file(local.credentials_file_path)}""
}
",provider,23,,28c28dd1c900c08d9cbfb0aaa22d7dfed990678c,d0ee13342a737a1bd2f7bfddb46c1f21a8c1562f,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/28c28dd1c900c08d9cbfb0aaa22d7dfed990678c/examples/simple_regional/main.tf#L23,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/d0ee13342a737a1bd2f7bfddb46c1f21a8c1562f/examples/simple_regional/main.tf,2018-10-01 18:09:56-04:00,2018-10-05 17:09:52-04:00,2,1,0,1,1,0,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,175,fargate.tf,fargate.tf,0,implement,# This is a homemade `depends_on` https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2,"# Hack to ensure ordering of resource creation. 
 # This is a homemade `depends_on` https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2 
 # Do not create node_groups before other resources are ready and removes race conditions 
 # Ensure these resources are created before ""unlocking"" the data source. 
 # Will be removed in Terraform 0.13","module ""fargate"" {
  source                            = ""./modules/fargate""
  cluster_name                      = coalescelist(aws_eks_cluster.this[*].name, [""""])[0]
  create_eks                        = var.create_eks
  create_fargate_pod_execution_role = var.create_fargate_pod_execution_role
  fargate_pod_execution_role_name   = var.fargate_pod_execution_role_name
  fargate_profiles                  = var.fargate_profiles
  iam_path                          = var.iam_path
  iam_policy_arn_prefix             = local.policy_arn_prefix
  subnets                           = var.subnets
  tags                              = var.tags

  # Hack to ensure ordering of resource creation.
  # This is a homemade `depends_on` https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2
  # Do not create node_groups before other resources are ready and removes race conditions
  # Ensure these resources are created before ""unlocking"" the data source.
  # Will be removed in Terraform 0.13
  eks_depends_on = [
    aws_eks_cluster.this,
    kubernetes_config_map.aws_auth,
  ]
}
",module,"module ""fargate"" {
  source = ""./modules/fargate""

  create_eks                        = var.create_eks
  create_fargate_pod_execution_role = var.create_fargate_pod_execution_role

  cluster_name                    = local.cluster_name
  fargate_pod_execution_role_name = var.fargate_pod_execution_role_name
  permissions_boundary            = var.permissions_boundary
  iam_path                        = var.iam_path
  subnets                         = coalescelist(var.fargate_subnets, var.subnets, [""""])

  fargate_profiles = var.fargate_profiles

  tags = var.tags
}
",module,14,,0d77e30075e20ff16fa7357987a240e93f5f28ce,2bdf7d7dd6e4705fdfa267bed40e147bd9287a21,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/0d77e30075e20ff16fa7357987a240e93f5f28ce/fargate.tf#L14,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/2bdf7d7dd6e4705fdfa267bed40e147bd9287a21/fargate.tf,2020-11-07 23:03:12+01:00,2021-09-16 11:35:44+02:00,5,1,1,1,1,0,0,0,0,1
https://github.com/oracle-terraform-modules/terraform-oci-oke,166,modules/workergroup/instancepools.tf,modules/workers/instancepools.tf,1,todo,# TODO From dynamic creation when no lb_id provided introspected fields when present,# TODO From dynamic creation when no lb_id provided introspected fields when present,"resource ""oci_core_instance_pool"" ""instance_pools"" {
  # Create an OCI Instance Pool resource for each enabled entry of the worker_groups map with that mode.
  for_each                  = local.enabled_instance_pools
  compartment_id            = each.value.compartment_id
  display_name              = ""${each.value.label_prefix}-${each.key}""
  size                      = each.value.size
  instance_configuration_id = oci_core_instance_configuration.instance_configuration[each.key].id
  defined_tags              = merge(local.defined_tags, contains(keys(each.value), ""defined_tags"") ? each.value.defined_tags : {})
  freeform_tags             = merge(local.freeform_tags, contains(keys(each.value), ""freeform_tags"") ? each.value.freeform_tags : { worker_group = each.key })

  dynamic ""placement_configurations"" {
    # Define each configured availability domain for placement, with bounds on # available
    # Configured AD numbers e.g. [1,2,3] are converted into tenancy/compartment-specific names
    iterator = ad_number
    for_each = (contains(keys(each.value), ""placement_ads"")
      ? tolist(setintersection(each.value.placement_ads, local.ad_numbers))
      : local.ad_numbers
    )

    content {
      availability_domain = lookup(local.ad_number_to_name, ad_number.value, local.first_ad_name)
      primary_subnet_id   = each.value.subnet_id
    }
  }

  lifecycle {
    ignore_changes = [
      display_name, defined_tags, freeform_tags,
      placement_configurations,
    ]
  }

  dynamic ""load_balancers"" {
    # Associate the instance pool with 0+ load balancers for ingress traffic
    # TODO Accept full definition to create
    for_each = contains(keys(each.value), ""load_balancers"") ? each.value.load_balancers : {}

    content {
      # TODO From dynamic creation when no lb_id provided; introspected fields when present
      backend_set_name = lookup(lb, ""backend_set_name"", display_name)
      load_balancer_id = lookup(lb, ""lb_id"", lb_id)
      port             = lookup(lb, ""port"", 8080)

      // Possible values are ""PrimaryVnic"" or the displayName of
      // one of the secondary VNICs on the instance configuration
      // that is associated with the instance pool.
      vnic_selection = lookup(lb, ""vnic_selection"", ""PrimaryVnic"") # TODO Support w/ named secondary VNICs
    }
  }

  depends_on = [
    oci_core_instance_configuration.instance_configuration,
  ]
}",resource,the block associated got renamed or deleted,,43,,4d2b3f3d672a8f41655da3a7c58fded42c6858f3,f49f1da39d79cf260d80dcb10ee8e399828e6e1c,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/4d2b3f3d672a8f41655da3a7c58fded42c6858f3/modules/workergroup/instancepools.tf#L43,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/f49f1da39d79cf260d80dcb10ee8e399828e6e1c/modules/workers/instancepools.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,4,1,1,1,0,0,1,0,0,0
https://github.com/Worklytics/psoxy,978,infra/modular-examples/gcp/main.tf,infra/modular-examples/gcp/main.tf,0,# todo,# TODO: what if multiple lookups from same source??,"bucket_name_suffix             = ""-lookup"" # TODO: what if multiple lookups from same source??","module ""lookup_output"" {
  for_each = var.lookup_tables

  source = ""../../modules/gcp-output-bucket""

  bucket_write_role_id           = module.psoxy.bucket_write_role_id
  function_service_account_email = module.psoxy-bulk[each.value.source_connector_id].instance_sa_email
  project_id                     = var.gcp_project_id
  region                         = var.gcp_region
  bucket_name_prefix             = module.psoxy-bulk[each.value.source_connector_id].bucket_prefix
  bucket_name_suffix             = ""-lookup"" # TODO: what if multiple lookups from same source??
  expiration_days                = each.value.expiration_days
  sanitizer_accessor_principals  = each.value.sanitized_accessor_principals
}
",module,,,334,0.0,4f41d480721ffa94079bbfb0cdcc181e22423300,f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e,https://github.com/Worklytics/psoxy/blob/4f41d480721ffa94079bbfb0cdcc181e22423300/infra/modular-examples/gcp/main.tf#L334,https://github.com/Worklytics/psoxy/blob/f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e/infra/modular-examples/gcp/main.tf#L0,2023-04-18 16:24:15-07:00,2023-06-16 14:08:45-07:00,13,2,1,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,1521,infra/modules/gcp-secrets/main.tf,infra/modules/gcp-secrets/main.tf,0,# todo,# TODO: avoid creating version here at all if value == null,"# TODO: avoid creating version here at all if value == null 
 # (problem is that Terraform complains if trying to use any derivative of var.secrets in a for_each, 
 #  bc it's sensitive - not sure why it doesn't complain about the for_each over var.secrets directly)","resource ""google_secret_manager_secret_version"" ""version"" {
  for_each = var.secrets

  secret      = google_secret_manager_secret.secret[each.key].id
  secret_data = coalesce(each.value.value, ""placeholder value - fill me"")
  # NOTE: can't set `enabled = false` here, bc we bind secret to env var so CloudFunction update will fail

  lifecycle {
    create_before_destroy = true
  }
}
",resource,"resource ""google_secret_manager_secret_version"" ""version"" {
  for_each = local.secrets_w_terraform_managed_values

  secret      = google_secret_manager_secret.secret[each.key].id
  secret_data = coalesce(each.value.value, ""placeholder value - fill me"")
  # NOTE: can't set `enabled = false` here in placeholder case, bc we bind secret to env var so
  # CloudFunction update will fail as can't bind to ':latest'

  lifecycle {
    create_before_destroy = true

    # TODO: remove this in v0.5
    ignore_changes = [
      enabled, # if secret version disabled after creation, let it be (placeholder case)
    ]
  }
}
",resource,31,,f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e,140ba148a513ad7b6a450120b0a8d39d58c1d908,https://github.com/Worklytics/psoxy/blob/f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e/infra/modules/gcp-secrets/main.tf#L31,https://github.com/Worklytics/psoxy/blob/140ba148a513ad7b6a450120b0a8d39d58c1d908/infra/modules/gcp-secrets/main.tf,2023-06-16 14:08:45-07:00,2023-07-25 08:16:19-07:00,6,1,0,1,1,1,0,0,0,0
https://github.com/kubernetes-sigs/kubespray,11,contrib/terraform/openstack/modules/compute/main.tf,contrib/terraform/openstack/modules/compute/main.tf,0,workaround,"# As a workaround for creating ""dynamic"" lists (when, for example, no bastion host is created)","# The join() hack is described here: https://github.com/hashicorp/terraform/issues/11566 
 # As a workaround for creating ""dynamic"" lists (when, for example, no bastion host is created) ","resource ""openstack_compute_instance_v2"" ""k8s_master"" {
  name              = ""${var.cluster_name}-k8s-master-${count.index+1}""
  count             = ""${var.number_of_k8s_masters}""
  availability_zone = ""${element(var.az_list, count.index)}""
  image_name        = ""${var.image}""
  flavor_id         = ""${var.flavor_k8s_master}""
  key_pair          = ""${openstack_compute_keypair_v2.k8s.name}""

  network {
    name = ""${var.network_name}""
  }

  # The join() hack is described here: https://github.com/hashicorp/terraform/issues/11566
  # As a workaround for creating ""dynamic"" lists (when, for example, no bastion host is created)

  security_groups = [""${compact(list(
    openstack_networking_secgroup_v2.k8s_master.name,
    join("" "", openstack_networking_secgroup_v2.bastion.*.id),
    openstack_networking_secgroup_v2.k8s.name,
    ""default"",
   ))}""]
  metadata = {
    ssh_user         = ""${var.ssh_user}""
    kubespray_groups = ""etcd,kube-master,${var.supplementary_master_groups},k8s-cluster,vault""
    depends_on       = ""${var.network_id}""
  }
  provisioner ""local-exec"" {
    command = ""sed s/USER/${var.ssh_user}/ contrib/terraform/openstack/ansible_bastion_template.txt | sed s/BASTION_ADDRESS/${element( concat(var.bastion_fips, var.k8s_master_fips), 0)}/ > contrib/terraform/group_vars/no-floating.yml""
  }
}
",resource,"resource ""openstack_compute_instance_v2"" ""k8s_master"" {
  name              = ""${var.cluster_name}-k8s-master-${count.index+1}""
  count             = ""${var.number_of_k8s_masters}""
  availability_zone = ""${element(var.az_list, count.index)}""
  image_name        = ""${var.image}""
  flavor_id         = ""${var.flavor_k8s_master}""
  key_pair          = ""${openstack_compute_keypair_v2.k8s.name}""

  network {
    name = ""${var.network_name}""
  }

  security_groups = [""${openstack_networking_secgroup_v2.k8s_master.name}"",
    ""${openstack_networking_secgroup_v2.k8s.name}"",
    ""default"",
  ]

  metadata = {
    ssh_user         = ""${var.ssh_user}""
    kubespray_groups = ""etcd,kube-master,${var.supplementary_master_groups},k8s-cluster,vault""
    depends_on       = ""${var.network_id}""
  }

  provisioner ""local-exec"" {
    command = ""sed s/USER/${var.ssh_user}/ contrib/terraform/openstack/ansible_bastion_template.txt | sed s/BASTION_ADDRESS/${element( concat(var.bastion_fips, var.k8s_master_fips), 0)}/ > contrib/terraform/group_vars/no-floating.yml""
  }
}
",resource,106,,20ebb49568547d9621bfdd13945c725a991d5916,7f1d9ff543247a4a1868eab44e79a7fa4438ab70,https://github.com/kubernetes-sigs/kubespray/blob/20ebb49568547d9621bfdd13945c725a991d5916/contrib/terraform/openstack/modules/compute/main.tf#L106,https://github.com/kubernetes-sigs/kubespray/blob/7f1d9ff543247a4a1868eab44e79a7fa4438ab70/contrib/terraform/openstack/modules/compute/main.tf,2019-04-09 04:01:09-07:00,2019-04-15 07:22:08-07:00,2,1,1,1,1,1,0,0,0,0
https://github.com/Worklytics/psoxy,1021,infra/modules/gcp-tf-runner/main.tf,infra/modules/gcp-tf-runner/main.tf,0,hack,# hacky way to determine if Terraform running as a service account or not,# hacky way to determine if Terraform running as a service account or not,"locals {
  # hacky way to determine if Terraform running as a service account or not
  tf_is_service_account = endswith(data.google_client_openid_userinfo.me.email, ""iam.gserviceaccount.com"")

  tf_qualifier          = local.tf_is_service_account ? ""serviceAccount:"" : ""user:""
  tf_principal          = ""${local.tf_qualifier}${data.google_client_openid_userinfo.me.email}""
}
",locals,"locals {
  jwt_payload = try(split(""."", data.google_service_account_id_token.identity[0].id_token)[1], """")

  # convert base64url encoding to base64 encoding
  padding                   = join("""", formatlist(""%s"", [for _ in range(4 - length(local.jwt_payload) % 4) : ""=""]))
  jwt_payload_padded        = ""${local.jwt_payload}${local.padding}""
  jwt_payload_base64encoded = replace(replace(local.jwt_payload_padded, ""-"", ""+""), ""_"", ""/"")

  # decode to JSON, then extract email field
  email_from_jwt = try(nonsensitive(jsondecode(base64decode(local.jwt_payload_base64encoded)).email), """")

  # coalesce failing here implies we failed to detect the auth'd gcp user
  authed_user_email = coalesce(
    try(data.external.identity.result.gcp_terraform_sa_account_email, """"), # """" if no such value
    data.google_client_openid_userinfo.me.email,
    local.email_from_jwt
  )

  # hacky way to determine if Terraform running as a service account or not
  tf_is_service_account = endswith(local.authed_user_email, ""iam.gserviceaccount.com"")

  tf_qualifier = local.tf_is_service_account ? ""serviceAccount:"" : ""user:""
  tf_principal = ""${local.tf_qualifier}${local.authed_user_email}""
}
",locals,13,60.0,5b70c56e12136c245e8032e353322be2ccf4c055,2c788b347439df06399004184cbfe1b9ce9987e7,https://github.com/Worklytics/psoxy/blob/5b70c56e12136c245e8032e353322be2ccf4c055/infra/modules/gcp-tf-runner/main.tf#L13,https://github.com/Worklytics/psoxy/blob/2c788b347439df06399004184cbfe1b9ce9987e7/infra/modules/gcp-tf-runner/main.tf#L60,2023-04-21 09:17:45-07:00,2023-10-24 20:36:35-07:00,5,0,0,0,0,1,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,30,modules/aws-eks-self-managed-node-groups/secgroups.tf,modules/aws-eks-self-managed-node-groups/secgroups.tf,0,#todo,#TODO This may not be required since cluster_security_group_id outbound is open to 0.0.0.0/0,"//------------------IMPORTANT  
 #TODO This may not be required since cluster_security_group_id outbound is open to 0.0.0.0/0","resource ""aws_security_group_rule"" ""control_plane_egress_to_worker_https"" {
  count = local.self_managed_node_group[""create_worker_security_group""] == true ? 1 : 0

  description              = ""Allow cluster security group to send communication to worker security groups""
  type                     = ""egress""
  from_port                = 443
  to_port                  = 443
  protocol                 = ""tcp""
  security_group_id        = var.cluster_security_group_id
  source_security_group_id = aws_security_group.self_managed_ng[0].id

}
",resource,,,102,0.0,fd23f8a3a15c3ce9bb1b0692a21bd43f6b0ec439,6f7908ab23255067c61f3614124ec7818b60ba8f,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/fd23f8a3a15c3ce9bb1b0692a21bd43f6b0ec439/modules/aws-eks-self-managed-node-groups/secgroups.tf#L102,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/6f7908ab23255067c61f3614124ec7818b60ba8f/modules/aws-eks-self-managed-node-groups/secgroups.tf#L0,2021-10-03 20:47:56+01:00,2022-03-17 17:03:58+00:00,7,2,0,1,0,1,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,24,modules/net-vpc/outputs.tf,modules/net-vpc/outputs.tf,0,# todo,# TODO(ludoo): use input names as keys,# TODO(ludoo): use input names as keys,"output ""subnets"" {
  description = ""Subnet resources.""
  value       = { for k, v in google_compute_subnetwork.subnetwork : k => v }
}
",output,"output ""subnets"" {
  description = ""Subnet resources.""
  value       = { for k, v in google_compute_subnetwork.subnetwork : k => v }
}
",output,45,,c486bfc66f9814e33b410602cb557a5e4d532912,884cb8b4bf4caa8baa1d0148deb5a000a70a9920,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c486bfc66f9814e33b410602cb557a5e4d532912/modules/net-vpc/outputs.tf#L45,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/884cb8b4bf4caa8baa1d0148deb5a000a70a9920/modules/net-vpc/outputs.tf,2020-04-03 14:06:48+02:00,2023-06-02 16:07:22+02:00,12,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,11,resources/third-party/compute/SchedMD-slurm-on-gcp-v5-partition/main.tf,resources/third-party/compute/SchedMD-slurm-on-gcp-v5-partition/main.tf,0,# todo,# TODO: this next one does not like '-',"source = ""git::https://gitlab.com/SchedMD/slurm-gcp.git//terraform/modules/slurm_partition?ref=dev-v5""  
 # TODO: this next one does not like '-'","module ""slurm_partition"" {
  source = ""git::https://gitlab.com/SchedMD/slurm-gcp.git//terraform/modules/slurm_partition?ref=dev-v5""

  # TODO: this next one does not like '-'
  slurm_cluster_name      = var.deployment_name
  partition_nodes         = local.partition_nodes
  enable_job_exclusive    = var.exclusive
  enable_placement_groups = var.enable_placement
  network_storage         = var.network_storage
  partition_name          = var.partition_name
  project_id              = var.project_id
  region                  = var.region
  slurm_cluster_id        = ""placeholder""
  subnetwork              = ""default""
}
",module,"module ""slurm_partition"" {
  source = ""git::https://gitlab.com/SchedMD/slurm-gcp.git//terraform/modules/slurm_partition?ref=dev-v5""

  slurm_cluster_name      = var.slurm_cluster_name
  partition_nodes         = local.partition_nodes
  enable_job_exclusive    = var.exclusive
  enable_placement_groups = var.enable_placement
  network_storage         = var.network_storage
  partition_name          = var.partition_name
  project_id              = var.project_id
  region                  = var.region
  slurm_cluster_id        = ""placeholder""
  subnetwork              = var.subnetwork_self_link
  partition_conf = {
    Default = ""YES""
  }
}
",module,57,,306743da2f4c8f924b52d7fe2a84de9666dba59f,ebb8cbdcdc388c89f8e050809fa8d7b2c186ebb2,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/306743da2f4c8f924b52d7fe2a84de9666dba59f/resources/third-party/compute/SchedMD-slurm-on-gcp-v5-partition/main.tf#L57,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/ebb8cbdcdc388c89f8e050809fa8d7b2c186ebb2/resources/third-party/compute/SchedMD-slurm-on-gcp-v5-partition/main.tf,2022-08-29 16:17:21+00:00,2022-08-29 16:17:21+00:00,2,1,1,0,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1642,modules/gke-cluster-standard/variables.tf,modules/gke-cluster-standard/variables.tf,0,todo,# TODO add kube state metrics and validation,"# TODO add kube state metrics and validation  
 # Google Cloud Managed Service for Prometheus","variable ""monitoring_config"" {
  description = ""Monitoring configuration. Google Cloud Managed Service for Prometheus is enabled by default.""
  type = object({
    enable_system_metrics = optional(bool, true)

    # Control plane metrics
    enable_api_server_metrics         = optional(bool, false)
    enable_controller_manager_metrics = optional(bool, false)
    enable_scheduler_metrics          = optional(bool, false)

    # TODO add kube state metrics and validation

    # Google Cloud Managed Service for Prometheus
    enable_managed_prometheus = optional(bool, true)
  })
  default  = {}
  nullable = false
  validation {
    condition = anytrue([
      var.monitoring_config.enable_api_server_metrics,
      var.monitoring_config.enable_controller_manager_metrics,
      var.monitoring_config.enable_scheduler_metrics,
    ]) ? var.monitoring_config.enable_system_metrics : true
    error_message = ""System metrics are the minimum required component for enabling metrics collection.""
  }
}
",variable,"variable ""monitoring_config"" {
  description = ""Monitoring configuration. Google Cloud Managed Service for Prometheus is enabled by default.""
  type = object({
    enable_system_metrics = optional(bool, true)

    # Control plane metrics
    enable_api_server_metrics         = optional(bool, false)
    enable_controller_manager_metrics = optional(bool, false)
    enable_scheduler_metrics          = optional(bool, false)

    # Kube state metrics
    enable_daemonset_metrics   = optional(bool, false)
    enable_deployment_metrics  = optional(bool, false)
    enable_hpa_metrics         = optional(bool, false)
    enable_pod_metrics         = optional(bool, false)
    enable_statefulset_metrics = optional(bool, false)
    enable_storage_metrics     = optional(bool, false)

    # Google Cloud Managed Service for Prometheus
    enable_managed_prometheus = optional(bool, true)
  })
  default  = {}
  nullable = false
  validation {
    condition = anytrue([
      var.monitoring_config.enable_api_server_metrics,
      var.monitoring_config.enable_controller_manager_metrics,
      var.monitoring_config.enable_scheduler_metrics,
      var.monitoring_config.enable_daemonset_metrics,
      var.monitoring_config.enable_deployment_metrics,
      var.monitoring_config.enable_hpa_metrics,
      var.monitoring_config.enable_pod_metrics,
      var.monitoring_config.enable_statefulset_metrics,
      var.monitoring_config.enable_storage_metrics,
    ]) ? var.monitoring_config.enable_system_metrics : true
    error_message = ""System metrics are the minimum required component for enabling metrics collection.""
  }
  validation {
    condition = anytrue([
      var.monitoring_config.enable_daemonset_metrics,
      var.monitoring_config.enable_deployment_metrics,
      var.monitoring_config.enable_hpa_metrics,
      var.monitoring_config.enable_pod_metrics,
      var.monitoring_config.enable_statefulset_metrics,
      var.monitoring_config.enable_storage_metrics,
    ]) ? var.monitoring_config.enable_managed_prometheus : true
    error_message = ""Kube state metrics collection requires Google Cloud Managed Service for Prometheus to be enabled.""
  }
}
",variable,209,,b3dc91b5cd6375d9e589f149d98985a895fffbc2,6eb862a7754e9796cc26386227ea763c579edcdc,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b3dc91b5cd6375d9e589f149d98985a895fffbc2/modules/gke-cluster-standard/variables.tf#L209,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/6eb862a7754e9796cc26386227ea763c579edcdc/modules/gke-cluster-standard/variables.tf,2023-09-14 23:25:57+01:00,2023-09-15 12:18:45+01:00,2,1,1,1,0,0,0,0,1,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,361,modules/network/subnets.tf,modules/network/subnets.tf,0,todo,# TODO reflect default security_list_id instead of ignore,# TODO reflect default security_list_id instead of ignore,"resource ""oci_core_subnet"" ""oke"" {
  for_each = local.subnets_to_create

  compartment_id             = var.compartment_id
  vcn_id                     = var.vcn_id
  cidr_block                 = lookup(local.subnet_cidrs_all, each.key)
  display_name               = ""${each.key}-${var.state_id}""
  dns_label                  = var.assign_dns ? lookup(var.subnets, ""id"", substr(each.key, 0, 2)) : null
  prohibit_public_ip_on_vnic = !tobool(lookup(each.value, ""is_public"", false))
  route_table_id             = !tobool(lookup(each.value, ""is_public"", false)) ? var.nat_route_table_id : var.ig_route_table_id
  security_list_ids          = compact([lookup(lookup(oci_core_security_list.oke, each.key, {}), ""id"", null)])
  defined_tags               = local.defined_tags
  freeform_tags              = local.freeform_tags

  lifecycle {
    # TODO reflect default security_list_id instead of ignore
    ignore_changes = [security_list_ids, freeform_tags, defined_tags, dns_label, display_name, cidr_block]
  }
}
",resource,"resource ""oci_core_subnet"" ""oke"" {
  for_each = local.subnets_to_create

  compartment_id             = var.compartment_id
  vcn_id                     = var.vcn_id
  cidr_block                 = lookup(local.subnet_cidrs_all, each.key)
  display_name               = format(""%v-%v"", each.key, var.state_id)
  dns_label                  = lookup(local.subnet_dns_labels, each.key)
  prohibit_public_ip_on_vnic = !tobool(lookup(each.value, ""is_public"", false))
  route_table_id             = !tobool(lookup(each.value, ""is_public"", false)) ? var.nat_route_table_id : var.ig_route_table_id
  security_list_ids          = compact([lookup(lookup(oci_core_security_list.oke, each.key, {}), ""id"", null)])
  defined_tags               = var.defined_tags
  freeform_tags              = var.freeform_tags

  lifecycle {
    ignore_changes = [
      freeform_tags, defined_tags, display_name,
      cidr_block, dns_label, security_list_ids,
    ]
  }
}
",resource,127,,f49f1da39d79cf260d80dcb10ee8e399828e6e1c,52588fd5e9123c180f01072f5a2f3c2f4a349a25,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/f49f1da39d79cf260d80dcb10ee8e399828e6e1c/modules/network/subnets.tf#L127,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/52588fd5e9123c180f01072f5a2f3c2f4a349a25/modules/network/subnets.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,6,1,0,1,0,1,1,0,0,0
https://github.com/camptocamp/devops-stack,95,examples/eks-aws/main.tf,examples/eks/main.tf,1,todo,# TODO Add variable when we configure the Traefik Dashboard,# TODO Add variable when we configure the Traefik Dashboard,"module ""helloworld_apps"" {
  source = ""git::https://github.com/camptocamp/devops-stack-module-applicationset.git""

  depends_on = [module.argocd]

  name                   = ""helloworld-apps""
  argocd_namespace       = local.argocd_namespace
  project_dest_namespace = ""*""
  project_source_repos = [
    ""https://github.com/camptocamp/devops-stack-helloworld-templates.git"",
  ]

  generators = [
    {
      git = {
        repoURL  = ""https://github.com/camptocamp/devops-stack-helloworld-templates.git""
        revision = ""main""

        directories = [
          {
            path = ""apps/*""
          }
        ]
      }
    }
  ]
  template = {
    metadata = {
      name = ""{{path.basename}}""
    }

    spec = {
      project = ""helloworld-apps""

      source = {
        repoURL        = ""https://github.com/camptocamp/devops-stack-helloworld-templates.git""
        targetRevision = ""main""
        path           = ""{{path}}""

        helm = {
          valueFiles = []
          # The following value defines this global variables that will be available to all apps in apps/*
          # These are needed to generate the ingresses containing the name and base domain of the cluster.
          values = <<-EOT
            cluster:
              name: ""${module.eks.cluster_name}""
              domain: ""${module.eks.base_domain}""
            apps:
              traefik_dashboard: false # TODO Add variable when we configure the Traefik Dashboard
              grafana: ${module.grafana.grafana_enabled || module.prometheus-stack.grafana_enabled}
              prometheus: ${module.prometheus-stack.prometheus_enabled}
              thanos: ${module.thanos.thanos_enabled}
              alertmanager: ${module.prometheus-stack.alertmanager_enabled}
          EOT
        }
      }

      destination = {
        name      = ""in-cluster""
        namespace = ""{{path.basename}}""
      }

      syncPolicy = {
        automated = {
          allowEmpty = false
          selfHeal   = true
          prune      = true
        }
        syncOptions = [
          ""CreateNamespace=true""
        ]
      }
    }
  }
}
",module,the block associated got renamed or deleted,,336,,23a76321726eca45b1852f9cbb9a5a46dd17c13e,9f09347de36de8f813051eae5c981dc8cae5c393,https://github.com/camptocamp/devops-stack/blob/23a76321726eca45b1852f9cbb9a5a46dd17c13e/examples/eks-aws/main.tf#L336,https://github.com/camptocamp/devops-stack/blob/9f09347de36de8f813051eae5c981dc8cae5c393/examples/eks/main.tf,2023-04-03 16:40:29+02:00,2023-08-18 14:48:32+02:00,4,1,1,1,0,0,1,0,1,0
https://github.com/ministryofjustice/aws-root-account,1,terraform/opg-roles.tf,terraform/opg-roles.tf,0,implemented,# Once SSO has been implemented we can get rid of these anyway,"# I've decided to hard code our ARNs, I don't want to make this repo depend on our accounts. 
 # Once SSO has been implemented we can get rid of these anyway","locals {
  # I've decided to hard code our ARNs, I don't want to make this repo depend on our accounts.
  # Once SSO has been implemented we can get rid of these anyway
  opg_engineers = [
    ""arn:aws:iam::631181914621:user/thomas.withers"",
    ""arn:aws:iam::631181914621:user/andrew.pearce"",
  ]
}
",locals,,,26,0.0,b0d4ef88fceb7d448a684c4480e756911c47f87d,432bff65375d36954c647b7a17f259addc2ccc40,https://github.com/ministryofjustice/aws-root-account/blob/b0d4ef88fceb7d448a684c4480e756911c47f87d/terraform/opg-roles.tf#L26,https://github.com/ministryofjustice/aws-root-account/blob/432bff65375d36954c647b7a17f259addc2ccc40/terraform/opg-roles.tf#L0,2020-09-30 14:06:38+01:00,2020-12-07 09:01:03+00:00,4,2,0,1,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,387,infra/aws/terraform/prow-build-cluster/iam.tf,infra/aws/terraform/prow-build-cluster/iam.tf,0,# todo,# TODO(pkprzekwas): remove after applying changes on prow-build-cluster,"/* 
 Copyright 2023 The Kubernetes Authors. 
  
 Licensed under the Apache License, Version 2.0 (the ""License""); 
 you may not use this file except in compliance with the License. 
 You may obtain a copy of the License at 
  
 http://www.apache.org/licenses/LICENSE-2.0 
  
 Unless required by applicable law or agreed to in writing, software 
 distributed under the License is distributed on an ""AS IS"" BASIS, 
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 See the License for the specific language governing permissions and 
 limitations under the License. 
 */  
 # TODO(pkprzekwas): remove after applying changes on prow-build-cluster","module ""iam"" {
  count = var.cluster_name == ""prow-build-cluster"" ? 1 : 0

  source = ""./modules/iam""

  eks_admins = var.eks_cluster_admins
}
",module,the block associated got renamed or deleted,,17,,d6f2d24051e866e453d920130509ce7b285311f9,1cdb9ebe968e412627b5334131803bbc8a9fecdb,https://github.com/kubernetes/k8s.io/blob/d6f2d24051e866e453d920130509ce7b285311f9/infra/aws/terraform/prow-build-cluster/iam.tf#L17,https://github.com/kubernetes/k8s.io/blob/1cdb9ebe968e412627b5334131803bbc8a9fecdb/infra/aws/terraform/prow-build-cluster/iam.tf,2023-04-27 18:01:09+02:00,2023-04-28 14:47:31+02:00,4,1,0,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,1456,infra/examples-dev/gcp/main.tf,infra/examples-dev/gcp/main.tf,0,# todo,# TODO: this has 5 remote modules combine some?,"# TODO: this has 5 remote modules; combine some? 
 #  eg, worklytics-connectors + gcp-host + worklytics-psoxy-connection-generic into a single 
 #     gcp-host-for-worklytics? poor TF style, but simplifies root module?  
 # in effect, these are for sources for which authentication/authorization cannot (or need not) 
 # be provisioned via Terraform, so doesn't add any dependencies 
 # call this 'generic_source_connectors'?","module ""worklytics_connectors"" {
  source = ""../../modules/worklytics-connectors""

  enabled_connectors    = var.enabled_connectors
  example_jira_issue_id = var.example_jira_issue_id
  jira_cloud_id         = var.jira_cloud_id
  jira_server_url       = var.jira_server_url
  salesforce_domain     = var.salesforce_domain
}
",module,"module ""worklytics_connectors"" {
  source = ""../../modules/worklytics-connectors""
  # source = ""git::https://github.com/worklytics/psoxy//infra/modules/worklytics-connectors?ref=v0.4.53""


  enabled_connectors               = var.enabled_connectors
  jira_cloud_id                    = var.jira_cloud_id
  jira_server_url                  = var.jira_server_url
  jira_example_issue_id            = var.jira_example_issue_id
  salesforce_domain                = var.salesforce_domain
  github_api_host                  = var.github_api_host
  github_enterprise_server_host    = var.github_enterprise_server_host
  github_enterprise_server_version = var.github_enterprise_server_version
  github_installation_id           = var.github_installation_id
  github_organization              = var.github_organization
  github_example_repository        = var.github_example_repository
  salesforce_example_account_id    = var.salesforce_example_account_id
}
",module,34,23.0,f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e,8f6786b90a7f0fcb6120cc8f822fd07cab49697f,https://github.com/Worklytics/psoxy/blob/f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e/infra/examples-dev/gcp/main.tf#L34,https://github.com/Worklytics/psoxy/blob/8f6786b90a7f0fcb6120cc8f822fd07cab49697f/infra/examples-dev/gcp/main.tf#L23,2023-06-16 14:08:45-07:00,2024-04-23 19:32:11-07:00,82,0,0,1,1,1,0,0,0,0
https://github.com/rust-lang/simpleinfra,24,terragrunt/modules/ecs-service/main.tf,terragrunt/modules/ecs-service/main.tf,0,// todo,// TODO: We assign a public IP address so that the service communicate,"// TODO: We assign a public IP address so that the service communicate 
 // to all the services it needs (e.g., SSM and ECR). Eventually, we'd 
 // like to shut down public access to the ecs service, but the work 
 // around is tediuous.","resource ""aws_ecs_service"" ""service"" {
  name             = var.name
  cluster          = var.cluster_config.cluster_id
  task_definition  = var.task_arn
  desired_count    = var.tasks_count
  launch_type      = ""FARGATE""
  platform_version = var.platform_version

  deployment_minimum_healthy_percent = var.deployment_minimum_healty_percent
  deployment_maximum_percent         = var.deployment_maximum_percent

  enable_ecs_managed_tags = true

  load_balancer {
    target_group_arn = aws_lb_target_group.service.arn
    container_name   = var.http_container
    container_port   = var.http_port
  }

  network_configuration {
    subnets = var.cluster_config.subnet_ids
    security_groups = concat(
      [var.cluster_config.service_security_group_id],
      var.additional_security_group_ids,
    )
    // TODO: We assign a public IP address so that the service communicate
    // to all the services it needs (e.g., SSM and ECR). Eventually, we'd
    // like to shut down public access to the ecs service, but the work
    // around is tediuous.
    assign_public_ip = true
  }
}
",resource,"resource ""aws_ecs_service"" ""service"" {
  name             = var.name
  cluster          = var.cluster_config.cluster_id
  task_definition  = var.task_arn
  desired_count    = var.tasks_count
  launch_type      = ""FARGATE""
  platform_version = var.platform_version

  deployment_minimum_healthy_percent = var.deployment_minimum_healty_percent
  deployment_maximum_percent         = var.deployment_maximum_percent

  enable_ecs_managed_tags = true

  load_balancer {
    target_group_arn = aws_lb_target_group.service.arn
    container_name   = var.http_container
    container_port   = var.http_port
  }

  network_configuration {
    subnets = var.cluster_config.subnet_ids
    security_groups = concat(
      [var.cluster_config.service_security_group_id],
      var.additional_security_group_ids,
    )
    // TODO: We assign a public IP address so that the service communicate
    // to all the services it needs (e.g., SSM and ECR). Eventually, we'd
    // like to shut down public access to the ecs service, but the work
    // around is tediuous.
    assign_public_ip = true
  }
}
",resource,26,26.0,4332ccbacf65427cb91e0e9b6f0c3ee53f37ea18,4332ccbacf65427cb91e0e9b6f0c3ee53f37ea18,https://github.com/rust-lang/simpleinfra/blob/4332ccbacf65427cb91e0e9b6f0c3ee53f37ea18/terragrunt/modules/ecs-service/main.tf#L26,https://github.com/rust-lang/simpleinfra/blob/4332ccbacf65427cb91e0e9b6f0c3ee53f37ea18/terragrunt/modules/ecs-service/main.tf#L26,2023-01-11 16:47:08+01:00,2023-01-11 16:47:08+01:00,1,0,1,1,0,1,1,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,343,azure/instances.tf,azure/instances.tf,0,todo,// TODO CHECK THIS group,// TODO CHECK THIS group,"resource ""azurerm_virtual_machine"" ""monitoring"" {
  name                  = ""${terraform.workspace}-monitoring""
  location              = var.az_region
  // TODO CHECK THIS group
  resource_group_name   = azurerm_resource_group.myrg.name
  // 
  network_interface_ids = [azurerm_network_interface.monitoring.id]
  availability_set_id   = azurerm_availability_set.myas.id
  vm_size               = ""Standard_D2s_v3""

  storage_os_disk {
    name              = ""iscsiOsDisk""
    caching           = ""ReadWrite""
    create_option     = ""FromImage""
    managed_disk_type = ""Premium_LRS""
  }

  storage_image_reference {
    id        = var.iscsi_srv_uri != """" ? join("","", azurerm_image.iscsi_srv.*.id) : """"
    publisher = var.iscsi_srv_uri != """" ? """" : ""SUSE""
    offer     = var.iscsi_srv_uri != """" ? """" : ""SLES-SAP-BYOS""
    sku       = var.iscsi_srv_uri != """" ? """" : ""12-sp4""
    version   = var.iscsi_srv_uri != """" ? """" : ""2019.03.06""
  }

  storage_data_disk {
    name              = ""iscsiDevices""
    caching           = ""ReadWrite""
    create_option     = ""Empty""
    disk_size_gb      = ""10""
    lun               = ""0""
    managed_disk_type = ""Standard_LRS""
  }

  os_profile {
    computer_name  = ""monitoring""
    admin_username = var.admin_user
  }

  os_profile_linux_config {
    disable_password_authentication = true

    ssh_keys {
      path     = ""/home/${var.admin_user}/.ssh/authorized_keys""
      key_data = file(var.public_key_location)
    }
  }

  boot_diagnostics {
    enabled     = ""true""
    storage_uri = azurerm_storage_account.mytfstorageacc.primary_blob_endpoint
  }

  tags = {
    workspace = terraform.workspace
  }
}
",resource,"resource ""azurerm_virtual_machine"" ""monitoring"" {
  name     = ""${terraform.workspace}-monitoring""
  location = var.az_region
  resource_group_name = azurerm_resource_group.myrg.name
  network_interface_ids = [azurerm_network_interface.monitoring.id]
  availability_set_id   = azurerm_availability_set.myas.id
  vm_size               = ""Standard_D2s_v3""

  storage_os_disk {
    name              = ""monitoringOsDisk""
    caching           = ""ReadWrite""
    create_option     = ""FromImage""
    managed_disk_type = ""Premium_LRS""
  }
  
  storage_image_reference {
    id        = azurerm_image.monitoring.0.id
    publisher = ""SUSE""
    offer     = ""SLES-SAP-BYOS""
    sku       = ""15""
    version   = ""2019.07.17""
  }

  storage_data_disk {
    name              = ""monitoringDevices""
    caching           = ""ReadWrite""
    create_option     = ""Empty""
    disk_size_gb      = ""10""
    lun               = ""0""
    managed_disk_type = ""Standard_LRS""
  }

  os_profile {
    computer_name  = ""monitoring""
    admin_username = var.admin_user
  }

  os_profile_linux_config {
    disable_password_authentication = true

    ssh_keys {
      path     = ""/home/${var.admin_user}/.ssh/authorized_keys""
      key_data = file(var.public_key_location)
    }
  }

  boot_diagnostics {
    enabled     = ""true""
    storage_uri = azurerm_storage_account.mytfstorageacc.primary_blob_endpoint
  }

  tags = {
    workspace = terraform.workspace
  }
}
",resource,138,,f41baea2a7a45b527e944b62bcab73612c693e02,9adcf152486838f9f5b600ead5e4ef373918a786,https://github.com/SUSE/ha-sap-terraform-deployments/blob/f41baea2a7a45b527e944b62bcab73612c693e02/azure/instances.tf#L138,https://github.com/SUSE/ha-sap-terraform-deployments/blob/9adcf152486838f9f5b600ead5e4ef373918a786/azure/instances.tf,2019-09-05 00:01:31+02:00,2019-09-05 18:08:13+02:00,5,1,1,0,0,1,0,0,0,1
https://github.com/alphagov/govuk-aws,129,terraform/projects/infra-security-groups/api.tf,terraform/projects/infra-security-groups/api.tf,0,# todo,# TODO: replace this with ingress from the api LBs when we build them.,# TODO: replace this with ingress from the api LBs when we build them.,"resource ""aws_security_group_rule"" ""allow_management_to_api_elb"" {
  type      = ""ingress""
  from_port = 443
  to_port   = 443
  protocol  = ""tcp""

  security_group_id        = ""${aws_security_group.api_elb.id}""
  source_security_group_id = ""${aws_security_group.management.id}""
}
",resource,the block associated got renamed or deleted,,47,,231e1767403d873f174f96001a2f84b9ecb2b12f,97bf0282ce176d8e44cf9475826feaa9094125e5,https://github.com/alphagov/govuk-aws/blob/231e1767403d873f174f96001a2f84b9ecb2b12f/terraform/projects/infra-security-groups/api.tf#L47,https://github.com/alphagov/govuk-aws/blob/97bf0282ce176d8e44cf9475826feaa9094125e5/terraform/projects/infra-security-groups/api.tf,2017-07-20 12:51:53+01:00,2017-08-14 11:25:10+01:00,2,1,0,1,0,1,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1763,modules/net-lb-app-ext-regional/main.tf,modules/net-lb-app-ext-regional/main.tf,0,# todo,# TODO(jccb): double check if this is true,"# external regional load balancer is always EXTERNAL_MANAGER. 
 # TODO(jccb): double check if this is true","resource ""google_compute_forwarding_rule"" ""default"" {
  provider    = google-beta
  project     = var.project_id
  name        = var.name
  region      = var.region
  description = var.description
  ip_address  = var.address
  ip_protocol = ""TCP""
  # external regional load balancer is always EXTERNAL_MANAGER.
  # TODO(jccb): double check if this is true
  load_balancing_scheme = ""EXTERNAL_MANAGED""
  port_range            = join("","", local.fwd_rule_ports)
  labels                = var.labels
  target                = local.fwd_rule_target
  network               = var.vpc
  # external regional app lb only supports standard tier
  network_tier = ""STANDARD""
}
",resource,"resource ""google_compute_forwarding_rule"" ""default"" {
  provider    = google-beta
  project     = var.project_id
  name        = var.name
  region      = var.region
  description = var.description
  ip_address  = var.address
  ip_protocol = ""TCP""
  # external regional load balancer is always EXTERNAL_MANAGER.
  # TODO(jccb): double check if this is true
  load_balancing_scheme = ""EXTERNAL_MANAGED""
  port_range            = join("","", local.fwd_rule_ports)
  labels                = var.labels
  target                = local.fwd_rule_target
  network               = var.vpc
  # external regional app lb only supports standard tier
  network_tier = ""STANDARD""
}
",resource,41,41.0,8beb621e070226b7f11a82807a706170ae7040ea,3af7e257d21f889ffaf7b32a3bab974fdbfda6e4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/8beb621e070226b7f11a82807a706170ae7040ea/modules/net-lb-app-ext-regional/main.tf#L41,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3af7e257d21f889ffaf7b32a3bab974fdbfda6e4/modules/net-lb-app-ext-regional/main.tf#L41,2024-01-05 16:59:27+01:00,2024-04-17 10:23:48+02:00,2,0,0,1,0,0,1,0,0,0
https://github.com/Azure/sap-automation,31,deploy/terraform/terraform-units/modules/sap_system/app_tier/vm-webdisp.tf,deploy/terraform/terraform-units/modules/sap_system/app_tier/vm-webdisp.tf,0,#todo,"#ToDo: Remove once feature is GA  patch_mode = ""Manual""","#ToDo: Remove once feature is GA  patch_mode = ""Manual""","resource ""azurerm_windows_virtual_machine"" ""web"" {
  provider            = azurerm.main
  count               = local.enable_deployment ? (upper(local.web_ostype) == ""WINDOWS"" ? local.webdispatcher_count : 0) : 0
  name                = format(""%s%s%s%s"", local.prefix, var.naming.separator, var.naming.virtualmachine_names.WEB_VMNAME[count.index], local.resource_suffixes.vm)
  computer_name       = var.naming.virtualmachine_names.WEB_COMPUTERNAME[count.index]
  location            = var.resource_group[0].location
  resource_group_name = var.resource_group[0].name

  //If no ppg defined do not put the web dispatchers in a proximity placement group
  proximity_placement_group_id = local.web_no_ppg ? (
    null) : (
    local.web_zonal_deployment ? var.ppg[count.index % max(local.web_zone_count, 1)].id : var.ppg[0].id
  )

  //If more than one servers are deployed into a single zone put them in an availability set and not a zone
  availability_set_id = local.use_web_avset ? azurerm_availability_set.web[count.index % max(local.web_zone_count, 1)].id : null

  //If length of zones > 1 distribute servers evenly across zones
  zone = local.use_web_avset ? null : local.web_zones[count.index % max(local.web_zone_count, 1)]

  network_interface_ids = var.application.dual_nics ? (
    var.options.legacy_nic_order ? (
      [azurerm_network_interface.web_admin[count.index].id, azurerm_network_interface.web[count.index].id]) : (
      [azurerm_network_interface.web[count.index].id, azurerm_network_interface.web_admin[count.index].id]
    )
    ) : (
    [azurerm_network_interface.web[count.index].id]
  )

  size           = local.web_sizing.compute.vm_size
  admin_username = var.sid_username
  admin_password = var.sid_password

  dynamic ""os_disk"" {
    iterator = disk
    for_each = flatten(
      [
        for storage_type in local.web_sizing.storage : [
          for disk_count in range(storage_type.count) :
          {
            name      = storage_type.name,
            id        = disk_count,
            disk_type = storage_type.disk_type,
            size_gb   = storage_type.size_gb,
            caching   = storage_type.caching
          }
        ]
        if storage_type.name == ""os""
      ]
    )

    content {
      name                   = format(""%s%s%s%s"", local.prefix, var.naming.separator, var.naming.virtualmachine_names.WEB_VMNAME[count.index], local.resource_suffixes.osdisk)
      caching                = disk.value.caching
      storage_account_type   = disk.value.disk_type
      disk_size_gb           = disk.value.size_gb
      disk_encryption_set_id = try(var.options.disk_encryption_set_id, null)
    }
  }

  source_image_id = local.web_custom_image ? local.web_os.source_image_id : null

  dynamic ""source_image_reference"" {
    for_each = range(local.web_custom_image ? 0 : 1)
    content {
      publisher = local.web_os.publisher
      offer     = local.web_os.offer
      sku       = local.web_os.sku
      version   = local.web_os.version
    }
  }

  boot_diagnostics {
    storage_account_uri = var.storage_bootdiag_endpoint
  }


  #ToDo: Remove once feature is GA  patch_mode = ""Manual""
  license_type = length(var.license_type) > 0 ? var.license_type : null

  tags = try(var.application.web_tags, {})
}
",resource,"resource ""azurerm_windows_virtual_machine"" ""web"" {
  provider                             = azurerm.main
  count                                = local.enable_deployment && upper(var.application_tier.web_os.os_type) == ""WINDOWS"" ? (
                                           local.webdispatcher_count) : (
                                           0
                                         )
  name                                 = format(""%s%s%s%s%s"",
                                           var.naming.resource_prefixes.vm,
                                           local.prefix,
                                           var.naming.separator,
                                           var.naming.virtualmachine_names.WEB_VMNAME[count.index],
                                           local.resource_suffixes.vm
                                         )
  computer_name                        = var.naming.virtualmachine_names.WEB_COMPUTERNAME[count.index]
  location                             = var.resource_group[0].location
  resource_group_name                  = var.resource_group[0].name

  proximity_placement_group_id         = var.application_tier.web_use_ppg ? (
                                           local.web_zonal_deployment ? var.ppg[count.index % max(local.web_zone_count, 1)] : var.ppg[0]) : (
                                           null
                                         )

  //If more than one servers are deployed into a single zone put them in an availability set and not a zone
  availability_set_id                  = local.use_web_avset ? (
                                           azurerm_availability_set.web[count.index % max(length(azurerm_availability_set.web), 1)].id
                                           ) : (
                                           null
                                         )

  virtual_machine_scale_set_id         = length(var.scale_set_id) > 0 ? var.scale_set_id : null

  //If length of zones > 1 distribute servers evenly across zones
  zone                                 = local.use_web_avset ? (
                                           null) : (
                                           try(local.web_zones[count.index % max(local.web_zone_count, 1)], null)
                                         )

  network_interface_ids                = var.application_tier.dual_nics ? (
                                           var.options.legacy_nic_order ? (
                                             [
                                               azurerm_network_interface.web_admin[count.index].id,
                                               azurerm_network_interface.web[count.index].id
                                             ]) : (
                                             [
                                               azurerm_network_interface.web[count.index].id,
                                               azurerm_network_interface.web_admin[count.index].id
                                             ]
                                           )
                                           ) : (
                                           [azurerm_network_interface.web[count.index].id]
                                         )

  size                                 = local.web_sizing.compute.vm_size
  admin_username                       = var.sid_username
  admin_password                       = var.sid_password

  source_image_id                      = var.application_tier.web_os.type == ""custom"" ? var.application_tier.web_os.source_image_id : null

  #ToDo: Remove once feature is GA  patch_mode = ""Manual""
  license_type                         = length(var.license_type) > 0 ? var.license_type : null
  # ToDo Add back later
# patch_mode                           = var.infrastructure.patch_mode

  tags                                 = merge(var.application_tier.web_tags, var.tags)

  dynamic ""os_disk"" {
                      iterator = disk
                      for_each = flatten(
                        [
                          for storage_type in local.web_sizing.storage : [
                            for disk_count in range(storage_type.count) :
                            {
                              name      = storage_type.name,
                              id        = disk_count,
                              disk_type = storage_type.disk_type,
                              size_gb   = storage_type.size_gb < 128 ? 128 : storage_type.size_gb,
                              caching   = storage_type.caching
                            }
                          ]
                          if storage_type.name == ""os""
                        ]
                      )

                      content {
                                name = format(""%s%s%s%s%s"",
                                  var.naming.resource_prefixes.osdisk,
                                  local.prefix,
                                  var.naming.separator,
                                  var.naming.virtualmachine_names.WEB_VMNAME[count.index],
                                  local.resource_suffixes.osdisk
                                )
                                caching                = disk.value.caching
                                storage_account_type   = disk.value.disk_type
                                disk_size_gb           = disk.value.size_gb
                                disk_encryption_set_id = try(var.options.disk_encryption_set_id, null)
                              }
                    }

  dynamic ""source_image_reference"" {
                                     for_each = range(var.application_tier.web_os.type == ""marketplace"" || var.application_tier.web_os.type == ""marketplace_with_plan"" ? 1 : 0)
                                     content {
                                               publisher = var.application_tier.web_os.publisher
                                               offer     = var.application_tier.web_os.offer
                                               sku       = var.application_tier.web_os.sku
                                               version   = var.application_tier.web_os.version
                                             }
                                   }
  dynamic ""plan"" {
                   for_each = range(var.application_tier.web_os.type == ""marketplace_with_plan"" ? 1 : 0)
                   content {
                             name      = var.application_tier.web_os.sku
                             publisher = var.application_tier.web_os.publisher
                             product   = var.application_tier.web_os.offer
                           }
                 }

  boot_diagnostics {
                     storage_account_uri = var.storage_bootdiag_endpoint
                   }
  dynamic ""identity""   {
                         for_each = range(length(var.application_tier.user_assigned_identity_id) > 0 ? 1 : 0)
                         content {
                                   type         = ""UserAssigned""
                                   identity_ids = [var.application_tier.user_assigned_identity_id]
                                 }
                       }
  lifecycle {
    ignore_changes = [
      source_image_id
    ]
  }

}
",resource,224,314.0,6ff0b891114c36d3aeccb850d830b698cd1fe52a,df063c58945a9efa2cb2ba303762c43f0b9c1d8f,https://github.com/Azure/sap-automation/blob/6ff0b891114c36d3aeccb850d830b698cd1fe52a/deploy/terraform/terraform-units/modules/sap_system/app_tier/vm-webdisp.tf#L224,https://github.com/Azure/sap-automation/blob/df063c58945a9efa2cb2ba303762c43f0b9c1d8f/deploy/terraform/terraform-units/modules/sap_system/app_tier/vm-webdisp.tf#L314,2021-11-17 19:29:07+02:00,2024-05-17 12:37:17+03:00,54,0,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,20,modules/dns/variables.tf,modules/dns/variables.tf,0,# todo,# TODO(ludoo): add support for forwarding path attribute,# TODO(ludoo): add support for forwarding path attribute,"variable ""forwarders"" {
  description = ""List of target name servers, only valid for 'forwarding' zone types.""
  type        = list(string)
  default     = []
}
",variable,"variable ""forwarders"" {
  description = ""Map of {IPV4_ADDRESS => FORWARDING_PATH} for 'forwarding' zone types. Path can be 'default', 'private', or null for provider default.""
  type        = map(string)
  default     = {}
}
",variable,59,,c486bfc66f9814e33b410602cb557a5e4d532912,27aa0aa64c7d3c3f9d35b2a2d54f8e95860cb913,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c486bfc66f9814e33b410602cb557a5e4d532912/modules/dns/variables.tf#L59,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/27aa0aa64c7d3c3f9d35b2a2d54f8e95860cb913/modules/dns/variables.tf,2020-04-03 14:06:48+02:00,2020-11-20 08:35:58+01:00,6,1,0,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,551,infra/modules/vault-psoxy/main.tf,infra/modules/vault-psoxy/main.tf,0,# todo,"# TODO: as of 4 Jan 2023, secret/* case needed here; please scope better for your prod use!!","# TODO: as of 4 Jan 2023, secret/* case needed here; please scope better for your prod use!!","resource ""vault_policy"" ""psoxy_instance"" {
  name   = var.instance_id

  # TODO: as of 4 Jan 2023, secret/* case needed here; please scope better for your prod use!!
  policy = <<EOT
path ""secret/*""
{
	capabilities = [""read""]
}
path ""${var.path_to_global_secrets}*""
{
	capabilities = [""read""]
}

path ""${local.path_to_instance_secrets}*""
{
    capabilities = [""create"", ""read"", ""update""]
}
EOT
}
",resource,"resource ""vault_policy"" ""psoxy_instance"" {
  name = var.instance_id

  # TODO: as of 4 Jan 2023, secret/* case needed here; please scope better for your prod use!!
  policy = <<EOT
path ""secret/*""
{
	capabilities = [""read""]
}
path ""${var.path_to_global_secrets}*""
{
	capabilities = [""read""]
}

path ""${local.path_to_instance_secrets}*""
{
    capabilities = [""create"", ""read"", ""update""]
}
EOT
}
",resource,12,12.0,516c6c050368e211634d3dd35047ab0e2fdc6b52,a71ebbfe78973520324010427ee3857040e087ee,https://github.com/Worklytics/psoxy/blob/516c6c050368e211634d3dd35047ab0e2fdc6b52/infra/modules/vault-psoxy/main.tf#L12,https://github.com/Worklytics/psoxy/blob/a71ebbfe78973520324010427ee3857040e087ee/infra/modules/vault-psoxy/main.tf#L12,2023-01-04 19:42:55-08:00,2023-01-13 11:00:08-08:00,2,0,0,1,0,1,0,1,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,21,main.tf,main.tf,0,#todo,"#TODO: terraform 0.12 will enable ""expiration_mode ? a_table_expiration : null"" (https://github.com/hashicorp/terraform/issues/17968)","#TODO: terraform 0.12 will enable ""expiration_mode ? a_table_expiration : null"" (https://github.com/hashicorp/terraform/issues/17968)","resource ""google_bigquery_dataset"" ""main"" {
  dataset_id    = ""${var.dataset_id}""
  friendly_name = ""${var.dataset_name}""
  description   = ""${var.description}""
  location      = ""${local.location}""

  #TODO: terraform 0.12 will enable ""expiration_mode ? a_table_expiration : null"" (https://github.com/hashicorp/terraform/issues/17968)
  default_table_expiration_ms = ""${var.expiration}""
  project                     = ""${var.project_id}""
  labels                      = ""${var.dataset_labels}""

  # TODO: revisit terraform 0.12 if arrays are available
  # access {
  #   role   = ""READER""
  #   domain = ""adigangi.com""
  # }
}
",resource,"resource ""google_bigquery_dataset"" ""main"" {
  dataset_id    = ""${var.dataset_id}""
  friendly_name = ""${var.dataset_name}""
  description   = ""${var.description}""
  location      = ""${local.location}""

  default_table_expiration_ms = ""${var.expiration}""
  project                     = ""${var.project_id}""
  labels                      = ""${var.dataset_labels}""
}
",resource,30,,7f922f7e9df197df38c9b09dfa6e3614d71f19f5,a7966ae4afc7bb73842fb9fd6f0f708b359cf36e,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/7f922f7e9df197df38c9b09dfa6e3614d71f19f5/main.tf#L30,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/a7966ae4afc7bb73842fb9fd6f0f708b359cf36e/main.tf,2019-01-16 18:10:54-05:00,2019-01-28 12:41:47-05:00,2,1,1,0,1,0,0,0,0,0
https://github.com/pingcap/tidb-operator,55,deploy/modules/aliyun/tidb-operator/operator.tf,deploy/modules/aliyun/tidb-operator/operator.tf,0,hack,"# Hack, instruct terraform that the kubeconfig_filename is only available until the k8s created","# Hack, instruct terraform that the kubeconfig_filename is only available until the k8s created","data ""template_file"" ""kubeconfig_filename"" {
  template = var.kubeconfig_file
  vars = {
    kubernetes_depedency = alicloud_cs_managed_kubernetes.k8s.client_cert
  }
}
",data,"data ""template_file"" ""kubeconfig_filename"" {
  template = var.kubeconfig_file
  vars = {
    kubernetes_dependency = alicloud_cs_managed_kubernetes.k8s.client_cert
  }
}
",data,1,1.0,042b1a97fbbdf342297002990564828e6644a3f0,32e1f58b34ea891d653e436f263485578df5ce63,https://github.com/pingcap/tidb-operator/blob/042b1a97fbbdf342297002990564828e6644a3f0/deploy/modules/aliyun/tidb-operator/operator.tf#L1,https://github.com/pingcap/tidb-operator/blob/32e1f58b34ea891d653e436f263485578df5ce63/deploy/modules/aliyun/tidb-operator/operator.tf#L1,2019-07-23 19:44:58+08:00,2020-07-23 15:01:18+08:00,6,0,1,1,1,0,0,0,0,0
https://github.com/Worklytics/psoxy,4989,infra/modules/gcp/main.tf,infra/modules/gcp/main.tf,0,# todo,"# TODO: This is will supported since 0.5 psoxy version, as google provider needs to be updated","# TODO: This is will supported since 0.5 psoxy version, as google provider needs to be updated 
 /*resource ""google_artifact_registry_repository"" ""psoxy-functions-repo"" { 
 location      = var.bucket_location 
 project       = var.project_id 
 repository_id = ""psoxy-functions"" 
 description   = ""Docker repository used on the cloud functions"" 
 format        = ""DOCKER"" 
  
 ## Not supported in current google providers, needs 5.14 as there it is GA 
 # See https://github.com/hashicorp/terraform-provider-google/blob/main/CHANGELOG.md#5140-jan-29-2024 
 # but even is present in the Documentation (https://registry.terraform.io/providers/hashicorp/google/4.80.0/docs/resources/artifact_registry_repository#argument-reference) 
 # when applied it throws an error with the message: ""An argument named ""cleanup_policy_dry_run"" is not expected here"" 
 # and ""no block for cleanup_policies"" is expected 
 *//*cleanup_policy_dry_run = false  
 # https://cloud.google.com/artifact-registry/docs/repositories/cleanup-policy#json_2 
 # https://registry.terraform.io/providers/hashicorp/google/4.80.0/docs/resources/artifact_registry_repository#argument-reference","resource ""google_secret_manager_secret"" ""pseudonym_salt"" {
  project   = var.project_id
  secret_id = ""${var.config_parameter_prefix}PSOXY_SALT""
  labels = merge(
    var.default_labels,
    {
      terraform_managed_value = true
    }
  )

  replication {
    user_managed {
      dynamic ""replicas"" {
        for_each = var.secret_replica_locations
        content {
          location = replicas.value
        }
      }
    }
  }

  lifecycle {
    ignore_changes = [
      replication, # can't change replication after creation
      labels
    ]
  }

  depends_on = [
    google_project_service.gcp_infra_api
  ]
}
",resource,"resource ""google_secret_manager_secret"" ""pseudonym_salt"" {
  project   = var.project_id
  secret_id = ""${var.config_parameter_prefix}PSOXY_SALT""
  labels = merge(
    var.default_labels,
    {
      terraform_managed_value = true
    }
  )

  replication {
    user_managed {
      dynamic ""replicas"" {
        for_each = var.secret_replica_locations
        content {
          location = replicas.value
        }
      }
    }
  }

  lifecycle {
    ignore_changes = [
      replication, # can't change replication after creation
      labels
    ]
  }

  depends_on = [
    google_project_service.gcp_infra_api
  ]
}
",resource,33,33.0,0db6077bf0549cf79dc2e0cb57563d5ac2453feb,0db6077bf0549cf79dc2e0cb57563d5ac2453feb,https://github.com/Worklytics/psoxy/blob/0db6077bf0549cf79dc2e0cb57563d5ac2453feb/infra/modules/gcp/main.tf#L33,https://github.com/Worklytics/psoxy/blob/0db6077bf0549cf79dc2e0cb57563d5ac2453feb/infra/modules/gcp/main.tf#L33,2024-04-03 10:54:59-07:00,2024-04-03 10:54:59-07:00,1,0,1,1,1,0,0,0,0,0
https://github.com/Worklytics/psoxy,1532,infra/modules/worklytics-connectors-msft-365/outputs.tf,infra/modules/worklytics-connectors-msft-365/outputs.tf,0,fix,# TODO: fix this. tf complain is:,"# TODO: fix this. tf complain is: 
 #       while calling max(numbers...) 
 #       local.next_todo_steps is empty list of dynamic 
 #        var.todo_step is 1 ","output ""next_todo_step"" {
  # TODO: fix this. tf complain is:
  #       while calling max(numbers...)
  #       local.next_todo_steps is empty list of dynamic
  #        var.todo_step is 1

  value = try(max(concat([var.todo_step], local.next_todo_steps)), var.todo_step + 1)
}
",output,"output ""next_todo_step"" {
  # TODO: fix this. tf complain is:
  #       while calling max(numbers...)
  #       local.next_todo_steps is empty list of dynamic
  #        var.todo_step is 1

  value = try(max(concat([var.todo_step], local.next_todo_steps)), var.todo_step + 1)
}
",output,18,18.0,f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e,69a63fd34e47423da0c3cd430eaccbc15d7286af,https://github.com/Worklytics/psoxy/blob/f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e/infra/modules/worklytics-connectors-msft-365/outputs.tf#L18,https://github.com/Worklytics/psoxy/blob/69a63fd34e47423da0c3cd430eaccbc15d7286af/infra/modules/worklytics-connectors-msft-365/outputs.tf#L18,2023-06-16 14:08:45-07:00,2023-08-25 09:02:49-07:00,3,0,0,1,1,0,0,0,0,0
https://github.com/ministryofjustice/modernisation-platform,197,terraform/environments/data-platform-apps-and-tools/eks-iam-roles.tf,terraform/environments/data-platform-apps-and-tools/eks-iam-roles.tf,0,// todo,// TODO: define SecretsManager path for cluster consumed secrets,"// TODO: define SecretsManager path for cluster consumed secrets 
 // external_secrets_secrets_manager_arns = [] ","module ""external_secrets_role"" {
  #checkov:skip=CKV_TF_1:Module is from Terraform registry

  source  = ""terraform-aws-modules/iam/aws//modules/iam-role-for-service-accounts-eks""
  version = ""~> 5.0""

  role_name_prefix               = ""external-secrets""
  attach_external_secrets_policy = true

  // TODO: define SecretsManager path for cluster consumed secrets
  // external_secrets_secrets_manager_arns = []

  oidc_providers = {
    main = {
      provider_arn               = module.eks.oidc_provider_arn
      namespace_service_accounts = [""${kubernetes_namespace.external_secrets.metadata[0].name}:external-secrets""]
    }
  }

  tags = local.tags
}
",module,,,128,0.0,90002b7667baf7c1eb23d0e49dcfa9d9eaee7567,951ffdb805c4255e3bb6d0a5febe5f1bb21407c9,https://github.com/ministryofjustice/modernisation-platform/blob/90002b7667baf7c1eb23d0e49dcfa9d9eaee7567/terraform/environments/data-platform-apps-and-tools/eks-iam-roles.tf#L128,https://github.com/ministryofjustice/modernisation-platform/blob/951ffdb805c4255e3bb6d0a5febe5f1bb21407c9/terraform/environments/data-platform-apps-and-tools/eks-iam-roles.tf#L0,2023-10-18 15:43:32+01:00,2023-12-19 15:49:47+00:00,4,2,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd,124,modules/gke-cloudbuild-private-pool/main.tf,modules/gke-cloudbuild-private-pool/main.tf,0,# todo,# TODO: for_each --> 1 module per destination GKE VPC,# TODO: for_each --> 1 module per destination GKE VPC,"module ""vpn_ha-2"" {
  # TODO: for_each --> 1 module per destination GKE VPC
  source     = ""terraform-google-modules/vpn/google//modules/vpn_ha""
  version    = ""~> 1.3.0""
  project_id = var.project_id
  region     = var.location
  network    = ""https://www.googleapis.com/compute/v1/projects/<PROJECT_ID>/global/networks/local-network"" ## TODO: GKE network self_link
  name       = ""gke-to-cloudbuild""
  router_asn = 64513
  peer_gcp_gateway = module.vpn_ha-1.self_link
  tunnels = {
    remote-0 = {
      bgp_peer = {
        address = ""169.254.1.2""
        asn     = 64514
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.1.1/30""
      ike_version       = 2
      vpn_gateway_interface = 0
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1.random_secret
    }
    remote-1 = {
      bgp_peer = {
        address = ""169.254.2.2""
        asn     = 64514
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.2.1/30""
      ike_version       = 2
      vpn_gateway_interface = 1
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1.random_secret
    }
  }
}",module,"module ""vpn_ha-2"" {
  count = length(local.gke_networks)

  source     = ""terraform-google-modules/vpn/google//modules/vpn_ha""
  version    = ""~> 1.3.0""
  project_id = local.gke_networks[count.index].project_id
  region     = local.gke_networks[count.index].location
  network    = local.gke_networks[count.index].network 
  name       = ""${local.gke_networks[count.index].network}-to-cloudbuild""
  router_asn = 65002+(count.index*2)
  peer_gcp_gateway = module.vpn_ha-1[count.index].self_link
  tunnels = {
    remote-0 = {
      bgp_peer = {
        address = ""169.254.${1+(count.index*2)}.1""
        asn     = 65001+(count.index*2)
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.${1+(count.index*2)}.2/30""
      ike_version       = 2
      vpn_gateway_interface = 0
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1[count.index].random_secret
    }
    remote-1 = {
      bgp_peer = {
        address = ""169.254.${2+(count.index*2)}.1""
        asn     = 65001+(count.index*2)
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.${2+(count.index*2)}.2/30""
      ike_version       = 2
      vpn_gateway_interface = 1
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1[count.index].random_secret
    }
  }
}",module,103,,a384bc29c9bcb80dd1b1f60ece9dee723b4bf378,e29ac91f2eedf8a48e82065434b81010d298a423,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/a384bc29c9bcb80dd1b1f60ece9dee723b4bf378/modules/gke-cloudbuild-private-pool/main.tf#L103,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/e29ac91f2eedf8a48e82065434b81010d298a423/modules/gke-cloudbuild-private-pool/main.tf,2021-11-30 12:07:36-06:00,2021-12-06 17:46:36-06:00,2,1,0,1,0,0,1,0,0,0
https://github.com/google/go-cloud,1,samples/guestbook/aws/main.tf,samples/guestbook/aws/main.tf,0,# todo,# TODO(light): Reuse credentials from Terraform.,# TODO(light): Reuse credentials from Terraform.,"resource ""aws_db_instance"" ""guestbook"" {
  identifier_prefix      = ""guestbook""
  engine                 = ""mysql""
  engine_version         = ""5.6.39""
  instance_class         = ""db.t2.micro""
  allocated_storage      = 20
  username               = ""root""
  password               = ""${random_string.db_password.result}""
  name                   = ""guestbook""
  publicly_accessible    = true
  vpc_security_group_ids = [""${aws_security_group.guestbook.id}""]
  skip_final_snapshot    = true

  provisioner ""local-exec"" {
    # TODO(light): Reuse credentials from Terraform.
    command = ""cat '${path.module}'/../schema.sql '${path.module}'/../roles.sql | '${path.module}'/provision-db.sh '${aws_db_instance.guestbook.address}' '${aws_security_group.guestbook.id}' guestbook '${random_string.db_password.result}'""
  }
}
",resource,"resource ""aws_db_instance"" ""guestbook"" {
  identifier_prefix      = ""guestbook""
  engine                 = ""mysql""
  engine_version         = ""5.6.39""
  instance_class         = ""db.t2.micro""
  allocated_storage      = 20
  username               = ""root""
  password               = random_string.db_password.result
  name                   = ""guestbook""
  publicly_accessible    = true
  vpc_security_group_ids = [aws_security_group.guestbook.id]
  skip_final_snapshot    = true

  provisioner ""local-exec"" {
    # TODO(light): Reuse credentials from Terraform.
    command = ""go run '${path.module}'/provision_db/main.go -host='${aws_db_instance.guestbook.address}' -region='${var.region}' -security_group='${aws_security_group.guestbook.id}' -database=guestbook -password='${random_string.db_password.result}' -schema='${path.module}'/../schema.sql""
  }
}
",resource,84,88.0,2f807406236dfb6b662001bea3c3dabe0dc79b1c,9b5268f0a2b98af6ac6714a0013f9ccd264b1606,https://github.com/google/go-cloud/blob/2f807406236dfb6b662001bea3c3dabe0dc79b1c/samples/guestbook/aws/main.tf#L84,https://github.com/google/go-cloud/blob/9b5268f0a2b98af6ac6714a0013f9ccd264b1606/samples/guestbook/aws/main.tf#L88,2018-06-14 14:45:08-07:00,2019-07-02 13:57:39-07:00,9,0,1,1,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,132,infra/gcp/clusters/projects/kubernetes-public/aaa/k8s-infra-prow.tf,infra/gcp/terraform/kubernetes-public/k8s-infra-prow.tf,1,//todo,//TODO(ameukam): switch to allUsers when https://github.com/kubernetes/k8s.io/issues/752 is closed.,//TODO(ameukam): switch to allUsers when https://github.com/kubernetes/k8s.io/issues/752 is closed.,"resource ""google_storage_bucket_iam_member"" ""k8s_infra_prow_oncall"" {
  bucket = google_storage_bucket.k8s_infra_prow_bucket.name
  role   = ""roles/storage.objectViewer""
  //TODO(ameukam): switch to allUsers when https://github.com/kubernetes/k8s.io/issues/752 is closed.
  member = ""group:k8s-infra-prow-oncall@kubernetes.io""
}
",resource,"resource ""google_storage_bucket_iam_member"" ""k8s_infra_prow_owners"" {
  bucket = google_storage_bucket.k8s_infra_prow_bucket.name
  role   = ""roles/storage.objectViewer""
  //TODO(ameukam): switch to allUsers when https://github.com/kubernetes/k8s.io/issues/752 is closed.
  member = ""group:${local.prow_owners}""
}
",resource,82,107.0,a4f7666b489b8f409f0edc3c728946e839b70879,5e69979eb5251d9a1ad6ca6ec8856c99a5927034,https://github.com/kubernetes/k8s.io/blob/a4f7666b489b8f409f0edc3c728946e839b70879/infra/gcp/clusters/projects/kubernetes-public/aaa/k8s-infra-prow.tf#L82,https://github.com/kubernetes/k8s.io/blob/5e69979eb5251d9a1ad6ca6ec8856c99a5927034/infra/gcp/terraform/kubernetes-public/k8s-infra-prow.tf#L107,2021-06-11 23:05:47+02:00,2024-01-03 18:16:49+00:00,9,0,1,1,1,1,0,0,0,0
https://github.com/compiler-explorer/infra,22,terraform/dynamodb.tf,terraform/dynamodb.tf,0,// todo,// TODO: change once terraform supports on-demand pricing. We are currently set to use,"// TODO: change once terraform supports on-demand pricing. We are currently set to use 
 // on-demand in the UI only.","resource ""aws_dynamodb_table"" ""links"" {
  name = ""links""
  lifecycle {
    ignore_changes = [
      ""read_capacity"",
      ""write_capacity""
    ]
  }
  // TODO: change once terraform supports on-demand pricing. We are currently set to use
  // on-demand in the UI only.
  read_capacity = 1
  write_capacity = 1
  hash_key = ""prefix""
  range_key = ""unique_subhash""

  attribute = [
    {
      name = ""prefix""
      type = ""S""
    },
    {
      name = ""unique_subhash""
      type = ""S""
    }
  ]

  point_in_time_recovery {
    enabled = true
  }

  tags {
    key = ""Site""
    value = ""CompilerExplorer""
  }
}
",resource,"resource ""aws_dynamodb_table"" ""links"" {
  name = ""links""
  lifecycle {
    ignore_changes = [
      ""read_capacity"",
      ""write_capacity""
    ]
  }
  billing_mode = ""PAY_PER_REQUEST""
  read_capacity = 1
  write_capacity = 1
  hash_key = ""prefix""
  range_key = ""unique_subhash""

  attribute = [
    {
      name = ""prefix""
      type = ""S""
    },
    {
      name = ""unique_subhash""
      type = ""S""
    }
  ]

  point_in_time_recovery {
    enabled = true
  }

  tags {
    key = ""Site""
    value = ""CompilerExplorer""
  }
}
",resource,9,,6701d62eda6ae70b38f4faa908ae23ce2662103f,79bb0fbfeb42154db0aa14f7ecb1d69b60197545,https://github.com/compiler-explorer/infra/blob/6701d62eda6ae70b38f4faa908ae23ce2662103f/terraform/dynamodb.tf#L9,https://github.com/compiler-explorer/infra/blob/79bb0fbfeb42154db0aa14f7ecb1d69b60197545/terraform/dynamodb.tf,2018-12-01 08:04:08-06:00,2019-03-04 09:56:50-06:00,2,1,1,1,1,0,0,0,0,0
https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules,49,modules/vmss/outputs.tf,modules/vmss/outputs.tf,0,todo,"# tflint-ignore: terraform_naming_convention # TODO rename to scale_set_name, but bundle with next breaking change","# tflint-ignore: terraform_naming_convention # TODO rename to scale_set_name, but bundle with next breaking change","output ""inbound-scale-set-name"" {
  description = ""Name of inbound scale set.""
  value       = azurerm_virtual_machine_scale_set.this.name
}
",output,the block associated got renamed or deleted,,1,,486a451e0009a3a296ff1d2c9683aede8176c3e9,d43a5b9c4a5127efc4f2d8c82c814e0e6a2bf796,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/486a451e0009a3a296ff1d2c9683aede8176c3e9/modules/vmss/outputs.tf#L1,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/d43a5b9c4a5127efc4f2d8c82c814e0e6a2bf796/modules/vmss/outputs.tf,2021-05-21 15:58:34+02:00,2021-09-17 16:38:00+02:00,2,1,0,1,0,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,199,module-iam.tf,module-iam.tf,0,implementation,# Default IAM sub-module implementation for OKE cluster,# Default IAM sub-module implementation for OKE cluster,"module ""iam"" {
  source                   = ""./modules/iam""
  compartment_id           = local.compartment_id
  state_id                 = random_id.state_id.id
  tenancy_id               = local.tenancy_id
  cluster_id               = local.cluster_id
  create_autoscaler_policy = local.create_autoscaler_policy
  create_kms_policy        = local.create_kms_policy
  create_operator_policy   = local.create_operator_policy
  create_worker_policy     = local.create_worker_policy

  create_tag_namespace = var.create_tag_namespace
  create_defined_tags  = var.create_defined_tags
  defined_tags         = lookup(var.defined_tags, ""policy"", {})
  freeform_tags        = lookup(var.freeform_tags, ""policy"", {})
  tag_namespace        = var.tag_namespace
  use_defined_tags     = var.use_defined_tags

  cluster_kms_key_id         = var.cluster_kms_key_id
  operator_volume_kms_key_id = var.operator_volume_kms_key_id
  worker_volume_kms_key_id   = var.worker_volume_kms_key_id

  autoscaler_compartments = local.autoscaler_compartments
  worker_compartments     = local.worker_compartments

  providers = {
    oci.home = oci.home
  }
}
",module,"module ""iam"" {
  source                       = ""./modules/iam""
  compartment_id               = local.compartment_id
  state_id                     = local.state_id
  tenancy_id                   = local.tenancy_id
  cluster_id                   = local.cluster_id
  create_iam_resources         = var.create_iam_resources
  create_iam_autoscaler_policy = local.create_iam_autoscaler_policy
  create_iam_kms_policy        = local.create_iam_kms_policy
  create_iam_operator_policy   = local.create_iam_operator_policy
  create_iam_worker_policy     = local.create_iam_worker_policy

  create_iam_tag_namespace = var.create_iam_tag_namespace
  create_iam_defined_tags  = var.create_iam_defined_tags
  defined_tags             = local.iam_defined_tags
  freeform_tags            = local.iam_freeform_tags
  tag_namespace            = var.tag_namespace
  use_defined_tags         = var.use_defined_tags

  cluster_kms_key_id         = var.cluster_kms_key_id
  operator_volume_kms_key_id = var.operator_volume_kms_key_id
  worker_volume_kms_key_id   = var.worker_volume_kms_key_id

  autoscaler_compartments = local.autoscaler_compartments
  worker_compartments     = local.worker_compartments

  providers = {
    oci.home = oci.home
  }
}
",module,35,53.0,6c867cd8e9cbf559742f56658989bcded0d1fd89,e2ac866a96bd7171c980727c46078cc438643225,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/module-iam.tf#L35,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/e2ac866a96bd7171c980727c46078cc438643225/module-iam.tf#L53,2023-10-25 16:40:02+11:00,2024-03-28 20:16:45+11:00,9,0,0,0,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,125,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/controller.tf,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/controller.tf,0,# todo,# TODO: add support for proper access_config,# TODO: add support for proper access_config,"locals {
  # TODO: add support for proper access_config
  access_config = {
    nat_ip       = null
    network_tier = ""STANDARD""
  }
}
",locals,"locals {
  # TODO: add support for proper access_config
  access_config = {
    nat_ip       = null
    network_tier = null
  }
}
",locals,88,90.0,c47d346676eb04eb46385f018a745fb865c081a0,367dbbc03d00a523c457d066229e6c9376bcd5c9,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/c47d346676eb04eb46385f018a745fb865c081a0/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/controller.tf#L88,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/367dbbc03d00a523c457d066229e6c9376bcd5c9/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/controller.tf#L90,2023-11-15 01:15:19+00:00,2024-05-01 19:59:12+00:00,23,0,0,1,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,429,infra/aws/terraform/kops-infra-ci/terraform.tf,infra/aws/terraform/kops-infra-ci/terraform.tf,0,// todo,// TODO(ameukam): stop used hardcoded account id. Preferably use SSO user,// TODO(ameukam): stop used hardcoded account id. Preferably use SSO user,"terraform {
  backend ""s3"" {
    bucket = ""k8s-infra-kops-ci-tf-state""
    region = ""us-east-2""
    key    = ""kops-infra-ci/terraform.tfstate""
    // TODO(ameukam): stop used hardcoded account id. Preferably use SSO user
    role_arn     = ""arn:aws:iam::808842816990:role/OrganizationAccountAccessRole""
    session_name = ""kops-infra-ci""
  }

  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.11.0""
    }
  }
}
",terraform,"terraform {
  backend ""s3"" {
    bucket = ""k8s-infra-kops-ci-tf-state""
    region = ""us-east-2""
    key    = ""kops-infra-ci/terraform.tfstate""
    // TODO(ameukam): stop used hardcoded account id. Preferably use SSO user
    role_arn     = ""arn:aws:iam::808842816990:role/OrganizationAccountAccessRole""
    session_name = ""kops-infra-ci""
  }

  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.40.0""
    }
  }
}
",terraform,22,22.0,fdb6862000701a42dfee5eacf413cf4bdc633ccb,d67626296482f3df01968377c828ffac093efee8,https://github.com/kubernetes/k8s.io/blob/fdb6862000701a42dfee5eacf413cf4bdc633ccb/infra/aws/terraform/kops-infra-ci/terraform.tf#L22,https://github.com/kubernetes/k8s.io/blob/d67626296482f3df01968377c828ffac093efee8/infra/aws/terraform/kops-infra-ci/terraform.tf#L22,2023-08-08 15:17:59+02:00,2024-03-12 17:14:02+01:00,3,0,1,1,0,1,0,0,0,0
https://github.com/chanzuckerberg/cztack,4,aws-params-writer/variables.tf,aws-params-writer/variables.tf,0,// todo,// TODO(el): Remove once tf 0.12 is released,// TODO(el): Remove once tf 0.12 is released,"variable ""parameters_count"" {
  type        = ""string""
  description = ""HACK: The number of keys in var.parameters. To avoid hitting value of count cannot be computed.""
}
",variable,"variable ""parameters_count"" {
  type        = string
  description = ""HACK: The number of keys in var.parameters. To avoid hitting value of count cannot be computed.""
}
",variable,26,26.0,9d5798e3a0ff47602b7db6343dfd114cdbf5c8fa,9df439500dee7468643ca03a844cf7a5b1e1b313,https://github.com/chanzuckerberg/cztack/blob/9d5798e3a0ff47602b7db6343dfd114cdbf5c8fa/aws-params-writer/variables.tf#L26,https://github.com/chanzuckerberg/cztack/blob/9df439500dee7468643ca03a844cf7a5b1e1b313/aws-params-writer/variables.tf#L26,2019-04-22 16:11:24-07:00,2021-04-13 14:52:04-04:00,3,0,0,1,1,0,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,3,eks.tf,eks.tf,0,todo,# TODO Create New SecGroup for each node group,worker_security_group_id  = module.eks.worker_security_group_id # TODO Create New SecGroup for each node group,"module ""managed-node-groups"" {
  for_each = var.managed_node_groups

  source     = ""./modules/aws-eks-managed-node-groups""
  managed_ng = each.value

  eks_cluster_name          = module.eks.cluster_id
  private_subnet_ids        = var.create_vpc == false ? var.private_subnet_ids : module.vpc.private_subnets
  public_subnet_ids         = var.create_vpc == false ? var.public_subnet_ids : module.vpc.public_subnets
  cluster_ca_base64         = module.eks.cluster_certificate_authority_data
  cluster_endpoint          = module.eks.cluster_endpoint
  cluster_autoscaler_enable = var.cluster_autoscaler_enable
  worker_security_group_id  = module.eks.worker_security_group_id # TODO Create New SecGroup for each node group
  tags                      = module.eks-label.tags

  depends_on = [module.eks]

}
",module,the block associated got renamed or deleted,,145,,50f6e2c2dcd3479177d5c5001732c013be2fe6de,354afae1274daf6f39d01eafa51b7c87a3533cb4,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/50f6e2c2dcd3479177d5c5001732c013be2fe6de/eks.tf#L145,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/354afae1274daf6f39d01eafa51b7c87a3533cb4/eks.tf,2021-08-27 00:35:58+01:00,2021-08-27 17:16:49+01:00,2,1,1,1,0,1,0,0,0,0
https://github.com/rust-lang/simpleinfra,20,terragrunt/modules/docs-rs/web-server.tf,terragrunt/modules/docs-rs/web-server.tf,0,# todo,# TODO: ensure that this is a secret in the SSM store,"# TODO: ensure that this is a secret in the SSM store 
 # DOCSRS_GITHUB_ACCESSTOKEN = ""/prod/docs-rs/github-access-token""","module ""web"" {
  source         = ""../ecs-app""
  cluster_config = var.cluster_config

  env  = ""dev""
  name = ""docs-rs-web""
  repo = ""rust-lang/docs.rs""

  cpu                  = 256
  memory               = 512
  tasks_count          = 1
  platform_version     = ""1.4.0""
  ephemeral_storage_gb = 40

  environment = {
    DOCSRS_PREFIX               = ""/tmp""
    DOCSRS_STORAGE_BACKEND      = ""s3""
    DOCSRS_LOG                  = ""docs_rs=debug,rustwide=info""
    RUST_BACKTRACE              = ""1""
    DOCSRS_STATIC_CLOUDFRONT_ID = ""${aws_cloudfront_distribution.static.id}""
  }

  secrets = {
    # TODO: ensure that this is a secret in the SSM store
    # DOCSRS_GITHUB_ACCESSTOKEN = ""/prod/docs-rs/github-access-token""
  }

  computed_secrets = {
    DOCSRS_DATABASE_URL = aws_ssm_parameter.connection_url.arn
  }

  expose_http = {
    container_port = 80
    prometheus     = ""/about/metrics""
    domains        = [local.web_domain]
    zone_id        = var.zone_id

    health_check_path     = ""/""
    health_check_interval = 5
    health_check_timeout  = 2
  }

  # Allow database access
  additional_security_group_ids = [aws_security_group.web.id]
}
",module,"module ""web"" {
  source         = ""../ecs-app""
  cluster_config = var.cluster_config

  env  = ""dev""
  name = ""docs-rs-web""
  repo = ""rust-lang/docs.rs""

  cpu                  = 256
  memory               = 512
  tasks_count          = 1
  platform_version     = ""1.4.0""
  ephemeral_storage_gb = 40

  environment = {
    DOCSRS_PREFIX               = ""/tmp""
    DOCSRS_STORAGE_BACKEND      = ""s3""
    DOCSRS_LOG                  = ""docs_rs=debug,rustwide=info""
    RUST_BACKTRACE              = ""1""
    DOCSRS_STATIC_CLOUDFRONT_ID = ""${aws_cloudfront_distribution.static.id}""
  }

  secrets = {
    DOCSRS_GITHUB_ACCESSTOKEN = ""/prod/docs-rs/github-access-token""
  }

  computed_secrets = {
    DOCSRS_DATABASE_URL = aws_ssm_parameter.connection_url.arn
  }

  expose_http = {
    container_port = 80
    prometheus     = ""/about/metrics""
    domains        = [local.web_domain]
    zone_id        = var.zone_id

    health_check_path     = ""/""
    health_check_interval = 5
    health_check_timeout  = 2
  }

  # Allow database access
  additional_security_group_ids = [aws_security_group.web.id]
}
",module,33,,8d588e18da3b236eaa53a9920a5af7219450cf75,cbd55946ef1f6ed122e73fa26ef14ec814f100c0,https://github.com/rust-lang/simpleinfra/blob/8d588e18da3b236eaa53a9920a5af7219450cf75/terragrunt/modules/docs-rs/web-server.tf#L33,https://github.com/rust-lang/simpleinfra/blob/cbd55946ef1f6ed122e73fa26ef14ec814f100c0/terragrunt/modules/docs-rs/web-server.tf,2023-01-10 20:16:01+01:00,2023-01-11 16:41:27+01:00,2,1,0,1,0,1,0,0,0,1
https://github.com/alphagov/govuk-aws,926,terraform/projects/infra-public-services/waf.tf,terraform/projects/infra-public-services/waf.tf,0,fix,# FIXME: Change this to BLOCK after 25th July 2019,"type = ""ALLOW"" # FIXME: Change this to BLOCK after 25th July 2019","resource ""aws_wafregional_web_acl"" ""default"" {
  name        = ""CachePublicWebACL""
  metric_name = ""CachePublicWebACL""

  default_action {
    type = ""ALLOW""
  }

  rule {
    action {
      type = ""BLOCK""
    }

    priority = 2
    rule_id  = ""${aws_wafregional_rule.x_always_block.id}""
  }

  rule {
    action {
      type = ""ALLOW"" # FIXME: Change this to BLOCK after 25th July 2019
    }

    priority = 3
    rule_id  = ""${aws_wafregional_rule.sqli.id}""
  }

  logging_configuration {
    log_destination = ""${aws_kinesis_firehose_delivery_stream.splunk.arn}""

    redacted_fields {
      field_to_match {
        type = ""URI""
      }

      field_to_match {
        data = ""referer""
        type = ""HEADER""
      }
    }
  }

  depends_on = [
    ""aws_wafregional_rule.x_always_block"",
    ""aws_wafregional_rule.sqli"",
  ]
}
",resource,"resource ""aws_wafregional_web_acl"" ""default"" {
  name        = ""CachePublicWebACL""
  metric_name = ""CachePublicWebACL""

  default_action {
    type = ""ALLOW""
  }

  rule {
    action {
      type = ""BLOCK""
    }

    priority = 2
    rule_id  = ""${aws_wafregional_rule.x_always_block.id}""
  }

  logging_configuration {
    log_destination = ""${aws_kinesis_firehose_delivery_stream.splunk.arn}""
  }

  depends_on = [
    ""aws_wafregional_rule.x_always_block"",
  ]
}
",resource,20,,8c2729fb92978bd4a778234d756943d31428c1e3,c1ecdd9a87ba22fcb5c0585636e4ebbe527f0137,https://github.com/alphagov/govuk-aws/blob/8c2729fb92978bd4a778234d756943d31428c1e3/terraform/projects/infra-public-services/waf.tf#L20,https://github.com/alphagov/govuk-aws/blob/c1ecdd9a87ba22fcb5c0585636e4ebbe527f0137/terraform/projects/infra-public-services/waf.tf,2019-07-22 16:00:21+01:00,2020-12-02 18:07:36+00:00,7,1,0,1,0,1,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,329,examples/oke-network-only/main.tf,examples/rms/oke-network-only/main.tf,1,todo,# TODO Remaining tags in schema,defined_tags = { # TODO Remaining tags in schema,"module ""oke"" {
  source         = ""github.com/devoncrouse/terraform-oci-oke.git?ref=5.x-stack&depth=1""
  providers      = { oci.home = oci.home }
  tenancy_id     = var.tenancy_ocid
  compartment_id = var.compartment_ocid

  # Identity
  create_iam_resources     = true
  create_iam_tag_namespace = var.create_iam_tag_namespace
  create_iam_defined_tags  = var.create_iam_tag_namespace || var.create_iam_defined_tags
  use_defined_tags         = var.use_defined_tags
  tag_namespace            = var.tag_namespace

  # Network
  create_vcn                  = var.create_vcn
  vcn_id                      = var.vcn_id
  vcn_cidrs                   = split("","", var.vcn_cidrs)
  vcn_create_internet_gateway = var.vcn_create_internet_gateway ? ""always"" : ""never""
  vcn_create_nat_gateway      = var.vcn_create_nat_gateway ? ""always"" : ""never""
  vcn_create_service_gateway  = var.vcn_create_service_gateway ? ""always"" : ""never""
  vcn_name                    = var.vcn_name
  vcn_dns_label               = var.vcn_dns_label
  assign_dns                  = var.assign_dns
  ig_route_table_id           = var.ig_route_table_id
  local_peering_gateways      = var.local_peering_gateways
  lockdown_default_seclist    = var.lockdown_default_seclist
  nat_gateway_public_ip_id    = var.nat_gateway_public_ip_id
  nat_route_table_id          = var.nat_route_table_id
  create_drg                  = var.create_drg
  drg_id                      = var.drg_id
  drg_display_name            = var.drg_display_name

  subnets = {
    bastion  = { create = var.bastion_subnet_create ? ""always"" : ""never"", newbits = var.bastion_subnet_newbits, id = var.bastion_subnet_id }
    operator = { create = var.operator_subnet_create ? ""always"" : ""never"", newbits = var.operator_subnet_newbits, id = var.operator_subnet_id }
    cp       = { create = var.control_plane_subnet_create ? ""always"" : ""never"", newbits = var.control_plane_subnet_newbits, id = var.control_plane_subnet_id }
    int_lb   = { create = var.int_lb_subnet_create ? ""always"" : ""never"", newbits = var.int_lb_subnet_newbits, id = var.int_lb_subnet_id }
    pub_lb   = { create = var.pub_lb_subnet_create ? ""always"" : ""never"", newbits = var.pub_lb_subnet_newbits, id = var.pub_lb_subnet_id }
    workers  = { create = var.worker_subnet_create ? ""always"" : ""never"", newbits = var.worker_subnet_newbits, id = var.worker_subnet_id }
    pods     = { create = var.pod_subnet_create ? ""always"" : ""never"", newbits = var.pod_subnet_newbits, id = var.pod_subnet_id }
    fss      = { create = var.fss_subnet_create ? ""always"" : ""never"", newbits = var.fss_subnet_newbits, id = var.fss_subnet_id }
  }

  # Network Security
  create_nsgs                  = var.create_nsgs
  create_nsgs_always           = true
  allow_node_port_access       = var.allow_node_port_access
  allow_pod_internet_access    = var.allow_pod_internet_access
  allow_rules_internal_lb      = var.allow_rules_internal_lb
  allow_rules_public_lb        = var.allow_rules_public_lb
  allow_worker_internet_access = var.allow_worker_internet_access
  allow_worker_ssh_access      = var.allow_worker_ssh_access
  enable_waf                   = var.enable_waf
  bastion_allowed_cidrs        = compact(split("","", var.bastion_allowed_cidrs))
  bastion_nsg_ids              = compact(split("","", var.bastion_nsg_id))
  control_plane_allowed_cidrs  = compact(split("","", var.control_plane_allowed_cidrs))
  control_plane_is_public      = var.control_plane_is_public
  control_plane_nsg_ids        = compact(split("","", var.control_plane_nsg_id))
  fss_nsg_ids                  = compact(split("","", var.fss_nsg_id))
  load_balancers               = lower(var.load_balancers)
  operator_nsg_ids             = compact(split("","", var.operator_nsg_id))
  pod_nsg_ids                  = compact(split("","", var.pod_nsg_id))
  worker_is_public             = var.worker_is_public
  worker_nsg_ids               = compact(split("","", var.worker_nsg_id))

  # Bastion
  bastion_availability_domain = var.bastion_availability_domain
  bastion_image_id            = var.bastion_image_id
  bastion_image_os            = var.bastion_image_os
  bastion_image_os_version    = var.bastion_image_os_version
  bastion_image_type          = lower(var.bastion_image_type)
  bastion_is_public           = var.bastion_is_public
  bastion_shape               = var.bastion_shape
  bastion_upgrade             = var.bastion_upgrade
  bastion_user                = var.bastion_user
  create_bastion              = var.create_bastion

  # SSH
  ssh_public_key  = local.ssh_public_key
  ssh_private_key = sensitive(local.ssh_key_bundle_content)

  # Cluster
  create_cluster          = false
  preferred_load_balancer = lower(var.preferred_load_balancer)
  create_fss              = false
  create_operator         = false

  freeform_tags = { # TODO Remaining tags in schema
    cluster           = {}
    persistent_volume = {}
    service_lb        = {}
    workers           = {}
    bastion           = lookup(var.bastion_tags, ""freeformTags"", {})
    operator          = {}
    vcn               = {}
  }

  defined_tags = { # TODO Remaining tags in schema
    cluster           = {}
    persistent_volume = {}
    service_lb        = {}
    workers           = {}
    bastion           = lookup(var.bastion_tags, ""definedTags"", {})
    operator          = {}
    vcn               = {}
  }
}
",module,"module ""oke"" {
  source         = ""github.com/oracle-terraform-modules/terraform-oci-oke.git?ref=5.x&depth=1""
  providers      = { oci.home = oci.home }
  tenancy_id     = var.tenancy_ocid
  compartment_id = var.compartment_ocid

  # Identity
  create_iam_resources     = true
  create_iam_tag_namespace = var.create_iam_tag_namespace
  create_iam_defined_tags  = var.create_iam_tag_namespace || var.create_iam_defined_tags
  use_defined_tags         = var.use_defined_tags
  tag_namespace            = var.tag_namespace

  # Network
  create_vcn                  = var.create_vcn
  vcn_id                      = var.vcn_id
  vcn_cidrs                   = split("","", var.vcn_cidrs)
  vcn_create_internet_gateway = var.vcn_create_internet_gateway ? ""always"" : ""never""
  vcn_create_nat_gateway      = var.vcn_create_nat_gateway ? ""always"" : ""never""
  vcn_create_service_gateway  = var.vcn_create_service_gateway ? ""always"" : ""never""
  vcn_name                    = var.vcn_name
  vcn_dns_label               = var.vcn_dns_label
  assign_dns                  = var.assign_dns
  ig_route_table_id           = var.ig_route_table_id
  local_peering_gateways      = var.local_peering_gateways
  lockdown_default_seclist    = var.lockdown_default_seclist
  create_drg                  = var.create_drg
  drg_id                      = var.drg_id
  drg_display_name            = var.drg_display_name

  subnets = {
    bastion = {
      create  = var.bastion_subnet_create ? ""always"" : ""never"",
      newbits = var.bastion_subnet_newbits,
      id      = var.bastion_subnet_id
    }

    operator = {
      create  = var.operator_subnet_create ? ""always"" : ""never"",
      newbits = var.operator_subnet_newbits,
      id      = var.operator_subnet_id
    }

    cp = {
      create  = var.control_plane_subnet_create ? ""always"" : ""never"",
      newbits = var.control_plane_subnet_newbits,
      id      = var.control_plane_subnet_id
    }

    int_lb = {
      create  = var.int_lb_subnet_create ? ""always"" : ""never"",
      newbits = var.int_lb_subnet_newbits,
      id      = var.int_lb_subnet_id
    }

    pub_lb = {
      create  = var.pub_lb_subnet_create ? ""always"" : ""never"",
      newbits = var.pub_lb_subnet_newbits,
      id      = var.pub_lb_subnet_id
    }

    workers = {
      create  = var.worker_subnet_create ? ""always"" : ""never"",
      newbits = var.worker_subnet_newbits,
      id      = var.worker_subnet_id
    }

    pods = {
      create  = var.pod_subnet_create ? ""always"" : ""never"",
      newbits = var.pod_subnet_newbits,
      id      = var.pod_subnet_id
    }
  }

  # Network Security
  nsgs = {
    bastion  = { create = var.create_nsgs ? ""always"" : ""never"" }
    operator = { create = var.create_nsgs ? ""always"" : ""never"" }
    cp       = { create = var.create_nsgs ? ""always"" : ""never"" }
    int_lb   = { create = var.create_nsgs ? ""always"" : ""never"" }
    pub_lb   = { create = var.create_nsgs ? ""always"" : ""never"" }
    workers  = { create = var.create_nsgs ? ""always"" : ""never"" }
    pods     = { create = var.create_nsgs ? ""always"" : ""never"" }
  }

  allow_node_port_access       = var.allow_node_port_access
  allow_pod_internet_access    = var.allow_pod_internet_access
  allow_rules_internal_lb      = var.allow_rules_internal_lb
  allow_rules_public_lb        = var.allow_rules_public_lb
  allow_worker_internet_access = var.allow_worker_internet_access
  allow_worker_ssh_access      = var.allow_worker_ssh_access
  enable_waf                   = var.enable_waf
  bastion_allowed_cidrs        = compact(split("","", var.bastion_allowed_cidrs))
  control_plane_allowed_cidrs  = compact(split("","", var.control_plane_allowed_cidrs))
  control_plane_is_public      = var.control_plane_is_public
  load_balancers               = lower(var.load_balancers)
  worker_is_public             = var.worker_is_public

  # Bastion
  bastion_availability_domain = var.bastion_availability_domain
  bastion_image_id            = var.bastion_image_id
  bastion_image_os            = var.bastion_image_os
  bastion_image_os_version    = var.bastion_image_os_version
  bastion_image_type          = lower(var.bastion_image_type)
  bastion_is_public           = var.bastion_is_public
  bastion_shape               = var.bastion_shape
  bastion_upgrade             = var.bastion_upgrade
  bastion_user                = var.bastion_user
  create_bastion              = var.create_bastion

  # SSH
  ssh_public_key  = local.ssh_public_key
  ssh_private_key = sensitive(local.ssh_key_bundle_content)

  # Cluster
  create_cluster          = false
  preferred_load_balancer = lower(var.preferred_load_balancer)
  create_operator         = false

  freeform_tags = { # TODO Remaining tags in schema
    bastion = lookup(var.bastion_tags, ""freeformTags"", {})
    vcn     = {}
  }

  defined_tags = { # TODO Remaining tags in schema
    bastion = lookup(var.bastion_tags, ""definedTags"", {})
    vcn     = {}
  }
}
",module,119,128.0,ba160d1800dbef893f2535db1e2e00a51df7238c,8c36e1160ec67dbd04cbe6a5e9fccf1fdce9a372,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/ba160d1800dbef893f2535db1e2e00a51df7238c/examples/oke-network-only/main.tf#L119,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/8c36e1160ec67dbd04cbe6a5e9fccf1fdce9a372/examples/rms/oke-network-only/main.tf#L128,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,4,0,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1437,blueprints/data-solutions/bq-ml/main.tf,blueprints/data-solutions/bq-ml/main.tf,0,#todo,#TODO Check/Implement P4SA logic for IAM role,"#TODO Check/Implement P4SA logic for IAM role 
 # encryption_spec { 
 #   kms_key_name = var.service_encryption_keys.ai_metadata_store 
 # }","resource ""google_vertex_ai_metadata_store"" ""store"" {
  provider    = google-beta
  project     = module.project.project_id
  name        = ""${var.prefix}-metadata-store""
  description = ""Vertex Ai Metadata Store""
  region      = var.region
  #TODO Check/Implement P4SA logic for IAM role
  # encryption_spec {
  #   kms_key_name = var.service_encryption_keys.ai_metadata_store
  # }
}
",resource,the block associated got renamed or deleted,,174,,50856e6951763237be2133781acb4a7714bc8c72,9e19f8960861fe61830801eab27111422f1d7a4e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/50856e6951763237be2133781acb4a7714bc8c72/blueprints/data-solutions/bq-ml/main.tf#L174,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9e19f8960861fe61830801eab27111422f1d7a4e/blueprints/data-solutions/bq-ml/main.tf,2023-02-23 18:36:03+01:00,2023-03-05 22:02:41+01:00,4,1,1,1,0,1,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,140,local.tf,locals.tf,1,fix,"# Windows nodes are available from k8s 1.14. If cluster version is less than 1.14, fix ami filter to some constant to not fail on 'terraform plan'.","# Windows nodes are available from k8s 1.14. If cluster version is less than 1.14, fix ami filter to some constant to not fail on 'terraform plan'.","locals {
  asg_tags = [
    for item in keys(var.tags) :
    map(
      ""key"", item,
      ""value"", element(values(var.tags), index(keys(var.tags), item)),
      ""propagate_at_launch"", ""true""
    )
  ]

  cluster_security_group_id = var.cluster_create_security_group ? join("""", aws_security_group.cluster.*.id) : var.cluster_security_group_id
  cluster_iam_role_name     = var.manage_cluster_iam_resources ? join("""", aws_iam_role.cluster.*.name) : var.cluster_iam_role_name
  cluster_iam_role_arn      = var.manage_cluster_iam_resources ? join("""", aws_iam_role.cluster.*.arn) : join("""", data.aws_iam_role.custom_cluster_iam_role.*.arn)
  worker_security_group_id  = var.worker_create_security_group ? join("""", aws_security_group.workers.*.id) : var.worker_security_group_id

  default_iam_role_id    = concat(aws_iam_role.workers.*.id, [""""])[0]
  default_ami_id_linux   = coalesce(local.workers_group_defaults.ami_id, data.aws_ami.eks_worker.id)
  default_ami_id_windows = coalesce(local.workers_group_defaults.ami_id_windows, data.aws_ami.eks_worker_windows.id)

  kubeconfig_name = var.kubeconfig_name == """" ? ""eks_${var.cluster_name}"" : var.kubeconfig_name

  worker_group_count                 = length(var.worker_groups)
  worker_group_launch_template_count = length(var.worker_groups_launch_template)

  worker_ami_name_filter = var.worker_ami_name_filter != """" ? var.worker_ami_name_filter : ""amazon-eks-node-${var.cluster_version}-v*""
  # Windows nodes are available from k8s 1.14. If cluster version is less than 1.14, fix ami filter to some constant to not fail on 'terraform plan'.
  worker_ami_name_filter_windows = (var.worker_ami_name_filter_windows != """" ?
    var.worker_ami_name_filter_windows : ""Windows_Server-2019-English-Core-EKS_Optimized-${tonumber(var.cluster_version) >= 1.14 ? var.cluster_version : 1.14}-*""
  )

  ec2_principal = ""ec2.${data.aws_partition.current.dns_suffix}""

  policy_arn_prefix = ""arn:${data.aws_partition.current.partition}:iam::aws:policy""
  workers_group_defaults_defaults = {
    name                          = ""count.index""               # Name of the worker group. Literal count.index will never be used but if name is not set, the count.index interpolation will be used.
    tags                          = []                          # A list of map defining extra tags to be applied to the worker group autoscaling group.
    ami_id                        = """"                          # AMI ID for the eks linux based workers. If none is provided, Terraform will search for the latest version of their EKS optimized worker AMI based on platform.
    ami_id_windows                = """"                          # AMI ID for the eks windows based workers. If none is provided, Terraform will search for the latest version of their EKS optimized worker AMI based on platform.
    asg_desired_capacity          = ""1""                         # Desired worker capacity in the autoscaling group and changing its value will not affect the autoscaling group's desired capacity because the cluster-autoscaler manages up and down scaling of the nodes. Cluster-autoscaler add nodes when pods are in pending state and remove the nodes when they are not required by modifying the desirec_capacity of the autoscaling group. Although an issue exists in which if the value of the asg_min_size is changed it modifies the value of asg_desired_capacity.
    asg_max_size                  = ""3""                         # Maximum worker capacity in the autoscaling group.
    asg_min_size                  = ""1""                         # Minimum worker capacity in the autoscaling group. NOTE: Change in this paramater will affect the asg_desired_capacity, like changing its value to 2 will change asg_desired_capacity value to 2 but bringing back it to 1 will not affect the asg_desired_capacity.
    asg_force_delete              = false                       # Enable forced deletion for the autoscaling group.
    asg_initial_lifecycle_hooks   = []                          # Initital lifecycle hook for the autoscaling group.
    asg_recreate_on_change        = false                       # Recreate the autoscaling group when the Launch Template or Launch Configuration change.
    default_cooldown              = null                        # The amount of time, in seconds, after a scaling activity completes before another scaling activity can start.
    health_check_grace_period     = null                        # Time in seconds after instance comes into service before checking health.
    instance_type                 = ""m4.large""                  # Size of the workers instances.
    spot_price                    = """"                          # Cost of spot instance.
    placement_tenancy             = """"                          # The tenancy of the instance. Valid values are ""default"" or ""dedicated"".
    root_volume_size              = ""100""                       # root volume size of workers instances.
    root_volume_type              = ""gp2""                       # root volume type of workers instances, can be 'standard', 'gp2', or 'io1'
    root_iops                     = ""0""                         # The amount of provisioned IOPS. This must be set with a volume_type of ""io1"".
    key_name                      = """"                          # The key name that should be used for the instances in the autoscaling group
    pre_userdata                  = """"                          # userdata to pre-append to the default userdata.
    userdata_template_file        = """"                          # alternate template to use for userdata
    userdata_template_extra_args  = {}                          # Additional arguments to use when expanding the userdata template file
    bootstrap_extra_args          = """"                          # Extra arguments passed to the bootstrap.sh script from the EKS AMI (Amazon Machine Image).
    additional_userdata           = """"                          # userdata to append to the default userdata.
    ebs_optimized                 = true                        # sets whether to use ebs optimization on supported types.
    enable_monitoring             = true                        # Enables/disables detailed monitoring.
    public_ip                     = false                       # Associate a public ip address with a worker
    kubelet_extra_args            = """"                          # This string is passed directly to kubelet if set. Useful for adding labels or taints.
    subnets                       = var.subnets                 # A list of subnets to place the worker nodes in. i.e. [""subnet-123"", ""subnet-456"", ""subnet-789""]
    additional_security_group_ids = []                          # A list of additional security group ids to include in worker launch config
    protect_from_scale_in         = false                       # Prevent AWS from scaling in, so that cluster-autoscaler is solely responsible.
    iam_instance_profile_name     = """"                          # A custom IAM instance profile name. Used when manage_worker_iam_resources is set to false. Incompatible with iam_role_id.
    iam_role_id                   = ""local.default_iam_role_id"" # A custom IAM role id. Incompatible with iam_instance_profile_name.  Literal local.default_iam_role_id will never be used but if iam_role_id is not set, the local.default_iam_role_id interpolation will be used.
    suspended_processes           = [""AZRebalance""]             # A list of processes to suspend. i.e. [""AZRebalance"", ""HealthCheck"", ""ReplaceUnhealthy""]
    target_group_arns             = null                        # A list of Application LoadBalancer (ALB) target group ARNs to be associated to the autoscaling group
    enabled_metrics               = []                          # A list of metrics to be collected i.e. [""GroupMinSize"", ""GroupMaxSize"", ""GroupDesiredCapacity""]
    placement_group               = """"                          # The name of the placement group into which to launch the instances, if any.
    service_linked_role_arn       = """"                          # Arn of custom service linked role that Auto Scaling group will use. Useful when you have encrypted EBS
    termination_policies          = []                          # A list of policies to decide how the instances in the auto scale group should be terminated.
    platform                      = ""linux""                     # Platform of workers. either ""linux"" or ""windows""
    additional_ebs_volumes        = []                          # A list of additional volumes to be attached to the instances on this Auto Scaling group. Each volume should be an object with the following: block_device_name (required), volume_size, volume_type, iops, encrypted, kms_key_id (only on launch-template), delete_on_termination. Optional values are grabbed from root volume or from defaults
    # Settings for launch templates
    root_block_device_name            = data.aws_ami.eks_worker.root_device_name # Root device name for workers. If non is provided, will assume default AMI was used.
    root_kms_key_id                   = """"                                       # The KMS key to use when encrypting the root storage device
    launch_template_version           = ""$Latest""                                # The lastest version of the launch template to use in the autoscaling group
    launch_template_placement_tenancy = ""default""                                # The placement tenancy for instances
    launch_template_placement_group   = """"                                       # The name of the placement group into which to launch the instances, if any.
    root_encrypted                    = false                                    # Whether the volume should be encrypted or not
    eni_delete                        = true                                     # Delete the Elastic Network Interface (ENI) on termination (if set to false you will have to manually delete before destroying)
    cpu_credits                       = ""standard""                               # T2/T3 unlimited mode, can be 'standard' or 'unlimited'. Used 'standard' mode as default to avoid paying higher costs
    market_type                       = null
    # Settings for launch templates with mixed instances policy
    override_instance_types                  = [""m5.large"", ""m5a.large"", ""m5d.large"", ""m5ad.large""] # A list of override instance types for mixed instances policy
    on_demand_allocation_strategy            = null                                                 # Strategy to use when launching on-demand instances. Valid values: prioritized.
    on_demand_base_capacity                  = ""0""                                                  # Absolute minimum amount of desired capacity that must be fulfilled by on-demand instances
    on_demand_percentage_above_base_capacity = ""0""                                                  # Percentage split between on-demand and Spot instances above the base on-demand capacity
    spot_allocation_strategy                 = ""lowest-price""                                       # Valid options are 'lowest-price' and 'capacity-optimized'. If 'lowest-price', the Auto Scaling group launches instances using the Spot pools with the lowest price, and evenly allocates your instances across the number of Spot pools. If 'capacity-optimized', the Auto Scaling group launches instances using Spot pools that are optimally chosen based on the available Spot capacity.
    spot_instance_pools                      = 10                                                   # ""Number of Spot pools per availability zone to allocate capacity. EC2 Auto Scaling selects the cheapest Spot pools and evenly allocates Spot capacity across the number of Spot pools that you specify.""
    spot_max_price                           = """"                                                   # Maximum price per unit hour that the user is willing to pay for the Spot instances. Default is the on-demand price
    max_instance_lifetime                    = 0                                                    # Maximum number of seconds instances can run in the ASG. 0 is unlimited.
  }

  workers_group_defaults = merge(
    local.workers_group_defaults_defaults,
    var.workers_group_defaults,
  )

  ebs_optimized_not_supported = [
    ""c1.medium"",
    ""c3.8xlarge"",
    ""c3.large"",
    ""c5d.12xlarge"",
    ""c5d.24xlarge"",
    ""c5d.metal"",
    ""cc2.8xlarge"",
    ""cr1.8xlarge"",
    ""g2.8xlarge"",
    ""g4dn.metal"",
    ""hs1.8xlarge"",
    ""i2.8xlarge"",
    ""m1.medium"",
    ""m1.small"",
    ""m2.xlarge"",
    ""m3.large"",
    ""m3.medium"",
    ""m5ad.16xlarge"",
    ""m5ad.8xlarge"",
    ""m5dn.metal"",
    ""m5n.metal"",
    ""r3.8xlarge"",
    ""r3.large"",
    ""r5ad.16xlarge"",
    ""r5ad.8xlarge"",
    ""r5dn.metal"",
    ""r5n.metal"",
    ""t1.micro"",
    ""t2.2xlarge"",
    ""t2.large"",
    ""t2.medium"",
    ""t2.micro"",
    ""t2.nano"",
    ""t2.small"",
    ""t2.xlarge""
  ]

  kubeconfig = var.create_eks ? templatefile(""${path.module}/templates/kubeconfig.tpl"", {
    kubeconfig_name                   = local.kubeconfig_name
    endpoint                          = aws_eks_cluster.this[0].endpoint
    cluster_auth_base64               = aws_eks_cluster.this[0].certificate_authority[0].data
    aws_authenticator_command         = var.kubeconfig_aws_authenticator_command
    aws_authenticator_command_args    = length(var.kubeconfig_aws_authenticator_command_args) > 0 ? var.kubeconfig_aws_authenticator_command_args : [""token"", ""-i"", aws_eks_cluster.this[0].name]
    aws_authenticator_additional_args = var.kubeconfig_aws_authenticator_additional_args
    aws_authenticator_env_variables   = var.kubeconfig_aws_authenticator_env_variables
  }) : """"

  userdata = [for worker in var.worker_groups : templatefile(
    lookup(
      worker,
      ""userdata_template_file"",
      lookup(worker, ""platform"", local.workers_group_defaults[""platform""]) == ""windows""
      ? ""${path.module}/templates/userdata_windows.tpl""
      : ""${path.module}/templates/userdata.sh.tpl""
    ),
    merge(
      {
        platform            = lookup(worker, ""platform"", local.workers_group_defaults[""platform""])
        cluster_name        = aws_eks_cluster.this[0].name
        endpoint            = aws_eks_cluster.this[0].endpoint
        cluster_auth_base64 = aws_eks_cluster.this[0].certificate_authority[0].data
        pre_userdata = lookup(
          worker,
          ""pre_userdata"",
          local.workers_group_defaults[""pre_userdata""],
        )
        additional_userdata = lookup(
          worker,
          ""additional_userdata"",
          local.workers_group_defaults[""additional_userdata""],
        )
        bootstrap_extra_args = lookup(
          worker,
          ""bootstrap_extra_args"",
          local.workers_group_defaults[""bootstrap_extra_args""],
        )
        kubelet_extra_args = lookup(
          worker,
          ""kubelet_extra_args"",
          local.workers_group_defaults[""kubelet_extra_args""],
        )
      },
      lookup(
        worker,
        ""userdata_template_extra_args"",
        local.workers_group_defaults[""userdata_template_extra_args""]
      )
    )
    ) if var.create_eks
  ]

  launch_template_userdata = [for worker in var.worker_groups_launch_template : templatefile(
    lookup(
      worker,
      ""userdata_template_file"",
      lookup(worker, ""platform"", local.workers_group_defaults[""platform""]) == ""windows""
      ? ""${path.module}/templates/userdata_windows.tpl""
      : ""${path.module}/templates/userdata.sh.tpl""
    ),
    merge(
      {
        platform            = lookup(worker, ""platform"", local.workers_group_defaults[""platform""])
        cluster_name        = aws_eks_cluster.this[0].name
        endpoint            = aws_eks_cluster.this[0].endpoint
        cluster_auth_base64 = aws_eks_cluster.this[0].certificate_authority[0].data
        pre_userdata = lookup(
          worker,
          ""pre_userdata"",
          local.workers_group_defaults[""pre_userdata""],
        )
        additional_userdata = lookup(
          worker,
          ""additional_userdata"",
          local.workers_group_defaults[""additional_userdata""],
        )
        bootstrap_extra_args = lookup(
          worker,
          ""bootstrap_extra_args"",
          local.workers_group_defaults[""bootstrap_extra_args""],
        )
        kubelet_extra_args = lookup(
          worker,
          ""kubelet_extra_args"",
          local.workers_group_defaults[""kubelet_extra_args""],
        )
      },
      lookup(
        worker,
        ""userdata_template_extra_args"",
        local.workers_group_defaults[""userdata_template_extra_args""]
      )
    )
    ) if var.create_eks
  ]
}
",locals,"locals {

  # EKS Cluster
  cluster_id                        = coalescelist(aws_eks_cluster.this[*].id, [""""])[0]
  cluster_arn                       = coalescelist(aws_eks_cluster.this[*].arn, [""""])[0]
  cluster_name                      = coalescelist(aws_eks_cluster.this[*].name, [""""])[0]
  cluster_endpoint                  = coalescelist(aws_eks_cluster.this[*].endpoint, [""""])[0]
  cluster_auth_base64               = coalescelist(aws_eks_cluster.this[*].certificate_authority[0].data, [""""])[0]
  cluster_oidc_issuer_url           = flatten(concat(aws_eks_cluster.this[*].identity[*].oidc[0].issuer, [""""]))[0]
  cluster_primary_security_group_id = coalescelist(aws_eks_cluster.this[*].vpc_config[0].cluster_security_group_id, [""""])[0]

  cluster_security_group_id = var.cluster_create_security_group ? join("""", aws_security_group.cluster.*.id) : var.cluster_security_group_id
  cluster_iam_role_name     = var.manage_cluster_iam_resources ? join("""", aws_iam_role.cluster.*.name) : var.cluster_iam_role_name
  cluster_iam_role_arn      = var.manage_cluster_iam_resources ? join("""", aws_iam_role.cluster.*.arn) : join("""", data.aws_iam_role.custom_cluster_iam_role.*.arn)

  # Worker groups
  worker_security_group_id = var.worker_create_security_group ? join("""", aws_security_group.workers.*.id) : var.worker_security_group_id

  default_iam_role_id    = concat(aws_iam_role.workers.*.id, [""""])[0]
  default_ami_id_linux   = local.workers_group_defaults.ami_id != """" ? local.workers_group_defaults.ami_id : concat(data.aws_ami.eks_worker.*.id, [""""])[0]
  default_ami_id_windows = local.workers_group_defaults.ami_id_windows != """" ? local.workers_group_defaults.ami_id_windows : concat(data.aws_ami.eks_worker_windows.*.id, [""""])[0]

  worker_group_launch_configuration_count = length(var.worker_groups)
  worker_group_launch_template_count      = length(var.worker_groups_launch_template)

  worker_groups_platforms = [for x in concat(var.worker_groups, var.worker_groups_launch_template) : try(x.platform, var.workers_group_defaults[""platform""], var.default_platform)]

  worker_ami_name_filter         = coalesce(var.worker_ami_name_filter, ""amazon-eks-node-${coalesce(var.cluster_version, ""cluster_version"")}-v*"")
  worker_ami_name_filter_windows = coalesce(var.worker_ami_name_filter_windows, ""Windows_Server-2019-English-Core-EKS_Optimized-${coalesce(var.cluster_version, ""cluster_version"")}-*"")

  ec2_principal     = ""ec2.${data.aws_partition.current.dns_suffix}""
  sts_principal     = ""sts.${data.aws_partition.current.dns_suffix}""
  client_id_list    = distinct(compact(concat([local.sts_principal], var.openid_connect_audiences)))
  policy_arn_prefix = ""arn:${data.aws_partition.current.partition}:iam::aws:policy""

  workers_group_defaults_defaults = {
    name                              = ""count.index""               # Name of the worker group. Literal count.index will never be used but if name is not set, the count.index interpolation will be used.
    tags                              = []                          # A list of maps defining extra tags to be applied to the worker group autoscaling group and volumes.
    ami_id                            = """"                          # AMI ID for the eks linux based workers. If none is provided, Terraform will search for the latest version of their EKS optimized worker AMI based on platform.
    ami_id_windows                    = """"                          # AMI ID for the eks windows based workers. If none is provided, Terraform will search for the latest version of their EKS optimized worker AMI based on platform.
    asg_desired_capacity              = ""1""                         # Desired worker capacity in the autoscaling group and changing its value will not affect the autoscaling group's desired capacity because the cluster-autoscaler manages up and down scaling of the nodes. Cluster-autoscaler add nodes when pods are in pending state and remove the nodes when they are not required by modifying the desired_capacity of the autoscaling group. Although an issue exists in which if the value of the asg_min_size is changed it modifies the value of asg_desired_capacity.
    asg_max_size                      = ""3""                         # Maximum worker capacity in the autoscaling group.
    asg_min_size                      = ""1""                         # Minimum worker capacity in the autoscaling group. NOTE: Change in this paramater will affect the asg_desired_capacity, like changing its value to 2 will change asg_desired_capacity value to 2 but bringing back it to 1 will not affect the asg_desired_capacity.
    asg_force_delete                  = false                       # Enable forced deletion for the autoscaling group.
    asg_initial_lifecycle_hooks       = []                          # Initital lifecycle hook for the autoscaling group.
    default_cooldown                  = null                        # The amount of time, in seconds, after a scaling activity completes before another scaling activity can start.
    health_check_type                 = null                        # Controls how health checking is done. Valid values are ""EC2"" or ""ELB"".
    health_check_grace_period         = null                        # Time in seconds after instance comes into service before checking health.
    instance_type                     = ""m4.large""                  # Size of the workers instances.
    instance_store_virtual_name       = ""ephemeral0""                # ""virtual_name"" of the instance store volume.
    spot_price                        = """"                          # Cost of spot instance.
    placement_tenancy                 = """"                          # The tenancy of the instance. Valid values are ""default"" or ""dedicated"".
    root_volume_size                  = ""100""                       # root volume size of workers instances.
    root_volume_type                  = ""gp2""                       # root volume type of workers instances, can be ""standard"", ""gp3"", ""gp2"", or ""io1""
    root_iops                         = ""0""                         # The amount of provisioned IOPS. This must be set with a volume_type of ""io1"".
    root_volume_throughput            = null                        # The amount of throughput to provision for a gp3 volume.
    key_name                          = """"                          # The key pair name that should be used for the instances in the autoscaling group
    pre_userdata                      = """"                          # userdata to pre-append to the default userdata.
    userdata_template_file            = """"                          # alternate template to use for userdata
    userdata_template_extra_args      = {}                          # Additional arguments to use when expanding the userdata template file
    bootstrap_extra_args              = """"                          # Extra arguments passed to the bootstrap.sh script from the EKS AMI (Amazon Machine Image).
    additional_userdata               = """"                          # userdata to append to the default userdata.
    ebs_optimized                     = true                        # sets whether to use ebs optimization on supported types.
    enable_monitoring                 = true                        # Enables/disables detailed monitoring.
    enclave_support                   = false                       # Enables/disables enclave support
    public_ip                         = false                       # Associate a public ip address with a worker
    kubelet_extra_args                = """"                          # This string is passed directly to kubelet if set. Useful for adding labels or taints.
    subnets                           = var.subnets                 # A list of subnets to place the worker nodes in. i.e. [""subnet-123"", ""subnet-456"", ""subnet-789""]
    additional_security_group_ids     = []                          # A list of additional security group ids to include in worker launch config
    protect_from_scale_in             = false                       # Prevent AWS from scaling in, so that cluster-autoscaler is solely responsible.
    iam_instance_profile_name         = """"                          # A custom IAM instance profile name. Used when manage_worker_iam_resources is set to false. Incompatible with iam_role_id.
    iam_role_id                       = ""local.default_iam_role_id"" # A custom IAM role id. Incompatible with iam_instance_profile_name.  Literal local.default_iam_role_id will never be used but if iam_role_id is not set, the local.default_iam_role_id interpolation will be used.
    suspended_processes               = [""AZRebalance""]             # A list of processes to suspend. i.e. [""AZRebalance"", ""HealthCheck"", ""ReplaceUnhealthy""]
    target_group_arns                 = null                        # A list of Application LoadBalancer (ALB) target group ARNs to be associated to the autoscaling group
    load_balancers                    = null                        # A list of Classic LoadBalancer (CLB)'s name to be associated to the autoscaling group
    enabled_metrics                   = []                          # A list of metrics to be collected i.e. [""GroupMinSize"", ""GroupMaxSize"", ""GroupDesiredCapacity""]
    placement_group                   = null                        # The name of the placement group into which to launch the instances, if any.
    service_linked_role_arn           = """"                          # Arn of custom service linked role that Auto Scaling group will use. Useful when you have encrypted EBS
    termination_policies              = []                          # A list of policies to decide how the instances in the auto scale group should be terminated.
    platform                          = var.default_platform        # Platform of workers. Either ""linux"" or ""windows"".
    additional_ebs_volumes            = []                          # A list of additional volumes to be attached to the instances on this Auto Scaling group. Each volume should be an object with the following: block_device_name (required), volume_size, volume_type, iops, throughput, encrypted, kms_key_id (only on launch-template), delete_on_termination. Optional values are grabbed from root volume or from defaults
    additional_instance_store_volumes = []                          # A list of additional instance store (local disk) volumes to be attached to the instances on this Auto Scaling group. Each volume should be an object with the following: block_device_name (required), virtual_name.
    warm_pool                         = null                        # If this block is configured, add a Warm Pool to the specified Auto Scaling group.
    timeouts                          = {}                          # A map of timeouts for create/update/delete operations

    # Settings for launch templates
    root_block_device_name               = concat(data.aws_ami.eks_worker.*.root_device_name, [""""])[0]         # Root device name for Linux workers. If not provided, will assume default Linux AMI was used.
    root_block_device_name_windows       = concat(data.aws_ami.eks_worker_windows.*.root_device_name, [""""])[0] # Root device name for Windows workers. If not provided, will assume default Windows AMI was used.
    root_kms_key_id                      = """"                                                                  # The KMS key to use when encrypting the root storage device
    launch_template_id                   = null                                                                # The id of the launch template used for managed node_groups
    launch_template_version              = ""$Latest""                                                           # The latest version of the launch template to use in the autoscaling group
    update_default_version               = false                                                               # Update the autoscaling group launch template's default version upon each update
    launch_template_placement_tenancy    = ""default""                                                           # The placement tenancy for instances
    launch_template_placement_group      = null                                                                # The name of the placement group into which to launch the instances, if any.
    root_encrypted                       = false                                                               # Whether the volume should be encrypted or not
    eni_delete                           = true                                                                # Delete the Elastic Network Interface (ENI) on termination (if set to false you will have to manually delete before destroying)
    interface_type                       = null                                                                # The type of network interface. To create an Elastic Fabric Adapter (EFA), specify 'efa'.
    cpu_credits                          = ""standard""                                                          # T2/T3 unlimited mode, can be 'standard' or 'unlimited'. Used 'standard' mode as default to avoid paying higher costs
    market_type                          = null
    metadata_http_endpoint               = ""enabled""  # The state of the metadata service: enabled, disabled.
    metadata_http_tokens                 = ""optional"" # If session tokens are required: optional, required.
    metadata_http_put_response_hop_limit = null       # The desired HTTP PUT response hop limit for instance metadata requests.
    # Settings for launch templates with mixed instances policy
    override_instance_types                  = [""m5.large"", ""m5a.large"", ""m5d.large"", ""m5ad.large""] # A list of override instance types for mixed instances policy
    on_demand_allocation_strategy            = null                                                 # Strategy to use when launching on-demand instances. Valid values: prioritized.
    on_demand_base_capacity                  = ""0""                                                  # Absolute minimum amount of desired capacity that must be fulfilled by on-demand instances
    on_demand_percentage_above_base_capacity = ""0""                                                  # Percentage split between on-demand and Spot instances above the base on-demand capacity
    spot_allocation_strategy                 = ""lowest-price""                                       # Valid options are 'lowest-price' and 'capacity-optimized'. If 'lowest-price', the Auto Scaling group launches instances using the Spot pools with the lowest price, and evenly allocates your instances across the number of Spot pools. If 'capacity-optimized', the Auto Scaling group launches instances using Spot pools that are optimally chosen based on the available Spot capacity.
    spot_instance_pools                      = 10                                                   # ""Number of Spot pools per availability zone to allocate capacity. EC2 Auto Scaling selects the cheapest Spot pools and evenly allocates Spot capacity across the number of Spot pools that you specify.""
    spot_max_price                           = """"                                                   # Maximum price per unit hour that the user is willing to pay for the Spot instances. Default is the on-demand price
    max_instance_lifetime                    = 0                                                    # Maximum number of seconds instances can run in the ASG. 0 is unlimited.
    elastic_inference_accelerator            = null                                                 # Type of elastic inference accelerator to be attached. Example values are eia1.medium, eia2.large, etc.
    instance_refresh_enabled                 = false                                                # Enable instance refresh for the worker autoscaling group.
    instance_refresh_strategy                = ""Rolling""                                            # Strategy to use for instance refresh. Default is 'Rolling' which the only valid value.
    instance_refresh_min_healthy_percentage  = 90                                                   # The amount of capacity in the ASG that must remain healthy during an instance refresh, as a percentage of the ASG's desired capacity.
    instance_refresh_instance_warmup         = null                                                 # The number of seconds until a newly launched instance is configured and ready to use. Defaults to the ASG's health check grace period.
    instance_refresh_triggers                = []                                                   # Set of additional property names that will trigger an Instance Refresh. A refresh will always be triggered by a change in any of launch_configuration, launch_template, or mixed_instances_policy.
    capacity_rebalance                       = false                                                # Enable capacity rebalance
  }

  workers_group_defaults = merge(
    local.workers_group_defaults_defaults,
    var.workers_group_defaults,
  )

  ebs_optimized_not_supported = [
    ""c1.medium"",
    ""c3.8xlarge"",
    ""c3.large"",
    ""c5d.12xlarge"",
    ""c5d.24xlarge"",
    ""c5d.metal"",
    ""cc2.8xlarge"",
    ""cr1.8xlarge"",
    ""g2.8xlarge"",
    ""g4dn.metal"",
    ""hs1.8xlarge"",
    ""i2.8xlarge"",
    ""m1.medium"",
    ""m1.small"",
    ""m2.xlarge"",
    ""m3.large"",
    ""m3.medium"",
    ""m5ad.16xlarge"",
    ""m5ad.8xlarge"",
    ""m5dn.metal"",
    ""m5n.metal"",
    ""r3.8xlarge"",
    ""r3.large"",
    ""r5ad.16xlarge"",
    ""r5ad.8xlarge"",
    ""r5dn.metal"",
    ""r5n.metal"",
    ""t1.micro"",
    ""t2.2xlarge"",
    ""t2.large"",
    ""t2.medium"",
    ""t2.micro"",
    ""t2.nano"",
    ""t2.small"",
    ""t2.xlarge""
  ]

  kubeconfig = var.create_eks ? templatefile(""${path.module}/templates/kubeconfig.tpl"", {
    kubeconfig_name                   = coalesce(var.kubeconfig_name, ""eks_${var.cluster_name}"")
    endpoint                          = local.cluster_endpoint
    cluster_auth_base64               = local.cluster_auth_base64
    aws_authenticator_command         = var.kubeconfig_aws_authenticator_command
    aws_authenticator_command_args    = coalescelist(var.kubeconfig_aws_authenticator_command_args, [""token"", ""-i"", local.cluster_name])
    aws_authenticator_additional_args = var.kubeconfig_aws_authenticator_additional_args
    aws_authenticator_env_variables   = var.kubeconfig_aws_authenticator_env_variables
  }) : """"

  launch_configuration_userdata_rendered = [
    for index in range(var.create_eks ? local.worker_group_launch_configuration_count : 0) : templatefile(
      lookup(
        var.worker_groups[index],
        ""userdata_template_file"",
        lookup(var.worker_groups[index], ""platform"", local.workers_group_defaults[""platform""]) == ""windows""
        ? ""${path.module}/templates/userdata_windows.tpl""
        : ""${path.module}/templates/userdata.sh.tpl""
      ),
      merge({
        platform            = lookup(var.worker_groups[index], ""platform"", local.workers_group_defaults[""platform""])
        cluster_name        = local.cluster_name
        endpoint            = local.cluster_endpoint
        cluster_auth_base64 = local.cluster_auth_base64
        pre_userdata = lookup(
          var.worker_groups[index],
          ""pre_userdata"",
          local.workers_group_defaults[""pre_userdata""],
        )
        additional_userdata = lookup(
          var.worker_groups[index],
          ""additional_userdata"",
          local.workers_group_defaults[""additional_userdata""],
        )
        bootstrap_extra_args = lookup(
          var.worker_groups[index],
          ""bootstrap_extra_args"",
          local.workers_group_defaults[""bootstrap_extra_args""],
        )
        kubelet_extra_args = lookup(
          var.worker_groups[index],
          ""kubelet_extra_args"",
          local.workers_group_defaults[""kubelet_extra_args""],
        )
        },
        lookup(
          var.worker_groups[index],
          ""userdata_template_extra_args"",
          local.workers_group_defaults[""userdata_template_extra_args""]
        )
      )
    )
  ]

  launch_template_userdata_rendered = [
    for index in range(var.create_eks ? local.worker_group_launch_template_count : 0) : templatefile(
      lookup(
        var.worker_groups_launch_template[index],
        ""userdata_template_file"",
        lookup(var.worker_groups_launch_template[index], ""platform"", local.workers_group_defaults[""platform""]) == ""windows""
        ? ""${path.module}/templates/userdata_windows.tpl""
        : ""${path.module}/templates/userdata.sh.tpl""
      ),
      merge({
        platform            = lookup(var.worker_groups_launch_template[index], ""platform"", local.workers_group_defaults[""platform""])
        cluster_name        = local.cluster_name
        endpoint            = local.cluster_endpoint
        cluster_auth_base64 = local.cluster_auth_base64
        pre_userdata = lookup(
          var.worker_groups_launch_template[index],
          ""pre_userdata"",
          local.workers_group_defaults[""pre_userdata""],
        )
        additional_userdata = lookup(
          var.worker_groups_launch_template[index],
          ""additional_userdata"",
          local.workers_group_defaults[""additional_userdata""],
        )
        bootstrap_extra_args = lookup(
          var.worker_groups_launch_template[index],
          ""bootstrap_extra_args"",
          local.workers_group_defaults[""bootstrap_extra_args""],
        )
        kubelet_extra_args = lookup(
          var.worker_groups_launch_template[index],
          ""kubelet_extra_args"",
          local.workers_group_defaults[""kubelet_extra_args""],
        )
        },
        lookup(
          var.worker_groups_launch_template[index],
          ""userdata_template_extra_args"",
          local.workers_group_defaults[""userdata_template_extra_args""]
        )
      )
    )
  ]
}
",locals,26,,9bfdba9fb86290b562c096cb404df4cd4e13223e,f37e5af88adfb86214d8c09424913067bac58a4f,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/9bfdba9fb86290b562c096cb404df4cd4e13223e/local.tf#L26,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/f37e5af88adfb86214d8c09424913067bac58a4f/locals.tf,2020-05-06 14:00:20+02:00,2021-10-07 14:21:13+02:00,38,1,1,1,1,0,0,0,0,0
https://github.com/rust-lang/simpleinfra,1,terraform/shared/modules/ecs-app/deployment.tf,terraform/shared/modules/ecs-app/deployment.tf,0,crap,// Security group to allow the monitoring instance to scrape metrics,// Security group to allow the monitoring instance to scrape metrics,"data ""aws_security_group"" ""monitoring"" {
  name = ""monitoring-instance-on-legacy-vpc""
}
",data,"data ""aws_security_group"" ""monitoring"" {
  name = ""monitoring-instance-on-legacy-vpc""
}
",data,79,79.0,b08f657adcb9ae34c6ad6a92a67c0b98d8519657,75291a2cdf0dd5ba60860701274f9500703102da,https://github.com/rust-lang/simpleinfra/blob/b08f657adcb9ae34c6ad6a92a67c0b98d8519657/terraform/shared/modules/ecs-app/deployment.tf#L79,https://github.com/rust-lang/simpleinfra/blob/75291a2cdf0dd5ba60860701274f9500703102da/terraform/shared/modules/ecs-app/deployment.tf#L79,2022-07-09 19:31:38+02:00,2023-04-15 16:55:17+02:00,3,0,0,0,0,1,0,0,1,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,24,modules/extensions/locals.tf,modules/extensions/locals.tf,0,workaround,# workaround for summing a list of numbers: https://github.com/hashicorp/terraform/issues/17239,# workaround for summing a list of numbers: https://github.com/hashicorp/terraform/issues/17239,"locals {

  node_pools_size_list = [
    for node_pool in data.oci_containerengine_node_pools.all_node_pools.node_pools :
    node_pool.node_config_details[0].size
  ]

  # workaround for summing a list of numbers: https://github.com/hashicorp/terraform/issues/17239
  total_nodes = length(flatten([
    for nodes in local.node_pools_size_list : range(nodes)
  ]))

  service_account_cluster_role_binding_name = var.service_account_cluster_role_binding == """" ? ""${var.service_account_name}-crb"" : var.service_account_cluster_role_binding

  # 1. get a list of available images for this cluster
  # 2. filter by version
  # 3. if more than 1 image found for this version, pick the latest
  node_pool_image_ids = data.oci_containerengine_node_pool_option.node_pool_options.sources

  # determine if post provisioning operations are possible
  # requires:
  ## 1. bastion to be enabled and in a running state
  ## 2. operation to be enabled and instance_principal to be enabled

  post_provisioning_ops = var.create_bastion_host == true && var.bastion_state == ""RUNNING"" && var.create_operator == true && var.operator_instance_principal == true ? true : false

  dynamic_group_rule_this_cluster = (var.use_encryption == true) ? ""ALL {resource.type = 'cluster', resource.id = '${var.cluster_id}'}"" : ""null""

  # scripting templates
  update_dynamic_group_template = templatefile(""${path.module}/scripts/update_dynamic_group.template.sh"",
    {
      dynamic_group_id   = var.use_encryption == true ? var.kms_dynamic_group_id : ""null""
      dynamic_group_rule = local.dynamic_group_rule_this_cluster
      home_region        = data.oci_identity_regions.home_region.regions[0].name
    }
  )
  
  check_active_worker_template = templatefile(""${path.module}/scripts/check_worker_active.template.sh"",
    {
      check_node_active = var.check_node_active
      total_nodes       = local.total_nodes
    }
  )

  install_calico_template = templatefile(""${path.module}/scripts/install_calico.template.sh"",
    {
      calico_version     = var.calico_version
      number_of_nodes    = local.total_nodes
      pod_cidr           = var.pods_cidr
      number_of_replicas = min(20, max((local.total_nodes) / 200, 3))
    }
  )

  drain_template = templatefile(""${path.module}/scripts/drain.template.sh"", {})

  drain_list_template = templatefile(""${path.module}/scripts/drainlist.py"",
    {
      cluster_id     = var.cluster_id
      compartment_id = var.compartment_id
      region         = var.region
      pools_to_drain = var.label_prefix == ""none"" ? trim(join("","", formatlist(""'%s'"", var.node_pools_to_drain)), ""'"") : trim(join("","", formatlist(""'%s-%s'"", var.label_prefix, var.node_pools_to_drain)), ""'"")
    }
  )

  install_kubectl_template = templatefile(""${path.module}/scripts/install_kubectl.template.sh"",
    {
      ol = var.operator_os_version
    }
  )

  install_helm_template = templatefile(""${path.module}/scripts/install_helm.template.sh"", {})

  metric_server_template = templatefile(""${path.module}/scripts/install_metricserver.template.sh"",
    {
      enable_vpa  = var.enable_vpa
      vpa_version = var.vpa_version
    }
  )

  secret_template = templatefile(""${path.module}/scripts/secret.py"",
    {
      compartment_id = var.compartment_id
      region         = var.region

      email_address     = var.email_address
      region_registry   = var.ocir_urls[var.region]
      secret_id         = var.secret_id
      secret_name       = var.secret_name
      secret_namespace  = var.secret_namespace
      tenancy_namespace = data.oci_objectstorage_namespace.object_storage_namespace.namespace
      username          = var.username
    }
  )

  create_service_account_template = templatefile(""${path.module}/scripts/create_service_account.template.sh"",
    {
      service_account_name                 = var.service_account_name
      service_account_namespace            = var.service_account_namespace
      service_account_cluster_role_binding = local.service_account_cluster_role_binding_name
    }
  )
}
",locals,"locals {
  ssh_private_key = (
    var.ssh_private_key != """"
    ? try(base64decode(var.ssh_private_key), var.ssh_private_key)
    : var.ssh_private_key_path != ""none""
    ? file(var.ssh_private_key_path)
  : null)

  service_account_cluster_role_binding_name = var.service_account_cluster_role_binding == """" ? ""${var.service_account_name}-crb"" : var.service_account_cluster_role_binding

  # 1. get a list of available images for this cluster
  # 2. filter by version
  # 3. if more than 1 image found for this version, pick the latest
  node_pool_image_ids = data.oci_containerengine_node_pool_option.node_pool_options.sources

  # determine if post provisioning operations are possible
  # requires:
  ## 1. bastion to be enabled and in a running state
  ## 2. operation to be enabled and instance_principal to be enabled

  post_provisioning_ops = var.create_bastion_host == true && var.bastion_state == ""RUNNING"" && var.create_operator == true && var.operator_state == ""RUNNING"" && var.enable_operator_instance_principal == true ? true : false

  dynamic_group_rule_this_cluster = (var.use_cluster_encryption == true) ? ""ALL {resource.type = 'cluster', resource.id = '${var.cluster_id}'}"" : ""null""

  dynamic_group_prefix = (var.label_prefix == ""none"") ? """" : ""${var.label_prefix}""
}
",locals,11,,1c53f2ae31789274bfe4ded6cf312f265e011307,4d2b3f3d672a8f41655da3a7c58fded42c6858f3,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/1c53f2ae31789274bfe4ded6cf312f265e011307/modules/extensions/locals.tf#L11,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/4d2b3f3d672a8f41655da3a7c58fded42c6858f3/modules/extensions/locals.tf,2021-10-26 10:35:29+11:00,2023-10-25 16:40:02+11:00,10,1,0,1,1,0,0,0,0,0
https://github.com/chanzuckerberg/cztack,126,snowflake-table-grant-all/main.tf,snowflake-table-grant-all/main.tf,0,hack,"// HACK(el): The way the provider works, we can only have one grant per (db, share, table, on_future, with_grant_option) grant","// HACK(el): The way the provider works, we can only have one grant per (db, share, table, on_future, with_grant_option) grant 
 //           because of this, if we simulate an ALL grant for a tuple X for role foo through this module 
 //           we couldn't then issue a SELECT grant for the same tuple X for a different role bar. 
 //           We therefore expand this module so that you can issue specific privilege grants to other roles and shares 
 //           It is a bit of a hack but probably can't do much better with the current structure of the provider (and its limitations).","resource snowflake_table_grant all {
  for_each = toset(local.privileges)

  database_name = var.database_name
  schema_name   = var.schema_name
  table_name    = var.table_name

  // HACK(el): The way the provider works, we can only have one grant per (db, share, table, on_future, with_grant_option) grant
  //           because of this, if we simulate an ALL grant for a tuple X for role foo through this module
  //           we couldn't then issue a SELECT grant for the same tuple X for a different role bar.
  //           We therefore expand this module so that you can issue specific privilege grants to other roles and shares
  //           It is a bit of a hack but probably can't do much better with the current structure of the provider (and its limitations).
  roles = setunion(
    var.roles,
    lookup(var.per_privilege_grants, each.value, { roles = [], shares = [] }).roles,
  )
  shares = setunion(
    var.shares,
    lookup(var.per_privilege_grants, each.value, { roles = [], shares = [] }).shares,
  )

  on_future         = var.on_future
  with_grant_option = var.with_grant_option

  privilege = each.value
}
",resource,,,20,0.0,fafd05ae12d4d162cb2ba2715fc61c36b14a53a7,7cebb2e1dd3a50f9ef31130f550d44a6ac273813,https://github.com/chanzuckerberg/cztack/blob/fafd05ae12d4d162cb2ba2715fc61c36b14a53a7/snowflake-table-grant-all/main.tf#L20,https://github.com/chanzuckerberg/cztack/blob/7cebb2e1dd3a50f9ef31130f550d44a6ac273813/snowflake-table-grant-all/main.tf#L0,2020-12-04 12:50:00-08:00,2020-12-10 10:30:42-08:00,2,2,1,0,1,1,0,0,0,0
https://github.com/alphagov/govuk-aws,1463,terraform/projects/infra-database-backups-bucket/reader.tf,terraform/projects/infra-database-backups-bucket/reader.tf,0,# todo,# TODO: Remove once Content Data API is using the content_data_api_production database.,# TODO: Remove once Content Data API is using the content_data_api_production database.,"data ""aws_iam_policy_document"" ""production_content_data_api_dbadmin_database_backups_reader"" {
  statement {
    sid       = ""ContentDataAPIDBAdminListBucket""
    actions   = [""s3:ListBucket""]
    resources = [""arn:aws:s3:::govuk-production-database-backups""]
  }

  statement {
    sid     = ""ContentDataAPIDBAdminGetObject""
    actions = [""s3:GetObject""]
    resources = [
      ""arn:aws:s3:::govuk-production-database-backups/content-data-api-postgresql/*-content_data_api_production.gz"",
      # TODO: Remove once Content Data API is using the content_data_api_production database.
      ""arn:aws:s3:::govuk-production-database-backups/content-data-api-postgresql/*-content_performance_manager_production.gz"",
    ]
  }
}
",data,the block associated got renamed or deleted,,325,,04b817e5e25eb8691b316964e87f01ab15d339aa,20bc67be4a7d62f92027f56128eeabe46679e8a2,https://github.com/alphagov/govuk-aws/blob/04b817e5e25eb8691b316964e87f01ab15d339aa/terraform/projects/infra-database-backups-bucket/reader.tf#L325,https://github.com/alphagov/govuk-aws/blob/20bc67be4a7d62f92027f56128eeabe46679e8a2/terraform/projects/infra-database-backups-bucket/reader.tf,2023-10-31 20:03:46+00:00,2023-11-22 15:00:38+00:00,2,1,1,1,0,0,0,1,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1056,examples/third-party-solutions/wordpress/cloudrun/variables.tf,examples/third-party-solutions/wordpress/cloudrun/variables.tf,0,# todo,# TODO: check locals,type        = string # TODO: check locals,"variable ""project_id"" {
  description = ""Project id, references existing project if `project_create` is null.""
  type        = string # TODO: check locals
}
",variable,"variable ""project_id"" {
  description = ""Project id, references existing project if `project_create` is null.""
  type        = string
}
",variable,34,,cc60ae850e4c62b900f0ce1c38beb329437ec54e,d27f09e7797bfd8567f05966646c94836ee908f2,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/cc60ae850e4c62b900f0ce1c38beb329437ec54e/examples/third-party-solutions/wordpress/cloudrun/variables.tf#L34,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d27f09e7797bfd8567f05966646c94836ee908f2/examples/third-party-solutions/wordpress/cloudrun/variables.tf,2022-08-19 12:24:58+00:00,2022-08-31 14:19:54+00:00,2,1,0,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,1706,infra/modules/gcp/main.tf,infra/modules/gcp/main.tf,0,# todo,"# TODO: name ends in .jar, but it's actually a zipped JAR","# TODO: name ends in .jar, but it's actually a zipped JAR 
 # (gcp doesn't like straight JAR)","resource ""google_storage_bucket_object"" ""function"" {
  # TODO: name ends in .jar, but it's actually a zipped JAR
  # (gcp doesn't like straight JAR)
  name           = ""${var.environment_id_prefix}${local.file_name_with_sha1}""
  content_type   = ""application/zip""
  bucket         = google_storage_bucket.artifacts.name
  # source         = module.psoxy_package.path_to_deployment_jar
  source         = data.archive_file.source.output_path
  detect_md5hash = true
}
",resource,"resource ""google_storage_bucket_object"" ""function"" {
  name           = ""${var.environment_id_prefix}${local.file_name_with_sha1}""
  content_type   = ""application/zip""
  bucket         = google_storage_bucket.artifacts.name
  source         = coalesce(var.deployment_bundle, data.archive_file.source[0].output_path)
  detect_md5hash = true
}
",resource,173,,658db10204a6a35dc5a3583d24a86c7c8f10c0af,b9eeb4eb92b5bf9e1fe52487f2ac918fe04e0808,https://github.com/Worklytics/psoxy/blob/658db10204a6a35dc5a3583d24a86c7c8f10c0af/infra/modules/gcp/main.tf#L173,https://github.com/Worklytics/psoxy/blob/b9eeb4eb92b5bf9e1fe52487f2ac918fe04e0808/infra/modules/gcp/main.tf,2023-06-21 15:21:51-07:00,2023-06-21 15:44:40-07:00,2,1,1,1,0,0,0,0,0,0
https://github.com/uyuni-project/sumaform,268,modules/libvirt/mirror/variables.tf,modules/libvirt/mirror/variables.tf,0,hack,"# HACK: """" cannot be used as a default because of https://github\.com/hashicorp/hil/issues/50","# HACK: """" cannot be used as a default because of https://github\.com/hashicorp/hil/issues/50","variable ""ssh_key_path"" {
  description = ""path of additional pub ssh key you want to use to access VMs, see libvirt/README.md""
  default = ""/dev/null""
  # HACK: """" cannot be used as a default because of https://github\.com/hashicorp/hil/issues/50
}
",variable,"variable ""ssh_key_path"" {
  description = ""path of additional pub ssh key you want to use to access VMs, see libvirt/README.md""
  default = ""/dev/null""
  # HACK: """" cannot be used as a default because of https://github.com/hashicorp/hil/issues/50
}
",variable,35,,3b7407fd3d6f1265ceb845c4fd21a9f0f0a76beb,141dd30bb2100f8edfd74b0ad149662dcdeb03ea,https://github.com/uyuni-project/sumaform/blob/3b7407fd3d6f1265ceb845c4fd21a9f0f0a76beb/modules/libvirt/mirror/variables.tf#L35,https://github.com/uyuni-project/sumaform/blob/141dd30bb2100f8edfd74b0ad149662dcdeb03ea/modules/libvirt/mirror/variables.tf,2017-08-04 15:28:57+02:00,2017-08-22 16:07:20+02:00,2,1,0,1,1,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1044,examples/third-party-solutions/wordpress/cloudrun/main.tf,examples/third-party-solutions/wordpress/cloudrun/main.tf,0,todo,# TODO https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/cloud_run_service,ports = [{ # TODO https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/cloud_run_service,"module ""cloud_run"" {
  source     = ""../../../../modules/cloud-run""
  project_id = module.project.project_id
  name     = ""${var.prefix}-cr-wordpress""
  region = var.region
  cloudsql_instances = module.cloudsql.connection_name
  vpc_connector = {
    create          = true
    name            = ""wp-connector""
    egress_settings = ""all-traffic""
  }
  vpc_connector_config = {
    network       = module.vpc.self_link
    ip_cidr_range = ""10.8.0.0/28"" # !!!
  }
#        ""run.googleapis.com/ingress"" = ""internal-and-cloud-load-balancing""

  containers = [{
    image = var.wordpress_image
    ports = [{ # TODO https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/cloud_run_service
      name           = ""http1""
      protocol       = ""TCP""
      container_port = 80
    }]
    options = {
      command  = null
      args     = null
      env_from = null
      env      = {
        ""DB_HOST""        : module.cloudsql.ip #google_sql_database_instance.cloud_sql.private_ip_address
        ""DB_USER""        : local.cloud_sql_conf.user
        ""DB_PASSWORD""    : local.cloud_sql_conf.pass
        ""DB_NAME""        : local.cloud_sql_conf.db
        ""WORDPRESS_DEBUG"": ""1""
      }
    }
    resources = null
    volume_mounts = null
  }]

  iam = {
    ""roles/run.invoker"": [var.cloud_run_invoker]
  }
}
",module,"module ""cloud_run"" { # create the Cloud Run service
  source     = ""../../../../modules/cloud-run""
  project_id = module.project.project_id
  name     = ""${local.prefix}cr-wordpress""
  region = var.region

  containers = [{
    image = var.wordpress_image
    ports = [{
      name           = ""http1""
      protocol       = null
      container_port = var.wordpress_port
    }]
    options = {
      command  = null
      args     = null
      env_from = null
      env      = { # set up the database connection
        ""APACHE_HTTP_PORT_NUMBER""    : var.wordpress_port
        ""WORDPRESS_DATABASE_HOST""    : module.cloudsql.ip
        ""WORDPRESS_DATABASE_NAME""    : local.cloud_sql_conf.db
        ""WORDPRESS_DATABASE_USER""    : local.cloud_sql_conf.user
        ""WORDPRESS_DATABASE_PASSWORD"": local.cloud_sql_conf.pass
        ""WORDPRESS_USERNAME""         : local.wp_user
        ""WORDPRESS_PASSWORD""         : random_password.wp_password.result
      }
    }
    resources = null
    volume_mounts = null
  }]

  iam = {
    ""roles/run.invoker"": [var.cloud_run_invoker]
  }

  revision_annotations = {
    autoscaling = {
      min_scale = 1
      max_scale = 2
    }
    # connect to CloudSQL
    cloudsql_instances = [ module.cloudsql.connection_name ]
    vpcaccess_connector = null
    vpcaccess_egress    = ""all-traffic"" # allow all traffic
  }
  ingress_settings = ""all""

  vpc_connector_create = { # create a VPC connector for the ClouSQL VPC
    ip_cidr_range = var.connector_cidr
    name          = ""${local.prefix}wp-connector""
    vpc_self_link = module.vpc.self_link
  }
}
",module,93,,bcb4a334a0fe06b39831a3c60d501a9fc2a6ca7c,d27f09e7797bfd8567f05966646c94836ee908f2,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/bcb4a334a0fe06b39831a3c60d501a9fc2a6ca7c/examples/third-party-solutions/wordpress/cloudrun/main.tf#L93,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d27f09e7797bfd8567f05966646c94836ee908f2/examples/third-party-solutions/wordpress/cloudrun/main.tf,2022-08-09 09:21:24+00:00,2022-08-31 14:19:54+00:00,3,1,1,1,0,0,1,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,165,examples/launch_templates_with_managed_node_groups/launchtemplate.tf,examples/launch_templates_with_managed_node_groups/launchtemplate.tf,0,xxx,// this is based on the LT that EKS would create if no custom one is specified (aws ec2 describe-launch-template-versions --launch-template-id xxx),"// this is based on the LT that EKS would create if no custom one is specified (aws ec2 describe-launch-template-versions --launch-template-id xxx) 
 // there are several more options one could set but you probably dont need to modify them 
 // you can take the default and add your custom AMI and/or custom tags 
 // 
 // Trivia: AWS transparently creates a copy of your LaunchTemplate and actually uses that copy then for the node group. If you DONT use a custom AMI, 
 // then the default user-data for bootstrapping a cluster is merged in the copy.","resource ""aws_launch_template"" ""default"" {
  name_prefix            = ""eks-example-""
  description            = ""Default Launch-Template""
  update_default_version = true

  block_device_mappings {
    device_name = ""/dev/xvda""

    ebs {
      volume_size           = 100
      volume_type           = ""gp2""
      delete_on_termination = true
      //encrypted             = true
      // enable this if you want to encrypt your node root volumes with a KMS/CMK. encryption of PVCs is handled via k8s StorageClass tho
      // you also need to attach data.aws_iam_policy_document.ebs_decryption.json from the disk_encryption_policy.tf to the KMS/CMK key then !!
      //kms_key_id            = var.kms_key_arn 
    }
  }

  instance_type = var.instance_type

  monitoring {
    enabled = true
  }

  network_interfaces {
    associate_public_ip_address = false
    delete_on_termination       = true
    security_groups             = [module.eks.worker_security_group_id]
  }

  //image_id      = var.ami_id // if you want to use a custom AMI

  // if you use a custom AMI, you need to supply via user-data, the bootstrap script as EKS DOESNT merge its managed user-data then
  // you can add more than the minimum code you see in the template, e.g. install SSM agent, see https://github.com/aws/containers-roadmap/issues/593#issuecomment-577181345
  //
  // (optionally you can use https://registry.terraform.io/providers/hashicorp/cloudinit/latest/docs/data-sources/cloudinit_config to render the script, example: https://github.com/terraform-aws-modules/terraform-aws-eks/pull/997#issuecomment-705286151)

  // user_data = base64encode(
  //   data.template_file.launch_template_userdata.rendered,
  // )


  // supplying custom tags to EKS instances is another use-case for LaunchTemplates
  tag_specifications {
    resource_type = ""instance""

    tags = {
      CustomTag = ""EKS example""
    }
  }

  // supplying custom tags to EKS instances root volumes is another use-case for LaunchTemplates. (doesnt add tags to dynamically provisioned volumes via PVC tho)
  tag_specifications {
    resource_type = ""volume""

    tags = {
      CustomTag = ""EKS example""
    }
  }

  // tag the LT itself
  tags = {
    CustomTag = ""EKS example""
  }

  lifecycle {
    create_before_destroy = true
  }
}
",resource,"resource ""aws_launch_template"" ""default"" {
  name_prefix            = ""eks-example-""
  description            = ""Default Launch-Template""
  update_default_version = true

  block_device_mappings {
    device_name = ""/dev/xvda""

    ebs {
      volume_size           = 100
      volume_type           = ""gp2""
      delete_on_termination = true
      # encrypted             = true

      # Enable this if you want to encrypt your node root volumes with a KMS/CMK. encryption of PVCs is handled via k8s StorageClass tho
      # you also need to attach data.aws_iam_policy_document.ebs_decryption.json from the disk_encryption_policy.tf to the KMS/CMK key then !!
      # kms_key_id            = var.kms_key_arn
    }
  }

  instance_type = var.instance_type

  monitoring {
    enabled = true
  }

  network_interfaces {
    associate_public_ip_address = false
    delete_on_termination       = true
    security_groups             = [module.eks.worker_security_group_id]
  }

  # if you want to use a custom AMI
  # image_id      = var.ami_id

  # If you use a custom AMI, you need to supply via user-data, the bootstrap script as EKS DOESNT merge its managed user-data then
  # you can add more than the minimum code you see in the template, e.g. install SSM agent, see https://github.com/aws/containers-roadmap/issues/593#issuecomment-577181345
  #
  # (optionally you can use https://registry.terraform.io/providers/hashicorp/cloudinit/latest/docs/data-sources/cloudinit_config to render the script, example: https://github.com/terraform-aws-modules/terraform-aws-eks/pull/997#issuecomment-705286151)

  # user_data = base64encode(
  #   data.template_file.launch_template_userdata.rendered,
  # )


  # Supplying custom tags to EKS instances is another use-case for LaunchTemplates
  tag_specifications {
    resource_type = ""instance""

    tags = {
      CustomTag = ""EKS example""
    }
  }

  # Supplying custom tags to EKS instances root volumes is another use-case for LaunchTemplates. (doesnt add tags to dynamically provisioned volumes via PVC tho)
  tag_specifications {
    resource_type = ""volume""

    tags = {
      CustomTag = ""EKS example""
    }
  }

  # Tag the LT itself
  tags = {
    CustomTag = ""EKS example""
  }

  lifecycle {
    create_before_destroy = true
  }
}
",resource,14,,127a3a883189d887eac3d0c9e91b7aa335d53b77,571e4e7f4bca0c30a6d714a4cbebb9aaaf69c88a,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/127a3a883189d887eac3d0c9e91b7aa335d53b77/examples/launch_templates_with_managed_node_groups/launchtemplate.tf#L14,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/571e4e7f4bca0c30a6d714a4cbebb9aaaf69c88a/examples/launch_templates_with_managed_node_groups/launchtemplate.tf,2020-11-02 08:19:10+01:00,2020-11-02 08:35:12+01:00,2,1,1,0,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-kubernetes-engine,7,modules/safer-cluster/main.tf,modules/safer-cluster/main.tf,0,// todo,// TODO(mmontan): we generally considered applying,"// TODO(mmontan): we generally considered applying 
 // just the cloud-platofrm scope and use Cloud IAM 
 // If we have Workload Identity, are there advantages 
 // in restricting scopes even more?","module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // Disable the dashboard. It creates risk by running as a very sensitive user.
  kubernetes_dashboard = false

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy          = true
  network_policy_provider = ""CALICO""

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  disable_legacy_metadata_endpoints = true

  node_pools        = var.node_pools
  node_pools_labels = var.node_pools_labels

  // TODO(mmontan): check whether we need to restrict these
  // settings.
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  // TODO(mmontan): we generally considered applying
  // just the cloud-platofrm scope and use Cloud IAM
  // If we have Workload Identity, are there advantages
  // in restricting scopes even more?
  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  // We should use IP Alias.
  configure_ip_masq = false

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = length(var.compute_engine_service_account) > 0 ? false : true
  service_account        = var.compute_engine_service_account

  // TODO(mmontan): define a registry_project parameter in the private_beta_cluster,
  // so that we can give GCS permissions to the service account on a project
  // that hosts only container-images and not data.
  grant_registry_access = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // TODO(mmontan): link to a couple of policies.
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id
  node_metadata                    = ""SECURE""

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // TODO(mmontan): investigate whether this should be a recommended setting
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group
}
",module,"module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy = true

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  node_pools        = var.node_pools
  node_pools_labels = var.node_pools_labels

  // TODO(mmontan): check whether we need to restrict these
  // settings.
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = var.compute_engine_service_account == """" ? true : false
  service_account        = var.compute_engine_service_account
  registry_project_id    = var.registry_project_id
  grant_registry_access  = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // Example: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#example
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // Intranode Visibility enables you to capture flow logs for traffic between pods and create FW rules that apply to traffic between pods.
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group

  enable_shielded_nodes = var.enable_shielded_nodes
}
",module,80,,e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5,5a194719faa144ad0a7ee578663d336358f5073c,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5/modules/safer-cluster/main.tf#L80,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/5a194719faa144ad0a7ee578663d336358f5073c/modules/safer-cluster/main.tf,2019-10-04 15:21:33-07:00,2019-11-13 15:10:12-06:00,6,1,1,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,218,infra/modules/azuread-local-cert/variables.tf,infra/modules/azuread-local-cert/variables.tf,0,# todo,# TODO: should be your organization!!,"default     = ""/C=US/ST=New York/L=New York/CN=www.worklytics.co"" # TODO: should be your organization!!","variable ""certificate_subject"" {
  type        = string
  description = ""value for 'subject' passed to openssl when generation certificate""
  default     = ""/C=US/ST=New York/L=New York/CN=www.worklytics.co"" # TODO: should be your organization!!
}
",variable,"variable ""certificate_subject"" {
  type        = string
  description = ""value for 'subject' passed to openssl when generation certificate (eg '/C=US/ST=New York/L=New York/CN=www.worklytics.co')""
}
",variable,21,,747196b6d081d59b6abb86a8f1d0f47b590c0cb7,a87c975913efccae7f4f6db646c8b57e9ab45106,https://github.com/Worklytics/psoxy/blob/747196b6d081d59b6abb86a8f1d0f47b590c0cb7/infra/modules/azuread-local-cert/variables.tf#L21,https://github.com/Worklytics/psoxy/blob/a87c975913efccae7f4f6db646c8b57e9ab45106/infra/modules/azuread-local-cert/variables.tf,2022-01-24 15:32:14-08:00,2022-01-24 22:10:59-08:00,2,1,0,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,204,infra/modules/psoxy-package/main.tf,infra/modules/psoxy-package/main.tf,0,implementation,# can't build implementation package to deploy until core pkg is built and installed to local mvn repo,# can't build implementation package to deploy until core pkg is built and installed to local mvn repo,"resource ""null_resource"" ""deployment_package"" {
  triggers = {
    pom_hash = filebase64sha256(""${local.path_to_impl_module}/pom.xml"")
  }

  provisioner ""local-exec"" {
    working_dir = local.path_to_impl_module
    command     = ""mvn package""
  }

  # can't build implementation package to deploy until core pkg is built and installed to local mvn repo
  depends_on = [
    null_resource.core_package
  ]
}
",resource,the block associated got renamed or deleted,,34,,450629d23c2f1ec4eb34e057d6d2c3ae5215abf2,d30ea8e93bb4e9b7f7eb27dfada16817ee3541f0,https://github.com/Worklytics/psoxy/blob/450629d23c2f1ec4eb34e057d6d2c3ae5215abf2/infra/modules/psoxy-package/main.tf#L34,https://github.com/Worklytics/psoxy/blob/d30ea8e93bb4e9b7f7eb27dfada16817ee3541f0/infra/modules/psoxy-package/main.tf,2022-01-19 08:22:58-08:00,2022-01-19 10:55:08-08:00,2,1,0,1,0,0,0,1,0,0
https://github.com/uyuni-project/sumaform,1383,backend_modules/aws/host/main.tf,backend_modules/aws/host/main.tf,0,workaround,# WORKAROUND: ephemeral block devices are defined in any case,"# WORKAROUND: ephemeral block devices are defined in any case 
 # they will only be used for instance types that provide them","resource ""aws_instance"" ""instance"" {
  ami                    = local.ami
  instance_type          = local.provider_settings[""instance_type""]
  count                  = var.quantity
  availability_zone      = local.availability_zone
  key_name               = local.provider_settings[""key_name""]
  subnet_id              = var.connect_to_base_network ? (local.provider_settings[""public_instance""] ? local.public_subnet_id : local.private_subnet_id) : var.connect_to_additional_network ? local.private_additional_subnet_id : local.private_subnet_id
  vpc_security_group_ids = [var.connect_to_base_network ? (local.provider_settings[""public_instance""] ? local.public_security_group_id : local.private_security_group_id) : var.connect_to_additional_network ? local.private_additional_security_group_id : local.private_security_group_id]

  root_block_device {
    volume_size = local.provider_settings[""volume_size""]
  }

  user_data = data.template_file.user_data[count.index].rendered

  # WORKAROUND: ephemeral block devices are defined in any case
  # they will only be used for instance types that provide them
  ephemeral_block_device {
    device_name  = ""xvdb""
    virtual_name = ""ephemeral0""
  }

  ephemeral_block_device {
    device_name  = ""xvdc""
    virtual_name = ""ephemeral1""
  }

  tags = {
    Name = ""${local.resource_name_prefix}${var.quantity > 1 ? ""-${count.index + 1}"" : """"}""
  }

  # WORKAROUND
  # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner"").
  # After the first `apply`, terraform removes those tags. The following block avoids this behavior.
  # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider
  # See github:terraform-providers/terraform-provider-aws#10689
  lifecycle {
    ignore_changes = [tags]
  }
}
",resource,"resource ""aws_instance"" ""instance"" {
  ami                    = local.ami
  instance_type          = local.provider_settings[""instance_type""]
  count                  = var.quantity
  availability_zone      = local.availability_zone
  key_name               = local.provider_settings[""key_name""]
  subnet_id              = var.connect_to_base_network ? (local.provider_settings[""public_instance""] ? local.public_subnet_id : local.private_subnet_id) : var.connect_to_additional_network ? local.private_additional_subnet_id : local.private_subnet_id
  vpc_security_group_ids = [var.connect_to_base_network ? (local.provider_settings[""public_instance""] ? local.public_security_group_id : local.private_security_group_id) : var.connect_to_additional_network ? local.private_additional_security_group_id : local.private_security_group_id]
  private_ip             = local.private_ip
  iam_instance_profile   = contains(var.roles, ""server"") ? var.base_configuration[""iam_instance_profile""] : null

  root_block_device {
    volume_size = local.provider_settings[""volume_size""]
  }

  user_data = data.template_file.user_data[count.index].rendered

  # WORKAROUND: ephemeral block devices are defined in any case
  # they will only be used for instance types that provide them
  ephemeral_block_device {
    device_name  = ""xvdb""
    virtual_name = ""ephemeral0""
  }

  ephemeral_block_device {
    device_name  = ""xvdc""
    virtual_name = ""ephemeral1""
  }

  tags = {
    Name = ""${local.resource_name_prefix}${var.quantity > 1 ? ""-${count.index + 1}"" : """"}""
  }

  connection {
    private_ip = self.private_ip
  }

  # WORKAROUND
  # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner"").
  # After the first `apply`, terraform removes those tags. The following block avoids this behavior.
  # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider
  # See github:terraform-providers/terraform-provider-aws#10689
  lifecycle {
    ignore_changes = [tags]
  }
}
",resource,62,92.0,12fc857978857ebd94f9b0906480004ca9b88c22,3a05fda9be0240b065bb2031b5812c595b77f973,https://github.com/uyuni-project/sumaform/blob/12fc857978857ebd94f9b0906480004ca9b88c22/backend_modules/aws/host/main.tf#L62,https://github.com/uyuni-project/sumaform/blob/3a05fda9be0240b065bb2031b5812c595b77f973/backend_modules/aws/host/main.tf#L92,2021-01-26 15:58:29+01:00,2024-04-12 13:39:31+12:00,35,0,1,0,0,0,0,0,0,0
https://github.com/compiler-explorer/infra,47,terraform/security.tf,terraform/security.tf,0,fix,# completely (with access only to admin node and the ALB) but this would make diagnosing and fixing,"# It's convenient for Compiler explorer nodes to be able to do things like `git pull` and `docker pull`, 
 # so need to be able to talk to the outside world. Ideally they'd be locked down 
 # completely (with access only to admin node and the ALB); but this would make diagnosing and fixing 
 # issues quickly on-box very difficult.","resource ""aws_security_group_rule"" ""CE_EgressToAll"" {
  security_group_id = aws_security_group.CompilerExplorer.id
  type              = ""egress""
  from_port         = 0
  to_port           = 65535
  cidr_blocks       = [""0.0.0.0/0""]
  ipv6_cidr_blocks  = [""::/0""]
  protocol          = ""-1""
  description       = ""Unfettered outbound access""
}
",resource,"resource ""aws_security_group_rule"" ""CE_EgressToAll"" {
  security_group_id = aws_security_group.CompilerExplorer.id
  type              = ""egress""
  from_port         = 0
  to_port           = 65535
  cidr_blocks       = [""0.0.0.0/0""]
  ipv6_cidr_blocks  = [""::/0""]
  protocol          = ""-1""
  description       = ""Unfettered outbound access""
}
",resource,13,12.0,bb786e00dceef7c73aca83bb026899d8bb90632c,e1b1416e3a1af2b54c905c5b90153208afdd87e7,https://github.com/compiler-explorer/infra/blob/bb786e00dceef7c73aca83bb026899d8bb90632c/terraform/security.tf#L13,https://github.com/compiler-explorer/infra/blob/e1b1416e3a1af2b54c905c5b90153208afdd87e7/terraform/security.tf#L12,2019-09-09 12:16:04-05:00,2024-04-22 22:51:52+02:00,36,0,0,0,0,1,0,0,0,0
https://github.com/wireapp/wire-server-deploy,2,terraform/modules/aws_vpc_security_groups/main.tf,terraform/modules/aws-vpc-security-groups/main.tf,1,fix,# FIXME: tighten this up.,# FIXME: tighten this up.,"resource ""aws_security_group"" ""k8s_node"" {
  name        = ""k8s_node""
  description = ""hosts that have kubernetes.""
  vpc_id      = var.vpc_id

  # incoming from the admin node (kubectl)
  ingress {
    from_port   = 6443
    to_port     = 6443
    protocol    = ""tcp""
    security_groups = [""${aws_security_group.talk_to_k8s.id}""]
  }

  # FIXME: tighten this up.
  ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = ""tcp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # FIXME: tighten this up. need UDP for flannel.
  ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = ""udp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  tags = {
    Name = ""k8s_node""
  }
}
",resource,"resource ""aws_security_group"" ""k8s_node"" {
  name        = ""k8s_node""
  description = ""hosts that have kubernetes.""
  vpc_id      = var.vpc_id

  # incoming from the admin node (kubectl)
  ingress {
    description     = """"
    from_port       = 6443
    to_port         = 6443
    protocol        = ""tcp""
    security_groups = [""${aws_security_group.talk_to_k8s.id}""]
  }

  # FIXME: tighten this up.
  ingress {
    description     = """"
    from_port       = 0
    to_port         = 65535
    protocol        = ""tcp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # FIXME: tighten this up. need UDP for flannel.
  ingress {
    description     = """"
    from_port       = 0
    to_port         = 65535
    protocol        = ""udp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # incoming traffic to the application.
  ingress {
    from_port   = 31772
    to_port     = 31773
    protocol    = ""tcp""
    # NOTE: NLBs dont allow security groups to be set on them, which is why
    # we go with the CIDR for now, which is hard-coded and needs fixing
    cidr_blocks = [""172.17.0.0/20""]
  }

  tags = {
    Name = ""k8s_node""
  }
}
",resource,213,225.0,cb61b733457d5a1ccdb5c9dcb5fde4cab9e5d7e0,e7f081ca744de60bf9d10cb037828b995883d2aa,https://github.com/wireapp/wire-server-deploy/blob/cb61b733457d5a1ccdb5c9dcb5fde4cab9e5d7e0/terraform/modules/aws_vpc_security_groups/main.tf#L213,https://github.com/wireapp/wire-server-deploy/blob/e7f081ca744de60bf9d10cb037828b995883d2aa/terraform/modules/aws-vpc-security-groups/main.tf#L225,2020-04-23 17:54:17+01:00,2020-08-26 16:29:39+02:00,5,0,0,0,0,1,0,0,0,0
https://github.com/camptocamp/devops-stack,99,examples/kind/main.tf,examples/kind/main.tf,0,todo,"# TODO remove useless base_domain and cluster_name variables from ""self-signed"" module.","source = ""git::https://github.com/camptocamp/devops-stack-module-cert-manager.git//self-signed?ref=v2.0.0""  
 # TODO remove useless base_domain and cluster_name variables from ""self-signed"" module.","module ""cert-manager"" {
  source = ""git::https://github.com/camptocamp/devops-stack-module-cert-manager.git//self-signed?ref=v2.0.0""

  # TODO remove useless base_domain and cluster_name variables from ""self-signed"" module.
  cluster_name     = local.cluster_name
  base_domain      = local.base_domain
  argocd_namespace = module.argocd_bootstrap.argocd_namespace

  dependency_ids = {
    argocd = module.argocd_bootstrap.id
  }
}
",module,"module ""cert-manager"" {
  source = ""git::https://github.com/camptocamp/devops-stack-module-cert-manager.git//self-signed?ref=v3.1.0""

  argocd_namespace = module.argocd_bootstrap.argocd_namespace

  enable_service_monitor = local.enable_service_monitor

  dependency_ids = {
    argocd = module.argocd_bootstrap.id
  }
}
",module,86,,e3e1a35b6a90a2990878d6c06775a6dba94637af,a620f2d89d7072298b6a5d5c8c8a9d4b7e8ec11c,https://github.com/camptocamp/devops-stack/blob/e3e1a35b6a90a2990878d6c06775a6dba94637af/examples/kind/main.tf#L86,https://github.com/camptocamp/devops-stack/blob/a620f2d89d7072298b6a5d5c8c8a9d4b7e8ec11c/examples/kind/main.tf,2023-05-16 13:05:31+02:00,2023-05-22 09:53:27+02:00,2,1,1,1,0,0,1,0,0,0
https://github.com/Worklytics/psoxy,505,infra/modules/gcp-psoxy-rest/main.tf,infra/modules/gcp-psoxy-rest/main.tf,0,# todo,"# TODO: Support version, by default now is latest","# TODO: Support version, by default now is latest","resource ""google_cloudfunctions_function"" ""function"" {
  name        = var.instance_id
  description = ""Psoxy Connector - ${var.source_kind}""
  runtime     = ""java11""
  project     = var.project_id
  region      = var.region

  available_memory_mb   = 1024
  source_archive_bucket = var.artifacts_bucket_name
  source_archive_object = var.deployment_bundle_object_name
  entry_point           = ""co.worklytics.psoxy.Route""
  service_account_email = var.service_account_email

  environment_variables = merge(
    var.path_to_config == null ? {} : yamldecode(file(var.path_to_config)),
    var.environment_variables
  )

  dynamic ""secret_environment_variables"" {
    for_each = local.secret_bindings
    iterator = secret_environment_variable

    content {
      key        = secret_environment_variable.key
      project_id = data.google_project.project.number
      secret     = secret_environment_variable.value.secret_id
      version    = secret_environment_variable.value.version_number
    }
  }

  dynamic ""secret_volumes"" {
    for_each = var.secret_volumes
    iterator = secret_volume

    content {
      project_id = data.google_project.project.number
      secret     = secret_volume.value.secret_id
      mount_path = ""/etc/secrets/${secret_volume.key}""

      # TODO: Support version, by default now is latest
    }
  }

  trigger_http = true

  lifecycle {
    ignore_changes = [
      labels
    ]
  }

  depends_on = [
    google_secret_manager_secret_iam_member.grant_sa_accessor_on_secret
  ]
}
",resource,"resource ""google_cloudfunctions_function"" ""function"" {
  name        = var.instance_id
  description = ""Psoxy Connector - ${var.source_kind}""
  runtime     = ""java11""
  project     = var.project_id
  region      = var.region

  available_memory_mb   = 1024
  source_archive_bucket = var.artifacts_bucket_name
  source_archive_object = var.deployment_bundle_object_name
  entry_point           = ""co.worklytics.psoxy.Route""
  service_account_email = var.service_account_email

  environment_variables = merge(
    var.path_to_config == null ? {} : yamldecode(file(var.path_to_config)),
    var.environment_variables
  )

  dynamic ""secret_environment_variables"" {
    for_each = local.secret_bindings
    iterator = secret_environment_variable

    content {
      key        = secret_environment_variable.key
      project_id = data.google_project.project.number
      secret     = secret_environment_variable.value.secret_id
      version    = secret_environment_variable.value.version_number
    }
  }

  trigger_http = true

  lifecycle {
    ignore_changes = [
      labels
    ]
  }

  depends_on = [
    google_secret_manager_secret_iam_member.grant_sa_accessor_on_secret
  ]
}
",resource,73,,5c45b21ff779761f52696850549448cfca58f601,454c610080da3d260517b8aa102bf2de5411189c,https://github.com/Worklytics/psoxy/blob/5c45b21ff779761f52696850549448cfca58f601/infra/modules/gcp-psoxy-rest/main.tf#L73,https://github.com/Worklytics/psoxy/blob/454c610080da3d260517b8aa102bf2de5411189c/infra/modules/gcp-psoxy-rest/main.tf,2022-11-09 21:46:45+01:00,2022-11-10 21:14:03+01:00,2,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,650,examples/data-solutions/data-platform-foundations/02-load.tf,examples/data-solutions/data-platform-foundations/02-load.tf,0,# todo,# TODO: these are needed on the shared VPC?,"# TODO: these are needed on the shared VPC? 
 # ""roles/compute.serviceAgent"" = [ 
 #   ""serviceAccount:${module.load-project.service_accounts.robots.compute}"" 
 # ] 
 # ""roles/dataflow.serviceAgent"" = [ 
 #   ""serviceAccount:${module.load-project.service_accounts.robots.dataflow}"" 
 # ]","module ""load-project"" {
  source          = ""../../../modules/project""
  parent          = var.folder_id
  billing_account = var.billing_account_id
  prefix          = var.prefix
  name            = ""lod""
  group_iam = {
    (local.groups.data-engineers) = [
      ""roles/compute.viewer"",
      ""roles/dataflow.admin"",
      ""roles/dataflow.developer"",
      ""roles/viewer"",
    ]
  }
  iam = {
    ""roles/bigquery.jobUser"" = [module.load-sa-df-0.iam_email]
    ""roles/dataflow.admin"" = [
      module.orch-sa-cmp-0.iam_email, module.load-sa-df-0.iam_email
    ]
    ""roles/dataflow.worker""     = [module.load-sa-df-0.iam_email]
    ""roles/storage.objectAdmin"" = local.load_service_accounts
    # TODO: these are needed on the shared VPC?
    # ""roles/compute.serviceAgent"" = [
    #   ""serviceAccount:${module.load-project.service_accounts.robots.compute}""
    # ]
    # ""roles/dataflow.serviceAgent"" = [
    #   ""serviceAccount:${module.load-project.service_accounts.robots.dataflow}""
    # ]
  }
  services = concat(var.project_services, [
    ""bigquery.googleapis.com"",
    ""bigqueryreservation.googleapis.com"",
    ""bigquerystorage.googleapis.com"",
    ""cloudkms.googleapis.com"",
    ""compute.googleapis.com"",
    ""dataflow.googleapis.com"",
    ""dlp.googleapis.com"",
    ""pubsub.googleapis.com"",
    ""servicenetworking.googleapis.com"",
    ""storage.googleapis.com"",
    ""storage-component.googleapis.com""
  ])
  service_encryption_key_ids = {
    pubsub   = [try(local.service_encryption_keys.pubsub, null)]
    dataflow = [try(local.service_encryption_keys.dataflow, null)]
    storage  = [try(local.service_encryption_keys.storage, null)]
  }
  shared_vpc_service_config = local.shared_vpc_project == null ? null : {
    attach       = true
    host_project = local.shared_vpc_project
    service_identity_iam = {
      # TODO: worker service account
      ""compute.networkUser"" = [""dataflow""]
    }
  }
}
",module,"module ""load-project"" {
  source          = ""../../../modules/project""
  parent          = var.folder_id
  billing_account = var.billing_account_id
  prefix          = var.prefix
  name            = ""lod""
  group_iam = {
    (local.groups.data-engineers) = [
      ""roles/compute.viewer"",
      ""roles/dataflow.admin"",
      ""roles/dataflow.developer"",
      ""roles/viewer"",
    ]
  }
  iam = {
    ""roles/bigquery.jobUser"" = [module.load-sa-df-0.iam_email]
    ""roles/dataflow.admin"" = [
      module.orch-sa-cmp-0.iam_email, module.load-sa-df-0.iam_email
    ]
    ""roles/dataflow.worker""     = [module.load-sa-df-0.iam_email]
    ""roles/storage.objectAdmin"" = local.load_service_accounts
  }
  services = concat(var.project_services, [
    ""bigquery.googleapis.com"",
    ""bigqueryreservation.googleapis.com"",
    ""bigquerystorage.googleapis.com"",
    ""cloudkms.googleapis.com"",
    ""compute.googleapis.com"",
    ""dataflow.googleapis.com"",
    ""dlp.googleapis.com"",
    ""pubsub.googleapis.com"",
    ""servicenetworking.googleapis.com"",
    ""storage.googleapis.com"",
    ""storage-component.googleapis.com""
  ])
  service_encryption_key_ids = {
    pubsub   = [try(local.service_encryption_keys.pubsub, null)]
    dataflow = [try(local.service_encryption_keys.dataflow, null)]
    storage  = [try(local.service_encryption_keys.storage, null)]
  }
  shared_vpc_service_config = local.shared_vpc_project == null ? null : {
    attach               = true
    host_project         = local.shared_vpc_project
    service_identity_iam = {}
  }
}
",module,57,,4f4a9cd7ac2cee3a1b2e413cd19f6b6b07c35622,12383ae72df0082cb52ef70803aa2d293a66d674,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/4f4a9cd7ac2cee3a1b2e413cd19f6b6b07c35622/examples/data-solutions/data-platform-foundations/02-load.tf#L57,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/12383ae72df0082cb52ef70803aa2d293a66d674/examples/data-solutions/data-platform-foundations/02-load.tf,2022-02-09 17:01:25+01:00,2022-02-12 09:48:16+01:00,7,1,0,1,0,1,1,0,0,1
https://github.com/terraform-aws-modules/terraform-aws-eks,408,examples/karpenter/main.tf,examples/karpenter/main.tf,0,workaround,# Workaround - https://github.com/hashicorp/terraform-provider-kubernetes/issues/1380#issuecomment-967022975,# Workaround - https://github.com/hashicorp/terraform-provider-kubernetes/issues/1380#issuecomment-967022975,"resource ""kubectl_manifest"" ""karpenter_provisioner"" {
  yaml_body = <<-YAML
  apiVersion: karpenter.sh/v1alpha5
  kind: Provisioner
  metadata:
    name: default
  spec:
    requirements:
      - key: karpenter.sh/capacity-type
        operator: In
        values: [""spot""]
    limits:
      resources:
        cpu: 1000
    provider:
      subnetSelector:
        karpenter.sh/discovery: ${local.name}
      securityGroupSelector:
        karpenter.sh/discovery: ${local.name}
      tags:
        karpenter.sh/discovery: ${local.name}
    ttlSecondsAfterEmpty: 30
  YAML

  depends_on = [
    helm_release.karpenter
  ]
}
",resource,"resource ""kubectl_manifest"" ""karpenter_provisioner"" {
  yaml_body = <<-YAML
    apiVersion: karpenter.sh/v1alpha5
    kind: Provisioner
    metadata:
      name: default
    spec:
      requirements:
        - key: karpenter.sh/capacity-type
          operator: In
          values: [""spot""]
      limits:
        resources:
          cpu: 1000
      providerRef:
        name: default
      ttlSecondsAfterEmpty: 30
  YAML

  depends_on = [
    helm_release.karpenter
  ]
}
",resource,154,,3ff17205a4ead51cca993547ef3de42cc080043b,f24de3326d3c12ce61fbaefe1e3dbe7418d8bc85,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/3ff17205a4ead51cca993547ef3de42cc080043b/examples/karpenter/main.tf#L154,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/f24de3326d3c12ce61fbaefe1e3dbe7418d8bc85/examples/karpenter/main.tf,2022-04-07 20:47:22+02:00,2022-11-21 13:50:34-05:00,4,1,1,0,1,0,0,0,0,0
https://github.com/rust-lang/simpleinfra,6,terragrunt/modules/crates-io/fastly-static.tf,terragrunt/modules/crates-io/fastly-static.tf,0,hack,# The below snippet is a hack to get the CNAME for the static domain from the,"# It is currently not possible to get the CNAME for TLS-enabled hostnames as a 
 # Terraform resource. But the ACME HTTP challenge redirects production traffic 
 # to Fastly, for which it uses the CNAME that we are looking for. 
 # 
 # The below snippet is a hack to get the CNAME for the static domain from the 
 # HTTP challenge, until Fastly exposes it in the Terraform provider.","locals {
  # It is currently not possible to get the CNAME for TLS-enabled hostnames as a
  # Terraform resource. But the ACME HTTP challenge redirects production traffic
  # to Fastly, for which it uses the CNAME that we are looking for.
  #
  # The below snippet is a hack to get the CNAME for the static domain from the
  # HTTP challenge, until Fastly exposes it in the Terraform provider.
  fastly_static_destinations = flatten([
    for record in fastly_tls_subscription.static.managed_http_challenges :
    record.record_values if record.record_type == ""CNAME""
  ])
}
",locals,"locals {
  fastly_domain_name = ""fastly-${var.static_domain_name}""

  primary_host_name  = aws_s3_bucket.static.region
  fallback_host_name = aws_s3_bucket.fallback.region

  dictionary_name = ""compute_static""

  request_logs_endpoint = ""s3-request-logs""
  service_logs_endpoint = ""s3-service-logs""
}
",locals,103,,da7cb3473f2c2ad29615132a9eff76ed1933dc60,6b807140a33708afac26894642fe13e46ab7a851,https://github.com/rust-lang/simpleinfra/blob/da7cb3473f2c2ad29615132a9eff76ed1933dc60/terragrunt/modules/crates-io/fastly-static.tf#L103,https://github.com/rust-lang/simpleinfra/blob/6b807140a33708afac26894642fe13e46ab7a851/terragrunt/modules/crates-io/fastly-static.tf,2022-12-21 12:24:08+01:00,2023-03-23 12:37:19+01:00,14,1,0,0,1,1,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,153,modules/organization/main.tf,modules/organization/main.tf,0,# todo,# TODO: add logging buckets support,"# TODO: add logging buckets support 
 # logging  = ""logging.googleapis.com""","locals {
  organization_id_numeric = split(""/"", var.organization_id)[1]
  iam_additive_pairs = flatten([
    for role, members in var.iam_additive : [
      for member in members : { role = role, member = member }
    ]
  ])
  iam_additive_member_pairs = flatten([
    for member, roles in var.iam_additive_members : [
      for role in roles : { role = role, member = member }
    ]
  ])
  iam_additive = {
    for pair in concat(local.iam_additive_pairs, local.iam_additive_member_pairs) :
    ""${pair.role}-${pair.member}"" => pair
  }
  extended_rules = flatten([
    for policy, rules in var.firewall_policies : [
      for rule_name, rule in rules :
      merge(rule, { policy = policy, name = rule_name })
    ]
  ])
  rules_map = {
    for rule in local.extended_rules :
    ""${rule.policy}-${rule.name}"" => rule
  }
  logging_sinks = coalesce(var.logging_sinks, {})
  sink_type_destination = {
    gcs      = ""storage.googleapis.com""
    bigquery = ""bigquery.googleapis.com""
    pubsub   = ""pubsub.googleapis.com""
    # TODO: add logging buckets support
    # logging  = ""logging.googleapis.com""
  }
  sink_bindings = {
    for type in [""gcs"", ""bigquery"", ""pubsub"", ""logging""] :
    type => {
      for name, sink in local.logging_sinks :
      name => sink
      if sink.grant && sink.type == type
    }
  }
}
",locals,"locals {
  organization_id_numeric = split(""/"", var.organization_id)[1]
  iam_additive_pairs = flatten([
    for role, members in var.iam_additive : [
      for member in members : { role = role, member = member }
    ]
  ])
  iam_additive_member_pairs = flatten([
    for member, roles in var.iam_additive_members : [
      for role in roles : { role = role, member = member }
    ]
  ])
  iam_additive = {
    for pair in concat(local.iam_additive_pairs, local.iam_additive_member_pairs) :
    ""${pair.role}-${pair.member}"" => pair
  }
  extended_rules = flatten([
    for policy, rules in var.firewall_policies : [
      for rule_name, rule in rules :
      merge(rule, { policy = policy, name = rule_name })
    ]
  ])
  rules_map = {
    for rule in local.extended_rules :
    ""${rule.policy}-${rule.name}"" => rule
  }
  logging_sinks = coalesce(var.logging_sinks, {})
  sink_type_destination = {
    gcs      = ""storage.googleapis.com""
    bigquery = ""bigquery.googleapis.com""
    pubsub   = ""pubsub.googleapis.com""
    logging  = ""logging.googleapis.com""
  }
  sink_bindings = {
    for type in [""gcs"", ""bigquery"", ""pubsub"", ""logging""] :
    type => {
      for name, sink in local.logging_sinks :
      name => sink
      if sink.iam && sink.type == type
    }
  }
}
",locals,48,,2c0f949f07d0563b621cb433c36cf13e117ea561,ad68fc4dfa576624a7e2caa1b96499161d8b0937,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2c0f949f07d0563b621cb433c36cf13e117ea561/modules/organization/main.tf#L48,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/ad68fc4dfa576624a7e2caa1b96499161d8b0937/modules/organization/main.tf,2020-12-05 08:31:35+01:00,2021-03-03 14:23:59+01:00,7,1,1,1,0,0,0,0,1,0
https://github.com/terraform-google-modules/terraform-google-bigquery,50,modules/data_warehouse/main.tf,modules/data_warehouse/main.tf,0,# todo,# # TODO: File bug for this to be a pickable service account,"# # Get the Pub/Sub service account to trigger the pub/sub notification 
 # # TODO: File bug for this to be a pickable service account","resource ""google_project_iam_member"" ""pub_sub_permissions_token"" {
  project = module.project-services.project_id
  role    = ""roles/iam.serviceAccountTokenCreator""
  member  = ""serviceAccount:service-${data.google_project.project.number}@gcp-sa-pubsub.iam.gserviceaccount.com""
}
",resource,the block associated got renamed or deleted,,667,,9e795e42f78b757e0a92100d368e6bd297a97418,d2c125676f176c8fa33eb9dad12b7ed992dee6ac,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/9e795e42f78b757e0a92100d368e6bd297a97418/modules/data_warehouse/main.tf#L667,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/d2c125676f176c8fa33eb9dad12b7ed992dee6ac/modules/data_warehouse/main.tf,2023-03-24 12:04:44-05:00,2023-09-20 06:49:10-06:00,10,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1307,blueprints/data-solutions/shielded-folder/variables.tf,blueprints/data-solutions/shielded-folder/variables.tf,0,#todo,"#TODO data-analysts  = ""gcp-data-analysts""","#TODO data-analysts  = ""gcp-data-analysts""","variable ""groups"" {
  description = ""User groups.""
  type        = map(string)
  default = {
    #TODO data-analysts  = ""gcp-data-analysts""
    data-engineers = ""gcp-data-engineers""
    #TODO data-security  = ""gcp-data-security""
  }
}
",variable,"variable ""groups"" {
  description = ""User groups.""
  type        = map(string)
  default = {
    workload-engineers = ""gcp-data-engineers""
    workload-security  = ""gcp-data-security""
  }
}
",variable,57,,84be665172b21220938ee702c4654e1a0cd0a584,840fc86b3e7a0b9f0734d7cb41abca813b84ab40,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/84be665172b21220938ee702c4654e1a0cd0a584/blueprints/data-solutions/shielded-folder/variables.tf#L57,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/840fc86b3e7a0b9f0734d7cb41abca813b84ab40/blueprints/data-solutions/shielded-folder/variables.tf,2023-01-17 08:49:04+01:00,2023-02-01 08:55:33+01:00,10,1,0,1,0,0,0,0,0,0
https://github.com/wireapp/wire-server-deploy,1,terraform/modules/aws_vpc_security_groups/main.tf,terraform/modules/aws-vpc-security-groups/main.tf,1,# todo,# TODO: refactor this when creating a second environment. re: names being unique for security groups.,"# TODO: refactor this when creating a second environment. re: names being unique for security groups.  
 # A security group for ssh from the outside world. should only be applied to our bastion hosts.","resource ""aws_security_group"" ""world_ssh_in"" {
  name        = ""world_ssh_in""
  description = ""ssh in from the outside world""
  vpc_id      = var.vpc_id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
    }

  tags = {
    Name = ""world_ssh_in""
  }
}
",resource,"resource ""aws_security_group"" ""world_ssh_in"" {
  name        = ""world_ssh_in""
  description = ""ssh in from the outside world""
  vpc_id      = var.vpc_id

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""world_ssh_in""
  }
}
",resource,1,1.0,cb61b733457d5a1ccdb5c9dcb5fde4cab9e5d7e0,e7f081ca744de60bf9d10cb037828b995883d2aa,https://github.com/wireapp/wire-server-deploy/blob/cb61b733457d5a1ccdb5c9dcb5fde4cab9e5d7e0/terraform/modules/aws_vpc_security_groups/main.tf#L1,https://github.com/wireapp/wire-server-deploy/blob/e7f081ca744de60bf9d10cb037828b995883d2aa/terraform/modules/aws-vpc-security-groups/main.tf#L1,2020-04-23 17:54:17+01:00,2020-08-26 16:29:39+02:00,5,0,0,1,0,1,0,1,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,19,modules/dns/variables.tf,modules/dns/variables.tf,0,# todo,# TODO(ludoo): add link to DNSSEC Documentation in README,"# TODO(ludoo): add link to DNSSEC Documentation in README 
 # https://www.terraform.io/docs/providers/google/r/dns_managed_zone.html#dnssec_config ","variable ""default_key_specs_key"" {
  description = ""DNSSEC default key signing specifications: algorithm, key_length, key_type, kind.""
  type        = any
  default     = {}
}
",variable,"variable ""default_key_specs_key"" {
  description = ""DNSSEC default key signing specifications: algorithm, key_length, key_type, kind.""
  type        = any
  default     = {}
}
",variable,33,,c486bfc66f9814e33b410602cb557a5e4d532912,da97405e31c48e470c5f30633f78cc6e052924c8,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c486bfc66f9814e33b410602cb557a5e4d532912/modules/dns/variables.tf#L33,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/da97405e31c48e470c5f30633f78cc6e052924c8/modules/dns/variables.tf,2020-04-03 14:06:48+02:00,2020-05-12 13:35:13+02:00,2,1,0,1,0,0,1,0,0,0
https://github.com/alphagov/govuk-aws,60,terraform/projects/govuk-security-groups/logs-elasticsearch.tf,terraform/projects/infra-security-groups/logs-elasticsearch.tf,1,# todo,# TODO: does anything other than icinga and logging need this?,# TODO: does anything other than icinga and logging need this?,"resource ""aws_security_group_rule"" ""allow_management_to_logs-elasticsearch_elb"" {
  type      = ""ingress""
  from_port = 9200
  to_port   = 9200
  protocol  = ""tcp""

  security_group_id = ""${aws_security_group.logs-elasticsearch_elb.id}""

  # TODO: does anything other than icinga and logging need this?
  source_security_group_id = ""${aws_security_group.management.id}""
}
",resource,,,71,0.0,00346bc2ba10d116f3e41951070ef0ebe0d9085d,0527d5b230dc008ee687667c45d1280de00b40bc,https://github.com/alphagov/govuk-aws/blob/00346bc2ba10d116f3e41951070ef0ebe0d9085d/terraform/projects/govuk-security-groups/logs-elasticsearch.tf#L71,https://github.com/alphagov/govuk-aws/blob/0527d5b230dc008ee687667c45d1280de00b40bc/terraform/projects/infra-security-groups/logs-elasticsearch.tf#L0,2017-07-13 14:53:36+01:00,2017-09-19 20:14:08+01:00,3,2,0,1,0,1,0,0,0,0
https://github.com/alphagov/govuk-aws,955,terraform/projects/infra-security/main.tf,terraform/projects/infra-security/main.tf,0,# todo,# TODO: this cloudtrail bucket is deprecated and is safe to remove,"# TODO: this cloudtrail bucket is deprecated and is safe to remove 
 # once terraform has been applied with force_destroy = true enabled","resource ""aws_s3_bucket"" ""cloudtrail"" {
  bucket        = ""${var.stackname}-${var.aws_environment}-cloudtrail""
  acl           = ""private""
  force_destroy = ""true""

  tags {
    Name        = ""${var.stackname}-${var.aws_environment}-cloudtrail""
    Environment = ""${var.aws_environment}""
  }
}
",resource,the block associated got renamed or deleted,,174,,d29ca713d4d8e26283ac6d693a28cdd0d56bfb1b,305b3fed7dc197332ada901217205b5aae30a0bf,https://github.com/alphagov/govuk-aws/blob/d29ca713d4d8e26283ac6d693a28cdd0d56bfb1b/terraform/projects/infra-security/main.tf#L174,https://github.com/alphagov/govuk-aws/blob/305b3fed7dc197332ada901217205b5aae30a0bf/terraform/projects/infra-security/main.tf,2019-08-12 12:51:32+01:00,2019-10-01 12:04:55+01:00,3,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,21,modules/gke-cluster/main.tf,modules/gke-cluster/main.tf,0,# todo,# TODO(ludomagno): support setting address ranges instead of range names,"# TODO(ludomagno): support setting address ranges instead of range names 
 # https://www.terraform.io/docs/providers/google/r/container_cluster.html#cluster_ipv4_cidr_block","resource ""google_container_cluster"" ""cluster"" {
  provider                    = google-beta
  project                     = var.project_id
  name                        = var.name
  description                 = var.description
  location                    = var.location
  node_locations              = length(var.node_locations) == 0 ? null : var.node_locations
  min_master_version          = var.min_master_version
  network                     = var.network
  subnetwork                  = var.subnetwork
  logging_service             = var.logging_service
  monitoring_service          = var.monitoring_service
  resource_labels             = var.labels
  default_max_pods_per_node   = var.default_max_pods_per_node
  enable_binary_authorization = var.enable_binary_authorization
  enable_intranode_visibility = var.enable_intranode_visibility
  enable_shielded_nodes       = var.enable_shielded_nodes
  enable_tpu                  = var.enable_tpu
  initial_node_count          = 1
  remove_default_node_pool    = true

  # node_config

  addons_config {
    http_load_balancing {
      disabled = ! var.addons.http_load_balancing
    }
    horizontal_pod_autoscaling {
      disabled = ! var.addons.horizontal_pod_autoscaling
    }
    network_policy_config {
      disabled = ! var.addons.network_policy_config
    }
    # beta addons
    # cloudrun is dynamic as it tends to trigger cluster recreation on change
    dynamic cloudrun_config {
      for_each = var.addons.istio_config.enabled && var.addons.cloudrun_config ? [""""] : []
      content {
        disabled = false
      }
    }
    istio_config {
      disabled = ! var.addons.istio_config.enabled
      auth     = var.addons.istio_config.tls ? ""AUTH_MUTUAL_TLS"" : ""AUTH_NONE""
    }
  }

  # TODO(ludomagno): support setting address ranges instead of range names
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#cluster_ipv4_cidr_block
  ip_allocation_policy {
    cluster_secondary_range_name  = var.secondary_range_pods
    services_secondary_range_name = var.secondary_range_services
  }

  # TODO(ludomagno): make optional, and support beta feature
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#daily_maintenance_window
  maintenance_policy {
    daily_maintenance_window {
      start_time = var.maintenance_start_time
    }
  }

  master_auth {
    client_certificate_config {
      issue_client_certificate = false
    }
  }

  dynamic master_authorized_networks_config {
    for_each = length(var.master_authorized_ranges) == 0 ? [] : list(var.master_authorized_ranges)
    iterator = ranges
    content {
      dynamic cidr_blocks {
        for_each = ranges.value
        iterator = range
        content {
          cidr_block   = range.value
          display_name = range.key
        }
      }
    }
  }

  dynamic network_policy {
    for_each = var.addons.network_policy_config ? [""""] : []
    content {
      enabled  = true
      provider = ""CALICO""
    }
  }

  dynamic private_cluster_config {
    for_each = var.private_cluster_config != null ? [var.private_cluster_config] : []
    iterator = config
    content {
      enable_private_nodes    = config.value.enable_private_nodes
      enable_private_endpoint = config.value.enable_private_endpoint
      master_ipv4_cidr_block  = config.value.master_ipv4_cidr_block
    }
  }

  # beta features

  dynamic authenticator_groups_config {
    for_each = var.authenticator_security_group == null ? [] : [""""]
    content {
      security_group = var.authenticator_security_group
    }
  }

  dynamic cluster_autoscaling {
    for_each = var.cluster_autoscaling.enabled ? [var.cluster_autoscaling] : []
    iterator = config
    content {
      enabled = true
      resource_limits {
        resource_type = ""cpu""
        minimum       = config.cpu_min
        maximum       = config.cpu_max
      }
      resource_limits {
        resource_type = ""memory""
        minimum       = config.memory_min
        maximum       = config.memory_max
      }
    }
  }

  dynamic database_encryption {
    for_each = var.database_encryption.enabled ? [var.database_encryption] : []
    iterator = config
    content {
      state    = config.value.state
      key_name = config.value.key_name
    }
  }

  dynamic pod_security_policy_config {
    for_each = var.pod_security_policy != null ? [""""] : []
    content {
      enabled = var.pod_security_policy
    }
  }

  dynamic release_channel {
    for_each = var.release_channel != null ? [""""] : []
    content {
      channel = var.release_channel
    }
  }

  dynamic resource_usage_export_config {
    for_each = (
      var.resource_usage_export_config.enabled != null
      &&
      var.resource_usage_export_config.dataset != null
      ? [""""] : []
    )
    content {
      enable_network_egress_metering = var.resource_usage_export_config.enabled
      bigquery_destination {
        dataset_id = var.resource_usage_export_config.dataset
      }
    }
  }

  dynamic vertical_pod_autoscaling {
    for_each = var.vertical_pod_autoscaling == null ? [] : [""""]
    content {
      enabled = var.vertical_pod_autoscaling
    }
  }

  dynamic workload_identity_config {
    for_each = var.workload_identity ? [""""] : []
    content {
      identity_namespace = ""${var.project_id}.svc.id.goog""
    }
  }

}
",resource,"resource ""google_container_cluster"" ""cluster"" {
  provider    = google-beta
  project     = var.project_id
  name        = var.name
  description = var.description
  location    = var.location
  node_locations = (
    length(var.node_locations) == 0 ? null : var.node_locations
  )
  min_master_version = var.min_master_version
  network            = var.vpc_config.network
  subnetwork         = var.vpc_config.subnetwork
  resource_labels    = var.labels
  default_max_pods_per_node = (
    var.enable_features.autopilot ? null : var.max_pods_per_node
  )
  enable_intranode_visibility = (
    var.enable_features.autopilot ? null : var.enable_features.intranode_visibility
  )
  enable_l4_ilb_subsetting = var.enable_features.l4_ilb_subsetting
  enable_shielded_nodes = (
    var.enable_features.autopilot ? null : var.enable_features.shielded_nodes
  )
  enable_tpu               = var.enable_features.tpu
  initial_node_count       = 1
  remove_default_node_pool = var.enable_features.autopilot ? null : true
  datapath_provider = (
    var.enable_features.dataplane_v2
    ? ""ADVANCED_DATAPATH""
    : ""DATAPATH_PROVIDER_UNSPECIFIED""
  )
  enable_autopilot = var.enable_features.autopilot ? true : null

  # the default nodepool is deleted here, use the gke-nodepool module instead
  # node_config {}

  addons_config {
    dynamic ""dns_cache_config"" {
      for_each = !var.enable_features.autopilot ? [""""] : []
      content {
        enabled = var.enable_addons.dns_cache
      }
    }
    http_load_balancing {
      disabled = !var.enable_addons.http_load_balancing
    }
    horizontal_pod_autoscaling {
      disabled = !var.enable_addons.horizontal_pod_autoscaling
    }
    dynamic ""network_policy_config"" {
      for_each = !var.enable_features.autopilot ? [""""] : []
      content {
        disabled = !var.enable_addons.network_policy
      }
    }
    cloudrun_config {
      disabled = !var.enable_addons.cloudrun
    }
    istio_config {
      disabled = var.enable_addons.istio == null
      auth = (
        try(var.enable_addons.istio.enable_tls, false) ? ""AUTH_MUTUAL_TLS"" : ""AUTH_NONE""
      )
    }
    gce_persistent_disk_csi_driver_config {
      enabled = var.enable_addons.gce_persistent_disk_csi_driver
    }
    dynamic ""gcp_filestore_csi_driver_config"" {
      for_each = !var.enable_features.autopilot ? [""""] : []
      content {
        enabled = var.enable_addons.gcp_filestore_csi_driver
      }
    }
    kalm_config {
      enabled = var.enable_addons.kalm
    }
    config_connector_config {
      enabled = var.enable_addons.config_connector
    }
    gke_backup_agent_config {
      enabled = var.enable_addons.gke_backup_agent
    }
  }

  dynamic ""authenticator_groups_config"" {
    for_each = var.enable_features.groups_for_rbac != null ? [""""] : []
    content {
      security_group = var.enable_features.groups_for_rbac
    }
  }

  dynamic ""binary_authorization"" {
    for_each = var.enable_features.binary_authorization ? [""""] : []
    content {
      evaluation_mode = ""PROJECT_SINGLETON_POLICY_ENFORCE""
    }
  }

  dynamic ""cluster_autoscaling"" {
    for_each = var.cluster_autoscaling == null ? [] : [""""]
    content {
      enabled = true
      dynamic ""resource_limits"" {
        for_each = var.cluster_autoscaling.cpu_limits != null ? [""""] : []
        content {
          resource_type = ""cpu""
          minimum       = var.cluster_autoscaling.cpu_limits.min
          maximum       = var.cluster_autoscaling.cpu_limits.max
        }
      }
      dynamic ""resource_limits"" {
        for_each = var.cluster_autoscaling.mem_limits != null ? [""""] : []
        content {
          resource_type = ""cpu""
          minimum       = var.cluster_autoscaling.mem_limits.min
          maximum       = var.cluster_autoscaling.mem_limits.max
        }
      }
      // TODO: support GPUs too
    }
  }

  dynamic ""database_encryption"" {
    for_each = var.enable_features.database_encryption != null ? [""""] : []
    content {
      state    = var.enable_features.database_encryption.state
      key_name = var.enable_features.database_encryption.key_name
    }
  }

  dynamic ""dns_config"" {
    for_each = var.enable_features.cloud_dns != null ? [""""] : []
    content {
      cluster_dns        = enable_features.cloud_dns.cluster_dns
      cluster_dns_scope  = enable_features.cloud_dns.cluster_dns_scope
      cluster_dns_domain = enable_features.cloud_dns.cluster_dns_domain
    }
  }

  dynamic ""ip_allocation_policy"" {
    for_each = var.vpc_config.secondary_range_blocks != null ? [""""] : []
    content {
      cluster_ipv4_cidr_block  = var.vpc_config.secondary_range_blocks.pods
      services_ipv4_cidr_block = var.vpc_config.secondary_range_blocks.services
    }
  }
  dynamic ""ip_allocation_policy"" {
    for_each = var.vpc_config.secondary_range_names != null ? [""""] : []
    content {
      cluster_secondary_range_name  = var.vpc_config.secondary_range_names.pods
      services_secondary_range_name = var.vpc_config.secondary_range_names.services
    }
  }

  dynamic ""logging_config"" {
    for_each = var.logging_config != null ? [""""] : []
    content {
      enable_components = var.logging_config
    }
  }

  maintenance_policy {
    dynamic ""daily_maintenance_window"" {
      for_each = (
        try(var.maintenance_config.daily_window_start_time, null) != null
        ? [""""]
        : []
      )
      content {
        start_time = var.maintenance_config.daily_window_start_time
      }
    }
    dynamic ""recurring_window"" {
      for_each = (
        try(var.maintenance_config.recurring_window, null) != null
        ? [""""]
        : []
      )
      content {
        start_time = var.maintenance_config.recurring_window.start_time
        end_time   = var.maintenance_config.recurring_window.end_time
        recurrence = var.maintenance_config.recurring_window.recurrence
      }
    }
    dynamic ""maintenance_exclusion"" {
      for_each = (
        try(var.maintenance_config.maintenance_exclusions, null) == null
        ? []
        : var.maintenance_config.maintenance_exclusions
      )
      iterator = exclusion
      content {
        exclusion_name = exclusion.value.name
        start_time     = exclusion.value.start_time
        end_time       = exclusion.value.end_time
      }
    }
  }

  master_auth {
    client_certificate_config {
      issue_client_certificate = var.issue_client_certificate
    }
  }

  dynamic ""master_authorized_networks_config"" {
    for_each = var.vpc_config.master_authorized_ranges != null ? [""""] : []
    content {
      dynamic ""cidr_blocks"" {
        for_each = var.vpc_config.master_authorized_ranges
        iterator = range
        content {
          cidr_block   = range.value
          display_name = range.key
        }
      }
    }
  }

  dynamic ""monitoring_config"" {
    for_each = var.monitoring_config != null ? [""""] : []
    content {
      enable_components = var.monitoring_config
    }
  }

  # dataplane v2 has bult-in network policies
  dynamic ""network_policy"" {
    for_each = (
      var.enable_addons.network_policy && !var.enable_features.dataplane_v2
      ? [""""]
      : []
    )
    content {
      enabled  = true
      provider = ""CALICO""
    }
  }

  dynamic ""notification_config"" {
    for_each = var.enable_features.upgrade_notifications != null ? [""""] : []
    content {
      pubsub {
        enabled = true
        topic = (
          try(var.enable_features.upgrade_notifications.topic_id, null) != null
          ? var.enable_features.upgrade_notifications.topic_id
          : google_pubsub_topic.notifications[0].id
        )
      }
    }
  }

  dynamic ""private_cluster_config"" {
    for_each = (
      var.private_cluster_config != null ? [""""] : []
    )
    content {
      enable_private_nodes    = true
      enable_private_endpoint = var.private_cluster_config.enable_private_endpoint
      master_ipv4_cidr_block  = var.private_cluster_config.master_ipv4_cidr_block
      master_global_access_config {
        enabled = var.private_cluster_config.master_global_access
      }
    }
  }

  dynamic ""pod_security_policy_config"" {
    for_each = var.enable_features.pod_security_policy ? [""""] : []
    content {
      enabled = var.enable_features.pod_security_policy
    }
  }

  dynamic ""release_channel"" {
    for_each = var.release_channel != null ? [""""] : []
    content {
      channel = var.release_channel
    }
  }

  dynamic ""resource_usage_export_config"" {
    for_each = (
      try(var.enable_features.resource_usage_export.dataset, null) != null
      ? [""""]
      : []
    )
    content {
      enable_network_egress_metering = (
        var.enable_features.resource_usage_export.enable_network_egress_metering
      )
      enable_resource_consumption_metering = (
        var.enable_features.resource_usage_export.enable_resource_consumption_metering
      )
      bigquery_destination {
        dataset_id = var.enable_features.resource_usage_export.dataset
      }
    }
  }

  dynamic ""vertical_pod_autoscaling"" {
    for_each = var.enable_features.vertical_pod_autoscaling ? [""""] : []
    content {
      enabled = var.enable_features.vertical_pod_autoscaling
    }
  }

  dynamic ""workload_identity_config"" {
    for_each = var.enable_features.workload_identity ? [""""] : []
    content {
      workload_pool = ""${var.project_id}.svc.id.goog""
    }
  }
}
",resource,64,,c486bfc66f9814e33b410602cb557a5e4d532912,16822e94ab70d75099214b9db786affcb231fbf6,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c486bfc66f9814e33b410602cb557a5e4d532912/modules/gke-cluster/main.tf#L64,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/16822e94ab70d75099214b9db786affcb231fbf6/modules/gke-cluster/main.tf,2020-04-03 14:06:48+02:00,2022-10-10 09:38:21+02:00,35,1,0,1,1,0,1,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,200,modules/aws-eks-managed-node-groups/iam.tf,modules/aws-eks-managed-node-groups/iam.tf,0,fix,# TODO - fix at next breaking change,"# TODO - fix at next breaking change 
 # tflint-ignore: terraform_naming_convention","resource ""aws_iam_role_policy_attachment"" ""managed_ng_AmazonEKSWorkerNodePolicy"" {
  policy_arn = ""${local.policy_arn_prefix}/AmazonEKSWorkerNodePolicy""
  role       = aws_iam_role.managed_ng.name
}
",resource,the block associated got renamed or deleted,,23,,deec7d5caea47c06dea48fa616ad2c56e52c3cce,5010ec9551e21fee27bda888d98fc106b1e54f8a,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/deec7d5caea47c06dea48fa616ad2c56e52c3cce/modules/aws-eks-managed-node-groups/iam.tf#L23,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/5010ec9551e21fee27bda888d98fc106b1e54f8a/modules/aws-eks-managed-node-groups/iam.tf,2022-04-29 14:37:13-07:00,2022-05-19 09:22:58+01:00,2,1,0,1,0,1,0,0,0,0
https://github.com/alphagov/govuk-aws,1447,terraform/projects/infra-assets/replication.tf,terraform/projects/infra-assets/replication.tf,0,# todo,"# TODO: govuk-assets-backup-production should really have timelock (AWS ""Object","# govuk-assets-production replicates to: 
 #   govuk-assets-backup-production (same account, different region, supposed to be a backup) 
 #   govuk-assets-staging (different account, objects owned by destination account) 
 #   govuk-assets-integration (different account, objects owned by destination account) 
 # 
 # See: 
 # https://docs.aws.amazon.com/AmazonS3/latest/userguide/setting-repl-config-perm-overview 
 # https://docs.aws.amazon.com/AmazonS3/latest/userguide/delete-marker-replication 
 # https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-change-owner 
 # 
 # TODO: govuk-assets-backup-production should really have timelock (AWS ""Object 
 # Lock"") or similar to properly serve as a backup. ","locals {
  replication_role_name                    = ""govuk-production-assets-s3-replication""
  replication_service_role_in_prod_account = ""arn:aws:iam::172025368201:role/${local.replication_role_name}""
}
",locals,"locals {
  replication_role_name                    = ""govuk-production-assets-s3-replication""
  replication_service_role_in_prod_account = ""arn:aws:iam::172025368201:role/${local.replication_role_name}""
}
",locals,11,11.0,c1d29a6caabcd4f041d87d76a10f2deef06685ef,c1d29a6caabcd4f041d87d76a10f2deef06685ef,https://github.com/alphagov/govuk-aws/blob/c1d29a6caabcd4f041d87d76a10f2deef06685ef/terraform/projects/infra-assets/replication.tf#L11,https://github.com/alphagov/govuk-aws/blob/c1d29a6caabcd4f041d87d76a10f2deef06685ef/terraform/projects/infra-assets/replication.tf#L11,2023-06-02 10:34:23+01:00,2023-06-02 10:34:23+01:00,1,0,1,1,0,0,0,1,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1598,fast/stages-multitenant/0-bootstrap-tenant/organization.tf,fast/stages-multitenant/0-bootstrap-tenant/organization.tf,0,# todo,# TODO: use tag IAM with id in the organization module,# TODO: use tag IAM with id in the organization module,"resource ""google_tags_tag_value_iam_member"" ""resman_tag_user"" {
  for_each  = var.tag_values
  tag_value = each.value
  role      = ""roles/resourcemanager.tagUser""
  member    = module.automation-tf-resman-sa.iam_email
}
",resource,,,79,0.0,819894d2bab4b440f1b52b1ac8035912fb107004,7a5dd4e6db197daa52da8a8d877ce86b5c93182e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/819894d2bab4b440f1b52b1ac8035912fb107004/fast/stages-multitenant/0-bootstrap-tenant/organization.tf#L79,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/7a5dd4e6db197daa52da8a8d877ce86b5c93182e/fast/stages-multitenant/0-bootstrap-tenant/organization.tf#L0,2023-08-20 09:44:20+02:00,2024-05-15 09:17:13+00:00,3,2,0,1,0,1,0,0,0,0
https://github.com/Azure/Avere,22,src/terraform/modules/hammerspace/anvil/outputs.tf,src/terraform/modules/hammerspace/anvil/outputs.tf,0,todo,// TODO - which password works?,// TODO - which password works?,"output ""web_ui_password"" {
  value = azurerm_linux_virtual_machine.anvilvm == null || length(azurerm_linux_virtual_machine.anvilvm) == 0 ? """" : local.is_high_availability ? azurerm_linux_virtual_machine.anvilvm[1].virtual_machine_id : azurerm_linux_virtual_machine.anvilvm[0].virtual_machine_id
}
",output,"output ""web_ui_password"" {
  value = azurerm_linux_virtual_machine.anvilvm == null || length(azurerm_linux_virtual_machine.anvilvm) == 0 ? [] : local.is_high_availability ? [azurerm_linux_virtual_machine.anvilvm[0].virtual_machine_id, azurerm_linux_virtual_machine.anvilvm[1].virtual_machine_id] : [azurerm_linux_virtual_machine.anvilvm[0].virtual_machine_id]
}
",output,9,,47847b35c4a7b8d24584e2c9690ea7a41d5bc3f1,a64c98d189a8f44b087f399cfa0864ecef7b3eeb,https://github.com/Azure/Avere/blob/47847b35c4a7b8d24584e2c9690ea7a41d5bc3f1/src/terraform/modules/hammerspace/anvil/outputs.tf#L9,https://github.com/Azure/Avere/blob/a64c98d189a8f44b087f399cfa0864ecef7b3eeb/src/terraform/modules/hammerspace/anvil/outputs.tf,2021-03-02 06:48:40-05:00,2021-05-02 10:11:01-04:00,6,1,0,1,0,0,0,0,0,1
https://github.com/Worklytics/psoxy,232,infra/modules/azuread-local-cert/main.tf,infra/modules/azuread-local-cert/main.tf,0,hack,# hackery to translate output of openssl fingerprint --> string of hex chars equivalent to how MSFT does,"# hackery to translate output of openssl fingerprint --> string of hex chars equivalent to how MSFT does 
 # (eg, MSFT will compute fingerprint server-side of the certificate value posted above)","output ""private_key_id"" {
  # hackery to translate output of openssl fingerprint --> string of hex chars equivalent to how MSFT does
  # (eg, MSFT will compute fingerprint server-side of the certificate value posted above)
  value = replace(replace(data.external.certificate.result.fingerprint, ""SHA1 Fingerprint="", """"), "":"", """")
}
",output,,,35,0.0,44a2da80a65900db472a11c8a598e4df58a339e6,242232edab4d3fd867c0b2833ded96cacf95ef21,https://github.com/Worklytics/psoxy/blob/44a2da80a65900db472a11c8a598e4df58a339e6/infra/modules/azuread-local-cert/main.tf#L35,https://github.com/Worklytics/psoxy/blob/242232edab4d3fd867c0b2833ded96cacf95ef21/infra/modules/azuread-local-cert/main.tf#L0,2022-01-28 21:48:32-08:00,2023-02-01 09:14:32-08:00,6,2,0,1,0,1,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,288,modules/iam/tagging.tf,modules/iam/tagging.tf,0,todo,// TODO Support reactivation of retired tag w/ update,"state            = ""ACTIVE"" // TODO Support reactivation of retired tag w/ update","data ""oci_identity_tags"" ""oke"" {
  count            = var.create_iam_resources ? 1 : 0
  provider         = oci.home
  tag_namespace_id = local.tag_namespace_id_found
  state            = ""ACTIVE"" // TODO Support reactivation of retired tag w/ update
}
",data,the block associated got renamed or deleted,,20,,cf7f4da8a0c56d0350bd86c2ef1011e3f6c2f3b2,e2ac866a96bd7171c980727c46078cc438643225,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/cf7f4da8a0c56d0350bd86c2ef1011e3f6c2f3b2/modules/iam/tagging.tf#L20,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/e2ac866a96bd7171c980727c46078cc438643225/modules/iam/tagging.tf,2023-10-25 16:40:02+11:00,2024-03-28 20:16:45+11:00,5,1,0,1,0,0,0,0,0,0
https://github.com/compiler-explorer/infra,106,terraform/lambda.tf,terraform/lambda.tf,0,fix,// Comment this in if you want to force a deploy every time... TODO ideally fix this,"  // Comment this in if you want to force a deploy every time... TODO ideally fix this
  //  source_code_hash  = data.aws_s3_bucket_object.lambda_zip.etag","resource ""aws_lambda_function"" ""stats"" {
  description       = ""Respond to various CE-specific stats records""
  s3_bucket         = data.aws_s3_bucket_object.lambda_zip.bucket
  s3_key            = data.aws_s3_bucket_object.lambda_zip.key
  s3_object_version = data.aws_s3_bucket_object.lambda_zip.version_id
  // Comment this in if you want to force a deploy every time... TODO ideally fix this
  //  source_code_hash  = data.aws_s3_bucket_object.lambda_zip.etag
  function_name     = ""stats""
  role              = aws_iam_role.iam_for_lambda.arn
  handler           = ""stats.lambda_handler""
  timeout           = 10

  runtime = ""python3.8""

  environment {
    variables = {
      S3_BUCKET_NAME  = aws_s3_bucket.compiler-explorer-logs.bucket
      SQS_STATS_QUEUE = aws_sqs_queue.stats_queue.id
    }
  }
}
",resource,"resource ""aws_lambda_function"" ""stats"" {
  description       = ""Respond to various CE-specific stats records""
  s3_bucket         = data.aws_s3_bucket_object.lambda_zip.bucket
  s3_key            = data.aws_s3_bucket_object.lambda_zip.key
  s3_object_version = data.aws_s3_bucket_object.lambda_zip.version_id
  source_code_hash  = chomp(data.aws_s3_bucket_object.lambda_zip_sha.body)
  function_name     = ""stats""
  role              = aws_iam_role.iam_for_lambda.arn
  handler           = ""stats.lambda_handler""
  timeout           = 10

  runtime = ""python3.8""

  environment {
    variables = {
      S3_BUCKET_NAME       = aws_s3_bucket.compiler-explorer-logs.bucket
      SQS_STATS_QUEUE      = aws_sqs_queue.stats_queue.id
      COMPILER_BUILD_TABLE = aws_dynamodb_table.compiler-builds.name
    }
  }
}
",resource,99,,e36de219309c76b81e7360525687bfb3e4f845a2,e5d5aff37a3bc047e6d6b093d4184359d92af12c,https://github.com/compiler-explorer/infra/blob/e36de219309c76b81e7360525687bfb3e4f845a2/terraform/lambda.tf#L99,https://github.com/compiler-explorer/infra/blob/e5d5aff37a3bc047e6d6b093d4184359d92af12c/terraform/lambda.tf,2021-07-11 17:06:05-05:00,2021-11-15 17:28:33-06:00,2,1,1,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,482,infra/modules/aws-psoxy-bulk-existing/main.tf,infra/modules/aws-psoxy-bulk-existing/main.tf,0,# todo,"# TODO: highly duplicative with regular `aws-psoxy-bulk` case, and could likely be unified in future","# creates a Bulk processing instance of Psoxy, with existing S3 bucket as the input 
 # TODO: highly duplicative with regular `aws-psoxy-bulk` case, and could likely be unified in future 
 # version ","terraform {
  required_providers {
    # for the infra that will host Psoxy instances
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.29""
    }
  }
}
",terraform,"module ""psoxy_lambda"" {
  source = ""../aws-psoxy-lambda""

  environment_name                     = var.environment_name
  instance_id                          = var.instance_id
  handler_class                        = ""co.worklytics.psoxy.S3Handler""
  timeout_seconds                      = 600 # 10 minutes
  memory_size_mb                       = var.memory_size_mb
  path_to_function_zip                 = var.path_to_function_zip
  function_zip_hash                    = var.function_zip_hash
  global_parameter_arns                = var.global_parameter_arns
  global_secrets_manager_secrets_arns  = var.global_secrets_manager_secret_arns
  path_to_instance_ssm_parameters      = var.path_to_instance_ssm_parameters
  path_to_shared_ssm_parameters        = var.path_to_shared_ssm_parameters
  function_env_kms_key_arn             = var.function_env_kms_key_arn
  logs_kms_key_arn                     = var.logs_kms_key_arn
  ssm_kms_key_ids                      = var.ssm_kms_key_ids
  vpc_config                           = var.vpc_config
  secrets_store_implementation         = var.secrets_store_implementation
  aws_lambda_execution_role_policy_arn = var.aws_lambda_execution_role_policy_arn

  environment_variables = merge(
    var.environment_variables,
    {
      INPUT_BUCKET  = var.input_bucket
      OUTPUT_BUCKET = module.sanitized_output_bucket.output_bucket
    }
  )
}
",module,2,2.0,d1d2ef9b4a358d71a7d8e228a8291a4569a6f3cf,a6fe806adeb28bbf5edc920030d9019c837df209,https://github.com/Worklytics/psoxy/blob/d1d2ef9b4a358d71a7d8e228a8291a4569a6f3cf/infra/modules/aws-psoxy-bulk-existing/main.tf#L2,https://github.com/Worklytics/psoxy/blob/a6fe806adeb28bbf5edc920030d9019c837df209/infra/modules/aws-psoxy-bulk-existing/main.tf#L2,2022-10-27 14:12:45-07:00,2024-03-15 08:20:09-07:00,17,0,0,1,0,0,0,0,0,0
https://github.com/kubernetes/k8s.io,307,infra/gcp/terraform/k8s-infra-oci-proxy-prod/oci-proxy.tf,infra/gcp/terraform/k8s-infra-oci-proxy-prod/oci-proxy.tf,0,// todo,// TODO: adjust to control costs,"""autoscaling.knative.dev/maxScale"" = ""10"" // TODO: adjust to control costs","resource ""google_cloud_run_service"" ""oci-proxy"" {
  project  = google_project.project.project_id
  for_each = var.cloud_run_config
  name     = ""${var.project_id}-${each.key}""
  location = each.key

  template {
    metadata {
      annotations = {
        ""autoscaling.knative.dev/maxScale"" = ""10"" // TODO: adjust to control costs
        ""run.googleapis.com/launch-stage""  = ""BETA""
      }
    }
    spec {
      service_account_name = google_service_account.oci-proxy.email
      containers {
        image = ""us.gcr.io/k8s-artifacts-prod/infra-tools/archeio:${var.tag}""
        args  = [""-v=3""]

        dynamic ""env"" {
          for_each = each.value.environment_variables
          content {
            name  = env.value[""name""]
            value = env.value[""value""]
          }
        }

        // ensure this macth the value for template.spec.containers.resources.limits
        env {
          name = ""GOMAXPROCS""
          value = ""1""
        }

        resources {
          limits = {
            ""cpu"" = ""1000m""
          }
        }
      }

      container_concurrency = 1000

      // 30 seconds less than cloud scheduler maximum.
      timeout_seconds = 570
    }
  }

  traffic {
    percent         = 100
    latest_revision = true
  }

  depends_on = [
    google_project_service.project[""run.googleapis.com""]
  ]

  lifecycle {
    ignore_changes = [
      // This gets added by the Cloud Run API post deploy and causes diffs, can be ignored...
      template[0].metadata[0].annotations[""client.knative.dev/sandbox""],
      template[0].metadata[0].annotations[""run.googleapis.com/user-image""],
      template[0].metadata[0].annotations[""run.googleapis.com/client-name""],
      template[0].metadata[0].annotations[""run.googleapis.com/client-version""],
    ]
  }
}
",resource,the block associated got renamed or deleted,,82,,819ad39b6bb53e044619abde05a99cd11383c15a,ffbaa4eb3b652a9cc7520593cae83a8004ef88a3,https://github.com/kubernetes/k8s.io/blob/819ad39b6bb53e044619abde05a99cd11383c15a/infra/gcp/terraform/k8s-infra-oci-proxy-prod/oci-proxy.tf#L82,https://github.com/kubernetes/k8s.io/blob/ffbaa4eb3b652a9cc7520593cae83a8004ef88a3/infra/gcp/terraform/k8s-infra-oci-proxy-prod/oci-proxy.tf,2022-10-03 19:00:50+02:00,2023-04-02 19:38:59-07:00,10,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1748,modules/net-vpc/subnets.tf,modules/net-vpc/subnets.tf,0,fix,# Until https://github.com/hashicorp/terraform-provider-google/issues/16804 is fixed,"# Until https://github.com/hashicorp/terraform-provider-google/issues/16804 is fixed 
 # ignore permadiff in ipv6_access_type for proxy_only subnets","resource ""google_compute_subnetwork"" ""proxy_only"" {
  for_each      = local.subnets_proxy_only
  project       = var.project_id
  network       = local.network.name
  name          = each.value.name
  region        = each.value.region
  ip_cidr_range = each.value.ip_cidr_range
  description = coalesce(
    each.value.description,
    ""Terraform-managed proxy-only subnet for Regional HTTPS, Internal HTTPS or Cross-Regional HTTPS Internal LB.""
  )
  purpose = each.value.global ? ""GLOBAL_MANAGED_PROXY"" : ""REGIONAL_MANAGED_PROXY""
  role    = each.value.active ? ""ACTIVE"" : ""BACKUP""

  lifecycle {
    # Until https://github.com/hashicorp/terraform-provider-google/issues/16804 is fixed
    # ignore permadiff in ipv6_access_type for proxy_only subnets
    ignore_changes = [ipv6_access_type]
  }
}
",resource,"resource ""google_compute_subnetwork"" ""proxy_only"" {
  for_each      = local.subnets_proxy_only
  project       = var.project_id
  network       = local.network.name
  name          = each.value.name
  region        = each.value.region
  ip_cidr_range = each.value.ip_cidr_range
  description = coalesce(
    each.value.description,
    ""Terraform-managed proxy-only subnet for Regional HTTPS, Internal HTTPS or Cross-Regional HTTPS Internal LB.""
  )
  purpose = each.value.global ? ""GLOBAL_MANAGED_PROXY"" : ""REGIONAL_MANAGED_PROXY""
  role    = each.value.active ? ""ACTIVE"" : ""BACKUP""
}
",resource,187,,0d486fb34e64c368f8a4cb8c58092486c2440b7c,93d9b60d54f8b4652cafbdb07a085a670702515b,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/0d486fb34e64c368f8a4cb8c58092486c2440b7c/modules/net-vpc/subnets.tf#L187,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/93d9b60d54f8b4652cafbdb07a085a670702515b/modules/net-vpc/subnets.tf,2023-12-19 11:01:03+01:00,2024-03-05 08:11:06+01:00,4,1,0,1,1,0,1,0,0,0
https://github.com/Worklytics/psoxy,230,infra/modules/azuread-grant-all-users/main.tf,infra/modules/azuread-grant-all-users/main.tf,0,todo,"# TODO: if grant can be made fully through API, do it here; until then, TODO file is best option","# TODO: if grant can be made fully through API, do it here; until then, TODO file is best option  
 # NOTE: using `azuread_service_principal_delegated_permission_grant` seems to NOT work for this, 
 # presumably it ONLY supports oauth scopes (delegated permissions) not application roles 
 # (application permissions) 
 # however, even using it JUST for delegated permissions seems to create an inconsistency when user 
 # manually grants the application permissions (resource seems to really create a 'delegated_permission_grant' 
 # entity in Azure, which is overwritten by the user and then missing on subsequent terraform runs) ","resource ""local_file"" ""todo"" {
  filename = ""TODO ${var.application_name}.md""

  content = <<EOT
# Authorize ${var.application_name}

Visit the following page in the Azure AD console and grant the required application permissions:

https://portal.azure.com/#blade/Microsoft_AAD_RegisteredApps/ApplicationMenuBlade/CallAnAPI/appId/${var.application_id}/isMSAApp/

If you are not a sufficiently privileged Azure AD Administrator (likely Application or Global
Administrator), you made need assistance from an appropriately privileged member of your IT team.

The required grants are:
```
${join(""\n"", concat(var.app_roles, var.oauth2_permission_scopes))}
```

EOT
}
",resource,"locals {
  instance_id  = coalesce(var.psoxy_instance_id, var.application_name)
  todo_content = <<EOT
# Authorize ${var.application_name}

Visit the following page in the Azure AD console and grant the required application permissions:

https://portal.azure.com/#blade/Microsoft_AAD_RegisteredApps/ApplicationMenuBlade/CallAnAPI/appId/${var.application_id}/isMSAApp/

If you are not a sufficiently privileged Azure AD Administrator (likely Application or Global
Administrator), you may need assistance from an appropriately privileged member of your IT team.

The required grants are:
```
${join(""\n"", concat(var.app_roles, var.oauth2_permission_scopes))}
```

EOT
}
",locals,13,5.0,e60f54ba57c150249298c3fcf1af52e6a2ea06ee,ce24a85e38bd513c005f315db934b61c950962a6,https://github.com/Worklytics/psoxy/blob/e60f54ba57c150249298c3fcf1af52e6a2ea06ee/infra/modules/azuread-grant-all-users/main.tf#L13,https://github.com/Worklytics/psoxy/blob/ce24a85e38bd513c005f315db934b61c950962a6/infra/modules/azuread-grant-all-users/main.tf#L5,2022-01-27 21:03:35-08:00,2024-04-17 08:37:10-07:00,10,0,0,1,0,1,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,639,modules/_user_data/main.tf,modules/_user_data/main.tf,0,todo,# TODO - platform will be removed in v21.0 and only `ami_type` will be valid,"# Converts AMI type into user data type that represents the underlying format (bash, toml, PS1, nodeadm) 
 # TODO - platform will be removed in v21.0 and only `ami_type` will be valid","locals {
  # Converts AMI type into user data type that represents the underlying format (bash, toml, PS1, nodeadm)
  # TODO - platform will be removed in v21.0 and only `ami_type` will be valid
  ami_type_to_user_data_type = {
    AL2_x86_64                 = ""linux""
    AL2_x86_64_GPU             = ""linux""
    AL2_ARM_64                 = ""linux""
    BOTTLEROCKET_ARM_64        = ""bottlerocket""
    BOTTLEROCKET_x86_64        = ""bottlerocket""
    BOTTLEROCKET_ARM_64_NVIDIA = ""bottlerocket""
    BOTTLEROCKET_x86_64_NVIDIA = ""bottlerocket""
    WINDOWS_CORE_2019_x86_64   = ""windows""
    WINDOWS_FULL_2019_x86_64   = ""windows""
    WINDOWS_CORE_2022_x86_64   = ""windows""
    WINDOWS_FULL_2022_x86_64   = ""windows""
    AL2023_x86_64_STANDARD     = ""al2023""
    AL2023_ARM_64_STANDARD     = ""al2023""
  }
  # Try to use `ami_type` first, but fall back to current, default behavior
  # TODO - will be removed in v21.0
  user_data_type = try(local.ami_type_to_user_data_type[var.ami_type], var.platform)

  template_path = {
    al2023       = ""${path.module}/../../templates/al2023_user_data.tpl""
    bottlerocket = ""${path.module}/../../templates/bottlerocket_user_data.tpl""
    linux        = ""${path.module}/../../templates/linux_user_data.tpl""
    windows      = ""${path.module}/../../templates/windows_user_data.tpl""
  }

  cluster_service_cidr = try(coalesce(var.cluster_service_ipv4_cidr, var.cluster_service_cidr), """")

  user_data = base64encode(templatefile(
    coalesce(var.user_data_template_path, local.template_path[local.user_data_type]),
    {
      # https://docs.aws.amazon.com/eks/latest/userguide/launch-templates.html#launch-template-custom-ami
      enable_bootstrap_user_data = var.enable_bootstrap_user_data

      # Required to bootstrap node
      cluster_name        = var.cluster_name
      cluster_endpoint    = var.cluster_endpoint
      cluster_auth_base64 = var.cluster_auth_base64

      cluster_service_cidr = local.cluster_service_cidr
      cluster_ip_family    = var.cluster_ip_family
      # Bottlerocket
      cluster_dns_ip = try(cidrhost(local.cluster_service_cidr, 10), """")

      # Optional
      bootstrap_extra_args     = var.bootstrap_extra_args
      pre_bootstrap_user_data  = var.pre_bootstrap_user_data
      post_bootstrap_user_data = var.post_bootstrap_user_data
    }
  ))

  user_data_type_to_rendered = {
    al2023 = {
      user_data = var.create ? try(data.cloudinit_config.al2023_eks_managed_node_group[0].rendered, local.user_data) : """"
    }
    bottlerocket = {
      user_data = var.create && local.user_data_type == ""bottlerocket"" && (var.enable_bootstrap_user_data || var.user_data_template_path != """" || var.bootstrap_extra_args != """") ? local.user_data : """"
    }
    linux = {
      user_data = var.create ? try(data.cloudinit_config.linux_eks_managed_node_group[0].rendered, local.user_data) : """"
    }
    windows = {
      user_data = var.create && local.user_data_type == ""windows"" && (var.enable_bootstrap_user_data || var.user_data_template_path != """" || var.pre_bootstrap_user_data != """") ? local.user_data : """"
    }
  }
}
",locals,"locals {
  # Converts AMI type into user data type that represents the underlying format (bash, toml, PS1, nodeadm)
  # TODO - platform will be removed in v21.0 and only `ami_type` will be valid
  ami_type_to_user_data_type = {
    AL2_x86_64                 = ""linux""
    AL2_x86_64_GPU             = ""linux""
    AL2_ARM_64                 = ""linux""
    BOTTLEROCKET_ARM_64        = ""bottlerocket""
    BOTTLEROCKET_x86_64        = ""bottlerocket""
    BOTTLEROCKET_ARM_64_NVIDIA = ""bottlerocket""
    BOTTLEROCKET_x86_64_NVIDIA = ""bottlerocket""
    WINDOWS_CORE_2019_x86_64   = ""windows""
    WINDOWS_FULL_2019_x86_64   = ""windows""
    WINDOWS_CORE_2022_x86_64   = ""windows""
    WINDOWS_FULL_2022_x86_64   = ""windows""
    AL2023_x86_64_STANDARD     = ""al2023""
    AL2023_ARM_64_STANDARD     = ""al2023""
  }
  # Try to use `ami_type` first, but fall back to current, default behavior
  # TODO - will be removed in v21.0
  user_data_type = try(local.ami_type_to_user_data_type[var.ami_type], var.platform)

  template_path = {
    al2023       = ""${path.module}/../../templates/al2023_user_data.tpl""
    bottlerocket = ""${path.module}/../../templates/bottlerocket_user_data.tpl""
    linux        = ""${path.module}/../../templates/linux_user_data.tpl""
    windows      = ""${path.module}/../../templates/windows_user_data.tpl""
  }

  cluster_service_cidr = try(coalesce(var.cluster_service_ipv4_cidr, var.cluster_service_cidr), """")

  user_data = base64encode(templatefile(
    coalesce(var.user_data_template_path, local.template_path[local.user_data_type]),
    {
      # https://docs.aws.amazon.com/eks/latest/userguide/launch-templates.html#launch-template-custom-ami
      enable_bootstrap_user_data = var.enable_bootstrap_user_data

      # Required to bootstrap node
      cluster_name        = var.cluster_name
      cluster_endpoint    = var.cluster_endpoint
      cluster_auth_base64 = var.cluster_auth_base64

      cluster_service_cidr = local.cluster_service_cidr
      cluster_ip_family    = var.cluster_ip_family
      # Bottlerocket
      cluster_dns_ip = try(cidrhost(local.cluster_service_cidr, 10), """")

      # Optional
      bootstrap_extra_args     = var.bootstrap_extra_args
      pre_bootstrap_user_data  = var.pre_bootstrap_user_data
      post_bootstrap_user_data = var.post_bootstrap_user_data
    }
  ))

  user_data_type_to_rendered = {
    al2023 = {
      user_data = var.create ? try(data.cloudinit_config.al2023_eks_managed_node_group[0].rendered, local.user_data) : """"
    }
    bottlerocket = {
      user_data = var.create && local.user_data_type == ""bottlerocket"" && (var.enable_bootstrap_user_data || var.user_data_template_path != """" || var.bootstrap_extra_args != """") ? local.user_data : """"
    }
    linux = {
      user_data = var.create ? try(data.cloudinit_config.linux_eks_managed_node_group[0].rendered, local.user_data) : """"
    }
    windows = {
      user_data = var.create && local.user_data_type == ""windows"" && (var.enable_bootstrap_user_data || var.user_data_template_path != """" || var.pre_bootstrap_user_data != """") ? local.user_data : """"
    }
  }
}
",locals,18,18.0,74d39187d855932dd976da6180eda42dcfe09873,74d39187d855932dd976da6180eda42dcfe09873,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/74d39187d855932dd976da6180eda42dcfe09873/modules/_user_data/main.tf#L18,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/74d39187d855932dd976da6180eda42dcfe09873/modules/_user_data/main.tf#L18,2024-05-08 08:04:19-04:00,2024-05-08 08:04:19-04:00,1,0,0,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,40,test/setup/iam.tf,test/setup/iam.tf,0,// todo,// TODO: Descope,"""roles/owner"" // TODO: Descope","locals {
  int_required_roles = [
    ""roles/bigquery.admin"",
    ""roles/cloudkms.cryptoKeyEncrypterDecrypter"",
    ""roles/owner"" // TODO: Descope
  ]
}
",locals,"locals {
  int_required_roles = [
    ""roles/bigquery.admin"",
    ""roles/cloudkms.cryptoKeyEncrypterDecrypter"",
    ""roles/owner"" // TODO: Descope
  ]
}
",locals,21,21.0,ad3c3472b644fe79c37ae1416b28faf5e0cbe271,d4f61d3ee2427d8d42cab767c0326074c56d2c17,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/ad3c3472b644fe79c37ae1416b28faf5e0cbe271/test/setup/iam.tf#L21,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/d4f61d3ee2427d8d42cab767c0326074c56d2c17/test/setup/iam.tf#L21,2023-02-17 11:54:10-06:00,2023-02-17 14:43:20-06:00,2,0,0,1,0,1,0,0,0,0
https://github.com/ministryofjustice/modernisation-platform,202,terraform/environments/data-platform-apps-and-tools/rds.tf,terraform/environments/data-platform-apps-and-tools/rds.tf,0,todo,"// TODO - Review this rule: ""Instance does not have IAM Authentication enabled"" IAM Auth not understood by our applications ","// TODO - Review this rule: ""Instance does not have IAM Authentication enabled"" IAM Auth not understood by our applications","module ""openmetadata_rds"" {
  #checkov:skip=CKV_TF_1:Module registry does not support commit hashes for versions
  source  = ""terraform-aws-modules/rds/aws""
  version = ""~> 6.0""

  identifier = ""openmetadata""

  engine               = ""postgres""
  engine_version       = ""15""
  family               = ""postgres15""
  major_engine_version = ""15""
  instance_class       = ""db.r6g.xlarge""

  ca_cert_identifier = ""rds-ca-rsa2048-g1""

  allocated_storage     = 128
  max_allocated_storage = 512

  multi_az               = true
  db_subnet_group_name   = module.vpc.database_subnet_group
  vpc_security_group_ids = [module.rds_security_group.security_group_id]

  username                    = ""openmetadata""
  db_name                     = ""openmetadata""
  manage_master_user_password = false
  password                    = random_password.openmetadata.result
  kms_key_id                  = module.openmetadata_rds_kms.key_arn

  parameters = [
    {
      name  = ""rds.force_ssl""
      value = 1
    },
    {
      name  = ""log_statement""
      value = ""all""
    },
    {
      name  = ""log_hostname""
      value = 1
    },
    {
      name  = ""log_connections""
      value = 1
    },
    {
      // Required as per Open Metadata's Documentation https://docs.open-metadata.org/v1.1.x/deployment/upgrade#update-sortbuffersize-mysql-or-workmem-postgres
      name  = ""work_mem""
      value = 10000
    }
  ]

  maintenance_window      = ""Mon:00:00-Mon:03:00""
  backup_window           = ""03:00-06:00""
  backup_retention_period = 7
  deletion_protection     = true

  apply_immediately = true

  performance_insights_enabled = true

  create_monitoring_role          = true
  monitoring_role_use_name_prefix = true
  monitoring_role_name            = ""openmetadata-rds-monitoring""
  monitoring_role_description     = ""Enhanced Monitoring for Open Metadata RDS""
  monitoring_interval             = 30
  enabled_cloudwatch_logs_exports = [""postgresql"", ""upgrade""]

  skip_final_snapshot = true

  tags = local.tags
}
",module,"module ""openmetadata_rds"" {
  #checkov:skip=CKV_TF_1:Module registry does not support commit hashes for versions
  source  = ""terraform-aws-modules/rds/aws""
  version = ""~> 6.0""

  identifier = ""openmetadata""

  engine               = ""postgres""
  engine_version       = ""15""
  family               = ""postgres15""
  major_engine_version = ""15""
  instance_class       = ""db.r6g.xlarge""

  ca_cert_identifier = ""rds-ca-rsa2048-g1""

  allocated_storage     = 128
  max_allocated_storage = 512

  multi_az               = true
  db_subnet_group_name   = module.vpc.database_subnet_group
  vpc_security_group_ids = [module.rds_security_group.security_group_id]

  username                    = ""openmetadata""
  db_name                     = ""openmetadata""
  manage_master_user_password = false
  password                    = random_password.openmetadata.result
  kms_key_id                  = module.openmetadata_rds_kms.key_arn

  parameters = [
    {
      name  = ""rds.force_ssl""
      value = 1
    },
    {
      name  = ""log_statement""
      value = ""all""
    },
    {
      name  = ""log_hostname""
      value = 1
    },
    {
      name  = ""log_connections""
      value = 1
    },
    {
      // Required as per Open Metadata's Documentation https://docs.open-metadata.org/v1.1.x/deployment/upgrade#update-sortbuffersize-mysql-or-workmem-postgres
      name  = ""work_mem""
      value = 10000
    }
  ]

  maintenance_window      = ""Mon:00:00-Mon:03:00""
  backup_window           = ""03:00-06:00""
  backup_retention_period = 7
  deletion_protection     = true

  apply_immediately = true

  performance_insights_enabled = true

  create_monitoring_role          = true
  monitoring_role_use_name_prefix = true
  monitoring_role_name            = ""openmetadata-rds-monitoring""
  monitoring_role_description     = ""Enhanced Monitoring for Open Metadata RDS""
  monitoring_interval             = 30
  enabled_cloudwatch_logs_exports = [""postgresql"", ""upgrade""]

  skip_final_snapshot = true

  tags = local.tags
}
",module,68,,90002b7667baf7c1eb23d0e49dcfa9d9eaee7567,672e8441b8463a463020857479268444f179be0c,https://github.com/ministryofjustice/modernisation-platform/blob/90002b7667baf7c1eb23d0e49dcfa9d9eaee7567/terraform/environments/data-platform-apps-and-tools/rds.tf#L68,https://github.com/ministryofjustice/modernisation-platform/blob/672e8441b8463a463020857479268444f179be0c/terraform/environments/data-platform-apps-and-tools/rds.tf,2023-10-18 15:43:32+01:00,2023-10-19 14:03:41+01:00,2,1,0,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,606,infra/modules/aws-psoxy-lambda/main.tf,infra/modules/aws-psoxy-lambda/main.tf,0,hack,# parse PATH_TO_SHARED_CONFIG in super-hacky way,"# parse PATH_TO_SHARED_CONFIG in super-hacky way 
 # expect something like: 
 # arn:aws:ssm:us-east-1:123123123123:parameter/PSOXY_SALT","locals {
  instance_ssm_prefix = coalesce(var.path_to_instance_ssm_parameters, ""${upper(replace(var.function_name, ""-"", ""_""))}_"")

  # parse PATH_TO_SHARED_CONFIG in super-hacky way
  # expect something like:
  # arn:aws:ssm:us-east-1:123123123123:parameter/PSOXY_SALT
  salt_arn              = [for l in var.global_parameter_arns : l if endswith(l, ""PSOXY_SALT"")][0]
  path_to_shared_config = regex(""arn.+parameter/(.*)PSOXY_SALT"", local.salt_arn)[0]
}
",locals,"locals {
  salt_parameter_name_suffix = ""PSOXY_SALT""
  function_name              = ""${module.env_id.id}-${var.instance_id}""

  kms_key_ids_to_allow = merge(
    var.ssm_kms_key_ids,
    var.kms_keys_to_allow
  )
}
",locals,14,,59ed477a2b92552a81c734405b9d0faf43f10330,005e1fed5f46b4310d81d41d29862bb1c4f360b0,https://github.com/Worklytics/psoxy/blob/59ed477a2b92552a81c734405b9d0faf43f10330/infra/modules/aws-psoxy-lambda/main.tf#L14,https://github.com/Worklytics/psoxy/blob/005e1fed5f46b4310d81d41d29862bb1c4f360b0/infra/modules/aws-psoxy-lambda/main.tf,2023-01-13 11:04:05-08:00,2024-02-06 19:07:07+00:00,14,1,0,1,0,0,1,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,201,modules/extensions/variables.tf,modules/extensions/variables.tf,0,todo,# TODO move to workers,"# Worker draining 
 # TODO move to workers","variable ""node_pools_to_drain"" { type = list(string) }
",variable,the block associated got renamed or deleted,,58,,6c867cd8e9cbf559742f56658989bcded0d1fd89,c76c39b0b3a6dcb10122f8ab5408f1b327640972,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/modules/extensions/variables.tf#L58,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/c76c39b0b3a6dcb10122f8ab5408f1b327640972/modules/extensions/variables.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,3,1,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1,organization-bootstrap/environments/main.tf,foundations/environments/main.tf,1,# todo,# TODO(ludomagno): move XPN admin role here after checking it now works on folders,"############################################################################### 
 #                              Top-level folders                              # 
 ###############################################################################  
 # TODO(ludomagno): move XPN admin role here after checking it now works on folders ","module ""folders-top-level"" {
  source            = ""terraform-google-modules/folders/google""
  version           = ""2.0.0""
  parent            = var.root_node
  names             = var.environments
  set_roles         = true
  per_folder_admins = module.service-accounts-tf-environments.iam_emails_list
  folder_admin_roles = [
    ""roles/resourcemanager.folderViewer"",
    ""roles/resourcemanager.projectCreator"",
    ""roles/owner"",
    ""roles/compute.networkAdmin"",
  ]
}
",module,"module ""folders-top-level"" {
  source            = ""terraform-google-modules/folders/google""
  version           = ""2.0.0""
  parent            = var.root_node
  names             = var.environments
  set_roles         = true
  per_folder_admins = module.service-accounts-tf-environments.iam_emails_list
  folder_admin_roles = compact(
    [
      ""roles/compute.networkAdmin"",
      ""roles/owner"",
      ""roles/resourcemanager.folderViewer"",
      ""roles/resourcemanager.projectCreator"",
      var.grant_xpn_folder_roles ? ""roles/compute.xpnAdmin"" : """"
    ]
  )
}
",module,79,,e4fa25f22d67f88aa2b46089d0ad4cba53a3dc99,f7d950b39f877a9bab60fa916d95906553ccb3cf,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/e4fa25f22d67f88aa2b46089d0ad4cba53a3dc99/organization-bootstrap/environments/main.tf#L79,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/f7d950b39f877a9bab60fa916d95906553ccb3cf/foundations/environments/main.tf,2019-09-07 05:44:24+02:00,2019-09-19 12:16:45+02:00,2,1,0,1,0,1,0,0,0,1
https://github.com/GoogleCloudPlatform/hpc-toolkit,149,community/modules/compute/schedmd-slurm-gcp-v6-nodeset-tpu/main.tf,community/modules/compute/schedmd-slurm-gcp-v6-nodeset-tpu/main.tf,0,# todo,"# TODO: rename to subnetwork_self_link, requires changes to the scripts","# TODO: rename to subnetwork_self_link, requires changes to the scripts","locals {

  nodeset_tpu = {
    node_count_static      = var.node_count_static
    node_count_dynamic_max = var.node_count_dynamic_max
    nodeset_name           = var.name
    node_type              = var.node_type

    accelerator_config = var.accelerator_config
    tf_version         = var.tf_version
    preemptible        = var.preemptible
    preserve_tpu       = var.preserve_tpu

    data_disks   = var.data_disks
    docker_image = var.docker_image

    enable_public_ip = !var.disable_public_ips
    # TODO: rename to subnetwork_self_link, requires changes to the scripts
    subnetwork      = var.subnetwork_self_link
    service_account = var.service_account
    zone            = var.zone
  }
}
",locals,"locals {

  nodeset_tpu = {
    node_count_static      = var.node_count_static
    node_count_dynamic_max = var.node_count_dynamic_max
    nodeset_name           = var.name
    node_type              = var.node_type

    accelerator_config = var.accelerator_config
    tf_version         = var.tf_version
    preemptible        = var.preemptible
    preserve_tpu       = var.preserve_tpu

    data_disks   = var.data_disks
    docker_image = var.docker_image

    enable_public_ip = !var.disable_public_ips
    subnetwork       = var.subnetwork_self_link
    service_account  = var.service_account
    zone             = var.zone
  }
}
",locals,37,,3d1072da48450aa22b844bb5c288415b270616cc,a7adc269a3069a1ab27baed7c8f5e136a3f46f3e,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/3d1072da48450aa22b844bb5c288415b270616cc/community/modules/compute/schedmd-slurm-gcp-v6-nodeset-tpu/main.tf#L37,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/a7adc269a3069a1ab27baed7c8f5e136a3f46f3e/community/modules/compute/schedmd-slurm-gcp-v6-nodeset-tpu/main.tf,2024-01-02 14:51:06-08:00,2024-02-13 17:40:08-08:00,3,1,0,1,0,0,1,0,0,0
https://github.com/nasa/cumulus,21,tf-modules/cumulus/archive.tf,tf-modules/cumulus/archive.tf,0,todo,# TODO Get these dynamically,# TODO Get these dynamically,"module ""archive"" {
  source = ""../archive""

  prefix = var.prefix

  permissions_boundary_arn = var.permissions_boundary_arn

  ecs_cluster_name = aws_ecs_cluster.default.name

  elasticsearch_arn      = ""XXX""
  elasticsearch_hostname = ""XXX""

  system_bucket     = var.system_bucket
  public_buckets    = var.public_buckets
  protected_buckets = var.protected_buckets
  private_buckets   = var.private_buckets

  vpc_id            = var.vpc_id
  lambda_subnet_ids = var.lambda_subnet_ids

  cmr_client_id   = var.cmr_client_id
  cmr_environment = var.cmr_environment
  cmr_provider    = var.cmr_provider
  cmr_username    = var.cmr_username
  cmr_password    = var.cmr_password

  urs_url             = ""https://uat.urs.earthdata.nasa.gov""
  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password

  # TODO Get these dynamically
  dynamo_tables = {
    AccessTokens    = ""${var.prefix}-AccessTokensTable""
    AsyncOperations = ""${var.prefix}-AsyncOperationsTable""
    Collections     = ""${var.prefix}-CollectionsTable""
    Executions      = ""${var.prefix}-ExecutionsTable""
    Granules        = ""${var.prefix}-GranulesTable""
    Pdrs            = ""${var.prefix}-PdrsTable""
    Providers       = ""${var.prefix}-ProvidersTable""
    Rules           = ""${var.prefix}-RulesTable""
    Users           = ""${var.prefix}-UsersTable""
  }
}
",module,"module ""archive"" {
  source = ""../archive""

  prefix = var.prefix

  permissions_boundary_arn = var.permissions_boundary_arn

  ecs_cluster_name = aws_ecs_cluster.default.name

  elasticsearch_domain_arn        = var.elasticsearch_domain_arn
  elasticsearch_hostname          = var.elasticsearch_hostname
  elasticsearch_security_group_id = var.elasticsearch_security_group_id

  ems_host = ""change-ems-host""

  system_bucket     = var.system_bucket
  public_buckets    = var.public_buckets
  protected_buckets = var.protected_buckets
  private_buckets   = var.private_buckets

  vpc_id            = var.vpc_id
  lambda_subnet_ids = var.lambda_subnet_ids

  cmr_client_id   = var.cmr_client_id
  cmr_environment = var.cmr_environment
  cmr_provider    = var.cmr_provider
  cmr_username    = var.cmr_username
  cmr_password    = var.cmr_password

  urs_url             = ""https://uat.urs.earthdata.nasa.gov""
  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password

  token_secret = var.token_secret

  dynamo_tables = var.dynamo_tables

  api_port = var.archive_api_port

  schedule_sf_function_arn      = data.aws_lambda_function.schedule_sf.arn
  message_consumer_function_arn = data.aws_lambda_function.message_consumer.arn
  # TODO This should eventually come from the ingest module
  kinesis_inbound_event_logger = var.kinesis_inbound_event_logger

  # TODO We need to figure out how to make this dynamic
  background_queue_name = ""backgroundProcessing""

  distribution_api_id = module.distribution.rest_api_id
  distribution_url    = module.distribution.distribution_url

  users = var.archive_api_users
}
",module,31,,1da53282470313085da6e713a94458500df71f6c,ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7,https://github.com/nasa/cumulus/blob/1da53282470313085da6e713a94458500df71f6c/tf-modules/cumulus/archive.tf#L31,https://github.com/nasa/cumulus/blob/ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7/tf-modules/cumulus/archive.tf,2019-08-02 16:32:51-04:00,2019-08-14 14:23:38-04:00,2,1,1,1,0,0,0,0,0,0
https://github.com/pingcap/tidb-operator,6,deploy/alicloud/main.tf,deploy/aliyun/main.tf,1,# todo,# TODO: support non-public apiserver,# TODO: support non-public apiserver,"module ""ack"" {
  source  = ""./ack""
  version = ""1.0.2""

  providers = {
    alicloud = ""alicloud.this""
  }

  # TODO: support non-public apiserver
  region           = ""${var.ALICLOUD_REGION}""
  cluster_name     = ""${var.cluster_name}""
  public_apiserver = true
  kubeconfig_file  = ""${local.kubeconfig}""
  key_file         = ""${local.key_file}""
  vpc_cidr         = ""${var.vpc_cidr}""
  k8s_pod_cidr     = ""${var.k8s_pod_cidr}""
  k8s_service_cidr = ""${var.k8s_service_cidr}""
  vpc_cidr_newbits = ""${var.vpc_cidr_newbits}""
  vpc_id           = ""${var.vpc_id}""
  group_id         = ""${var.group_id}""

  worker_groups = [
    {
      name          = ""pd_worker_group""
      instance_type = ""${data.alicloud_instance_types.pd.instance_types.0.id}""
      min_size      = ""${var.pd_count}""
      max_size      = ""${var.pd_count}""
      node_taints   = ""dedicated=pd:NoSchedule""
      node_labels   = ""dedicated=pd""
      post_userdata = ""${file(""userdata/pd-userdata.sh"")}""
    },
    {
      name          = ""tikv_worker_group""
      instance_type = ""${data.alicloud_instance_types.tikv.instance_types.0.id}""
      min_size      = ""${var.tikv_count}""
      max_size      = ""${var.tikv_count}""
      node_taints   = ""dedicated=tikv:NoSchedule""
      node_labels   = ""dedicated=tikv""
      post_userdata = ""${file(""userdata/tikv-userdata.sh"")}""
    },
    {
      name          = ""tidb_worker_group""
      instance_type = ""${var.tidb_instance_type != """" ? var.tidb_instance_type : data.alicloud_instance_types.tidb.instance_types.0.id}""
      min_size      = ""${var.tidb_count}""
      max_size      = ""${var.tidb_count}""
      node_taints   = ""dedicated=tidb:NoSchedule""
      node_labels   = ""dedicated=tidb""
    },
    {
      name          = ""monitor_worker_group""
      instance_type = ""${var.monitor_intance_type != """" ? var.monitor_intance_type : data.alicloud_instance_types.monitor.instance_types.0.id}""
      min_size      = 1
      max_size      = 1
    },
  ]
}
",module,the block associated got renamed or deleted,,36,,eebd686956c0b2adf67a10e1a376659c95c571a3,042b1a97fbbdf342297002990564828e6644a3f0,https://github.com/pingcap/tidb-operator/blob/eebd686956c0b2adf67a10e1a376659c95c571a3/deploy/alicloud/main.tf#L36,https://github.com/pingcap/tidb-operator/blob/042b1a97fbbdf342297002990564828e6644a3f0/deploy/aliyun/main.tf,2019-05-06 19:59:43+08:00,2019-07-23 19:44:58+08:00,4,1,1,1,0,0,1,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,163,modules/workergroup/cloudinit.tf,modules/workerpools/cloudinit.tf,1,todo,# TODO Collapse w/ variable content_type for cloud-init versions,"# Copyright (c) 2022, 2023 Oracle Corporation and/or its affiliates. 
 # Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl  
 # TODO Collapse w/ variable content_type for cloud-init versions","data ""cloudinit_config"" ""worker_once"" {
  gzip          = false
  base64_encode = true

  part {
    filename     = ""worker.template.sh""
    content_type = ""text/x-shellscript""
    content = templatefile(""${path.module}/cloudinit/worker.template.sh"", {
      cluster_ca_cert = local.cluster_ca_cert
      apiserver_host  = var.apiserver_private_host
    })
  }
}
",data,,,4,0.0,4d2b3f3d672a8f41655da3a7c58fded42c6858f3,6c867cd8e9cbf559742f56658989bcded0d1fd89,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/4d2b3f3d672a8f41655da3a7c58fded42c6858f3/modules/workergroup/cloudinit.tf#L4,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/modules/workerpools/cloudinit.tf#L0,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,2,2,0,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,4,main.tf,main.tf,0,//todo,//TODO: array of users or groups needs to be added to have access. Need to figure out the best method of customers to allocate users or groups.,"//TODO: array of users or groups needs to be added to have access. Need to figure out the best method of customers to allocate users or groups. 
 # access { 
 #   role   = ""READER"" 
 #   domain = ""adigangi.com"" 
 # } 
 # 
 # access { 
 #   role           = ""WRITER"" 
 #   user_by_email = ""adigangi@adigangi.com"" 
 # } 
 # 
 # access { 
 #   role           = ""OWNER"" 
 #   special_group  = ""projectOwners"" 
 # }","resource ""google_bigquery_dataset"" ""default"" {
  dataset_id                  = ""${var.dataset_id}""
  friendly_name               = ""${var.dataset_name}""
  description                 = ""${var.description}""
  #TODO: add if condition to validate if neither US or EU are supplied
  location                    = ""${var.region}""
  #TODO: format this ne excluded by default but can optionally be defined if the user wishes
  default_table_expiration_ms = ""${var.expiration}""
  project                     = ""${var.project_id}""

  #TODO: Need to find a way to dynamically assign a dict object(s)
  labels {
    env = ""default""
    foo = ""bar""
    tonyd = ""tonyd""
  }

  //TODO: array of users or groups needs to be added to have access. Need to figure out the best method of customers to allocate users or groups.
  # access {
  #   role   = ""READER""
  #   domain = ""adigangi.com""
  # }
  #
  # access {
  #   role           = ""WRITER""
  #   user_by_email = ""adigangi@adigangi.com""
  # }
  #
  # access {
  #   role           = ""OWNER""
  #   special_group  = ""projectOwners""
  # }
}
",resource,the block associated got renamed or deleted,,39,,d56aa2c9a80343d60eed3e1a7d24962be31ee0b6,7f922f7e9df197df38c9b09dfa6e3614d71f19f5,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/d56aa2c9a80343d60eed3e1a7d24962be31ee0b6/main.tf#L39,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/7f922f7e9df197df38c9b09dfa6e3614d71f19f5/main.tf,2018-11-20 10:30:15-05:00,2019-01-16 18:10:54-05:00,3,1,0,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,3,main.tf,main.tf,0,#todo,#TODO: Need to find a way to dynamically assign a dict object(s),#TODO: Need to find a way to dynamically assign a dict object(s),"resource ""google_bigquery_dataset"" ""default"" {
  dataset_id                  = ""${var.dataset_id}""
  friendly_name               = ""${var.dataset_name}""
  description                 = ""${var.description}""
  #TODO: add if condition to validate if neither US or EU are supplied
  location                    = ""${var.region}""
  #TODO: format this ne excluded by default but can optionally be defined if the user wishes
  default_table_expiration_ms = ""${var.expiration}""
  project                     = ""${var.project_id}""

  #TODO: Need to find a way to dynamically assign a dict object(s)
  labels {
    env = ""default""
    foo = ""bar""
    tonyd = ""tonyd""
  }

  //TODO: array of users or groups needs to be added to have access. Need to figure out the best method of customers to allocate users or groups.
  # access {
  #   role   = ""READER""
  #   domain = ""adigangi.com""
  # }
  #
  # access {
  #   role           = ""WRITER""
  #   user_by_email = ""adigangi@adigangi.com""
  # }
  #
  # access {
  #   role           = ""OWNER""
  #   special_group  = ""projectOwners""
  # }
}
",resource,the block associated got renamed or deleted,,32,,d56aa2c9a80343d60eed3e1a7d24962be31ee0b6,7f922f7e9df197df38c9b09dfa6e3614d71f19f5,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/d56aa2c9a80343d60eed3e1a7d24962be31ee0b6/main.tf#L32,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/7f922f7e9df197df38c9b09dfa6e3614d71f19f5/main.tf,2018-11-20 10:30:15-05:00,2019-01-16 18:10:54-05:00,3,1,1,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,94,infra/modules/aws-psoxy-instance/main.tf,infra/modules/aws-psoxy-instance/main.tf,0,#todo,#TODO: verify that it's also going to pass through GET??,"integration_method        = ""POST"" #TODO: verify that it's also going to pass through GET?? 
 #TODO: match on subpath equivalent to var.function_name","resource ""aws_apigatewayv2_integration"" ""map"" {
  api_id                    = var.api_gateway.id
  integration_type          = ""AWS_PROXY""
  connection_type           = ""INTERNET""

  integration_method        = ""POST"" #TODO: verify that it's also going to pass through GET??
  #TODO: match on subpath equivalent to var.function_name
  integration_uri           = aws_lambda_function.psoxy-instance.invoke_arn
}
",resource,"resource ""aws_apigatewayv2_integration"" ""map"" {
  api_id                    = var.api_gateway.id
  integration_type          = ""AWS_PROXY""
  connection_type           = ""INTERNET""

  #TODO: match on subpath equivalent to var.function_name ?

  integration_method        = ""POST""
  integration_uri           = aws_lambda_function.psoxy-instance.invoke_arn
  request_parameters        = {}
  request_templates         = {}
}
",resource,19,,10047f65e7f6a188f52736d1178dd326651a0662,7444d0de8dc052089b9c9dc91394cf5172660cdd,https://github.com/Worklytics/psoxy/blob/10047f65e7f6a188f52736d1178dd326651a0662/infra/modules/aws-psoxy-instance/main.tf#L19,https://github.com/Worklytics/psoxy/blob/7444d0de8dc052089b9c9dc91394cf5172660cdd/infra/modules/aws-psoxy-instance/main.tf,2022-01-06 11:38:28-08:00,2022-01-06 11:41:26-08:00,2,1,1,1,0,0,1,0,0,0
https://github.com/CDCgov/prime-simplereport,95,ops/services/web_application_firewall/main.tf,ops/services/web_application_firewall/main.tf,0,//todo,//TODO: add exception for whoami,"rule_group_name = ""REQUEST-932-APPLICATION-ATTACK-RCE"" //TODO: add exception for whoami","resource ""azurerm_web_application_firewall_policy"" ""sr_waf_policy"" {
  name                = ""${var.name}-wafpolicy""
  resource_group_name = var.resource_group_name
  location            = var.resource_group_location

  custom_rules {
    name      = ""Block_Sanctioned_Entities""
    priority  = 1
    rule_type = ""MatchRule""

    match_conditions {
      match_variables {
        variable_name = ""RemoteAddr""
      }
      operator           = ""GeoMatch""
      negation_condition = false
      match_values = [
        ""AF"", //Afghanistan (ITAR)
        ""AL"", //Albania (OFAC)
        ""BA"", //Bosnia and Herzegovinia (OFAC)
        ""BG"", //Bulgaria (OFAC)
        ""BY"", //Belarus (ITAR, OFAC)
        ""CD"", //Democratic Republic of the Congo (ITAR)
        ""CF"", //Central African Republic (ITAR, OFAC)
        ""CG"", //Congo (ITAR, OFAC)
        ""CI"", //Cte d'Ivoire (ITAR)
        ""CN"", //People's Republic of China (EAR, ITAR)
        ""CU"", //Cuba (EAR, ITAR, OFAC)
        ""CY"", //Cyprus (ITAR)
        ""ER"", //Eritrea (ITAR)
        ""ET"", //Ethiopia (ITAR-adjacent, due to ongoing conflict with Eritrea)
        ""GE"", //Georgia (preemptive, due to presence of separatist regions sympathetic to Russian Federation)
        ""HK"", //Hong Kong SAR (due to oversight by People's Republic of China)
        ""HR"", //Croatia (OFAC)
        ""HT"", //Haiti (ITAR)
        ""IQ"", //Iraq (EAR, ITAR, OFAC)
        ""IR"", //Iran, Islamic Republic of (EAR, ITAR, OFAC)
        ""KP"", //Korea, Democratic People's Republic of (EAR, ITAR, OFAC)
        ""LB"", //Lebanon (ITAR, OFAC)
        ""LK"", //Sri Lanka (ITAR)
        ""LR"", //Liberia (ITAR, OFAC)
        ""LY"", //Libya (ITAR, OFAC)
        ""MD"", //Moldova, Republic of (preemptive, due to presence of separatist regions sympathetic to Russian Federation)
        ""ME"", //Montenegro (OFAC)
        ""MK"", //North Macedonia (OFAC)
        ""MM"", //Myanmar (ITAR)
        ""MO"", //Macao SAR (due to oversight by People's Republic of China)
        ""NI"", //Nicaragua (preemptive, due to association with sanctioned entities)
        ""RS"", //Serbia (OFAC)
        ""RU"", //Russian Federation (EAR, OFAC)
        ""SO"", //Somalia (ITAR, OFAC)
        ""SY"", //Syrian Arab Republic (EAR, ITAR, OFAC)
        ""UA"", //Ukraine (OFAC, due to occupation by the Russian Federation)
        ""VE"", //Venezuela (EAR, ITAR, OFAC)
        ""VN"", //Vietnam (ITAR)
        ""XK"", //Kosovo (OFAC)
        ""YE"", //Yemen (OFAC)
        ""ZW""  //Zimbabwe (ITAR)
      ]
    }
    action = ""Block""
  }

  managed_rules {
    exclusion {
      match_variable          = ""RequestCookieNames""
      selector                = ""ai_session""
      selector_match_operator = ""StartsWith""
    }
    managed_rule_set {
      type    = ""OWASP""
      version = ""3.2""

      rule_group_override {
        rule_group_name = ""REQUEST-942-APPLICATION-ATTACK-SQLI""
        disabled_rules = [
          ""942430"",
          ""942260"",
          ""942200""
        ]
      }

      rule_group_override {
        rule_group_name = ""REQUEST-932-APPLICATION-ATTACK-RCE"" //TODO: add exception for whoami
        disabled_rules = [
          ""932100"",
          ""932105"",
          ""932115""
        ]
      }
    }
  }

  policy_settings {
    enabled                     = true
    mode                        = ""Detection"" //Can use ""Detection"" for testing, to see which requests would be blocked. ""Prevention"" turns on active blocking.
    request_body_check          = true
    file_upload_limit_in_mb     = 100
    max_request_body_size_in_kb = 128 //Can go to 2000 in modern provider version. Proposed is 1024.
  }
}",resource,"resource ""azurerm_web_application_firewall_policy"" ""sr_waf_policy"" {
  name                = ""${var.name}-wafpolicy""
  resource_group_name = var.resource_group_name
  location            = var.resource_group_location

  custom_rules {
    name      = ""Block_Sanctioned_Entities""
    priority  = 1
    rule_type = ""MatchRule""

    match_conditions {
      match_variables {
        variable_name = ""RemoteAddr""
      }
      operator           = ""GeoMatch""
      negation_condition = false
      match_values = [
        ""AF"", //Afghanistan (ITAR)
        ""AL"", //Albania (OFAC)
        ""BA"", //Bosnia and Herzegovinia (OFAC)
        ""BG"", //Bulgaria (OFAC)
        ""BY"", //Belarus (ITAR, OFAC)
        ""CD"", //Democratic Republic of the Congo (ITAR)
        ""CF"", //Central African Republic (ITAR, OFAC)
        ""CG"", //Congo (ITAR, OFAC)
        ""CI"", //Cte d'Ivoire (ITAR)
        ""CN"", //People's Republic of China (EAR, ITAR)
        ""CU"", //Cuba (EAR, ITAR, OFAC)
        ""CY"", //Cyprus (ITAR)
        ""ER"", //Eritrea (ITAR)
        ""ET"", //Ethiopia (ITAR-adjacent, due to ongoing conflict with Eritrea)
        ""GE"", //Georgia (preemptive, due to presence of separatist regions sympathetic to Russian Federation)
        ""HK"", //Hong Kong SAR (due to oversight by People's Republic of China)
        ""HR"", //Croatia (OFAC)
        ""HT"", //Haiti (ITAR)
        ""IQ"", //Iraq (EAR, ITAR, OFAC)
        ""IR"", //Iran, Islamic Republic of (EAR, ITAR, OFAC)
        ""KP"", //Korea, Democratic People's Republic of (EAR, ITAR, OFAC)
        ""LB"", //Lebanon (ITAR, OFAC)
        ""LK"", //Sri Lanka (ITAR)
        ""LR"", //Liberia (ITAR, OFAC)
        ""LY"", //Libya (ITAR, OFAC)
        ""MD"", //Moldova, Republic of (preemptive, due to presence of separatist regions sympathetic to Russian Federation)
        ""ME"", //Montenegro (OFAC)
        ""MK"", //North Macedonia (OFAC)
        ""MM"", //Myanmar (ITAR)
        ""MO"", //Macao SAR (due to oversight by People's Republic of China)
        ""NI"", //Nicaragua (preemptive, due to association with sanctioned entities)
        ""RS"", //Serbia (OFAC)
        ""RU"", //Russian Federation (EAR, OFAC)
        ""SO"", //Somalia (ITAR, OFAC)
        ""SY"", //Syrian Arab Republic (EAR, ITAR, OFAC)
        ""UA"", //Ukraine (OFAC, due to occupation by the Russian Federation)
        ""VE"", //Venezuela (EAR, ITAR, OFAC)
        ""VN"", //Vietnam (ITAR)
        ""XK"", //Kosovo (OFAC)
        ""YE"", //Yemen (OFAC)
        ""ZW""  //Zimbabwe (ITAR)
      ]
    }
    action = ""Block""
  }

  managed_rules {

    /*
     * Exclusions for specific request components.
     * Azure supports three specific values for match_variable:
     *  - RequestArgNames
     *  - RequestCookieNames
     *  - RequestHeaderNames
     */
    exclusion {
      match_variable          = ""RequestCookieNames""
      selector                = ""ai_session"" //Part of Azure Application Insights
      selector_match_operator = ""StartsWith""
    }
    exclusion {
      match_variable          = ""RequestCookieNames""
      selector                = ""ai_user"" //Part of Azure Application Insights
      selector_match_operator = ""StartsWith""
    }
    exclusion {
      match_variable          = ""RequestCookieNames""
      selector                = ""iss""
      selector_match_operator = ""Equals""
    }
    exclusion {
      match_variable          = ""RequestCookieNames""
      selector                = ""ssm_au""
      selector_match_operator = ""Equals""
    }
    exclusion {
      match_variable          = ""RequestCookieNames""
      selector                = ""ssm_au_c""
      selector_match_operator = ""Equals""
    }

    exclusion {
      match_variable          = ""RequestArgNames""
      selector                = ""iss""
      selector_match_operator = ""Equals""
    }
    exclusion {
      match_variable          = ""RequestArgNames""
      selector                = ""variables.testResultList""
      selector_match_operator = ""Equals""
    }
    exclusion {
      match_variable          = ""RequestArgNames""
      selector                = ""variables.namePrefixMatch""
      selector_match_operator = ""Equals""
    }
    exclusion {
      match_variable          = ""RequestArgNames""
      selector                = ""variables.street""
      selector_match_operator = ""Equals""
    }
    exclusion {
      match_variable          = ""RequestArgNames""
      selector                = ""variables.orderingProviderStreet""
      selector_match_operator = ""Equals""
    }
    exclusion {
      match_variable          = ""RequestArgNames""
      selector                = ""operations""
      selector_match_operator = ""Equals""
    }
    exclusion {
      match_variable          = ""RequestArgNames""
      selector                = ""map""
      selector_match_operator = ""Equals""
    }
    exclusion {
      match_variable          = ""RequestArgNames""
      selector                = ""phoneNumbers.number""
      selector_match_operator = ""Contains""
    }
    exclusion {
      match_variable          = ""RequestArgNames""
      selector                = ""variables.model""
      selector_match_operator = ""Equals""
    }

    exclusion {
      match_variable          = ""RequestArgNames""
      selector                = ""query""
      selector_match_operator = ""Equals""
    }

    exclusion {
      match_variable          = ""RequestArgNames""
      selector                = ""visualization_settings""
      selector_match_operator = ""Equals""
    }

    exclusion {
      match_variable          = ""RequestArgNames""
      selector                = ""query.filters""
      selector_match_operator = ""Contains""
    }

    exclusion {
      match_variable          = ""RequestArgNames""
      selector                = ""variables.email""
      selector_match_operator = ""Contains""
    }

    managed_rule_set {
      type    = ""OWASP""
      version = ""3.2""

      /*
       * Each rule group in the OWASP ruleset can be overridden. These blocks contain a list of
       * rules within each specific group that we've chosen to override, due to how the application
       * is structured.
       *
       * These rules should be periodically reviewed for relevance.
       */
      rule_group_override {
        rule_group_name = ""REQUEST-920-PROTOCOL-ENFORCEMENT""
        dynamic ""rule"" {
          for_each = [
            ""920300"",
            ""920320""
          ]
          content {
            id      = rule.value
            enabled = false
          }
        }
      }

      rule_group_override {
        rule_group_name = ""REQUEST-932-APPLICATION-ATTACK-RCE""
        dynamic ""rule"" {
          for_each = [
            ""932100"",
            ""932105"",
            ""932115""
          ]
          content {
            id      = rule.value
            enabled = false
          }
        }
      }

      rule_group_override {
        rule_group_name = ""REQUEST-942-APPLICATION-ATTACK-SQLI""
        dynamic ""rule"" {
          for_each = [
            ""942110"",
            ""942150"",
            ""942190"",
            ""942200"",
            ""942260"",
            ""942330"",
            ""942361"",
            ""942370"",
            ""942410"",
            ""942430"",
            ""942440""
          ]
          content {
            id      = rule.value
            enabled = false
          }
        }
      }
    }
  }

  policy_settings {
    enabled                     = true
    mode                        = ""Prevention"" //Can use ""Detection"" for testing, to see which requests would be blocked. ""Prevention"" turns on active blocking.
    request_body_check          = true
    file_upload_limit_in_mb     = 100
    max_request_body_size_in_kb = 128 //Can go to 2000 in modern provider version. Proposed is 1024.
  }
}",resource,84,,1b6d28d585168f92bb42a25b50549d825f1fbb7f,331706d3af8e41e76113d6e6f72ee5ae0a862a9b,https://github.com/CDCgov/prime-simplereport/blob/1b6d28d585168f92bb42a25b50549d825f1fbb7f/ops/services/web_application_firewall/main.tf#L84,https://github.com/CDCgov/prime-simplereport/blob/331706d3af8e41e76113d6e6f72ee5ae0a862a9b/ops/services/web_application_firewall/main.tf,2022-05-09 14:43:05-05:00,2023-05-18 08:54:04-07:00,12,1,0,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,1547,infra/modules/google-workspace-dwd-connection/main.tf,infra/modules/google-workspace-dwd-connection/main.tf,0,# todo,"# TODO: md5 here is 32 chars of hex, so some risk of collision by truncating","# hash if too long 
 # TODO: md5 here is 32 chars of hex, so some risk of collision by truncating","locals {
  # sa_account_ids must be 6-30 chars, and must start with a letter, use only lowercase letters,
  # numbers and - (inside)
  # see https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_service_account

  # trim trailing replaces, replace enclosed spaces with dashes, and everything as lower case
  trimmed_id = lower(replace(trim(var.connector_service_account_id, "" ""), "" "", ""-""))

  # pad if too short
  padded_id = length(local.trimmed_id) < 6 ? ""psoxy-${local.trimmed_id}"" : local.trimmed_id

  # hash if too long
  # TODO: md5 here is 32 chars of hex, so some risk of collision by truncating
  sa_account_id = length(local.padded_id) < 31 ? local.padded_id : substr(md5(local.padded_id), 0, 30)

  instance_id = coalesce(var.instance_id, var.display_name)
}
",locals,"locals {
  # sa_account_ids must be 6-30 chars, and must start with a letter, use only lowercase letters,
  # numbers and - (inside)
  # see https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_service_account

  # trim trailing replaces, replace enclosed spaces with dashes, and everything as lower case
  trimmed_id = lower(replace(trim(var.connector_service_account_id, "" ""), "" "", ""-""))

  # pad if too short
  padded_id = length(local.trimmed_id) < 6 ? ""psoxy-${local.trimmed_id}"" : local.trimmed_id

  # hash if too long
  # TODO: md5 here is 32 chars of hex, so some risk of collision by truncating
  sa_account_id = length(local.padded_id) < 31 ? local.padded_id : substr(md5(local.padded_id), 0, 30)

  instance_id = coalesce(var.instance_id, var.display_name)
}
",locals,19,19.0,2f240146ca64d11927aac88550c4df4b094c45ee,419ab7426298f38d950186bd64303ef628cc2fc5,https://github.com/Worklytics/psoxy/blob/2f240146ca64d11927aac88550c4df4b094c45ee/infra/modules/google-workspace-dwd-connection/main.tf#L19,https://github.com/Worklytics/psoxy/blob/419ab7426298f38d950186bd64303ef628cc2fc5/infra/modules/google-workspace-dwd-connection/main.tf#L19,2023-06-20 14:50:05+00:00,2023-12-20 09:36:51-08:00,7,0,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1351,blueprints/data-solutions/shielded-folder/main.tf,blueprints/data-solutions/shielded-folder/main.tf,0,#todo,#TODO VPCSC: Access levels,#TODO VPCSC: Access levels,"data ""google_projects"" ""folder-projects"" {
  filter = ""parent.id:${split(""/"", module.folder.id)[1]}""
}
",data,"data ""google_projects"" ""folder-projects"" {
  filter = ""parent.id:${split(""/"", module.folder.id)[1]}""

  depends_on = [
    module.sec-project,
    module.log-export-project
  ]
}
",data,80,,1189e38788529cd0de5483d3505e436778644b38,79373721df7c0d803aba0fbc8eb7cae3c63d747c,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/1189e38788529cd0de5483d3505e436778644b38/blueprints/data-solutions/shielded-folder/main.tf#L80,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/79373721df7c0d803aba0fbc8eb7cae3c63d747c/blueprints/data-solutions/shielded-folder/main.tf,2023-01-25 18:30:21+01:00,2023-08-09 11:23:07+00:00,10,1,0,1,0,1,1,0,0,0
https://github.com/alphagov/govuk-aws,128,terraform/projects/app-logs-elasticsearch/main.tf,terraform/projects/app-logs-elasticsearch/main.tf,0,# todo,# TODO: Instance 2 and 3,# TODO: Instance 2 and 3,"resource ""aws_iam_policy"" ""logs_elasticsearch_iam_policy"" {
  name   = ""${var.stackname}-logs-elasticsearch-additional""
  path   = ""/""
  policy = ""${file(""${path.module}/additional_policy.json"")}""
}
",resource,"resource ""aws_iam_policy"" ""logs_elasticsearch_iam_policy"" {
  name   = ""${var.stackname}-logs-elasticsearch-additional""
  path   = ""/""
  policy = ""${file(""${path.module}/additional_policy.json"")}""
}
",resource,143,,feace08700bf50ef4a40578685362bb1cc004190,06852b4e39e5c4f33e658b99ebee182ea59a55ad,https://github.com/alphagov/govuk-aws/blob/feace08700bf50ef4a40578685362bb1cc004190/terraform/projects/app-logs-elasticsearch/main.tf#L143,https://github.com/alphagov/govuk-aws/blob/06852b4e39e5c4f33e658b99ebee182ea59a55ad/terraform/projects/app-logs-elasticsearch/main.tf,2017-07-20 12:48:33+01:00,2017-07-26 18:30:00+01:00,3,1,0,0,0,1,0,0,0,0
https://github.com/chanzuckerberg/cztack,168,databricks-cluster-log-permissions/main.tf,databricks-cluster-log-permissions/main.tf,0,hack,# hacky way to validate if this workspace/cluster should have read permissions,"# hacky way to validate if this workspace/cluster should have read permissions 
 # tflint-ignore: terraform_unused_declarations","locals {
  default_role_name    = ""cluster_log_cluster_role"" # standard role for clusters - allows both writing and reading cluster logs for only the same workspace
  read_write_role_name = ""cluster_log_rw_role""      # special role - allows both writing and reading cluster logs for all workspaces
  path                 = ""/databricks/""

  # hacky way to validate if this workspace/cluster should have read permissions
  # tflint-ignore: terraform_unused_declarations
  validate_add_reader = (var.add_reader == true && var.env != var.global_reader_env) ? tobool(""add_reader is not supported for this environment"") : true

  databricks_bucket_cluster_log_prefix = ""cluster-logs""

  # kms grants - all roles can read and write
  read_write_operations = [""Encrypt"", ""GenerateDataKey"", ""Decrypt""]
}
",locals,"locals {
  default_role_name    = ""cluster_log_cluster_role"" # standard role for clusters - allows both writing and reading cluster logs for only the same workspace
  read_write_role_name = ""cluster_log_rw_role""      # special role - allows both writing and reading cluster logs for all workspaces
  path                 = ""/databricks/""

  # hacky way to validate if this workspace/cluster should have read permissions
  # tflint-ignore: terraform_unused_declarations
  validate_add_reader = (var.add_reader == true && var.env != var.global_reader_env) ? tobool(""add_reader is not supported for this environment"") : true

  databricks_bucket_cluster_log_prefix = ""cluster-logs""

  # kms grants - all roles can read and write
  read_write_operations = [""Encrypt"", ""GenerateDataKey"", ""Decrypt""]
}
",locals,10,10.0,2e5974a61defa36d339a1a28ce7c90a17bd22685,2e5974a61defa36d339a1a28ce7c90a17bd22685,https://github.com/chanzuckerberg/cztack/blob/2e5974a61defa36d339a1a28ce7c90a17bd22685/databricks-cluster-log-permissions/main.tf#L10,https://github.com/chanzuckerberg/cztack/blob/2e5974a61defa36d339a1a28ce7c90a17bd22685/databricks-cluster-log-permissions/main.tf#L10,2023-10-31 13:12:50-07:00,2023-10-31 13:12:50-07:00,1,0,0,0,0,1,0,0,0,1
https://github.com/SUSE/ha-sap-terraform-deployments,303,libvirt/main.tf,libvirt/main.tf,0,// todo,// TODO: check this better,// TODO: check this better,"resource ""libvirt_volume"" ""base_image"" {
  name   = ""${terraform.workspace}-baseimage""
  source = var.base_image
  // TODO: check this better
  pool   = ""terraform""
}
",resource,"resource ""libvirt_volume"" ""base_image"" {
  // the base image will be ""cloned"" and used by other domains, 
  // it is the central  image.
  name   = ""${terraform.workspace}-baseimage""
  source = var.base_image
  // TODO: this can moved to a tfvars
  pool   = var.storage_pool
}
",resource,16,,31fa6cffe7c41a22121d877d5a4244227a875687,b11930acacfa1ed7479d9490adc7b066a3973790,https://github.com/SUSE/ha-sap-terraform-deployments/blob/31fa6cffe7c41a22121d877d5a4244227a875687/libvirt/main.tf#L16,https://github.com/SUSE/ha-sap-terraform-deployments/blob/b11930acacfa1ed7479d9490adc7b066a3973790/libvirt/main.tf,2019-08-28 13:25:20+02:00,2019-08-28 13:42:24+02:00,2,1,1,1,0,0,0,0,0,0
https://github.com/Azure/sap-automation,15,deploy/terraform/terraform-units/modules/sap_landscape/iscsi.tf,deploy/terraform/terraform-units/modules/sap_landscape/iscsi.tf,0,// todo,// TODO: Add nsr to iSCSI's nsg,"// TODO: Add nsr to iSCSI's nsg  
 /* 
 iSCSI device IP address range: .4 - 
 */ 
 // Creates the NIC and IP address for iSCSI device","resource ""azurerm_network_interface"" ""iscsi"" {
  provider            = azurerm.main
  count               = local.iscsi_count
  name                = format(""%s%s%s%s"", local.prefix, var.naming.separator, local.virtualmachine_names[count.index], local.resource_suffixes.nic)
  location            = local.rg_exists ? data.azurerm_resource_group.resource_group[0].location : azurerm_resource_group.resource_group[0].location
  resource_group_name = local.rg_exists ? data.azurerm_resource_group.resource_group[0].name : azurerm_resource_group.resource_group[0].name

  ip_configuration {
    name                          = ""ipconfig1""
    subnet_id                     = local.sub_iscsi_exists ? data.azurerm_subnet.iscsi[0].id : azurerm_subnet.iscsi[0].id
    private_ip_address            = local.use_DHCP ? null : local.sub_iscsi_exists ? local.iscsi_nic_ips[count.index] : cidrhost(local.sub_iscsi_prefix, tonumber(count.index) + 4)
    private_ip_address_allocation = local.use_DHCP ? ""Dynamic"" : ""static""
  }
}
",resource,"resource ""azurerm_network_interface"" ""iscsi"" {
  provider                             = azurerm.main
  count                                = local.iscsi_count
  name                                 = format(""%s%s%s%s%s"",
                                           var.naming.resource_prefixes.nic,
                                           local.prefix,
                                           var.naming.separator,
                                           local.virtualmachine_names[count.index],
                                           local.resource_suffixes.nic
                                         )
  resource_group_name                  = local.resource_group_exists ? (
                                           data.azurerm_resource_group.resource_group[0].name) : (
                                           azurerm_resource_group.resource_group[0].name
                                         )
  location                             = local.resource_group_exists ? (
                                          data.azurerm_resource_group.resource_group[0].location) : (
                                          azurerm_resource_group.resource_group[0].location
                                        )
  tags                                 = var.tags

  ip_configuration {
                     name = ""ipconfig1""
                     subnet_id = local.sub_iscsi_exists ? (
                       data.azurerm_subnet.iscsi[0].id) : (
                       azurerm_subnet.iscsi[0].id
                     )
                     private_ip_address = local.use_DHCP ? (
                       null) : (
                       local.sub_iscsi_exists ? (
                         local.iscsi_nic_ips[count.index]) : (
                         cidrhost(local.sub_iscsi_prefix, tonumber(count.index) + 4)
                       )
                     )
                     private_ip_address_allocation = local.use_DHCP ? ""Dynamic"" : ""Static""
                   }
}
",resource,46,62.0,6ff0b891114c36d3aeccb850d830b698cd1fe52a,df063c58945a9efa2cb2ba303762c43f0b9c1d8f,https://github.com/Azure/sap-automation/blob/6ff0b891114c36d3aeccb850d830b698cd1fe52a/deploy/terraform/terraform-units/modules/sap_landscape/iscsi.tf#L46,https://github.com/Azure/sap-automation/blob/df063c58945a9efa2cb2ba303762c43f0b9c1d8f/deploy/terraform/terraform-units/modules/sap_landscape/iscsi.tf#L62,2021-11-17 19:29:07+02:00,2024-05-17 12:37:17+03:00,14,0,0,1,0,1,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1281,modules/net-vpn-ha/main.tf,modules/net-vpn-ha/main.tf,0,fix,# FIXME: can bgp_session_range be null?,# FIXME: can bgp_session_range be null?,"resource ""google_compute_router_interface"" ""router_interface"" {
  for_each = var.tunnels
  project  = var.project_id
  region   = var.region
  name     = ""${var.name}-${each.key}""
  router   = local.router
  # FIXME: can bgp_session_range be null?
  ip_range   = each.value.bgp_session_range == """" ? null : each.value.bgp_session_range
  vpn_tunnel = google_compute_vpn_tunnel.tunnels[each.key].name
}
",resource,"resource ""google_compute_router_interface"" ""router_interface"" {
  for_each = var.tunnels
  project  = var.project_id
  region   = var.region
  name     = ""${var.name}-${each.key}""
  router   = local.router
  # FIXME: can bgp_session_range be null?
  ip_range   = each.value.bgp_session_range == """" ? null : each.value.bgp_session_range
  vpn_tunnel = google_compute_vpn_tunnel.tunnels[each.key].name
}
",resource,117,139.0,798d3a413681b624391eabf9895ca52d0e810e6c,a95e681f059af9e112636befd03efb124439115f,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/798d3a413681b624391eabf9895ca52d0e810e6c/modules/net-vpn-ha/main.tf#L117,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/a95e681f059af9e112636befd03efb124439115f/modules/net-vpn-ha/main.tf#L139,2022-11-30 10:52:24+01:00,2024-04-28 12:11:07+02:00,11,0,0,1,0,0,1,0,0,0
https://github.com/kubernetes/k8s.io,373,infra/aws/terraform/prow-build-cluster/prow.tf,infra/aws/terraform/prow-build-cluster/prow.tf,0,# todo,# TODO(xmudrii): This is a temporary condition. To be deleted after making canary cluster a build cluster.,# TODO(xmudrii): This is a temporary condition. To be deleted after making canary cluster a build cluster.,"resource ""aws_iam_openid_connect_provider"" ""k8s_prow"" {
  # TODO(xmudrii): This is a temporary condition. To be deleted after making canary cluster a build cluster.
  count = var.cluster_name == ""prow-build-cluster"" ? 1 : 0

  url             = ""https://container.googleapis.com/v1/projects/k8s-prow/locations/us-central1-f/clusters/prow""
  client_id_list  = [""sts.amazonaws.com""]
  thumbprint_list = [""08745487e891c19e3078c1f2a07e452950ef36f6""]
}
",resource,"resource ""aws_iam_openid_connect_provider"" ""k8s_prow"" {
  count = local.configure_prow ? 1 : 0

  url             = ""https://container.googleapis.com/v1/projects/k8s-prow/locations/us-central1-f/clusters/prow""
  client_id_list  = [""sts.amazonaws.com""]
  thumbprint_list = [""08745487e891c19e3078c1f2a07e452950ef36f6""]
}
",resource,22,,db6b5a5b1de7cac3d8f08c4b44a6a19b36855b58,3cf0ef275d51659e041a4663921016a73d4eb7b8,https://github.com/kubernetes/k8s.io/blob/db6b5a5b1de7cac3d8f08c4b44a6a19b36855b58/infra/aws/terraform/prow-build-cluster/prow.tf#L22,https://github.com/kubernetes/k8s.io/blob/3cf0ef275d51659e041a4663921016a73d4eb7b8/infra/aws/terraform/prow-build-cluster/prow.tf,2023-04-26 13:27:36+02:00,2023-04-26 14:58:43+02:00,2,1,1,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,5,infra/modules/google-workspace-dwd-connection/main.tf,infra/modules/google-workspace-dwd-connection/main.tf,0,# todo,"# TODO: extract this to its own repo or something, so can consume from our main infra repo. it's","# infra for a Google Workspace API connector 
 #  (eg, OAuth Client in a GCP project)  
 # TODO: extract this to its own repo or something, so can consume from our main infra repo. it's 
 # similar to src/modules/google-workspace-dwd-connector/main.tf in the main infra repo  
 # service account to personify connector","resource ""google_service_account"" ""connector-sa"" {
  account_id   = var.connector_service_account_id
  display_name = var.display_name
  description  = var.description
  project      = var.project_id
}
",resource,"locals {
  # sa_account_ids must be 6-30 chars, and must start with a letter, use only lowercase letters,
  # numbers and - (inside)
  # see https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_service_account

  # trim trailing replaces, replace enclosed spaces with dashes, and everything as lower case
  trimmed_id = lower(replace(trim(var.connector_service_account_id, "" ""), "" "", ""-""))

  # pad if too short
  padded_id = length(local.trimmed_id) < 6 ? ""psoxy-${local.trimmed_id}"" : local.trimmed_id

  # hash if too long
  # TODO: md5 here is 32 chars of hex, so some risk of collision by truncating
  sa_account_id = length(local.padded_id) < 31 ? local.padded_id : substr(md5(local.padded_id), 0, 30)

  instance_id = coalesce(var.instance_id, var.display_name)
}
",locals,4,4.0,1259c535e4d315fea708946a07b95f255b249721,419ab7426298f38d950186bd64303ef628cc2fc5,https://github.com/Worklytics/psoxy/blob/1259c535e4d315fea708946a07b95f255b249721/infra/modules/google-workspace-dwd-connection/main.tf#L4,https://github.com/Worklytics/psoxy/blob/419ab7426298f38d950186bd64303ef628cc2fc5/infra/modules/google-workspace-dwd-connection/main.tf#L4,2021-10-06 09:58:36-07:00,2023-12-20 09:36:51-08:00,24,0,0,1,0,1,0,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,396,azure/network.tf,azure/network.tf,0,todo,# TODO check if this is needed,"# TODO check if this is needed 
 # Load balancing rules for HANA 1.0 ","resource ""azurerm_lb_rule"" ""lb_30315"" {
  resource_group_name            = azurerm_resource_group.myrg.name
  loadbalancer_id                = azurerm_lb.mylb.id
  name                           = ""hana-lb-30315""
  protocol                       = ""Tcp""
  frontend_ip_configuration_name = ""mylb-frontend""
  frontend_port                  = 30315
  backend_port                   = 30315
  backend_address_pool_id        = azurerm_lb_backend_address_pool.mylb.id
  probe_id                       = azurerm_lb_probe.mylb.id
  idle_timeout_in_minutes        = 30
  enable_floating_ip             = ""true""
}
",resource,"resource ""azurerm_lb_rule"" ""lb_30315"" {
  resource_group_name            = azurerm_resource_group.myrg.name
  loadbalancer_id                = azurerm_lb.mylb.id
  name                           = ""hana-lb-30315""
  protocol                       = ""Tcp""
  frontend_ip_configuration_name = ""mylb-frontend""
  frontend_port                  = 30315
  backend_port                   = 30315
  backend_address_pool_id        = azurerm_lb_backend_address_pool.mylb.id
  probe_id                       = azurerm_lb_probe.mylb.id
  idle_timeout_in_minutes        = 30
  enable_floating_ip             = ""true""
}
",resource,143,,5daf59b3bbcb57130d80f6d844ad35171f7c010a,9adcf152486838f9f5b600ead5e4ef373918a786,https://github.com/SUSE/ha-sap-terraform-deployments/blob/5daf59b3bbcb57130d80f6d844ad35171f7c010a/azure/network.tf#L143,https://github.com/SUSE/ha-sap-terraform-deployments/blob/9adcf152486838f9f5b600ead5e4ef373918a786/azure/network.tf,2019-09-05 00:05:54+02:00,2019-09-05 18:08:13+02:00,2,1,0,1,0,0,1,0,0,1
https://github.com/SUSE/ha-sap-terraform-deployments,347,azure/salt_provisioner.tf,azure/salt_provisioner.tf,0,workaround,# Workaround to let the process start in background properly,] # Workaround to let the process start in background properly,"resource ""null_resource"" ""monitoring_provisioner"" {
  count = var.provisioner == ""salt"" ? 1 : 0


  triggers = {
    monitoring_id = azurerm_virtual_machine.monitoring.id
  }

  connection {
    host        = data.azurerm_public_ip.monitoring.ip_address
    type        = ""ssh""
    user        = var.admin_user
    private_key = file(var.private_key_location)
  }

  provisioner ""file"" {
    source      = ""../salt""
    destination = ""/tmp""
  }

  provisioner ""file"" {
    content     = data.template_file.salt_provisioner.rendered
    destination = ""/tmp/salt_provisioner.sh""
  }

// TODO: add or don't add this (from libvirt)
// network_domain: ${var.network_domain}


  provisioner ""file"" {
    content = <<EOF
provider: azure
role: monitoring
name_prefix: ${terraform.workspace}-${var.name}
hostname: ${terraform.workspace}-${var.name}${var.monitoring_count > 1 ? ""0${count.index + 1}"" : """"}
timezone: ${var.timezone}
reg_code: ${var.reg_code}
reg_email: ${var.reg_email}
reg_additional_modules: {${join("", "",formatlist(""'%s': '%s'"",keys(var.reg_additional_modules),values(var.reg_additional_modules),),)}}
additional_repos: {${join("", "",formatlist(""'%s': '%s'"",keys(var.additional_repos),values(var.additional_repos),),)}}
additional_packages: [${join("", "", formatlist(""'%s'"", var.additional_packages))}]
authorized_keys: [${trimspace(file(var.public_key_location))},${trimspace(file(var.public_key_location))}]
host_ips: [${join("", "", formatlist(""'%s'"", [var.monitoring_srv_ip]))}]
host_ip: ${var.monitoring_srv_ip}
role: monitoring
provider: libvirt
ha_sap_deployment_repo: ${var.ha_sap_deployment_repo}
monitored_services: [${join("", "", formatlist(""'%s'"", var.monitored_services))}]
EOF

destination = ""/tmp/grains""
}

provisioner ""remote-exec"" {
  inline = [
    ""${var.background ? ""nohup"" : """"} sudo sh /tmp/salt_provisioner.sh > /tmp/provisioning.log ${var.background ? ""&"" : """"}"",
    ""return_code=$? && sleep 1 && exit $return_code"",
  ] # Workaround to let the process start in background properly
}

}
",resource,"resource ""null_resource"" ""monitoring_provisioner"" {
  count = var.provisioner == ""salt"" && var.monitoring_enabled ? 1 : 0

  triggers = {
    monitoring_id = azurerm_virtual_machine.monitoring.0.id
  }

  connection {
    host        = data.azurerm_public_ip.monitoring.0.ip_address
    type        = ""ssh""
    user        = var.admin_user
    private_key = file(var.private_key_location)
  }

  provisioner ""file"" {
    source      = ""../salt""
    destination = ""/tmp""
  }

  provisioner ""file"" {
    content     = data.template_file.salt_provisioner.rendered
    destination = ""/tmp/salt_provisioner.sh""
  }

  provisioner ""file"" {
    content = <<EOF
provider: azure
role: monitoring
name_prefix: vmmonitoring
hostname: ""vmmonitoring""
timezone: ${var.timezone}
reg_code: ${var.reg_code}
reg_email: ${var.reg_email}
reg_additional_modules: {${join("", "", formatlist(""'%s': '%s'"", keys(var.reg_additional_modules), values(var.reg_additional_modules), ), )}}
additional_packages: [${join("", "", formatlist(""'%s'"", var.additional_packages))}]
authorized_keys: [${trimspace(file(var.public_key_location))},${trimspace(file(var.public_key_location))}]
host_ips: [${join("", "", formatlist(""'%s'"", [var.monitoring_srv_ip]))}]
host_ip: ${var.monitoring_srv_ip}
ha_sap_deployment_repo: ${var.ha_sap_deployment_repo}
monitored_hosts: [${join("", "", formatlist(""'%s'"", var.host_ips))}]
drbd_monitored_hosts: [${join("", "", formatlist(""'%s'"", var.drbd_enabled ? var.drbd_ips : []))}]
nw_monitored_hosts: [${join("", "", formatlist(""'%s'"", var.netweaver_enabled ? var.netweaver_ips : []))}]
network_domain: ""tf.local""
EOF

    destination = ""/tmp/grains""
  }

  provisioner ""remote-exec"" {
    inline = [
      ""${var.background ? ""nohup"" : """"} sudo sh /tmp/salt_provisioner.sh > /tmp/provisioning.log ${var.background ? ""&"" : """"}"",
      ""return_code=$? && sleep 1 && exit $return_code"",
    ] # Workaround to let the process start in background properly
  }

}
",resource,224,139.0,f41baea2a7a45b527e944b62bcab73612c693e02,2ab3e131e002872c45a5d1aa0293246437fa3009,https://github.com/SUSE/ha-sap-terraform-deployments/blob/f41baea2a7a45b527e944b62bcab73612c693e02/azure/salt_provisioner.tf#L224,https://github.com/SUSE/ha-sap-terraform-deployments/blob/2ab3e131e002872c45a5d1aa0293246437fa3009/azure/salt_provisioner.tf#L139,2019-09-05 00:01:31+02:00,2020-01-28 16:54:26-08:00,24,0,0,1,0,0,0,1,0,0
https://github.com/Azure/terraform-azurerm-caf-enterprise-scale,51,resources.management.tf,resources.management.tf,0,fix,# workspace and Automation Account to fix issue #109.,"# Set explicit dependency on Resource Group, Log Analytics 
 # workspace and Automation Account to fix issue #109. 
 # Ideally we would limit to specific solutions, but the 
 # depends_on block only supports static values.","resource ""azurerm_log_analytics_solution"" ""enterprise_scale"" {
  for_each = local.azurerm_log_analytics_solution_enterprise_scale

  # Mandatory resource attributes
  solution_name         = each.value.template.solution_name
  location              = each.value.template.location
  resource_group_name   = each.value.template.resource_group_name
  workspace_resource_id = each.value.template.workspace_resource_id
  workspace_name        = each.value.template.workspace_name

  plan {
    publisher = each.value.template.plan.publisher
    product   = each.value.template.plan.product
  }

  # Optional resource attributes
  tags = each.value.template.tags

  # Set explicit dependency on Resource Group, Log Analytics
  # workspace and Automation Account to fix issue #109.
  # Ideally we would limit to specific solutions, but the
  # depends_on block only supports static values.
  depends_on = [
    azurerm_resource_group.enterprise_scale,
    azurerm_log_analytics_workspace.enterprise_scale,
    azurerm_automation_account.enterprise_scale,
  ]

}
",resource,"resource ""azurerm_log_analytics_solution"" ""management"" {
  for_each = local.azurerm_log_analytics_solution_management

  provider = azurerm.management

  # Mandatory resource attributes
  solution_name         = each.value.template.solution_name
  location              = each.value.template.location
  resource_group_name   = each.value.template.resource_group_name
  workspace_resource_id = each.value.template.workspace_resource_id
  workspace_name        = each.value.template.workspace_name

  plan {
    publisher = each.value.template.plan.publisher
    product   = each.value.template.plan.product
  }

  # Optional resource attributes
  tags = each.value.template.tags

  # Set explicit dependency on Resource Group, Log Analytics
  # workspace and Automation Account to fix issue #109.
  # Ideally we would limit to specific solutions, but the
  # depends_on block only supports static values.
  depends_on = [
    azurerm_resource_group.management,
    azurerm_log_analytics_workspace.management,
    azurerm_automation_account.management,
    azurerm_log_analytics_linked_service.management,
  ]

}
",resource,49,62.0,18297293ca702ce6610f0117baacdaafcc148ce8,be69a5d92787faec1d895f5d0b35e165ce9de8fd,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/18297293ca702ce6610f0117baacdaafcc148ce8/resources.management.tf#L49,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/be69a5d92787faec1d895f5d0b35e165ce9de8fd/resources.management.tf#L62,2021-06-07 13:21:51+01:00,2022-12-23 09:19:54+00:00,5,0,0,1,0,0,0,0,1,0
https://github.com/clong/DetectionLab,1,Azure/Terraform/main.tf,Azure/Terraform/main.tf,0,fix,# FIXME!,"# terraform init, plan, apply, destroy 
 # Note: does not support idempotence, don't execute twice with same scope. 
 # https://www.terraform.io/docs/providers/azurerm/index.html 
 # latest test: terraform 0.12.18 
 # 
 # FIXME! 
 # * apply: provisioning not working on Windows 
 # Error: Unsupported argument [...] An argument named ""connection"" is not expected here. 
 #    apply => Error: timeout - last error: SSH authentication failed (root@:22): ssh: handshake failed: ssh: unable to authenticate, attempted methods [none publickey], no supported methods remain 
 # * apply: linux provisioning 
 #        => works but script ends with error code for some reason (post bro install and splunk restart)  
 # Specify the provider and access details","provider ""azurerm"" {
  version = ""=2.12.0""
  features {}
}
",provider,"provider ""azurerm"" {
  features {}
}
",provider,6,6.0,5791b99c8fb8b6de6552e91e5ed6da5607e90401,0a61dc1d44e9a4c4331f054ebf0b225e065739ac,https://github.com/clong/DetectionLab/blob/5791b99c8fb8b6de6552e91e5ed6da5607e90401/Azure/Terraform/main.tf#L6,https://github.com/clong/DetectionLab/blob/0a61dc1d44e9a4c4331f054ebf0b225e065739ac/Azure/Terraform/main.tf#L6,2020-06-14 18:45:18-07:00,2022-04-21 13:40:51-05:00,19,0,0,1,1,0,0,0,0,0
https://github.com/cloudfoundry/bosh-bootloader,1,plan-patches/cfcr-azure/terraform/cfcr_lb_override.tf,plan-patches/cfcr-azure/terraform/cfcr_lb_override.tf,0,todo,# TODO move this to cfcr resource group.,# TODO move this to cfcr resource group.,"resource ""azurerm_lb"" ""cfcr-balancer"" {
  name                = ""${var.env_id}-cfcr-lb""
  location            = ""${var.region}""
  sku                 = ""Standard""
  # TODO move this to cfcr resource group.
  resource_group_name = ""${azurerm_resource_group.bosh.name}""

  frontend_ip_configuration {
    name                 = ""${azurerm_public_ip.cfcr-balancer-ip.name}""
    public_ip_address_id = ""${azurerm_public_ip.cfcr-balancer-ip.id}""
  }
}
",resource,"resource ""azurerm_lb"" ""cfcr-balancer"" {
  name                = ""${var.env_id}-cfcr-lb""
  location            = ""${var.region}""
  sku                 = ""Standard""
  resource_group_name = ""${azurerm_resource_group.cfcr.name}""

  frontend_ip_configuration {
    name                 = ""${azurerm_public_ip.cfcr-balancer-ip.name}""
    public_ip_address_id = ""${azurerm_public_ip.cfcr-balancer-ip.id}""
  }
}
",resource,13,,5169911b36b56c3d7da1815cfc1b4b5e2c9e46eb,f098a6c6ca76caf96d25a1469be65f7cb7d5c514,https://github.com/cloudfoundry/bosh-bootloader/blob/5169911b36b56c3d7da1815cfc1b4b5e2c9e46eb/plan-patches/cfcr-azure/terraform/cfcr_lb_override.tf#L13,https://github.com/cloudfoundry/bosh-bootloader/blob/f098a6c6ca76caf96d25a1469be65f7cb7d5c514/plan-patches/cfcr-azure/terraform/cfcr_lb_override.tf,2018-10-09 15:43:19-07:00,2018-10-29 09:34:36-07:00,2,1,0,1,0,0,1,0,0,0
https://github.com/Worklytics/psoxy,287,infra/modules/aws-psoxy-rest/main.tf,infra/modules/aws-psoxy-rest/main.tf,0,# todo,# TODO: configurable? not all require POST,"allow_methods     = [""POST"", ""GET"", ""HEAD""] # TODO: configurable? not all require POST","resource ""aws_lambda_function_url"" ""lambda_url"" {
  function_name      = var.function_name
  authorization_type = ""AWS_IAM""

  cors {
    allow_credentials = true
    allow_origins     = [""*""]
    allow_methods     = [""POST"", ""GET"", ""HEAD""] # TODO: configurable? not all require POST
    allow_headers     = [""date"", ""keep-alive""]
    expose_headers    = [""keep-alive"", ""date""]
    max_age           = 86400
  }
}
",resource,"resource ""aws_lambda_function_url"" ""lambda_url"" {
  function_name      = module.psoxy_lambda.function_name
  authorization_type = ""AWS_IAM""

  depends_on = [
    module.psoxy_lambda
  ]
}
",resource,40,,83172700daa197caa267997675d3ec6acb23c229,1b9ca4eaa917e0f3fce28bafc58d33529de22312,https://github.com/Worklytics/psoxy/blob/83172700daa197caa267997675d3ec6acb23c229/infra/modules/aws-psoxy-rest/main.tf#L40,https://github.com/Worklytics/psoxy/blob/1b9ca4eaa917e0f3fce28bafc58d33529de22312/infra/modules/aws-psoxy-rest/main.tf,2022-05-09 21:48:24-07:00,2023-08-09 09:57:11-07:00,51,1,1,1,0,0,0,0,0,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,210,locals.tf,locals.tf,0,fix,"# After manually correcting the file on the server the installation works fine and the taint is removed as planned, so fixing the indentation is up next.","# ToDo: The cilium.yaml file after it has been copied to the server currently has wrong indentation causing the helm template to fail. 
 # After manually correcting the file on the server the installation works fine and the taint is removed as planned, so fixing the indentation is up next.","locals {
  # ssh_agent_identity is not set if the private key is passed directly, but if ssh agent is used, the public key tells ssh agent which private key to use.
  # For terraforms provisioner.connection.agent_identity, we need the public key as a string.
  ssh_agent_identity = var.ssh_private_key == null ? var.ssh_public_key : null

  # If passed, a key already registered within hetzner is used. 
  # Otherwise, a new one will be created by the module.
  hcloud_ssh_key_id = var.hcloud_ssh_key_id == null ? hcloud_ssh_key.k3s[0].id : var.hcloud_ssh_key_id

  ccm_version   = var.hetzner_ccm_version != null ? var.hetzner_ccm_version : data.github_release.hetzner_ccm.release_tag
  csi_version   = var.hetzner_csi_version != null ? var.hetzner_csi_version : data.github_release.hetzner_csi.release_tag
  kured_version = var.kured_version != null ? var.kured_version : data.github_release.kured.release_tag

  common_commands_install_k3s = [
    ""set -ex"",
    # prepare the k3s config directory
    ""mkdir -p /etc/rancher/k3s"",
    # move the config file into place
    ""mv /tmp/config.yaml /etc/rancher/k3s/config.yaml"",
    # if the server has already been initialized just stop here
    ""[ -e /etc/rancher/k3s/k3s.yaml ] && exit 0"",
  ]

  apply_k3s_selinux = [""/sbin/semodule -v -i /usr/share/selinux/packages/k3s.pp""]

  install_k3s_server = concat(local.common_commands_install_k3s, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC=server sh -""
  ], local.apply_k3s_selinux)
  install_k3s_agent = concat(local.common_commands_install_k3s, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC=agent sh -""
  ], local.apply_k3s_selinux)

  control_plane_nodes = merge([
    for pool_index, nodepool_obj in var.control_plane_nodepools : {
      for node_index in range(nodepool_obj.count) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        location : nodepool_obj.location,
        labels : concat(local.default_control_plane_labels, nodepool_obj.labels),
        taints : concat(local.default_control_plane_taints, nodepool_obj.taints),
        index : node_index
      }
    }
  ]...)

  agent_nodes = merge([
    for pool_index, nodepool_obj in var.agent_nodepools : {
      for node_index in range(nodepool_obj.count) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        location : nodepool_obj.location,
        labels : concat(local.default_agent_labels, nodepool_obj.labels),
        taints : concat(local.default_agent_taints, nodepool_obj.taints),
        index : node_index
      }
    }
  ]...)

  # The main network cidr that all subnets will be created upon
  network_ipv4_cidr = ""10.0.0.0/8""

  # The first two subnets are respectively the default subnet 10.0.0.0/16 use for potientially anything and 10.1.0.0/16 used for control plane nodes.
  # the rest of the subnets are for agent nodes in each nodepools.
  network_ipv4_subnets = [for index in range(256) : cidrsubnet(local.network_ipv4_cidr, 8, index)]

  # if we are in a single cluster config, we use the default klipper lb instead of Hetzner LB
  control_plane_count    = sum([for v in var.control_plane_nodepools : v.count])
  agent_count            = sum([for v in var.agent_nodepools : v.count])
  is_single_node_cluster = (local.control_plane_count + local.agent_count) == 1

  using_klipper_lb = var.use_klipper_lb || local.is_single_node_cluster

  # disable k3s extras
  disable_extras = concat([""local-storage""], local.using_klipper_lb ? [] : [""servicelb""], var.traefik_enabled ? [] : [
    ""traefik""
  ], var.metrics_server_enabled ? [] : [""metrics-server""])

  # Default k3s node labels
  default_agent_labels         = concat([], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])
  default_control_plane_labels = concat([], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])

  allow_scheduling_on_control_plane = local.is_single_node_cluster ? true : var.allow_scheduling_on_control_plane

  # Default k3s node taints
  default_control_plane_taints = concat([], local.allow_scheduling_on_control_plane ? [] : [""node-role.kubernetes.io/control-plane:NoSchedule""], var.cni_plugin == ""cilium"" ? [""node.cilium.io/agent-not-ready:NoExecute""] : [])
  default_agent_taints         = concat([], var.cni_plugin == ""cilium"" ? [""node.cilium.io/agent-not-ready:NoExecute""] : [])


  packages_to_install = concat(var.enable_longhorn ? [""open-iscsi"", ""nfs-client""] : [], var.extra_packages_to_install)

  # The following IPs are important to be whitelisted because they communicate with Hetzner services and enable the CCM and CSI to work properly.
  # Source https://github.com/hetznercloud/csi-driver/issues/204#issuecomment-848625566
  hetzner_metadata_service_ipv4 = ""169.254.169.254/32""
  hetzner_cloud_api_ipv4        = ""213.239.246.1/32""

  # internal Pod CIDR, used for the controller and currently for calico
  cluster_cidr_ipv4 = ""10.42.0.0/16""

  whitelisted_ips = [
    local.network_ipv4_cidr,
    local.hetzner_metadata_service_ipv4,
    local.hetzner_cloud_api_ipv4,
    ""127.0.0.1/32"",
  ]

  base_firewall_rules = concat([
    # Allowing internal cluster traffic and Hetzner metadata service and cloud API IPs
    {
      direction  = ""in""
      protocol   = ""tcp""
      port       = ""any""
      source_ips = local.whitelisted_ips
    },
    {
      direction  = ""in""
      protocol   = ""udp""
      port       = ""any""
      source_ips = local.whitelisted_ips
    },
    {
      direction  = ""in""
      protocol   = ""icmp""
      source_ips = local.whitelisted_ips
    },

    # Allow all traffic to the kube api server
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""6443""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },

    # Allow all traffic to the ssh port
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""22""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },

    # Allow basic out traffic
    # ICMP to ping outside services
    {
      direction = ""out""
      protocol  = ""icmp""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    # DNS
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""53""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""out""
      protocol  = ""udp""
      port      = ""53""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    # HTTP(s)
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""80""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""443""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    #NTP
    {
      direction = ""out""
      protocol  = ""udp""
      port      = ""123""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    }
    ], !local.using_klipper_lb ? [] : [
    # Allow incoming web traffic for single node clusters, because we are using k3s servicelb there,
    # not an external load-balancer.
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""80""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""443""
      source_ips = [
        ""0.0.0.0/0""
      ]
    }
    ], var.block_icmp_ping_in ? [] : [
    {
      direction = ""in""
      protocol  = ""icmp""
      source_ips = [
        ""0.0.0.0/0""
      ]
    }
  ])

  labels = {
    ""provisioner"" = ""terraform"",
    ""engine""      = ""k3s""
    ""cluster""     = var.cluster_name
  }

  labels_control_plane_node = {
    role = ""control_plane_node""
  }
  labels_control_plane_lb = {
    role = ""control_plane_lb""
  }

  labels_agent_node = {
    role = ""agent_node""
  }

  cni_install_resources = {
    ""calico"" = [""https://projectcalico.docs.tigera.io/manifests/calico.yaml""]
    ""cilium"" = [""cilium.yaml""]
  }

  cni_install_resource_patches = {
    ""calico"" = [""calico.yaml""]
  }

  cni_k3s_settings = {
    ""flannel"" = {
      disable-network-policy = var.disable_network_policy
    }
    ""calico"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
    ""cilium"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
  }

  # ToDo: The cilium.yaml file after it has been copied to the server currently has wrong indentation causing the helm template to fail.
  # After manually correcting the file on the server the installation works fine and the taint is removed as planned, so fixing the indentation is up next.
  default_cilium_values = <<EOT
ipam:
 operator:
  clusterPoolIPv4PodCIDRList:
   - ${local.cluster_cidr_ipv4}
devices: ""eth1""
EOT
}
",locals,"locals {
  # ssh_agent_identity is not set if the private key is passed directly, but if ssh agent is used, the public key tells ssh agent which private key to use.
  # For terraforms provisioner.connection.agent_identity, we need the public key as a string.
  ssh_agent_identity = var.ssh_private_key == null ? var.ssh_public_key : null

  # If passed, a key already registered within hetzner is used. 
  # Otherwise, a new one will be created by the module.
  hcloud_ssh_key_id = var.hcloud_ssh_key_id == null ? hcloud_ssh_key.k3s[0].id : var.hcloud_ssh_key_id

  ccm_version   = var.hetzner_ccm_version != null ? var.hetzner_ccm_version : data.github_release.hetzner_ccm.release_tag
  csi_version   = var.hetzner_csi_version != null ? var.hetzner_csi_version : data.github_release.hetzner_csi.release_tag
  kured_version = var.kured_version != null ? var.kured_version : data.github_release.kured.release_tag

  common_commands_install_k3s = [
    ""set -ex"",
    # prepare the k3s config directory
    ""mkdir -p /etc/rancher/k3s"",
    # move the config file into place
    ""mv /tmp/config.yaml /etc/rancher/k3s/config.yaml"",
    # if the server has already been initialized just stop here
    ""[ -e /etc/rancher/k3s/k3s.yaml ] && exit 0"",
  ]

  apply_k3s_selinux = [""/sbin/semodule -v -i /usr/share/selinux/packages/k3s.pp""]

  install_k3s_server = concat(local.common_commands_install_k3s, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC=server sh -""
  ], local.apply_k3s_selinux)
  install_k3s_agent = concat(local.common_commands_install_k3s, [
    ""curl -sfL https://get.k3s.io | INSTALL_K3S_SKIP_START=true INSTALL_K3S_SKIP_SELINUX_RPM=true INSTALL_K3S_CHANNEL=${var.initial_k3s_channel} INSTALL_K3S_EXEC=agent sh -""
  ], local.apply_k3s_selinux)

  control_plane_nodes = merge([
    for pool_index, nodepool_obj in var.control_plane_nodepools : {
      for node_index in range(nodepool_obj.count) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        location : nodepool_obj.location,
        labels : concat(local.default_control_plane_labels, nodepool_obj.labels),
        taints : concat(local.default_control_plane_taints, nodepool_obj.taints),
        index : node_index
      }
    }
  ]...)

  agent_nodes = merge([
    for pool_index, nodepool_obj in var.agent_nodepools : {
      for node_index in range(nodepool_obj.count) :
      format(""%s-%s-%s"", pool_index, node_index, nodepool_obj.name) => {
        nodepool_name : nodepool_obj.name,
        server_type : nodepool_obj.server_type,
        location : nodepool_obj.location,
        labels : concat(local.default_agent_labels, nodepool_obj.labels),
        taints : concat(local.default_agent_taints, nodepool_obj.taints),
        index : node_index
      }
    }
  ]...)

  # The main network cidr that all subnets will be created upon
  network_ipv4_cidr = ""10.0.0.0/8""

  # The first two subnets are respectively the default subnet 10.0.0.0/16 use for potientially anything and 10.1.0.0/16 used for control plane nodes.
  # the rest of the subnets are for agent nodes in each nodepools.
  network_ipv4_subnets = [for index in range(256) : cidrsubnet(local.network_ipv4_cidr, 8, index)]

  # if we are in a single cluster config, we use the default klipper lb instead of Hetzner LB
  control_plane_count    = sum([for v in var.control_plane_nodepools : v.count])
  agent_count            = sum([for v in var.agent_nodepools : v.count])
  is_single_node_cluster = (local.control_plane_count + local.agent_count) == 1

  using_klipper_lb = var.use_klipper_lb || local.is_single_node_cluster

  # disable k3s extras
  disable_extras = concat([""local-storage""], local.using_klipper_lb ? [] : [""servicelb""], var.traefik_enabled ? [] : [
    ""traefik""
  ], var.metrics_server_enabled ? [] : [""metrics-server""])

  # Default k3s node labels
  default_agent_labels         = concat([], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])
  default_control_plane_labels = concat([], var.automatically_upgrade_k3s ? [""k3s_upgrade=true""] : [])

  allow_scheduling_on_control_plane = local.is_single_node_cluster ? true : var.allow_scheduling_on_control_plane

  # Default k3s node taints
  default_control_plane_taints = concat([], local.allow_scheduling_on_control_plane ? [] : [""node-role.kubernetes.io/control-plane:NoSchedule""], var.cni_plugin == ""cilium"" ? [""node.cilium.io/agent-not-ready:NoExecute""] : [])
  default_agent_taints         = concat([], var.cni_plugin == ""cilium"" ? [""node.cilium.io/agent-not-ready:NoExecute""] : [])


  packages_to_install = concat(var.enable_longhorn ? [""open-iscsi"", ""nfs-client""] : [], var.extra_packages_to_install)

  # The following IPs are important to be whitelisted because they communicate with Hetzner services and enable the CCM and CSI to work properly.
  # Source https://github.com/hetznercloud/csi-driver/issues/204#issuecomment-848625566
  hetzner_metadata_service_ipv4 = ""169.254.169.254/32""
  hetzner_cloud_api_ipv4        = ""213.239.246.1/32""

  # internal Pod CIDR, used for the controller and currently for calico
  cluster_cidr_ipv4 = ""10.42.0.0/16""

  whitelisted_ips = [
    local.network_ipv4_cidr,
    local.hetzner_metadata_service_ipv4,
    local.hetzner_cloud_api_ipv4,
    ""127.0.0.1/32"",
  ]

  base_firewall_rules = concat([
    # Allowing internal cluster traffic and Hetzner metadata service and cloud API IPs
    {
      direction  = ""in""
      protocol   = ""tcp""
      port       = ""any""
      source_ips = local.whitelisted_ips
    },
    {
      direction  = ""in""
      protocol   = ""udp""
      port       = ""any""
      source_ips = local.whitelisted_ips
    },
    {
      direction  = ""in""
      protocol   = ""icmp""
      source_ips = local.whitelisted_ips
    },

    # Allow all traffic to the kube api server
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""6443""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },

    # Allow all traffic to the ssh port
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""22""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },

    # Allow basic out traffic
    # ICMP to ping outside services
    {
      direction = ""out""
      protocol  = ""icmp""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    # DNS
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""53""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""out""
      protocol  = ""udp""
      port      = ""53""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    # HTTP(s)
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""80""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""out""
      protocol  = ""tcp""
      port      = ""443""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    },

    #NTP
    {
      direction = ""out""
      protocol  = ""udp""
      port      = ""123""
      destination_ips = [
        ""0.0.0.0/0""
      ]
    }
    ], !local.using_klipper_lb ? [] : [
    # Allow incoming web traffic for single node clusters, because we are using k3s servicelb there,
    # not an external load-balancer.
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""80""
      source_ips = [
        ""0.0.0.0/0""
      ]
    },
    {
      direction = ""in""
      protocol  = ""tcp""
      port      = ""443""
      source_ips = [
        ""0.0.0.0/0""
      ]
    }
    ], var.block_icmp_ping_in ? [] : [
    {
      direction = ""in""
      protocol  = ""icmp""
      source_ips = [
        ""0.0.0.0/0""
      ]
    }
  ])

  labels = {
    ""provisioner"" = ""terraform"",
    ""engine""      = ""k3s""
    ""cluster""     = var.cluster_name
  }

  labels_control_plane_node = {
    role = ""control_plane_node""
  }
  labels_control_plane_lb = {
    role = ""control_plane_lb""
  }

  labels_agent_node = {
    role = ""agent_node""
  }

  cni_install_resources = {
    ""calico"" = [""https://projectcalico.docs.tigera.io/manifests/calico.yaml""]
  }

  cni_install_resource_patches = {
    ""calico"" = [""calico.yaml""]
  }

  cni_k3s_settings = {
    ""flannel"" = {
      disable-network-policy = var.disable_network_policy
    }
    ""calico"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
    ""cilium"" = {
      disable-network-policy = true
      flannel-backend        = ""none""
    }
  }

  default_cilium_values = <<EOT
ipam:
 operator:
  clusterPoolIPv4PodCIDRList:
   - ${local.cluster_cidr_ipv4}
devices: ""eth1""
EOT
}
",locals,273,,ea97125426c35d9572981496464344a0bbd830a0,dd90ec37ba3ede05e2ec5c10f0e3024423ed1ab8,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/ea97125426c35d9572981496464344a0bbd830a0/locals.tf#L273,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/dd90ec37ba3ede05e2ec5c10f0e3024423ed1ab8/locals.tf,2022-08-14 05:16:56+02:00,2022-08-14 11:19:58+02:00,2,1,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1249,blueprints/cloud-operations/network-dashboard/main.tf,blueprints/cloud-operations/network-dashboard/main.tf,0,# todo,# TODO: service_account_email,# TODO: service_account_email,"module ""cloud-function"" {
  v2          = var.cf_version == ""V2""
  source      = ""../../../modules/cloud-function""
  project_id  = local.monitoring_project
  name        = ""network-dashboard-cloud-function""
  bucket_name = ""${local.monitoring_project}-network-dashboard-bucket""
  bucket_config = {
    location = var.region
  }
  region = var.region

  bundle_config = {
    source_dir  = ""cloud-function""
    output_path = ""cloud-function.zip""
  }

  function_config = {
    timeout     = 480 # Timeout in seconds, increase it if your CF timeouts and use v2 if > 9 minutes.
    entry_point = ""main""
    runtime     = ""python39""
    instances   = 1
    memory      = 256 # Memory in MB

  }

  environment_variables = {
    MONITORED_PROJECTS_LIST = local.projects
    MONITORED_FOLDERS_LIST  = local.folders
    MONITORING_PROJECT_ID   = local.monitoring_project
    ORGANIZATION_ID         = var.organization_id
    CF_VERSION              = var.cf_version
  }

  service_account = module.service-account-function.email
  # Internal only doesn't seem to work with CFv2:
  ingress_settings = var.cf_version == ""V2"" ? ""ALLOW_ALL"" : ""ALLOW_INTERNAL_ONLY""

  trigger_config = var.cf_version == ""V2"" ? {
    v2 = {
      event_type   = ""google.cloud.pubsub.topic.v1.messagePublished""
      pubsub_topic = module.pubsub.topic.id
      # TODO: service_account_email
    }
    } : {
    v1 = {
      event    = ""google.pubsub.topic.publish""
      resource = module.pubsub.topic.id
    }
  }
}
",module,"module ""cloud-function"" {
  v2          = var.cf_version == ""V2""
  source      = ""../../../modules/cloud-function""
  project_id  = local.monitoring_project
  name        = ""network-dashboard-cloud-function""
  bucket_name = ""${local.monitoring_project}-network-dashboard-bucket""
  bucket_config = {
    location = var.region
  }
  region = var.region

  bundle_config = {
    source_dir  = ""cloud-function""
    output_path = ""cloud-function.zip""
  }

  function_config = {
    timeout     = 480 # Timeout in seconds, increase it if your CF timeouts and use v2 if > 9 minutes.
    entry_point = ""main""
    runtime     = ""python39""
    instances   = 1
    memory      = 256 # Memory in MB

  }

  environment_variables = {
    MONITORED_PROJECTS_LIST = local.projects
    MONITORED_FOLDERS_LIST  = local.folders
    MONITORING_PROJECT_ID   = local.monitoring_project
    ORGANIZATION_ID         = var.organization_id
    CF_VERSION              = var.cf_version
  }

  service_account = module.service-account-function.email
  # Internal only doesn't seem to work with CFv2:
  ingress_settings = var.cf_version == ""V2"" ? ""ALLOW_ALL"" : ""ALLOW_INTERNAL_ONLY""

  trigger_config = var.cf_version == ""V2"" ? {
    v2 = {
      event_type             = ""google.cloud.pubsub.topic.v1.messagePublished""
      pubsub_topic           = module.pubsub.topic.id
      service_account_create = true
    }
    } : {
    v1 = {
      event    = ""google.pubsub.topic.publish""
      resource = module.pubsub.topic.id
    }
  }
}
",module,174,,9e37a915c8ae3a675dfd0815f654a665bd701d21,755ff7b1d2fb9aef3627efd19cbd720fa8cddf68,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9e37a915c8ae3a675dfd0815f654a665bd701d21/blueprints/cloud-operations/network-dashboard/main.tf#L174,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/755ff7b1d2fb9aef3627efd19cbd720fa8cddf68/blueprints/cloud-operations/network-dashboard/main.tf,2022-11-16 16:44:01+01:00,2022-11-16 16:44:01+01:00,2,1,1,1,0,1,0,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,356,azure/instances.tf,azure/instances.tf,0,todo,// TODO add variable later,// TODO add variable later,"resource ""azurerm_virtual_machine"" ""monitoring"" {
  name                  = ""${terraform.workspace}-monitoring""
  location              = var.az_region
  // TODO CHECK THIS group
  resource_group_name   = azurerm_resource_group.myrg.name
  // 
  network_interface_ids = [azurerm_network_interface.monitoring.id]
  availability_set_id   = azurerm_availability_set.myas.id
  vm_size               = ""Standard_D2s_v3""

  storage_os_disk {
    name              = ""monitoringOsDisk""
    caching           = ""ReadWrite""
    create_option     = ""FromImage""
    managed_disk_type = ""Premium_LRS""
  }
   // TODO add variable later
  storage_image_reference {
    id        = azurerm_image.monitoring.0.id
    publisher = ""SUSE""
    offer     = ""SLES-SAP-BYOS""
    sku       = ""12-sp4""
    version   = ""2019.03.06""
  }

  storage_data_disk {
    name              = ""monitoringDevices""
    caching           = ""ReadWrite""
    create_option     = ""Empty""
    disk_size_gb      = ""10""
    lun               = ""0""
    managed_disk_type = ""Standard_LRS""
  }

  os_profile {
    computer_name  = ""monitoring""
    admin_username = var.admin_user
  }

  os_profile_linux_config {
    disable_password_authentication = true

    ssh_keys {
      path     = ""/home/${var.admin_user}/.ssh/authorized_keys""
      key_data = file(var.public_key_location)
    }
  }

  boot_diagnostics {
    enabled     = ""true""
    storage_uri = azurerm_storage_account.mytfstorageacc.primary_blob_endpoint
  }

  tags = {
    workspace = terraform.workspace
  }
}
",resource,"resource ""azurerm_virtual_machine"" ""monitoring"" {
  name     = ""${terraform.workspace}-monitoring""
  location = var.az_region
  resource_group_name = azurerm_resource_group.myrg.name
  network_interface_ids = [azurerm_network_interface.monitoring.id]
  availability_set_id   = azurerm_availability_set.myas.id
  vm_size               = ""Standard_D2s_v3""

  storage_os_disk {
    name              = ""monitoringOsDisk""
    caching           = ""ReadWrite""
    create_option     = ""FromImage""
    managed_disk_type = ""Premium_LRS""
  }
  
  storage_image_reference {
    id        = azurerm_image.monitoring.0.id
    publisher = ""SUSE""
    offer     = ""SLES-SAP-BYOS""
    sku       = ""15""
    version   = ""2019.07.17""
  }

  storage_data_disk {
    name              = ""monitoringDevices""
    caching           = ""ReadWrite""
    create_option     = ""Empty""
    disk_size_gb      = ""10""
    lun               = ""0""
    managed_disk_type = ""Standard_LRS""
  }

  os_profile {
    computer_name  = ""monitoring""
    admin_username = var.admin_user
  }

  os_profile_linux_config {
    disable_password_authentication = true

    ssh_keys {
      path     = ""/home/${var.admin_user}/.ssh/authorized_keys""
      key_data = file(var.public_key_location)
    }
  }

  boot_diagnostics {
    enabled     = ""true""
    storage_uri = azurerm_storage_account.mytfstorageacc.primary_blob_endpoint
  }

  tags = {
    workspace = terraform.workspace
  }
}
",resource,151,,5c0d2ffe158dbf6db6c58414cdbc56fa872b4197,9adcf152486838f9f5b600ead5e4ef373918a786,https://github.com/SUSE/ha-sap-terraform-deployments/blob/5c0d2ffe158dbf6db6c58414cdbc56fa872b4197/azure/instances.tf#L151,https://github.com/SUSE/ha-sap-terraform-deployments/blob/9adcf152486838f9f5b600ead5e4ef373918a786/azure/instances.tf,2019-09-05 00:01:31+02:00,2019-09-05 18:08:13+02:00,4,1,1,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-kubernetes-engine,9,modules/safer-cluster/main.tf,modules/safer-cluster/main.tf,0,// todo,// TODO(mmontan): link to a couple of policies.,"// Define PodSecurityPolicies for differnet applications. 
 // TODO(mmontan): link to a couple of policies.","module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // Disable the dashboard. It creates risk by running as a very sensitive user.
  kubernetes_dashboard = false

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy          = true
  network_policy_provider = ""CALICO""

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  disable_legacy_metadata_endpoints = true

  node_pools        = var.node_pools
  node_pools_labels = var.node_pools_labels

  // TODO(mmontan): check whether we need to restrict these
  // settings.
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  // TODO(mmontan): we generally considered applying
  // just the cloud-platofrm scope and use Cloud IAM
  // If we have Workload Identity, are there advantages
  // in restricting scopes even more?
  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  // We should use IP Alias.
  configure_ip_masq = false

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = length(var.compute_engine_service_account) > 0 ? false : true
  service_account        = var.compute_engine_service_account

  // TODO(mmontan): define a registry_project parameter in the private_beta_cluster,
  // so that we can give GCS permissions to the service account on a project
  // that hosts only container-images and not data.
  grant_registry_access = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // TODO(mmontan): link to a couple of policies.
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id
  node_metadata                    = ""SECURE""

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // TODO(mmontan): investigate whether this should be a recommended setting
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group
}
",module,"module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy = true

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  node_pools        = var.node_pools
  node_pools_labels = var.node_pools_labels

  // TODO(mmontan): check whether we need to restrict these
  // settings.
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = var.compute_engine_service_account == """" ? true : false
  service_account        = var.compute_engine_service_account
  registry_project_id    = var.registry_project_id
  grant_registry_access  = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // Example: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#example
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // Intranode Visibility enables you to capture flow logs for traffic between pods and create FW rules that apply to traffic between pods.
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group

  enable_shielded_nodes = var.enable_shielded_nodes
}
",module,145,,e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5,5a194719faa144ad0a7ee578663d336358f5073c,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5/modules/safer-cluster/main.tf#L145,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/5a194719faa144ad0a7ee578663d336358f5073c/modules/safer-cluster/main.tf,2019-10-04 15:21:33-07:00,2019-11-13 15:10:12-06:00,6,1,1,0,0,1,0,0,0,0
https://github.com/ministryofjustice/modernisation-platform,230,terraform/environments/core-network-services/cidr-ranges.tf,terraform/environments/core-network-services/cidr-ranges.tf,0,implemented,"# csr-prod = ""10.27.10.0/22"" not yet implemented","# csr-prod = ""10.27.10.0/22"" not yet implemented ","locals {
  mp_core_cidr_ranges = {
    mp-core                     = ""10.20.0.0/16""
    mp-development-test         = ""10.26.0.0/16""
    mp-preproduction-production = ""10.27.0.0/16""
  }

  # This will take the vpc CIDR ranges for each business/env general subnet set, from the environments-networks files
  # e.g. `hmpps-development = ""10.26.24.0/21""
  platform_general_set_cidr_ranges = {
    for key, value in local.core-vpcs : key => value.cidr.subnet_sets.general.cidr
  }

  other_cidr_ranges = {
    alpha-vpn                        = ""100.64.0.0/16""
    analytical-platform-airflow-dev  = ""10.200.0.0/16""
    analytical-platform-airflow-prod = ""10.201.0.0/16""
    atos_arkc_ras                    = ""10.175.0.0/16"" # for DOM1 devices connected to Cisco RAS VPN
    atos_arkf_ras                    = ""10.176.0.0/16"" # for DOM1 devices connected to Cisco RAS VPN
    cloud-platform                   = ""172.20.0.0/16""
    global-protect                   = ""10.184.0.0/16""
    i2n                              = ""10.110.0.0/16""
    moj-core-azure-1                 = ""10.50.25.0/27""
    moj-core-azure-2                 = ""10.50.26.0/24""
    parole-board                     = ""10.50.0.0/16""
    psn                              = ""51.0.0.0/8""
    psn-ppud                         = ""51.247.2.115/32""
    vodafone_wan_nicts_aggregate     = ""10.80.0.0/12"" # for devices connected to Prison Networks

    # hmpps azure cidr ranges
    noms-live-vnet                 = ""10.40.0.0/18""
    noms-live-dr-vnet              = ""10.40.64.0/18""
    noms-mgmt-live-vnet            = ""10.40.128.0/20""
    noms-mgmt-live-dr-vnet         = ""10.40.144.0/20""
    noms-transit-live-vnet         = ""10.40.160.0/20""
    noms-transit-live-dr-vnet      = ""10.40.176.0/20""
    noms-test-vnet                 = ""10.101.0.0/16""
    noms-mgmt-vnet                 = ""10.102.0.0/16""
    noms-test-dr-vnet              = ""10.111.0.0/16""
    noms-mgmt-dr-vnet              = ""10.112.0.0/16""
    aks-studio-hosting-live-1-vnet = ""10.244.0.0/20""
    aks-studio-hosting-dev-1-vnet  = ""10.247.0.0/20""
    aks-studio-hosting-ops-1-vnet  = ""10.247.32.0/20""
    nomisapi-t2-root-vnet          = ""10.47.0.192/26""
    nomisapi-t3-root-vnet          = ""10.47.0.0/26""
    nomisapi-preprod-root-vnet     = ""10.47.0.64/26""
    nomisapi-prod-root-vnet        = ""10.47.0.128/26""

    # hmpps aws cidr ranges
    delius-eng-dev  = ""10.161.98.0/25""
    delius-eng-prod = ""10.160.98.0/25""
    delius-core-dev = ""10.161.20.0/22""
    delius-mis-dev  = ""10.162.32.0/20""
    delius-test     = ""10.162.0.0/20""
    delius-stage    = ""10.160.32.0/20""
    delius-pre-prod = ""10.160.0.0/20""
    delius-training = ""10.162.96.0/20""
    delius-prod     = ""10.160.16.0/20""

    # laa landing zone cidr ranges
    laa-lz-development             = ""10.202.0.0/20""
    laa-lz-test                    = ""10.203.0.0/20""
    laa-lz-uat                     = ""10.206.0.0/20""
    laa-lz-staging                 = ""10.204.0.0/20""
    laa-lz-production              = ""10.205.0.0/20""
    laa-lz-shared-services-nonprod = ""10.200.0.0/20""
    laa-lz-shared-services-prod    = ""10.200.16.0/20""
    laa-appstream-vpc              = ""10.200.32.0/19""
    laa-appstream-vpc_additional   = ""10.200.68.0/22""

    # csr app cidr ranges
    csr-preprod = ""10.27.0.0/22""
    # csr-prod = ""10.27.10.0/22"" not yet implemented

  }

  all_cidr_ranges = merge(
    local.mp_core_cidr_ranges,
    local.platform_general_set_cidr_ranges,
    local.other_cidr_ranges
  )
}
",locals,"locals {
  mp_core_cidr_ranges = {
    mp-core                     = ""10.20.0.0/16""
    mp-development-test         = ""10.26.0.0/16""
    mp-preproduction-production = ""10.27.0.0/16""
  }

  # This will take the vpc CIDR ranges for each business/env general subnet set, from the environments-networks files
  # e.g. `hmpps-development = ""10.26.24.0/21""
  platform_general_set_cidr_ranges = {
    for key, value in local.core-vpcs : key => value.cidr.subnet_sets.general.cidr
  }

  other_cidr_ranges = {
    alpha-vpn                        = ""100.64.0.0/16""
    analytical-platform-airflow-dev  = ""10.200.0.0/16""
    analytical-platform-airflow-prod = ""10.201.0.0/16""
    atos_arkc_ras                    = ""10.175.0.0/16"" # for DOM1 devices connected to Cisco RAS VPN
    atos_arkf_ras                    = ""10.176.0.0/16"" # for DOM1 devices connected to Cisco RAS VPN
    cloud-platform                   = ""172.20.0.0/16""
    global-protect                   = ""10.184.0.0/16""
    i2n                              = ""10.110.0.0/16""
    moj-core-azure-1                 = ""10.50.25.0/27""
    moj-core-azure-2                 = ""10.50.26.0/24""
    parole-board                     = ""10.50.0.0/16""
    psn                              = ""51.0.0.0/8""
    psn-ppud                         = ""51.247.2.115/32""
    vodafone_wan_nicts_aggregate     = ""10.80.0.0/12"" # for devices connected to Prison Networks

    # hmpps azure cidr ranges
    noms-live-vnet                 = ""10.40.0.0/18""
    noms-live-dr-vnet              = ""10.40.64.0/18""
    noms-mgmt-live-vnet            = ""10.40.128.0/20""
    noms-mgmt-live-dr-vnet         = ""10.40.144.0/20""
    noms-transit-live-vnet         = ""10.40.160.0/20""
    noms-transit-live-dr-vnet      = ""10.40.176.0/20""
    noms-test-vnet                 = ""10.101.0.0/16""
    noms-mgmt-vnet                 = ""10.102.0.0/16""
    noms-test-dr-vnet              = ""10.111.0.0/16""
    noms-mgmt-dr-vnet              = ""10.112.0.0/16""
    aks-studio-hosting-live-1-vnet = ""10.244.0.0/20""
    aks-studio-hosting-dev-1-vnet  = ""10.247.0.0/20""
    aks-studio-hosting-ops-1-vnet  = ""10.247.32.0/20""
    nomisapi-t2-root-vnet          = ""10.47.0.192/26""
    nomisapi-t3-root-vnet          = ""10.47.0.0/26""
    nomisapi-preprod-root-vnet     = ""10.47.0.64/26""
    nomisapi-prod-root-vnet        = ""10.47.0.128/26""

    # hmpps aws cidr ranges
    delius-eng-dev  = ""10.161.98.0/25""
    delius-eng-prod = ""10.160.98.0/25""
    delius-core-dev = ""10.161.20.0/22""
    delius-mis-dev  = ""10.162.32.0/20""
    delius-test     = ""10.162.0.0/20""
    delius-stage    = ""10.160.32.0/20""
    delius-pre-prod = ""10.160.0.0/20""
    delius-training = ""10.162.96.0/20""
    delius-prod     = ""10.160.16.0/20""

    # laa landing zone cidr ranges
    laa-lz-development             = ""10.202.0.0/20""
    laa-lz-test                    = ""10.203.0.0/20""
    laa-lz-uat                     = ""10.206.0.0/20""
    laa-lz-staging                 = ""10.204.0.0/20""
    laa-lz-production              = ""10.205.0.0/20""
    laa-lz-shared-services-nonprod = ""10.200.0.0/20""
    laa-lz-shared-services-prod    = ""10.200.16.0/20""
    laa-appstream-vpc              = ""10.200.32.0/19""
    laa-appstream-vpc_additional   = ""10.200.68.0/22""

    hmpps-preproduction-general-private-subnets = ""10.27.0.0/22""
    # hmpps-production-general-private-subnets = ""10.27.10.0/22"" not yet implemented
  }

  all_cidr_ranges = merge(
    local.mp_core_cidr_ranges,
    local.platform_general_set_cidr_ranges,
    local.other_cidr_ranges
  )
}
",locals,73,,1066bcea639188a84ef4f579d78dc1bf91cdcda4,c40f0f81d52046129fb0dd1c39b5f6a628b53717,https://github.com/ministryofjustice/modernisation-platform/blob/1066bcea639188a84ef4f579d78dc1bf91cdcda4/terraform/environments/core-network-services/cidr-ranges.tf#L73,https://github.com/ministryofjustice/modernisation-platform/blob/c40f0f81d52046129fb0dd1c39b5f6a628b53717/terraform/environments/core-network-services/cidr-ranges.tf,2023-11-07 16:51:39+00:00,2023-11-07 17:40:47+00:00,2,1,0,1,0,0,1,1,0,0
https://github.com/alphagov/govuk-aws,440,terraform/projects/app-mysql/main.tf,terraform/projects/app-mysql/main.tf,0,# todo,# TODO: this should be set to 0 when we have migrated to Production,"# TODO: this should be set to 0 when we have migrated to Production 
 # as it not recommended to set this option","resource ""aws_db_parameter_group"" ""mysql-primary"" {
  name_prefix = ""mysql-primary""
  family      = ""mysql5.6""

  parameter {
    name  = ""max_allowed_packet""
    value = 1073741824
  }

  # TODO: this should be set to 0 when we have migrated to Production
  # as it not recommended to set this option
  parameter {
    name  = ""log_bin_trust_function_creators""
    value = 1
  }

  tags {
    aws_stackname = ""${var.stackname}""
  }
}
",resource,,,74,0.0,9a70d18e581efc1a56f74139df69864237d9b017,060aa26bb8ae887ea52c2a00949d13a6faf07fa9,https://github.com/alphagov/govuk-aws/blob/9a70d18e581efc1a56f74139df69864237d9b017/terraform/projects/app-mysql/main.tf#L74,https://github.com/alphagov/govuk-aws/blob/060aa26bb8ae887ea52c2a00949d13a6faf07fa9/terraform/projects/app-mysql/main.tf#L0,2017-11-21 12:34:06+00:00,2022-01-28 14:26:02+00:00,31,2,1,1,0,0,0,1,0,0
https://github.com/zenml-io/mlstacks,74,src/mlstacks/terraform/k3d-modular/k3d.tf,src/mlstacks/terraform/k3d-modular/k3d.tf,0,hack,# This is a hack to attempt get the local stores path from the zenml config,"# This is a hack to attempt get the local stores path from the zenml config 
 # and pass it to the k3d cluster resource, if not set in the local config.","data ""external"" ""zenml_local_stores_path"" {
  program = [
    ""python"",
    ""-u"",
    ""-c"",
    <<-ZENML
%{if local.k3d.local_stores_path != """"}
path = ""${local.k3d.local_stores_path}""
%{else}
try:
  from zenml.config.global_config import GlobalConfiguration
  path = GlobalConfiguration().local_stores_path
except Exception:
  path = """"
%{endif}
print('{""path"": ""' + path + '""}')
    ZENML
  ]
}
",data,"data ""external"" ""zenml_local_stores_path"" {
  program = [
    ""python"",
    ""-u"",
    ""-c"",
    <<-ZENML
%{if local.k3d.local_stores_path != """"}
path = ""${local.k3d.local_stores_path}""
%{else}
try:
  from zenml.config.global_config import GlobalConfiguration
  path = GlobalConfiguration().local_stores_path
except Exception:
  path = """"
%{endif}
print('{""path"": ""' + path + '""}')
    ZENML
  ]
}
",data,28,28.0,513352b0abbf05afdd65e6b869f5619acd1d8c66,14973148b3d05063d333fd9d9ad523be79198c71,https://github.com/zenml-io/mlstacks/blob/513352b0abbf05afdd65e6b869f5619acd1d8c66/src/mlstacks/terraform/k3d-modular/k3d.tf#L28,https://github.com/zenml-io/mlstacks/blob/14973148b3d05063d333fd9d9ad523be79198c71/src/mlstacks/terraform/k3d-modular/k3d.tf#L28,2023-08-23 11:34:20+02:00,2023-12-22 09:15:10+01:00,2,0,1,1,0,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,168,modules/workergroup/locals.tf,modules/workerpools/locals.tf,1,todo,# TODO Deprecate,label_prefix     = var.label_prefix # TODO Deprecate,"locals {
  # Stable availability domain selection
  ads        = data.oci_identity_availability_domains.ad_list.availability_domains
  ad_numbers = local.ads != null ? sort(keys(local.ad_number_to_name)) : []
  ad_number_to_name = local.ads != null ? {
    for ad in local.ads : parseint(substr(ad.name, -1, -1), 10) => ad.name
  } : { -1 : """" } # Fallback handles failure when unavailable but not required
  first_ad_name = local.ad_number_to_name[1]

  k8s_version_length = length(var.kubernetes_version)
  k8s_version_only   = substr(var.kubernetes_version, 1, local.k8s_version_length)

  kubeconfig          = try(yamldecode(lookup(data.oci_containerengine_cluster_kube_config.kube_config, ""content"", """")), { ""error"" : ""yamldecode"" })
  kubeconfig_clusters = try(lookup(local.kubeconfig, ""clusters"", []), [])
  kubeconfig_ca_cert  = try(lookup(lookup(local.kubeconfig_clusters[0], ""cluster"", {}), ""certificate-authority-data"", """"), """")
  cluster_ca_cert     = length(var.cluster_ca_cert) > 0 ? var.cluster_ca_cert : local.kubeconfig_ca_cert

  # Instance tags - variables + constant
  defined_tags  = coalesce(var.defined_tags, {})
  freeform_tags = merge(coalesce(var.freeform_tags, {}), { ""role"" = ""worker"" })

  # OKE managed node pool images
  node_pool_images = try(data.oci_containerengine_node_pool_option.np_options.sources, [{
    source_type = ""IMAGE""
  }])

  # Parse platform/operating system information from node pool image names
  parsed_images = {
    for k, v in local.node_pool_images : v.image_id => merge(
      try(element(regexall(""OKE-(?P<k8s_version>[0-9\\.]+)-(?P<build>[0-9]+)"", v.source_name), 0), { k8s_version = ""none"" }),
      {
        arch        = length(regexall(""aarch64"", v.source_name)) > 0 ? ""aarch64"" : ""x86_64""
        image_type  = length(regexall(""OKE"", v.source_name)) > 0 ? ""oke"" : ""platform""
        is_gpu      = length(regexall(""GPU"", v.source_name)) > 0 ? true : false
        os          = trimspace(replace(element(regexall(""^[a-zA-Z-]+"", v.source_name), 0), ""-"", "" ""))
        os_version  = element(regexall(""[0-9\\.]+"", v.source_name), 0)
        source_name = v.source_name
    })
  }

  image_ids = {
    x86_64   = [for k, v in local.parsed_images : k if v.arch == ""x86_64""]
    aarch64  = [for k, v in local.parsed_images : k if v.arch == ""aarch64""]
    oke      = [for k, v in local.parsed_images : k if v.image_type == ""oke"" && v.k8s_version == local.k8s_version_only]
    platform = [for k, v in local.parsed_images : k if v.image_type == ""platform""]
    gpu      = [for k, v in local.parsed_images : k if v.is_gpu]
    nongpu   = [for k, v in local.parsed_images : k if !v.is_gpu]
  }

  worker_groups_default = {
    mode             = var.mode
    size             = var.size
    shape            = var.shape
    image_id         = var.image_id
    image_type       = var.image_type
    os               = var.os
    os_version       = var.os_version
    boot_volume_size = var.boot_volume_size
    memory           = var.memory
    ocpus            = var.ocpus
    compartment_id   = local.worker_compartment_id
    subnet_id        = var.subnet_id
    pod_subnet_id    = var.pod_subnet_id
    pod_nsgs         = var.pod_nsg_ids
    worker_nsgs      = var.worker_nsg_ids
    assign_public_ip = var.assign_public_ip
    label_prefix     = var.label_prefix # TODO Deprecate
    node_labels      = {}
  }

  # Filter worker_groups map variable for enabled entries
  worker_groups_enabled = {
    for k, v in var.worker_groups : k => merge(local.worker_groups_default, v) if lookup(v, ""enabled"", var.enabled)
  }

  worker_compartments = distinct(compact([for k, v in local.worker_groups_enabled : lookup(v, ""compartment_id"", """")]))

  # Number of nodes expected from enabled worker groups
  expected_node_count = length(local.worker_groups_enabled) == 0 ? 0 : sum([for k, v in local.worker_groups_enabled : lookup(v, ""size"", 0)])

  # Filter worker_groups map variable for entries with image_id defined, returning a distinct list
  enabled_worker_group_image_ids = distinct([
    for v in local.worker_groups_enabled : v.image_id if contains(keys(v), ""image_id"")
  ])

  # Intermediate worker image result from data source
  enabled_worker_group_images = data.oci_core_image.worker_images

  # Filter enabled worker_group map entries for node pools
  enabled_node_pools = {
    for k, v in local.worker_groups_enabled : k => v if lookup(v, ""mode"", """") == ""node-pool""
  }

  # Filter enabled worker_group map entries for instance pools
  enabled_instance_configs = {
    for k, v in local.worker_groups_enabled : k => v
    if contains([""cluster-network"", ""instance-pool""], lookup(v, ""mode"", """"))
  }

  # Filter enabled worker_group map entries for instance pools
  enabled_instance_pools = {
    for k, v in local.worker_groups_enabled : k => v if lookup(v, ""mode"", """") == ""instance-pool""
  }

  # Filter enabled worker_group map entries for cluster networks
  enabled_cluster_networks = {
    for k, v in local.worker_groups_enabled : k => v if lookup(v, ""mode"", """") == ""cluster-network""
  }

  # Worker group OCI resources enriched with desired/custom parameters
  worker_node_pools       = { for k, v in oci_containerengine_node_pool.node_pools : k => merge(v, lookup(local.worker_groups_enabled, k, {})) }
  worker_instance_pools   = { for k, v in oci_core_instance_pool.instance_pools : k => merge(v, lookup(local.worker_groups_enabled, k, {})) }
  worker_cluster_networks = { for k, v in oci_core_cluster_network.cluster_networks : k => merge(v, lookup(local.worker_groups_enabled, k, {})) }

  # Intermediate reference to the enabled worker group NLBs to be reconciled
  enabled_worker_group_nlbs = [
    for k, v in local.worker_groups_enabled : {
      for lb_k, lb_v in(contains(keys(v), ""load_balancers"") ? v.load_balancers : {}) : lb_k => lb_v
    } if contains(keys(v), ""load_balancers"")
  ]

  # Sanitized worker_groups output; some conditionally-used defaults would be misleading
  worker_groups_enabled_out = {
    for k, v in local.worker_groups_enabled : k => { for a, b in v : a => b
      if a != ""enabled""                                                                # implied
      && !(a == ""node_labels"" && b == {})                                              # exclude empty
      && !(contains([""os"", ""os_version""], a) && v.image_type == ""custom"")              # unused defaults for custom
      && !(contains([""pod_nsgs"", ""pod_subnet_id""], a) && var.cni_type != ""npn"")        # unused defaults for NPN
      && !(contains([""ocpus"", ""memory""], a) && length(regexall(""Flex"", v.shape)) == 0) # unused defaults for non-Flex shapes
    }
  }

  # Group resource outputs
  worker_groups_active = merge(
    local.worker_cluster_networks,
    local.worker_instance_pools,
    local.worker_node_pools,
  )
}
",locals,"locals {
  # Stable availability domain selection
  ads        = data.oci_identity_availability_domains.ad_list.availability_domains
  ad_numbers = local.ads != null ? sort(keys(local.ad_number_to_name)) : []
  ad_number_to_name = local.ads != null ? {
    for ad in local.ads : parseint(substr(ad.name, -1, -1), 10) => ad.name
  } : { -1 : """" } # Fallback handles failure when unavailable but not required

  k8s_version_length = length(var.kubernetes_version)
  k8s_version_only   = substr(var.kubernetes_version, 1, local.k8s_version_length)

  kubeconfig          = try(yamldecode(lookup(data.oci_containerengine_cluster_kube_config.kube_config, ""content"", """")), { ""error"" : ""yamldecode"" })
  kubeconfig_clusters = try(lookup(local.kubeconfig, ""clusters"", []), [])
  kubeconfig_ca_cert  = try(lookup(lookup(local.kubeconfig_clusters[0], ""cluster"", {}), ""certificate-authority-data"", """"), """")
  cluster_ca_cert     = length(var.cluster_ca_cert) > 0 ? var.cluster_ca_cert : local.kubeconfig_ca_cert

  # Instance tags - variables + constant
  defined_tags  = coalesce(var.defined_tags, {})
  freeform_tags = merge(coalesce(var.freeform_tags, {}), { ""role"" = ""worker"" })

  # OKE managed node pool images
  node_pool_images = try(data.oci_containerengine_node_pool_option.np_options.sources, [])

  # Parse platform/operating system information from node pool image names
  parsed_images = {
    for k, v in local.node_pool_images : v.image_id => merge(
      try(element(regexall(""OKE-(?P<k8s_version>[0-9\\.]+)-(?P<build>[0-9]+)"", v.source_name), 0), { k8s_version = ""none"" }),
      {
        arch        = length(regexall(""aarch64"", v.source_name)) > 0 ? ""aarch64"" : ""x86_64""
        image_type  = length(regexall(""OKE"", v.source_name)) > 0 ? ""oke"" : ""platform""
        is_gpu      = length(regexall(""GPU"", v.source_name)) > 0 ? true : false
        os          = trimspace(replace(element(regexall(""^[a-zA-Z-]+"", v.source_name), 0), ""-"", "" ""))
        os_version  = element(regexall(""[0-9\\.]+"", v.source_name), 0)
        source_name = v.source_name
    })
  }

  image_ids = {
    x86_64   = [for k, v in local.parsed_images : k if v.arch == ""x86_64""]
    aarch64  = [for k, v in local.parsed_images : k if v.arch == ""aarch64""]
    oke      = [for k, v in local.parsed_images : k if v.image_type == ""oke"" && v.k8s_version == local.k8s_version_only]
    platform = [for k, v in local.parsed_images : k if v.image_type == ""platform""]
    gpu      = [for k, v in local.parsed_images : k if v.is_gpu]
    nongpu   = [for k, v in local.parsed_images : k if !v.is_gpu]
  }

  worker_pools_default = {
    mode              = var.mode
    size              = var.size
    shape             = var.shape
    image_id          = var.image_id
    image_type        = var.image_type
    os                = var.os
    os_version        = var.os_version
    boot_volume_size  = var.boot_volume_size
    memory            = var.memory
    ocpus             = var.ocpus
    compartment_id    = local.worker_compartment_id
    placement_ads     = local.ad_numbers
    block_volume_type = var.block_volume_type
    pv_encryption     = var.enable_pv_encryption_in_transit
    subnet_id         = var.subnet_id
    pod_subnet_id     = var.pod_subnet_id
    pod_nsgs          = var.pod_nsg_ids
    worker_nsgs       = var.worker_nsg_ids
    assign_public_ip  = var.assign_public_ip
    node_labels       = {}
  }

  # Filter worker_pools map variable for enabled entries
  worker_pools_enabled = { for x, y in { # Final dynamic configuration for pool requirements
    # Merge desired pool configuration onto defaults
    for k, v in var.worker_pools : k => merge(local.worker_pools_default, v) if lookup(v, ""enabled"", var.enabled)
    } : x => merge(y, {
      # Translate configured + available  AD numbers e.g. 2 into a tenancy/compartment-specific name
      availability_domains = compact([for ad_number in tolist(setintersection(y.placement_ads, local.ad_numbers)) :
        lookup(local.ad_number_to_name, ad_number, null)
      ])
      block_volume_type = y.mode == ""cluster-network"" ? ""iscsi"" : var.block_volume_type
      pv_encryption     = var.enable_pv_encryption_in_transit && y.block_volume_type == ""paravirtualized"" && y.mode != ""cluster-network""
      image_id = (y.image_type == ""custom"" ? y.image_id : element(tolist(setintersection([
        lookup(local.image_ids, y.image_type, null),
        length(regexall(""GPU"", y.shape)) > 0 ? local.image_ids.gpu : local.image_ids.nongpu,
        length(regexall(""A1"", y.shape)) > 0 ? local.image_ids.aarch64 : local.image_ids.x86_64,
        [for parsed_image_id, iv in local.parsed_images : parsed_image_id
          if length(regexall(iv.os, y.os)) > 0 && trimprefix(iv.os_version, y.os_version) != iv.os_version
        ],
      ]...)), 0))
    })
  }

  worker_compartments = distinct(compact([for k, v in local.worker_pools_enabled : lookup(v, ""compartment_id"", """")]))

  # Number of nodes expected from enabled worker pools
  expected_node_count = length(local.worker_pools_enabled) == 0 ? 0 : sum([for k, v in local.worker_pools_enabled : lookup(v, ""size"", 0)])

  # Filter enabled worker_pool map entries for node pools
  enabled_node_pools = {
    for k, v in local.worker_pools_enabled : k => v if lookup(v, ""mode"", """") == ""node-pool""
  }

  # Filter enabled worker_pool map entries for instance pools
  enabled_instance_configs = {
    for k, v in local.worker_pools_enabled : k => v
    if contains([""cluster-network"", ""instance-pool""], lookup(v, ""mode"", """"))
  }

  # Filter enabled worker_pool map entries for instance pools
  enabled_instance_pools = {
    for k, v in local.worker_pools_enabled : k => v if lookup(v, ""mode"", """") == ""instance-pool""
  }

  # Filter enabled worker_pool map entries for cluster networks
  enabled_cluster_networks = {
    for k, v in local.worker_pools_enabled : k => v if lookup(v, ""mode"", """") == ""cluster-network""
  }

  # Worker pool OCI resources enriched with desired/custom parameters
  worker_node_pools       = { for k, v in oci_containerengine_node_pool.workers : k => merge(v, lookup(local.worker_pools_enabled, k, {})) }
  worker_instance_pools   = { for k, v in oci_core_instance_pool.workers : k => merge(v, lookup(local.worker_pools_enabled, k, {})) }
  worker_cluster_networks = { for k, v in oci_core_cluster_network.workers : k => merge(v, lookup(local.worker_pools_enabled, k, {})) }

  # Intermediate reference to the enabled worker pool NLBs to be reconciled
  enabled_worker_pool_nlbs = [
    for k, v in local.worker_pools_enabled : {
      for lb_k, lb_v in(contains(keys(v), ""load_balancers"") ? v.load_balancers : {}) : lb_k => lb_v
    } if contains(keys(v), ""load_balancers"")
  ]

  # Sanitized worker_pools output; some conditionally-used defaults would be misleading
  worker_pools_enabled_out = {
    for k, v in local.worker_pools_enabled : k => { for a, b in v : a => b
      if a != ""enabled""                                                                # implied
      && !(a == ""node_labels"" && b == {})                                              # exclude empty
      && !(contains([""os"", ""os_version""], a) && v.image_type == ""custom"")              # unused defaults for custom
      && !(contains([""pod_nsgs"", ""pod_subnet_id""], a) && var.cni_type != ""npn"")        # unused defaults for NPN
      && !(contains([""ocpus"", ""memory""], a) && length(regexall(""Flex"", v.shape)) == 0) # unused defaults for non-Flex shapes
    }
  }

  # Group resource outputs
  worker_pools_active = merge(
    local.worker_cluster_networks,
    local.worker_instance_pools,
    local.worker_node_pools,
  )
}
",locals,70,,4d2b3f3d672a8f41655da3a7c58fded42c6858f3,897bae1fd6cdbd22478066e6f93643a8f7482757,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/4d2b3f3d672a8f41655da3a7c58fded42c6858f3/modules/workergroup/locals.tf#L70,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/897bae1fd6cdbd22478066e6f93643a8f7482757/modules/workerpools/locals.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,3,1,0,1,0,0,0,0,0,0
https://github.com/CDCgov/prime-simplereport,13,ops/prod/app_gateway_url_redirects.tf,ops/prod/app_gateway_url_redirects.tf,0,todo,# TODO = extant but not yet managed by Terraform,"# Production has three special App Gateways that perform global redirects instead of routing 
 # to apps: simplereport-gateway and simplereport-internal-gw 
 # 
 # simplereport-gateway:       redirects simplereport.cdc.gov -> www.simplereport.gov (TODO) 
 # simplereport-internal-gw:   redirects simplereport.org     -> www.simplereport.gov (TODO) 
 # simple-report-www-redirect: redirects simplereport.gov     -> www.simplereport.gov 
 # 
 # TODO = extant but not yet managed by Terraform ","locals {
  static_backend_pool          = ""${local.name}-${local.env}-fe-static""
  static_backend_http_setting  = ""${local.name}-${local.env}-fe-static-http""
  static_backend_https_setting = ""${local.name}-${local.env}-fe-static-https""
  http_listener                = ""${local.name}-http""
  https_listener               = ""${local.name}-https""
  frontend_config              = ""${local.name}-config""
}
",locals,"locals {
  static_backend_pool          = ""${local.name}-${local.env}-fe-static""
  static_backend_http_setting  = ""${local.name}-${local.env}-fe-static-http""
  static_backend_https_setting = ""${local.name}-${local.env}-fe-static-https""
  http_listener                = ""${local.name}-http""
  https_listener               = ""${local.name}-https""
  frontend_config              = ""${local.name}-config""
}
",locals,8,,40834da4606790f5a553f9e2e9bc300869aeacda,4f6c71db585cbb2bd83553aeb21c8eb21fe5773e,https://github.com/CDCgov/prime-simplereport/blob/40834da4606790f5a553f9e2e9bc300869aeacda/ops/prod/app_gateway_url_redirects.tf#L8,https://github.com/CDCgov/prime-simplereport/blob/4f6c71db585cbb2bd83553aeb21c8eb21fe5773e/ops/prod/app_gateway_url_redirects.tf,2021-10-15 23:06:17-04:00,2021-11-23 00:12:13-05:00,3,1,0,0,1,0,0,0,0,0
https://github.com/kubernetes/k8s.io,385,infra/aws/terraform/prow-build-cluster/variables.tf,infra/aws/terraform/prow-build-cluster/variables.tf,0,# todo,# TODO: remove once applied on prow-build-cluster,"# TODO: remove once applied on prow-build-cluster 
 # This variable is required in the installation process as we cannot 
 # assume a role that is yet to be created.","variable ""assume_role"" {
  type        = bool
  description = ""Assumes role to get access to EKS cluster after provisioning.""
  default     = true
}
",variable,the block associated got renamed or deleted,,29,,ea61b21c1c9188ea5df32e9b08d1a51fe706715a,d670c1931be3c8304aaf368d71c6b3a537de4aac,https://github.com/kubernetes/k8s.io/blob/ea61b21c1c9188ea5df32e9b08d1a51fe706715a/infra/aws/terraform/prow-build-cluster/variables.tf#L29,https://github.com/kubernetes/k8s.io/blob/d670c1931be3c8304aaf368d71c6b3a537de4aac/infra/aws/terraform/prow-build-cluster/variables.tf,2023-04-26 15:07:51+02:00,2023-04-28 10:05:00+02:00,3,1,0,1,0,1,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,250,modules/utilities/drain.tf,modules/utilities/drain.tf,0,todo,# TODO List nodes by label for draining pools,"""echo kubectl get nodes ..."",             # TODO List nodes by label for draining pools","resource ""null_resource"" ""drain_workers"" {
  triggers = {
    drain_count = jsonencode(keys(local.worker_pools_draining))
  }

  connection {
    bastion_host        = var.bastion_host
    bastion_user        = var.bastion_user
    bastion_private_key = var.ssh_private_key
    host                = var.operator_host
    user                = var.operator_user
    private_key         = var.ssh_private_key
    timeout             = ""40m""
    type                = ""ssh""
  }

  provisioner ""remote-exec"" {
    inline = [
      ""echo kubectl get nodes ..."",             # TODO List nodes by label for draining pools
      ""echo kubectl drain --ignore-daemonsets"", # TODO Drain nodes for draining pools
    ]
  }
}
",resource,"resource ""null_resource"" ""drain_workers"" {
  count = local.drain_enabled ? 1 : 0
  triggers = {
    drain_pools    = jsonencode(sort(local.drain_pools))
    drain_commands = jsonencode(local.drain_commands)
  }

  connection {
    bastion_host        = var.bastion_host
    bastion_user        = var.bastion_user
    bastion_private_key = var.ssh_private_key
    host                = var.operator_host
    user                = var.operator_user
    private_key         = var.ssh_private_key
    timeout             = ""40m""
    type                = ""ssh""
  }

  provisioner ""remote-exec"" {
    inline = local.drain_commands
  }
}
",resource,26,,79845fb791998bdde1b58fa656b6c381f7d26510,5c4fc186cf8eeac1c0af855954593058edae675b,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/79845fb791998bdde1b58fa656b6c381f7d26510/modules/utilities/drain.tf#L26,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/5c4fc186cf8eeac1c0af855954593058edae675b/modules/utilities/drain.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,4,1,0,0,0,0,0,1,0,0
https://github.com/CDCgov/prime-simplereport,38,ops/services/postgres_db/main.tf,ops/services/postgres_db/main.tf,0,# todo,### TODO: delete the old configuration above once all environments have been cut,"### New Postgres Flexible Server configuration 
 ### TODO: delete the old configuration above once all environments have been cut 
 ### over to the new DBs so Terraform can clean up the old infrastructure. ","resource ""azurerm_postgresql_flexible_server"" ""db"" {
  name                = ""simple-report-${var.env}-flexible-db""
  location            = var.rg_location
  resource_group_name = var.rg_name
  sku_name            = var.env == ""prod"" ? ""MO_Standard_E8ds_v4"" : ""MO_Standard_E4ds_v4""
  version             = ""13""
  delegated_subnet_id = var.subnet_id
  private_dns_zone_id = var.dns_zone_id

  // TODO: replace with commented-out line below when removing old DB config
  administrator_login = ""simple_report""
  //administrator_login    = var.administrator_login
  administrator_password = data.azurerm_key_vault_secret.db_password.value

  storage_mb                   = 524288 // 512 GB
  backup_retention_days        = 7
  geo_redundant_backup_enabled = false

  tags = var.tags

  // Time is Eastern
  maintenance_window {
    day_of_week  = 0
    start_hour   = 0
    start_minute = 0
  }

  # See note at https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/postgresql_flexible_server#high_availability
  lifecycle {
    ignore_changes = [
      zone,
      high_availability.0.standby_availability_zone
    ]
  }
}
",resource,"resource ""azurerm_postgresql_flexible_server"" ""db"" {
  name                = ""simple-report-${var.env}-flexible-db""
  location            = var.rg_location
  resource_group_name = var.rg_name
  sku_name            = var.env == ""prod"" ? ""MO_Standard_E8ds_v4"" : ""MO_Standard_E4ds_v4""
  version             = ""13""
  delegated_subnet_id = var.subnet_id


  administrator_login    = var.administrator_login
  administrator_password = data.azurerm_key_vault_secret.db_password.value

  storage_mb                   = 524288 // 512 GB
  backup_retention_days        = 7
  geo_redundant_backup_enabled = false

  tags = var.tags

  // Time is Eastern
  maintenance_window {
    day_of_week  = 0
    start_hour   = 0
    start_minute = 0
  }

  # See note at https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/postgresql_flexible_server#high_availability
  lifecycle {
    ignore_changes = [
      zone,
      high_availability.0.standby_availability_zone
    ]
  }
}
",resource,65,,3e4185fa549937cff9213c325bc0fee780e41eb0,fb9a18eb71f31044ca7a58958a25dea489263ca7,https://github.com/CDCgov/prime-simplereport/blob/3e4185fa549937cff9213c325bc0fee780e41eb0/ops/services/postgres_db/main.tf#L65,https://github.com/CDCgov/prime-simplereport/blob/fb9a18eb71f31044ca7a58958a25dea489263ca7/ops/services/postgres_db/main.tf,2022-02-16 12:59:39-05:00,2022-04-28 23:27:57-04:00,2,1,1,1,0,0,0,1,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,164,modules/workergroup/instanceconfig.tf,modules/workers/instanceconfig.tf,1,todo,# TODO Instance Configuration replacement without delete when supported:,"# TODO Instance Configuration replacement without delete when supported: 
 # https://github.com/hashicorp/terraform/issues/15485","resource ""oci_core_instance_configuration"" ""instance_configuration"" {
  # Create an OCI Instance Configuration resource for each enabled entry of the worker_groups map with a mode that uses one.
  for_each       = local.enabled_instance_configs
  compartment_id = each.value.compartment_id
  display_name   = ""${each.value.label_prefix}-${each.key}""

  instance_details {
    instance_type = ""compute""

    launch_details {
      # Define each configured availability domain for placement, with bounds on # available
      # Configured AD numbers e.g. [1,2,3] are converted into tenancy/compartment-specific names
      availability_domain = lookup(local.ad_number_to_name, (
        contains(keys(each.value), ""placement_ads"")
        ? element(tolist(setintersection(each.value.placement_ads, local.ad_numbers)), 1)
        : element(local.ad_numbers, 1)
      ), """")
      compartment_id = each.value.compartment_id
      defined_tags = merge(
        local.defined_tags,
        lookup(each.value, ""defined_tags"", {}),
      )
      freeform_tags = merge(local.freeform_tags, contains(keys(each.value), ""freeform_tags"") ? each.value.freeform_tags : { worker_group = each.key })

      instance_options {
        are_legacy_imds_endpoints_disabled = false
      }

      create_vnic_details {
        assign_private_dns_record = var.assign_dns
        assign_public_ip          = each.value.assign_public_ip
        nsg_ids                   = each.value.worker_nsgs
        subnet_id                 = each.value.subnet_id
      }

      metadata = {
        apiserver_host           = var.apiserver_private_host
        cluster_ca_cert          = local.cluster_ca_cert
        kubedns_svc_ip           = var.cluster_dns
        oke-k8version            = var.kubernetes_version
        oke-kubeproxy-proxy-mode = var.kubeproxy_mode
        oke-tenancy-id           = local.tenancy_id
        oke-initial-node-labels = join("","", [
          for k, v in merge(var.node_labels, each.value.node_labels) : join(""="", [k, v])
        ])
        ssh_authorized_keys = local.ssh_public_key
        user_data           = data.cloudinit_config.worker_per_boot.rendered
      }

      shape = each.value.shape

      dynamic ""shape_config"" {
        for_each = length(regexall(""Flex"", each.value.shape)) > 0 ? [1] : []
        content {
          ocpus = each.value.ocpus
          memory_in_gbs = ( # If > 64GB memory/core, correct input to exactly 64GB memory/core
            (each.value.memory / each.value.ocpus) > 64 ? each.value.ocpus * 64 : each.value.memory
          )
        }
      }

      source_details {
        boot_volume_size_in_gbs = each.value.boot_volume_size
        image_id                = each.value.image_id
        source_type             = ""image""
      }

      is_pv_encryption_in_transit_enabled = var.enable_pv_encryption_in_transit
    }

    block_volumes {
      attach_details {
        type                                = var.block_volume_type
        is_pv_encryption_in_transit_enabled = var.block_volume_type == ""paravirtualized"" && var.enable_pv_encryption_in_transit
      }

      create_details {
        display_name   = ""${each.value.label_prefix}-${each.key}""
        kms_key_id     = var.volume_kms_key_id
        compartment_id = each.value.compartment_id
      }
    }
  }

  lifecycle {

    # TODO Instance Configuration replacement without delete when supported:
    # https://github.com/hashicorp/terraform/issues/15485
    create_before_destroy = true
    ignore_changes = [
      defined_tags, freeform_tags, display_name,
      instance_details[0].launch_details[0].metadata,
      instance_details[0].launch_details[0].defined_tags,
      instance_details[0].launch_details[0].freeform_tags,
    ]
  }
}",resource,"resource ""oci_core_instance_configuration"" ""workers"" {
  # Create an OCI Instance Configuration resource for each enabled entry of the worker_pools map with a mode that uses one.
  for_each       = local.enabled_instance_configs
  compartment_id = each.value.compartment_id
  display_name   = each.key
  defined_tags   = each.value.defined_tags
  freeform_tags  = each.value.freeform_tags

  instance_details {
    instance_type = ""compute""

    launch_details {
      agent_config {
        are_all_plugins_disabled = each.value.agent_config.are_all_plugins_disabled
        is_management_disabled   = each.value.agent_config.is_management_disabled
        is_monitoring_disabled   = each.value.agent_config.is_monitoring_disabled
        dynamic ""plugins_config"" {
          for_each = each.value.agent_config.plugins_config
          content {
            name          = plugins_config.key
            desired_state = plugins_config.value
          }
        }
      }

      availability_domain = element(each.value.availability_domains, 1)

      # First value specified on pool, or null to select automatically
      fault_domain = try(each.value.placement_fds[0], null)

      compartment_id          = each.value.compartment_id
      defined_tags            = each.value.defined_tags
      freeform_tags           = each.value.freeform_tags
      extended_metadata       = each.value.extended_metadata
      capacity_reservation_id = each.value.capacity_reservation_id

      instance_options {
        are_legacy_imds_endpoints_disabled = false
      }

      create_vnic_details {
        assign_private_dns_record = var.assign_dns
        assign_public_ip          = each.value.assign_public_ip
        nsg_ids                   = each.value.nsg_ids
        subnet_id                 = each.value.subnet_id
        defined_tags              = each.value.defined_tags
        freeform_tags             = each.value.freeform_tags
      }

      metadata = merge(
        {
          apiserver_host           = var.apiserver_private_host
          cluster_ca_cert          = var.cluster_ca_cert
          oke-k8version            = var.kubernetes_version
          oke-kubeproxy-proxy-mode = var.kubeproxy_mode
          oke-tenancy-id           = var.tenancy_id
          oke-initial-node-labels  = join("","", [for k, v in each.value.node_labels : format(""%v=%v"", k, v)])
          secondary_vnics          = jsonencode(lookup(each.value, ""secondary_vnics"", {}))
          ssh_authorized_keys      = var.ssh_public_key
          user_data                = lookup(lookup(data.cloudinit_config.workers, each.key, {}), ""rendered"", """")
        },

        # Only provide cluster DNS service address if set explicitly; determined automatically in practice.
        coalesce(var.cluster_dns, ""none"") == ""none"" ? {} : { kubedns_svc_ip = var.cluster_dns },

        # Extra user-defined fields merged last
        var.node_metadata,                       # global
        lookup(each.value, ""node_metadata"", {}), # pool-specific
      )

      shape = each.value.shape

      dynamic ""shape_config"" {
        for_each = length(regexall(""Flex"", each.value.shape)) > 0 ? [1] : []
        content {
          ocpus = each.value.ocpus
          memory_in_gbs = ( # If > 64GB memory/core, correct input to exactly 64GB memory/core
            (each.value.memory / each.value.ocpus) > 64 ? each.value.ocpus * 64 : each.value.memory
          )
        }
      }

      dynamic ""platform_config"" {
        for_each = each.value.platform_config != null ? [1] : []
        content {
          type = lookup(
            # Attempt lookup against data source for the associated 'type' of configured worker shape
            lookup(local.platform_config_by_shape, each.value.shape, {}), ""type"",
            # Fall back to 'type' on pool with custom platform_config, or INTEL_VM default
            lookup(each.value.platform_config, ""type"", ""INTEL_VM"")
          )
          # Remaining parameters as configured, validated by instance/instance config resource
          are_virtual_instructions_enabled               = lookup(each.value.platform_config, ""are_virtual_instructions_enabled"", null)
          is_access_control_service_enabled              = lookup(each.value.platform_config, ""is_access_control_service_enabled"", null)
          is_input_output_memory_management_unit_enabled = lookup(each.value.platform_config, ""is_input_output_memory_management_unit_enabled"", null)
          is_measured_boot_enabled                       = lookup(each.value.platform_config, ""is_measured_boot_enabled"", null)
          is_memory_encryption_enabled                   = lookup(each.value.platform_config, ""is_memory_encryption_enabled"", null)
          is_secure_boot_enabled                         = lookup(each.value.platform_config, ""is_secure_boot_enabled"", null)
          is_symmetric_multi_threading_enabled           = lookup(each.value.platform_config, ""is_symmetric_multi_threading_enabled"", null)
          is_trusted_platform_module_enabled             = lookup(each.value.platform_config, ""is_trusted_platform_module_enabled"", null)
          numa_nodes_per_socket                          = lookup(each.value.platform_config, ""numa_nodes_per_socket"", null)
          percentage_of_cores_enabled                    = lookup(each.value.platform_config, ""percentage_of_cores_enabled"", null)
        }
      }

      source_details {
        boot_volume_size_in_gbs = each.value.boot_volume_size
        boot_volume_vpus_per_gb = each.value.boot_volume_vpus_per_gb
        image_id                = each.value.image_id
        source_type             = ""image""
      }

      is_pv_encryption_in_transit_enabled = each.value.pv_transit_encryption
    }

    block_volumes {
      attach_details {
        type                                = each.value.block_volume_type
        is_pv_encryption_in_transit_enabled = each.value.pv_transit_encryption
      }

      create_details {
        // Limit to first candidate placement AD for cluster-network; undefined for all otherwise
        availability_domain = each.value.mode == ""cluster-network"" ? element(each.value.availability_domains, 1) : null
        compartment_id      = each.value.compartment_id
        display_name        = each.key
        kms_key_id          = each.value.volume_kms_key_id
      }
    }

    dynamic ""secondary_vnics"" {
      for_each = lookup(each.value, ""secondary_vnics"", {})
      iterator = vnic

      content {
        display_name = vnic.key
        nic_index    = lookup(vnic.value, ""nic_index"", null)

        create_vnic_details {
          assign_private_dns_record = lookup(vnic.value, ""assign_private_dns_record"", null)
          assign_public_ip          = lookup(vnic.value, ""assign_public_ip"", null)
          display_name              = vnic.key
          defined_tags              = lookup(vnic.value, ""defined_tags"", null)
          freeform_tags             = lookup(vnic.value, ""freeform_tags"", null)
          hostname_label            = lookup(vnic.value, ""hostname_label"", null)
          nsg_ids                   = lookup(vnic.value, ""nsg_ids"", null)
          private_ip                = lookup(vnic.value, ""private_ip"", null)
          skip_source_dest_check    = lookup(vnic.value, ""skip_source_dest_check"", null)
          subnet_id                 = lookup(vnic.value, ""subnet_id"", each.value.subnet_id)
        }
      }
    }
  }

  lifecycle {
    # TODO Instance Configuration replacement without delete when supported:
    # https://github.com/hashicorp/terraform/issues/15485
    create_before_destroy = true
    ignore_changes = [
      defined_tags, freeform_tags, display_name,
      instance_details[0].launch_details[0].metadata,
      instance_details[0].launch_details[0].defined_tags,
      instance_details[0].launch_details[0].freeform_tags,
      instance_details[0].launch_details[0].create_vnic_details[0].defined_tags,
      instance_details[0].launch_details[0].create_vnic_details[0].freeform_tags,
      instance_details[0].secondary_vnics,
    ]
  }
}
",resource,90,159.0,4d2b3f3d672a8f41655da3a7c58fded42c6858f3,a1fdfcb7de5e777f0191a43940ef276175a20ba9,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/4d2b3f3d672a8f41655da3a7c58fded42c6858f3/modules/workergroup/instanceconfig.tf#L90,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/a1fdfcb7de5e777f0191a43940ef276175a20ba9/modules/workers/instanceconfig.tf#L159,2023-10-25 16:40:02+11:00,2024-02-12 09:53:40+11:00,22,0,1,0,1,0,0,0,0,0
https://github.com/kubernetes/k8s.io,34,infra/gcp/clusters/modules/gke-cluster/main.tf,infra/gcp/terraform/modules/gke-cluster/main.tf,1,workaround,// Uses a workaround from https://github.com/hashicorp/terraform/issues/22544#issuecomment-582974372,"// Create GKE cluster, but with no node pools. Node pools are provisioned via another module. 
 // 
 // Uses a workaround from https://github.com/hashicorp/terraform/issues/22544#issuecomment-582974372 
 // to set lifecycle.prevent_destroy to false if is_prod_cluster 
 // 
 // IMPORTANT: The prod_ and test_ forms of this resource MUST be kept in sync. 
 //            Any changes in one MUST be reflected in the other.","resource ""google_container_cluster"" ""prod_cluster"" {
  count     = var.is_prod_cluster == ""true"" ? 1 : 0
  
  name     = var.cluster_name
  location = var.cluster_location

  provider = google-beta
  project  = var.project_name

  // NOTE: unique to prod_cluster
  // GKE clusters are critical objects and should not be destroyed
  lifecycle {
    prevent_destroy = true
  }

  // Network config
  network = ""default""

  // Start with a single node, because we're going to delete the default pool
  initial_node_count = 1

  // Removes the default node pool, so we can custom create them as separate
  // objects
  remove_default_node_pool = true

  // Disable local and certificate auth
  master_auth {
    username = """"
    password = """"

    client_certificate_config {
      issue_client_certificate = false
    }
  }

  // Enable google-groups for RBAC
  authenticator_groups_config {
    security_group = ""gke-security-groups@kubernetes.io""
  }

  // Enable workload identity for GCP IAM
  workload_identity_config {
    identity_namespace = ""${var.project_name}.svc.id.goog""
  }

  // Enable Stackdriver Kubernetes Monitoring
  logging_service    = ""logging.googleapis.com/kubernetes""
  monitoring_service = ""monitoring.googleapis.com/kubernetes""

  // Set maintenance time
  maintenance_policy {
    daily_maintenance_window {
      start_time = ""11:00"" // (in UTC), 03:00 PST
    }
  }

  // Restrict master to Google IP space; use Cloud Shell to access
  master_authorized_networks_config {
  }

  // Enable GKE Usage Metering
  resource_usage_export_config {
    enable_network_egress_metering = true
    bigquery_destination {
      dataset_id = google_bigquery_dataset.prod_usage_metering[0].dataset_id
    }
  }

  // Enable GKE Network Policy
  network_policy {
    enabled  = true
    provider = ""CALICO""
  }

  // Configure cluster addons
  addons_config {
    horizontal_pod_autoscaling {
      disabled = false
    }
    http_load_balancing {
      disabled = false
    }
    network_policy_config {
      disabled = false
    }
  }

  // Enable PodSecurityPolicy enforcement
  pod_security_policy_config {
    enabled = false // TODO: we should turn this on
  }

  // Enable VPA
  vertical_pod_autoscaling {
    enabled = true
  }
}
",resource,"resource ""google_container_cluster"" ""prod_cluster"" {
  count = var.is_prod_cluster == ""true"" ? 1 : 0

  name     = var.cluster_name
  location = var.cluster_location

  provider = google-beta
  project  = var.project_name

  // NOTE: unique to prod_cluster
  // GKE clusters are critical objects and should not be destroyed
  lifecycle {
    prevent_destroy = true
  }

  // Network config
  network = ""default""

  // Start with a single node, because we're going to delete the default pool
  initial_node_count = 1

  // Removes the default node pool, so we can custom create them as separate
  // objects
  remove_default_node_pool = true

  // Enable google-groups for RBAC
  authenticator_groups_config {
    security_group = ""gke-security-groups@kubernetes.io""
  }

  // Enable workload identity for GCP IAM
  workload_identity_config {
    workload_pool = ""${var.project_name}.svc.id.goog""
  }

  // Enable Stackdriver Kubernetes Monitoring
  logging_service    = ""logging.googleapis.com/kubernetes""
  monitoring_service = ""monitoring.googleapis.com/kubernetes""

  // Set maintenance time
  maintenance_policy {
    daily_maintenance_window {
      start_time = ""11:00"" // (in UTC), 03:00 PST
    }
  }

  // Restrict master to Google IP space; use Cloud Shell to access
  dynamic ""master_authorized_networks_config"" {
    for_each = var.cloud_shell_access ? [1] : []
    content {
    }
  }

  // Enable GKE Usage Metering
  resource_usage_export_config {
    enable_network_egress_metering = true
    bigquery_destination {
      dataset_id = google_bigquery_dataset.prod_usage_metering[0].dataset_id
    }
  }

  network_policy {
    enabled  = false
  }

  // Configure cluster addons
  addons_config {
    gce_persistent_disk_csi_driver_config {
      enabled = true
    }
    horizontal_pod_autoscaling {
      disabled = false
    }
    http_load_balancing {
      disabled = false
    }
    network_policy_config {
      disabled = false
    }
    dns_cache_config {
      enabled = var.dns_cache_enabled
    }
  }

  // Enable Shielded Nodes feature
  enable_shielded_nodes = var.enable_shielded_nodes

  release_channel {
    channel = var.release_channel
  }

  // Enable VPA
  vertical_pod_autoscaling {
    enabled = true
  }
}
",resource,90,90.0,0d83cf8820bd2d550c7032d8557aacb836bca743,1d001d33e9635f72f381199a008cbee3ec89cf21,https://github.com/kubernetes/k8s.io/blob/0d83cf8820bd2d550c7032d8557aacb836bca743/infra/gcp/clusters/modules/gke-cluster/main.tf#L90,https://github.com/kubernetes/k8s.io/blob/1d001d33e9635f72f381199a008cbee3ec89cf21/infra/gcp/terraform/modules/gke-cluster/main.tf#L90,2020-05-15 18:34:32-07:00,2024-03-07 11:20:21+00:00,10,0,1,1,1,0,0,1,0,0
https://github.com/ministryofjustice/cloud-platform-infrastructure,221,terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf,terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf,0,fix,# Pinned the version until this fix get merged https://github.com/terraform-aws-modules/terraform-aws-eks/pull/1447,"# Issue in v17.1.0, where each plan will have a change for the templates, this cause our divergence pipeline fail"" 
 # Pinned the version until this fix get merged https://github.com/terraform-aws-modules/terraform-aws-eks/pull/1447","locals {
  node_groups_count = {
    live    = ""54""
    manager = ""4""
    default = ""3""
  }

  node_size = {
    live    = [""r5.xlarge"", ""r4.xlarge""]
    manager = [""m5.xlarge"", ""m4.xlarge""]
    default = [""m5.large"", ""m4.large""]
  }

  monitoring_node_size = {
    live    = [""r5.2xlarge"", ""r4.2xlarge""]
    manager = [""t3.medium"", ""t2.medium""]
    default = [""t3.medium"", ""t2.medium""]
  }

  default_ng = {
    desired_capacity = lookup(local.node_groups_count, terraform.workspace, local.node_groups_count[""default""])
    max_capacity     = 60
    min_capacity     = 1
    subnets          = data.aws_subnet_ids.private.ids

    create_launch_template = true
    pre_userdata = templatefile(""${path.module}/templates/user-data.tpl"", {
      dockerhub_credentials = base64encode(""${var.dockerhub_user}:${var.dockerhub_token}"")
    })

    # Issue in v17.1.0, where each plan will have a change for the templates, this cause our divergence pipeline fail""
    # Pinned the version until this fix get merged https://github.com/terraform-aws-modules/terraform-aws-eks/pull/1447
    launch_template_version = ""1""

    instance_types = lookup(local.node_size, terraform.workspace, local.node_size[""default""])
    k8s_labels = {
      Terraform = ""true""
      Cluster   = terraform.workspace
      Domain    = local.fqdn
    }
    additional_tags = {
      default_ng    = ""true""
      application   = ""moj-cloud-platform""
      business-unit = ""platforms""
    }
  }

  monitoring_ng = {
    desired_capacity = 2
    max_capacity     = 3
    min_capacity     = 1
    subnets          = [sort(data.aws_subnet_ids.private.ids)[2]]

    create_launch_template = true
    pre_userdata = templatefile(""${path.module}/templates/user-data.tpl"", {
      dockerhub_credentials = base64encode(""${var.dockerhub_user}:${var.dockerhub_token}"")
    })

    # Issue in v17.1.0, where each plan will have a change for the templates, this cause our divergence pipeline fail""
    # Pinned the version until this fix get merged https://github.com/terraform-aws-modules/terraform-aws-eks/pull/1447
    launch_template_version = ""1""

    instance_types = lookup(local.monitoring_node_size, terraform.workspace, local.monitoring_node_size[""default""])
    k8s_labels = {
      Terraform                                     = ""true""
      ""cloud-platform.justice.gov.uk/monitoring-ng"" = ""true""
      Cluster                                       = terraform.workspace
      Domain                                        = local.fqdn
    }
    additional_tags = {
      monitoring_ng = ""true""
      application   = ""moj-cloud-platform""
      business-unit = ""platforms""
    }
    taints = [
      {
        key    = ""monitoring-node""
        value  = true
        effect = ""NO_SCHEDULE""
      }
    ]
  }

}
",locals,"locals {
  # desired_capcity change is a manual step after initial cluster creation (when no cluster-autoscaler)
  # https://github.com/terraform-aws-modules/terraform-aws-eks/issues/835
  node_groups_count = {
    live    = ""54""
    manager = ""4""
    default = ""3""
  }

  node_size = {
    live    = [""r5.xlarge"", ""r4.xlarge""]
    manager = [""m5.xlarge"", ""m4.xlarge""]
    default = [""m5.large"", ""m4.large""]
  }

  monitoring_node_size = {
    live    = [""r5.2xlarge"", ""r4.2xlarge""]
    manager = [""t3.medium"", ""t2.medium""]
    default = [""t3.medium"", ""t2.medium""]
  }

  default_ng = {
    desired_capacity = lookup(local.node_groups_count, terraform.workspace, local.node_groups_count[""default""])
    max_capacity     = 60
    min_capacity     = 1
    subnets          = data.aws_subnet_ids.private.ids

    create_launch_template = true
    pre_userdata = templatefile(""${path.module}/templates/user-data.tpl"", {
      dockerhub_credentials = base64encode(""${var.dockerhub_user}:${var.dockerhub_token}"")
    })

    instance_types = lookup(local.node_size, terraform.workspace, local.node_size[""default""])
    k8s_labels = {
      Terraform = ""true""
      Cluster   = terraform.workspace
      Domain    = local.fqdn
    }
    additional_tags = {
      default_ng    = ""true""
      application   = ""moj-cloud-platform""
      business-unit = ""platforms""
    }
  }

  monitoring_ng = {
    desired_capacity = 2
    max_capacity     = 3
    min_capacity     = 1
    subnets          = data.aws_subnet_ids.private_zone_2b.ids

    create_launch_template = true
    pre_userdata = templatefile(""${path.module}/templates/user-data.tpl"", {
      dockerhub_credentials = base64encode(""${var.dockerhub_user}:${var.dockerhub_token}"")
    })

    instance_types = lookup(local.monitoring_node_size, terraform.workspace, local.monitoring_node_size[""default""])
    k8s_labels = {
      Terraform                                     = ""true""
      ""cloud-platform.justice.gov.uk/monitoring-ng"" = ""true""
      Cluster                                       = terraform.workspace
      Domain                                        = local.fqdn
    }
    additional_tags = {
      monitoring_ng = ""true""
      application   = ""moj-cloud-platform""
      business-unit = ""platforms""
    }
    taints = [
      {
        key    = ""monitoring-node""
        value  = true
        effect = ""NO_SCHEDULE""
      }
    ]
  }

}
",locals,79,,811baa9677ab082b6f6b3f88277bdb3e259ddf0f,708bae2fed32eb1c2d97e6929605a5f021f572a2,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/811baa9677ab082b6f6b3f88277bdb3e259ddf0f/terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf#L79,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/708bae2fed32eb1c2d97e6929605a5f021f572a2/terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf,2021-08-26 14:44:12+01:00,2021-11-08 20:11:02+00:00,9,1,0,1,1,0,0,0,0,0
https://github.com/CDCgov/prime-simplereport,35,ops/services/postgres_db/_output.tf,ops/services/postgres_db/_output.tf,0,todo,// TODO - change these to azurerm_postgresql_flexible_server when removing the old DB config,// TODO - change these to azurerm_postgresql_flexible_server when removing the old DB config,"output ""server_name"" {
  value = azurerm_postgresql_server.db.name
}
",output,"output ""server_name"" {
  value = azurerm_postgresql_flexible_server.db.name
}
",output,1,,3e4185fa549937cff9213c325bc0fee780e41eb0,fb9a18eb71f31044ca7a58958a25dea489263ca7,https://github.com/CDCgov/prime-simplereport/blob/3e4185fa549937cff9213c325bc0fee780e41eb0/ops/services/postgres_db/_output.tf#L1,https://github.com/CDCgov/prime-simplereport/blob/fb9a18eb71f31044ca7a58958a25dea489263ca7/ops/services/postgres_db/_output.tf,2022-02-16 12:59:39-05:00,2022-04-28 23:27:57-04:00,2,1,1,1,0,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,108,modules/extensions/iam.tf,modules/extensions/iam.tf,0,todo,# TODO Move to Operator module,# TODO Move to Operator module,"resource ""oci_identity_policy"" ""operator_use_dynamic_group_policy"" {
  provider       = oci.home
  compartment_id = random_id.dynamic_group_suffix.keepers.tenancy_id
  description    = ""policy to allow operator host to manage dynamic group""
  name           = join(""-"", compact([
    random_id.dynamic_group_suffix.keepers.label_prefix,
    ""operator-instance-principal-dynamic-group"",
    random_id.dynamic_group_suffix.hex
  ]))
  statements     = [""Allow dynamic-group ${var.operator_dynamic_group} to use dynamic-groups in tenancy""]
  count          = (local.create_operator_dynamic_group_policy == true) ? 1 : 0
}
",resource,,,30,0.0,269d3fdd896309157e667558bb885f0c54a3e11d,6c867cd8e9cbf559742f56658989bcded0d1fd89,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/269d3fdd896309157e667558bb885f0c54a3e11d/modules/extensions/iam.tf#L30,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/modules/extensions/iam.tf#L0,2022-10-06 16:42:51+11:00,2023-10-25 16:40:02+11:00,3,2,0,1,0,1,0,0,0,0
https://github.com/compiler-explorer/infra,17,terraform/lc.tf,terraform/lc.tf,0,// todo,"// TODO: once this is proven right, add `name` to each of these","// TODO: once this is proven right, add `name` to each of these","resource ""aws_launch_configuration"" ""CompilerExplorer-beta-c5"" {
  image_id = ""${local.image_id}""
  instance_type = ""c5.large""
  iam_instance_profile = ""XaniaBlog""
  key_name = ""mattgodbolt""
  security_groups = [
    ""${aws_security_group.CompilerExplorer.id}""]
  associate_public_ip_address = true
  user_data = ""${local.beta_user_data}""
  enable_monitoring = false
  ebs_optimized = true
  spot_price = ""0.05""

  root_block_device {
    volume_type = ""gp2""
    volume_size = 10
    delete_on_termination = true
  }
}
",resource,"resource ""aws_launch_configuration"" ""CompilerExplorer-beta-c5"" {
  lifecycle {
    create_before_destroy = true
  }

  name = ""compiler-explorer-beta-c5""
  image_id = ""${local.image_id}""
  instance_type = ""c5.large""
  iam_instance_profile = ""XaniaBlog""
  key_name = ""mattgodbolt""
  security_groups = [
    ""${aws_security_group.CompilerExplorer.id}""
  ]
  associate_public_ip_address = true
  user_data = ""${local.beta_user_data}""
  enable_monitoring = false
  ebs_optimized = true
  spot_price = ""0.05""

  root_block_device {
    volume_type = ""gp2""
    volume_size = 10
    delete_on_termination = true
  }
}
",resource,6,,52b74b9730d4cb1537f8ff48e00f9cde7727eaf5,6f10aecb7bbf12c4f77a19f10c88e9f8770fb28c,https://github.com/compiler-explorer/infra/blob/52b74b9730d4cb1537f8ff48e00f9cde7727eaf5/terraform/lc.tf#L6,https://github.com/compiler-explorer/infra/blob/6f10aecb7bbf12c4f77a19f10c88e9f8770fb28c/terraform/lc.tf,2018-11-05 17:43:45-06:00,2018-11-05 18:27:54-06:00,2,1,1,1,0,0,0,0,0,0
https://github.com/uyuni-project/sumaform,208,modules/libvirt/base/main.tf,modules/libvirt/base/main.tf,0,hack,// HACK: work around https://github.com/hashicorp/terraform/issues/9549,// HACK: work around https://github.com/hashicorp/terraform/issues/9549,"output ""configuration"" {
  // HACK: work around https://github.com/hashicorp/terraform/issues/9549
  value = ""${
    map(
      ""opensuse421"", ""${libvirt_volume.opensuse421.id}"",
      ""sles11sp3"", ""${libvirt_volume.sles11sp3.id}"",
      ""sles11sp4"", ""${libvirt_volume.sles11sp4.id}"",
      ""sles12"", ""${libvirt_volume.sles12.id}"",
      ""sles12sp1"", ""${libvirt_volume.sles12sp1.id}"",
      ""network_name"", ""${var.network_name}"",

      ""cc_username"", ""${var.cc_username}"",
      ""cc_password"", ""${var.cc_password}"",
      ""package_mirror"", ""${replace(var.package_mirror, ""/^$/"", ""null"")}"",
      ""pool"", ""${var.pool}"",
      ""bridge"", ""${var.bridge}"",
      ""use_avahi"", ""${element(list(""False"", ""True""), var.use_avahi)}"",
      ""domain"", ""${var.domain}"",
      ""name_prefix"", ""${var.name_prefix}""
    )
  }""
}
",output,"output ""configuration"" {
  value = {
    network_name = ""${var.network_name}""
    cc_username = ""${var.cc_username}""
    cc_password = ""${var.cc_password}""
    package_mirror = ""${var.package_mirror == """" ? ""null"" : var.package_mirror}""
    pool = ""${var.pool}""
    bridge = ""${var.bridge}""
    use_avahi = ""${var.use_avahi == 1 ? ""True"" : ""False""}""
    domain = ""${var.domain}""
    name_prefix = ""${var.name_prefix}""
  }
}
",output,32,,0d3a83d22f57360dec522bcf1e5dd7adb3b9f9c3,e61be3bacea9a5e1a8f88284838880959fe67bfd,https://github.com/uyuni-project/sumaform/blob/0d3a83d22f57360dec522bcf1e5dd7adb3b9f9c3/modules/libvirt/base/main.tf#L32,https://github.com/uyuni-project/sumaform/blob/e61be3bacea9a5e1a8f88284838880959fe67bfd/modules/libvirt/base/main.tf,2016-11-04 17:34:25+01:00,2017-02-23 07:20:04+01:00,10,1,0,0,1,0,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,206,examples/tls-with-aws-pca-issuer/main.tf,patterns/tls-with-aws-pca-issuer/main.tf,1,workaround,# Using kubectl to workaround kubernetes provider issue https://github.com/hashicorp/terraform-provider-kubernetes/issues/1453,"#------------------------------- 
 # This resource creates a CRD of Certificate Kind, which then represents certificate issued from ACM PCA, 
 # mounted as K8 secret 
 #-------------------------------  
 # Using kubectl to workaround kubernetes provider issue https://github.com/hashicorp/terraform-provider-kubernetes/issues/1453","resource ""kubectl_manifest"" ""example_pca_certificate"" {
  yaml_body = yamlencode({
    apiVersion = ""cert-manager.io/v1""
    kind       = ""Certificate""

    metadata = {
      name      = var.certificate_name
      namespace = ""default""
    }

    spec = {
      commonName = var.certificate_dns
      duration   = ""2160h0m0s""
      issuerRef = {
        group = ""awspca.cert-manager.io""
        kind  = ""AWSPCAClusterIssuer""
        name : module.eks_blueprints.eks_cluster_id
      }
      renewBefore = ""360h0m0s""
      secretName  = join(""-"", [var.certificate_name, ""clusterissuer""]) # This is the name with which the K8 Secret will be available
      usages = [
        ""server auth"",
        ""client auth""
      ]
      privateKey = {
        algorithm : ""RSA""
        size : 2048
      }
    }
  })

  depends_on = [
    module.eks_blueprints_kubernetes_addons,
    kubectl_manifest.cluster_pca_issuer,
  ]
}
",resource,"resource ""kubectl_manifest"" ""pca_certificate"" {
  yaml_body = yamlencode({
    apiVersion = ""cert-manager.io/v1""
    kind       = ""Certificate""

    metadata = {
      name      = var.certificate_name
      namespace = ""default""
    }

    spec = {
      commonName = var.certificate_dns
      duration   = ""2160h0m0s""
      issuerRef = {
        group = ""awspca.cert-manager.io""
        kind  = ""AWSPCAClusterIssuer""
        name : module.eks.cluster_name
      }
      renewBefore = ""360h0m0s""
      secretName  = join(""-"", [var.certificate_name, ""clusterissuer""]) # This is the name with which the K8 Secret will be available
      usages = [
        ""server auth"",
        ""client auth""
      ]
      privateKey = {
        algorithm : ""RSA""
        size : 2048
      }
    }
  })

  depends_on = [
    kubectl_manifest.cluster_pca_issuer,
  ]
}
",resource,175,194.0,5602e3dad7e60945fd99df1bd0d52345e8a495f5,26309565d04b8b1cc2fc3bed7b227768ebb25e3a,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/5602e3dad7e60945fd99df1bd0d52345e8a495f5/examples/tls-with-aws-pca-issuer/main.tf#L175,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/26309565d04b8b1cc2fc3bed7b227768ebb25e3a/patterns/tls-with-aws-pca-issuer/main.tf#L194,2022-05-03 09:05:20-07:00,2024-04-22 09:49:57-07:00,20,0,1,0,1,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-vpc-service-controls,1,examples/simple_example/main.tf,examples/simple_example/main.tf,0,todo,## TODO make sure take out interpolation and test again.,## TODO make sure take out interpolation and test again.,"module ""regular-service-perimeter-1"" {
  source         = ""../../modules/regular_service_perimeter""
  policy         = ""${module.org-policy.policy_id}""
  perimeter_name = ""regular_perimeter_1""

  ## TODO make sure take out interpolation and test again.
  description    = ""Perimeter shielding bigquery project ${module.bigquery.dataset_project}""
  resources      = [""743286545054""]

  access_levels = [""${module.access-level-members.name}""]
  restricted_services = [""bigquery.googleapis.com"", ""storage.googleapis.com""]

  shared_resources = {
    all = [""743286545054""]
  }
}
",module,"module ""regular-service-perimeter-1"" {
  source         = ""../../modules/regular_service_perimeter""
  policy         = ""${module.org-policy.policy_id}""
  perimeter_name = ""regular_perimeter_1""

  description = ""Perimeter shielding bigquery project""
  resources   = [""${var.protected_project_ids[""number""]}""]

  access_levels       = [""${module.access-level-members.name}""]
  restricted_services = [""bigquery.googleapis.com"", ""storage.googleapis.com""]

  shared_resources = {
    all = [""${var.protected_project_ids[""number""]}""]
  }
}
",module,43,,07ef41e99d7389c0b4a31c4a67a12a7ab02627bd,62b68f8bcd72cd9bd8ea3d9b2d7cdc38de03fa98,https://github.com/terraform-google-modules/terraform-google-vpc-service-controls/blob/07ef41e99d7389c0b4a31c4a67a12a7ab02627bd/examples/simple_example/main.tf#L43,https://github.com/terraform-google-modules/terraform-google-vpc-service-controls/blob/62b68f8bcd72cd9bd8ea3d9b2d7cdc38de03fa98/examples/simple_example/main.tf,2019-04-26 00:29:28+00:00,2019-05-07 22:41:50+00:00,8,1,0,1,0,0,0,0,0,1
https://github.com/aws-observability/terraform-aws-observability-accelerator,4,main.tf,main.tf,0,# todo,# TODO: support custom alert manager config,# TODO: support custom alert manager config,"resource ""aws_prometheus_alert_manager_definition"" ""this"" {
  count = var.enable_alertmanager ? 1 : 0

  workspace_id = local.amp_ws_id

  # TODO: support custom alert manager config
  definition = <<EOF
alertmanager_config: |
    route:
      receiver: 'default'
    receivers:
      - name: 'default'
EOF
}
",resource,"resource ""aws_prometheus_alert_manager_definition"" ""this"" {
  count = var.enable_alertmanager ? 1 : 0

  workspace_id = local.amp_ws_id

  definition = <<EOF
alertmanager_config: |
    route:
      receiver: 'default'
    receivers:
      - name: 'default'
EOF
}
",resource,42,,333d46cccca511ba7ec2b83f695324b4c03c342e,c93e2cffc1d180af9d5c9a3623f0a18b4d6130b2,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/333d46cccca511ba7ec2b83f695324b4c03c342e/main.tf#L42,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/c93e2cffc1d180af9d5c9a3623f0a18b4d6130b2/main.tf,2022-08-26 17:30:03+02:00,2022-08-30 10:47:21+02:00,9,1,0,1,0,0,0,0,1,0
https://github.com/ministryofjustice/aws-root-account,35,organisation-security/terraform/license-manager.tf,organisation-security/terraform/license-manager.tf,0,todo,# TODO get some values below from stack above or make dependant on stack,# TODO get some values below from stack above or make dependant on stack,"resource ""aws_ssm_association"" ""license_manager"" {
  name             = ""OracleDbLTS-Orchestrate""
  association_name = ""OracleDbLicenseTrackingSolutionAssociation""

  # schedule_expression = ""0 1 * * *""
  max_concurrency = 4
  max_errors      = 4
  parameters = {
    AutomationAssumeRole = ""arn:aws:iam::${data.aws_caller_identity.current.id}:role/OracleDbLTS-SystemsManagerAutomationAdministrationRole""
    DeploymentTargets    = local.ou_example
    TargetRegions        = ""eu-west-2""
  }
}
",resource,"resource ""aws_ssm_association"" ""license_manager"" {
  name             = ""OracleDbLTS-Orchestrate""
  association_name = ""OracleDbLicenseTrackingSolutionAssociation""

  schedule_expression = ""0 1 * * *""
  max_concurrency = 4
  max_errors      = 4
  parameters = {
    AutomationAssumeRole = ""arn:aws:iam::${data.aws_caller_identity.current.id}:role/OracleDbLTS-SystemsManagerAutomationAdministrationRole""
    DeploymentTargets    = local.ou_example
    # DeploymentTargets    = local.ou_modernisation_platform_member_id
    TargetRegions        = ""eu-west-2""
  }

  depends_on = [
    aws_cloudformation_stack.oracleblts
  ]
}
",resource,108,,a00bc6abc4748ab2df805939dae7b5e7638befe8,f78db4476f9f701d6b0d34a43ef3b7471e5d7e77,https://github.com/ministryofjustice/aws-root-account/blob/a00bc6abc4748ab2df805939dae7b5e7638befe8/organisation-security/terraform/license-manager.tf#L108,https://github.com/ministryofjustice/aws-root-account/blob/f78db4476f9f701d6b0d34a43ef3b7471e5d7e77/organisation-security/terraform/license-manager.tf,2023-09-13 15:13:48+01:00,2023-09-13 15:13:48+01:00,2,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,258,modules/project/service_accounts.tf,modules/project/service_accounts.tf,0,# todo,# TODO: Find a better place to store BQ service account,# TODO: Find a better place to store BQ service account,"locals {
  service_account_cloud_services = ""${local.project.number}@cloudservices.gserviceaccount.com""
  service_accounts_default = {
    # TODO: Find a better place to store BQ service account
    bq      = ""bq-${local.project.number}@bigquery-encryption.iam.gserviceaccount.com""
    compute = ""${local.project.number}-compute@developer.gserviceaccount.com""
    gae     = ""${local.project.project_id}@appspot.gserviceaccount.com""
  }
  service_accounts_robot_services = {
    cloudasset        = ""gcp-sa-cloudasset""
    cloudbuild        = ""gcp-sa-cloudbuild""
    compute           = ""compute-system""
    container-engine  = ""container-engine-robot""
    containerregistry = ""containerregistry""
    dataflow          = ""dataflow-service-producer-prod""
    dataproc          = ""dataproc-accounts""
    gae-flex          = ""gae-api-prod""
    gcf               = ""gcf-admin-robot""
    pubsub            = ""gcp-sa-pubsub""
    storage           = ""gs-project-accounts""
  }
  service_accounts_robots = {
    for service, name in local.service_accounts_robot_services :
    service => ""service-${local.project.number}@${name}.iam.gserviceaccount.com""
  }
}
",locals,"locals {
  service_account_cloud_services = ""${local.project.number}@cloudservices.gserviceaccount.com""
  service_accounts_default = {
    compute = ""${local.project.number}-compute@developer.gserviceaccount.com""
    gae     = ""${local.project.project_id}@appspot.gserviceaccount.com""
  }
  service_accounts_robot_services = {
    bq                = ""bigquery-encryption""
    cloudasset        = ""gcp-sa-cloudasset""
    cloudbuild        = ""gcp-sa-cloudbuild""
    compute           = ""compute-system""
    container-engine  = ""container-engine-robot""
    containerregistry = ""containerregistry""
    dataflow          = ""dataflow-service-producer-prod""
    dataproc          = ""dataproc-accounts""
    gae-flex          = ""gae-api-prod""
    gcf               = ""gcf-admin-robot""
    pubsub            = ""gcp-sa-pubsub""
    secretmanager     = ""gcp-sa-secretmanager""
    storage           = ""gs-project-accounts""
  }
  service_accounts_robots = {
    for service, name in local.service_accounts_robot_services :
    service => ""${service == ""bq"" ? ""bq"" : ""service""}-${local.project.number}@${name}.iam.gserviceaccount.com""
  }
}
",locals,20,,476d2c79e972ec91a5f1b72ea79a00ce372d81d3,12e69c71e37f125a4cd2927e95d8cfd0a8411cbb,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/476d2c79e972ec91a5f1b72ea79a00ce372d81d3/modules/project/service_accounts.tf#L20,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/12e69c71e37f125a4cd2927e95d8cfd0a8411cbb/modules/project/service_accounts.tf,2021-06-11 16:00:20+02:00,2021-06-14 18:35:53+02:00,3,1,0,1,0,1,0,0,0,0
https://github.com/uyuni-project/sumaform,1667,modules/proxy_containerized/main.tf,modules/proxy_containerized/main.tf,0,implemented,# Not yet implemented in sumaform salt states,helm_chart_url            = var.helm_chart_url # Not yet implemented in sumaform salt states,"module ""proxy_containerized"" {
  source = ""../host""

  roles                         = var.roles
  connect_to_base_network       = true
  connect_to_additional_network = true
  quantity                      = var.quantity
  base_configuration            = var.base_configuration
  image                         = var.image == ""default"" || var.product_version == ""head"" ? var.images[var.product_version] : var.image
  name                          = var.name
  use_os_released_updates       = true
  ssh_key_path                  = var.ssh_key_path
  additional_repos              = var.additional_repos
  additional_repos_only         = var.additional_repos_only
  additional_packages           = var.additional_packages
  provider_settings             = var.provider_settings
  additional_disk_size          = var.repository_disk_size
  second_additional_disk_size   = var.database_disk_size
  volume_provider_settings      = var.volume_provider_settings

  grains = {
    product_version           = var.product_version
    server                    = var.server_configuration[""hostname""]
    first_user_present        = var.server_configuration[""create_first_user""]
    server_username           = var.server_configuration[""username""]
    server_password           = var.server_configuration[""password""]
    auto_configure            = var.auto_configure
    container_runtime         = var.runtime
    container_repository      = var.container_repository
    helm_chart_url            = var.helm_chart_url # Not yet implemented in sumaform salt states
    mirror                    = var.base_configuration[""mirror""]
    avahi_reflector           = var.avahi_reflector
    repository_disk_size      = var.repository_disk_size
    database_disk_size        = var.database_disk_size
  }
}
",module,"module ""proxy_containerized"" {
  source = ""../host""

  roles                         = var.roles
  connect_to_base_network       = true
  connect_to_additional_network = true
  quantity                      = var.quantity
  base_configuration            = var.base_configuration
  image                         = var.image == ""default"" || var.product_version == ""head"" ? var.images[var.product_version] : var.image
  name                          = var.name
  use_os_released_updates       = true
  install_salt_bundle           = var.install_salt_bundle
  ssh_key_path                  = var.ssh_key_path
  additional_repos              = var.additional_repos
  additional_repos_only         = var.additional_repos_only
  additional_packages           = var.additional_packages
  provider_settings             = var.provider_settings
  additional_disk_size          = var.repository_disk_size
  second_additional_disk_size   = var.database_disk_size
  volume_provider_settings      = var.volume_provider_settings

  grains = {
    product_version           = var.product_version
    server                    = var.server_configuration[""hostname""]
    server_username           = var.server_configuration[""username""]
    server_password           = var.server_configuration[""password""]
    auto_configure            = var.auto_configure
    container_runtime         = var.runtime
    container_repository      = var.container_repository
    helm_chart_url            = var.helm_chart_url # Not yet implemented in sumaform salt states
    mirror                    = var.base_configuration[""mirror""]
    avahi_reflector           = var.avahi_reflector
    main_disk_size            = var.main_disk_size
    repository_disk_size      = var.repository_disk_size
    database_disk_size        = var.database_disk_size
  }
}
",module,41,39.0,2601871d9b76286f60fbfd5ddae395a64d944c0f,31f3f833cfe72cfc8efc420eb4b9507f00e16d4b,https://github.com/uyuni-project/sumaform/blob/2601871d9b76286f60fbfd5ddae395a64d944c0f/modules/proxy_containerized/main.tf#L41,https://github.com/uyuni-project/sumaform/blob/31f3f833cfe72cfc8efc420eb4b9507f00e16d4b/modules/proxy_containerized/main.tf#L39,2024-02-01 15:23:28+01:00,2024-04-10 13:00:59+02:00,6,0,1,1,0,0,0,0,0,0
https://github.com/camptocamp/devops-stack,97,examples/kind/main.tf,examples/kind/main.tf,0,fix,# TODO fix: the base domain is defined later. Proposal: remove redirection from traefik module and add it in dependent modules.,"# TODO fix: the base domain is defined later. Proposal: remove redirection from traefik module and add it in dependent modules. 
 # For now random value is passed to base_domain. Redirections will not work before fix.","module ""traefik"" {
  source = ""git::https://github.com/camptocamp/devops-stack-module-traefik.git//kind?ref=v1.0.0""

  cluster_name = local.cluster_name

  # TODO fix: the base domain is defined later. Proposal: remove redirection from traefik module and add it in dependent modules.
  # For now random value is passed to base_domain. Redirections will not work before fix.
  base_domain = ""placeholder.com""

  argocd_namespace = module.argocd_bootstrap.argocd_namespace

  dependency_ids = {
    argocd = module.argocd_bootstrap.id
  }
}
",module,"module ""traefik"" {
  source = ""git::https://github.com/camptocamp/devops-stack-module-traefik.git//kind?ref=v6.2.0""

  argocd_project = local.cluster_name

  app_autosync           = local.app_autosync
  enable_service_monitor = local.enable_service_monitor

  dependency_ids = {
    argocd = module.argocd_bootstrap.id
  }
}
",module,72,,e3e1a35b6a90a2990878d6c06775a6dba94637af,10d0d93a6e34b9b8e29beca8c6e01ec6ca3a8244,https://github.com/camptocamp/devops-stack/blob/e3e1a35b6a90a2990878d6c06775a6dba94637af/examples/kind/main.tf#L72,https://github.com/camptocamp/devops-stack/blob/10d0d93a6e34b9b8e29beca8c6e01ec6ca3a8244/examples/kind/main.tf,2023-05-16 13:05:31+02:00,2024-03-15 09:27:05+01:00,29,1,1,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,7,frontend/tf/main.tf,community/front-end/ofe/tf/main.tf,1,# todo,# TODO:  SSH Keys,# TODO:  SSH Keys,"resource ""google_compute_instance"" ""server_vm"" {

    name = ""${var.deployment_name}-server""
    machine_type = var.server_instance_type
    zone = var.zone

    hostname = length(trimspace(var.webserver_hostname)) > 0 ? var.webserver_hostname : null
    
    metadata = {
        startup-script-url = ""${module.control_bucket.bucket.url}/webserver/startup.sh"",
        webserver-config-bucket = module.control_bucket.bucket.name,
        ghpcfe-c2-topic = module.pubsub.topic,
        hostname = var.webserver_hostname
        deploy_mode = var.deployment_mode
        # TODO:  SSH Keys
    }

    service_account {
        email = module.service_account.email
        scopes = [
            ""storage-full"",
            ""logging-write"",
            ""monitoring-write"",
            ""trace"",
            ""service-control"",
            ""service-management"",
            ""pubsub""
        ]
    }
    scheduling {
        on_host_maintenance = ""MIGRATE""
    }

    labels = local.labels
    tags = [""http-server"", ""https-server"", ""ssh-server""]

    boot_disk {
        initialize_params {
            image = ""projects/rocky-linux-cloud/global/images/rocky-linux-8-v20220126""
            size = 30
            type = ""pd-ssd""
        }
    }

    network_interface {
        subnetwork = length(trimspace(var.subnet)) > 0 ? var.subnet : module.network[0].subnet_name
        access_config {
            nat_ip = length(trimspace(var.static_ip)) > 0 ? var.static_ip : null
        }
    }

}
",resource,"resource ""google_compute_instance"" ""server_vm"" {

  name         = ""${var.deployment_name}-server""
  machine_type = var.server_instance_type
  zone         = var.zone

  hostname = length(trimspace(var.webserver_hostname)) > 0 ? var.webserver_hostname : null

  metadata = {
    startup-script-url      = ""${module.control_bucket.bucket.url}/webserver/startup.sh"",
    webserver-config-bucket = module.control_bucket.bucket.name,
    ghpcfe-c2-topic         = module.pubsub.topic,
    hostname                = var.webserver_hostname
    deploy_mode             = var.deployment_mode
  }

  service_account {
    email = module.service_account.email
    scopes = [
      ""storage-full"",
      ""logging-write"",
      ""monitoring-write"",
      ""trace"",
      ""service-control"",
      ""service-management"",
      ""pubsub""
    ]
  }
  scheduling {
    on_host_maintenance = ""MIGRATE""
  }

  labels = local.labels
  tags   = [""http-server"", ""https-server"", ""ssh-server""]

  boot_disk {
    initialize_params {
      image = ""projects/rocky-linux-cloud/global/images/rocky-linux-8-v20220126""
      size  = 30
      type  = ""pd-ssd""
    }
  }

  network_interface {
    subnetwork = length(trimspace(var.subnet)) > 0 ? var.subnet : module.network[0].subnet_name
    access_config {
      nat_ip = length(trimspace(var.static_ip)) > 0 ? var.static_ip : null
    }
  }

}
",resource,136,,67c9341c36b25f7c4fb6bc3b8aef45fddff8c2cc,d0b943737fd33716875d6b7e24bce99dda940918,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/67c9341c36b25f7c4fb6bc3b8aef45fddff8c2cc/frontend/tf/main.tf#L136,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/d0b943737fd33716875d6b7e24bce99dda940918/community/front-end/ofe/tf/main.tf,2022-03-31 13:08:42-05:00,2022-11-10 17:18:21+00:00,2,1,1,1,0,1,0,0,0,0
https://github.com/CDCgov/prime-simplereport,93,ops/services/postgres_db/vault.tf,ops/services/postgres_db/vault.tf,0,//todo,"//TODO: Change this to use a TF-generated password, like db-password-no-phi. See #3673 for the additional work.","//TODO: Change this to use a TF-generated password, like db-password-no-phi. See #3673 for the additional work.","data ""azurerm_key_vault_secret"" ""db_password"" {
  name         = ""simple-report-${var.env_level}-db-password""
  key_vault_id = var.global_vault_id
}
",data,"data ""azurerm_key_vault_secret"" ""db_password"" {
  name         = ""simple-report-${var.env}-db-password""
  key_vault_id = var.global_vault_id
}
",data,8,8.0,c617c1ab4838016aa686ecff84fde0e6be45b8a5,295bb12754a8f740634cd16d7a3442d18f5d0216,https://github.com/CDCgov/prime-simplereport/blob/c617c1ab4838016aa686ecff84fde0e6be45b8a5/ops/services/postgres_db/vault.tf#L8,https://github.com/CDCgov/prime-simplereport/blob/295bb12754a8f740634cd16d7a3442d18f5d0216/ops/services/postgres_db/vault.tf#L8,2022-05-05 00:48:15-05:00,2023-10-05 19:10:51+00:00,2,0,0,1,0,1,0,0,0,0
https://github.com/nebari-dev/nebari,34,src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/monitoring/loki/main.tf,src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/monitoring/loki/main.tf,0,implement,# We configure MinIO by using the AWS config because MinIO implements the S3 API,# We configure MinIO by using the AWS config because MinIO implements the S3 API,"resource ""helm_release"" ""grafana-loki"" {
  name       = ""nebari-loki""
  namespace  = var.namespace
  repository = ""https://grafana.github.io/helm-charts""
  chart      = ""loki""
  version    = var.loki-helm-chart-version

  values = concat([
    file(""${path.module}/values_loki.yaml""),
    jsonencode({
      loki : {
        storage : {
          s3 : {
            endpoint : local.minio-url,
            accessKeyId : ""admin""
            secretAccessKey : random_password.minio_root_password.result,
            s3ForcePathStyle : true
          }
        }
      }
      storageConfig : {
        # We configure MinIO by using the AWS config because MinIO implements the S3 API
        aws : {
          s3 : local.minio-url
          s3ForcePathStyle : true
        }
      }
      write : { nodeSelector : local.node-selector }
      read : { nodeSelector : local.node-selector }
      backend : { nodeSelector : local.node-selector }
      gateway : { nodeSelector : local.node-selector }
    })
  ], var.grafana-loki-overrides)

  depends_on = [helm_release.loki-minio]
}
",resource,"resource ""helm_release"" ""grafana-loki"" {
  name       = ""nebari-loki""
  namespace  = var.namespace
  repository = ""https://grafana.github.io/helm-charts""
  chart      = ""loki""
  version    = var.loki-helm-chart-version

  values = concat([
    file(""${path.module}/values_loki.yaml""),
    jsonencode({
      loki : {
        storage : {
          s3 : {
            endpoint : local.minio-url,
            accessKeyId : ""admin""
            secretAccessKey : random_password.minio_root_password.result,
            s3ForcePathStyle : true
          }
        }
      }
      storageConfig : {
        # We configure MinIO by using the AWS config because MinIO implements the S3 API
        aws : {
          s3 : local.minio-url
          s3ForcePathStyle : true
        }
      }
      write : { nodeSelector : local.node-selector }
      read : { nodeSelector : local.node-selector }
      backend : { nodeSelector : local.node-selector }
      gateway : { nodeSelector : local.node-selector }
    })
  ], var.grafana-loki-overrides)

  depends_on = [helm_release.loki-minio]
}
",resource,72,72.0,0210a47a1acbe4940e0f6ff18fb72f630e224230,0210a47a1acbe4940e0f6ff18fb72f630e224230,https://github.com/nebari-dev/nebari/blob/0210a47a1acbe4940e0f6ff18fb72f630e224230/src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/monitoring/loki/main.tf#L72,https://github.com/nebari-dev/nebari/blob/0210a47a1acbe4940e0f6ff18fb72f630e224230/src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/monitoring/loki/main.tf#L72,2024-03-08 16:15:52-03:00,2024-03-08 16:15:52-03:00,1,0,1,0,0,1,0,0,1,0
https://github.com/wireapp/wire-server-deploy,34,terraform/modules/aws-vpc-security-groups/main.tf,terraform/modules/aws-vpc-security-groups/main.tf,0,hack,"# HACK: running out of security groups, adding this here since all k8s nodes need to talk to S3.","# HACK: running out of security groups, adding this here since all k8s nodes need to talk to S3. 
 # S3","resource ""aws_security_group"" ""k8s_private"" {
  name        = ""k8s_private""
  description = ""hosts that are allowed to the private ports of the kubernetes nodes.""
  vpc_id      = var.vpc_id

  # FIXME: tighten this up.
  egress {
    from_port   = 0
    to_port     = 65535
    protocol    = ""tcp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  # FIXME: tighten this up. need UDP for flannel.
  egress {
    from_port   = 0
    to_port     = 65535
    protocol    = ""udp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  # HACK: running out of security groups, adding this here since all k8s nodes need to talk to S3.
  # S3
  egress {
    description = """"
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = var.s3_CIDRs
  }

  tags = {
    Name = ""k8s_private""
  }
}
",resource,"resource ""aws_security_group"" ""k8s_private"" {
  name        = ""k8s_private""
  description = ""hosts that are allowed to the private ports of the kubernetes nodes.""
  vpc_id      = var.vpc_id

  # FIXME: tighten this up.
  egress {
    from_port   = 0
    to_port     = 65535
    protocol    = ""tcp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  # FIXME: tighten this up. need UDP for flannel.
  egress {
    from_port   = 0
    to_port     = 65535
    protocol    = ""udp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  tags = {
    Name = ""k8s_private""
  }
}
",resource,291,,f239eeced44a73fb235171e9af52b1776e6ed6fc,e7f081ca744de60bf9d10cb037828b995883d2aa,https://github.com/wireapp/wire-server-deploy/blob/f239eeced44a73fb235171e9af52b1776e6ed6fc/terraform/modules/aws-vpc-security-groups/main.tf#L291,https://github.com/wireapp/wire-server-deploy/blob/e7f081ca744de60bf9d10cb037828b995883d2aa/terraform/modules/aws-vpc-security-groups/main.tf,2020-07-13 19:15:38+01:00,2020-08-26 16:29:39+02:00,2,1,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,761,examples/cloud-operations/network-dashboard/variables.tf,blueprints/cloud-operations/network-dashboard/variables.tf,1,# todo,# TODO: support folder instead of a list of projects?,# TODO: support folder instead of a list of projects?,"variable ""monitored_projects_list"" {
  type        = list(string)
  description = ""ID of the projects to be monitored (where limits and quotas data will be pulled)""
}
",variable,"variable ""monitored_projects_list"" {
  type        = list(string)
  description = ""ID of the projects to be monitored (where limits and quotas data will be pulled)""
}
",variable,36,,9f3ee4dc2299a7e9034cf25988d9c4b59c0fd70b,a709febfdb4234cec10da9d4949ebe889f20a6b9,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9f3ee4dc2299a7e9034cf25988d9c4b59c0fd70b/examples/cloud-operations/network-dashboard/variables.tf#L36,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/a709febfdb4234cec10da9d4949ebe889f20a6b9/blueprints/cloud-operations/network-dashboard/variables.tf,2022-03-08 18:36:02+01:00,2022-09-30 10:51:16+02:00,8,1,0,1,0,0,0,0,0,0
https://github.com/cattle-ops/terraform-aws-gitlab-runner,103,modules/terminate-agent-hook/iam.tf,modules/terminate-agent-hook/iam.tf,0,fix,# checkov:skip=CKV_AWS_356:False positive and fixed with version 2.3.293,"  # checkov:skip=CKV_AWS_111:I didn't found any condition to limit the access.
  # checkov:skip=CKV_AWS_356:False positive and fixed with version 2.3.293","data ""aws_iam_policy_document"" ""spot_request_housekeeping"" {
  # checkov:skip=CKV_AWS_111:I didn't found any condition to limit the access.
  # checkov:skip=CKV_AWS_356:False positive and fixed with version 2.3.293
  statement {
    sid = ""SpotRequestHousekeepingList""

    effect = ""Allow""
    actions = [
      ""ec2:CancelSpotInstanceRequests"",
      ""ec2:DescribeSpotInstanceRequests""
    ]
    # I didn't found any condition to limit the access
    resources = [""*""]
  }
}
",data,"data ""aws_iam_policy_document"" ""spot_request_housekeeping"" {
  # checkov:skip=CKV_AWS_111:I didn't found any condition to limit the access.
  # checkov:skip=CKV_AWS_356:False positive and fixed with version 2.3.293
  statement {
    sid = ""SpotRequestHousekeepingList""

    effect = ""Allow""
    actions = [
      ""ec2:CancelSpotInstanceRequests"",
      ""ec2:DescribeSpotInstanceRequests""
    ]
    # I didn't found any condition to limit the access
    resources = [""*""]
  }
}
",data,121,123.0,f25a86b5ada3e78a56c33db07a0110354f5e2e5d,8b76c0fd68deca6b40bd372ddb07f38abc7e668c,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/f25a86b5ada3e78a56c33db07a0110354f5e2e5d/modules/terminate-agent-hook/iam.tf#L121,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/8b76c0fd68deca6b40bd372ddb07f38abc7e668c/modules/terminate-agent-hook/iam.tf#L123,2023-06-15 09:42:30+02:00,2023-11-09 10:29:27+01:00,2,0,0,0,0,1,0,0,0,1
https://github.com/Worklytics/psoxy,97,infra/modules/aws-psoxy-instance/main.tf,infra/modules/aws-psoxy-instance/main.tf,0,#todo,#TODO: match on subpath equivalent to var.function_name ?,#TODO: match on subpath equivalent to var.function_name ?,"resource ""aws_apigatewayv2_integration"" ""map"" {
  api_id                    = var.api_gateway.id
  integration_type          = ""AWS_PROXY""
  connection_type           = ""INTERNET""

  #TODO: match on subpath equivalent to var.function_name ?

  integration_method        = ""POST""
  integration_uri           = aws_lambda_function.psoxy-instance.invoke_arn
  request_parameters        = {}
  request_templates         = {}
}
",resource,"resource ""aws_apigatewayv2_integration"" ""map"" {
  api_id                    = var.api_gateway.id
  integration_type          = ""AWS_PROXY""
  connection_type           = ""INTERNET""

  integration_method        = ""POST""
  integration_uri           = aws_lambda_function.psoxy-instance.invoke_arn
  request_parameters        = {}
  request_templates         = {}
}
",resource,18,,7444d0de8dc052089b9c9dc91394cf5172660cdd,2ed09153e988500e594a556bbc9f6bd64c986bad,https://github.com/Worklytics/psoxy/blob/7444d0de8dc052089b9c9dc91394cf5172660cdd/infra/modules/aws-psoxy-instance/main.tf#L18,https://github.com/Worklytics/psoxy/blob/2ed09153e988500e594a556bbc9f6bd64c986bad/infra/modules/aws-psoxy-instance/main.tf,2022-01-06 11:41:26-08:00,2022-01-12 16:14:43-08:00,9,1,0,1,0,0,1,0,0,0
https://github.com/pingcap/tidb-operator,3,deploy/alicloud/ack/data.tf,deploy/aliyun/ack/data.tf,1,workaround,"# Workaround map to list transformation, see stackoverflow.com/questions/43893295","# Workaround map to list transformation, see stackoverflow.com/questions/43893295","data ""template_file"" ""vswitch_id"" {
  count    = ""${var.vpc_id == """" ? 0 : length(data.alicloud_vswitches.default.vswitches)}""
  template = ""${lookup(data.alicloud_vswitches.default.0.vswitches[count.index], ""id"")}""
}
",data,,,14,0.0,eebd686956c0b2adf67a10e1a376659c95c571a3,042b1a97fbbdf342297002990564828e6644a3f0,https://github.com/pingcap/tidb-operator/blob/eebd686956c0b2adf67a10e1a376659c95c571a3/deploy/alicloud/ack/data.tf#L14,https://github.com/pingcap/tidb-operator/blob/042b1a97fbbdf342297002990564828e6644a3f0/deploy/aliyun/ack/data.tf#L0,2019-05-06 19:59:43+08:00,2019-07-23 19:44:58+08:00,2,2,0,1,0,0,0,0,0,0
https://github.com/pingcap/tidb-operator,73,deploy/modules/gcp/tidb-operator/versions.tf,deploy/modules/gcp/tidb-operator/versions.tf,0,fix,# TODO: remove the restriction of < 2.19 once the `ip_allocation_policy.0.use_ip_aliases` error fixes,"# TODO: remove the restriction of < 2.19 once the `ip_allocation_policy.0.use_ip_aliases` error fixes 
 # https://github.com/terraform-providers/terraform-provider-google/blob/master/CHANGELOG.md#2190-november-05-2019","terraform {
  required_version = "">= 0.12""
  required_providers {
    # TODO: remove the restriction of < 2.19 once the `ip_allocation_policy.0.use_ip_aliases` error fixes
    # https://github.com/terraform-providers/terraform-provider-google/blob/master/CHANGELOG.md#2190-november-05-2019
    google      = "">= 2.16, < 2.19""
    google-beta = "">= 2.16, < 2.19""
    external    = ""~> 1.2""
    helm        = ""~> 0.10""
    null        = ""~> 2.1""
  }
}
",terraform,"terraform {
  required_version = "">= 0.12""
  required_providers {
    google = ""~> 2.16""
    helm   = ""~> 0.10""
    null   = ""~> 2.1""
  }
}
",terraform,5,,ebc4486509e029ba81b1c3bc6b0de54962196eeb,b2d38ef46b9e52c2c2c2b5334793a9f9a274e95b,https://github.com/pingcap/tidb-operator/blob/ebc4486509e029ba81b1c3bc6b0de54962196eeb/deploy/modules/gcp/tidb-operator/versions.tf#L5,https://github.com/pingcap/tidb-operator/blob/b2d38ef46b9e52c2c2c2b5334793a9f9a274e95b/deploy/modules/gcp/tidb-operator/versions.tf,2019-11-21 13:49:04+08:00,2019-12-31 16:08:33+08:00,2,1,0,1,1,1,1,0,0,0
https://github.com/alphagov/govuk-aws,1,terraform/projects/govuk-networking/main.tf,terraform/projects/infra-networking/main.tf,1,workaround,"# There are a few workarounds to get around this limitation,","# Intermediate variables in Terraform are not supported. 
 # There are a few workarounds to get around this limitation, 
 # https://github.com/hashicorp/terraform/issues/4084 
 # The template_file resources allow us to use a private_subnet_nat_gateway_association 
 # variable to select which NAT gateway, if any, each private 
 # subnet must use to route public traffic.","data ""template_file"" ""nat_gateway_association_subnet_id"" {
  count    = ""${length(keys(var.private_subnet_nat_gateway_association))}""
  template = ""$${subnet_id}""

  vars {
    subnet_id = ""${lookup(module.govuk_public_subnet.subnet_names_ids_map, element(values(var.private_subnet_nat_gateway_association), count.index))}""
  }
}
",data,"data ""template_file"" ""nat_gateway_association_subnet_id"" {
  count    = length(keys(var.private_subnet_nat_gateway_association))
  template = ""$${subnet_id}""

  vars = {
    subnet_id = ""${lookup(module.infra_public_subnet.subnet_names_ids_map, element(values(var.private_subnet_nat_gateway_association), count.index))}""
  }
}
",data,114,167.0,7770005f878b16b548c854f6a22e817b5438d3ca,7a0cb9b14717825fe20ec66dde2591851d2de47b,https://github.com/alphagov/govuk-aws/blob/7770005f878b16b548c854f6a22e817b5438d3ca/terraform/projects/govuk-networking/main.tf#L114,https://github.com/alphagov/govuk-aws/blob/7a0cb9b14717825fe20ec66dde2591851d2de47b/terraform/projects/infra-networking/main.tf#L167,2017-07-05 16:12:02+01:00,2023-12-01 16:46:15+00:00,45,0,0,0,1,0,1,0,0,0
https://github.com/magma/magma,1,orc8r/cloud/deploy/terraform/main.tf,orc8r/cloud/deploy/terraform/main.tf,0,# todo,# TODO: custom userdata to claim and mount the EBS volume,"# TODO: custom userdata to claim and mount the EBS volume 
 # for now, you'll have to mount the volume to the node manually  
 # Have to specify this here otherwise it forces a new resource","module ""eks"" {
  source       = ""terraform-aws-modules/eks/aws""
  cluster_name = var.cluster_name
  vpc_id       = module.vpc.vpc_id
  subnets      = module.vpc.public_subnets

  worker_additional_security_group_ids = [aws_security_group.default.id]

  # asg max capacity is 3
  # 1 worker group for orc8r (3 boxes total)
  # 1 worker group for metrics (1 box)
  worker_groups = [
    {
      name                 = ""wg-1""
      instance_type        = ""m4.xlarge""
      asg_desired_capacity = 3
      key_name             = var.key_name

      # Have to specify this here otherwise it forces a new resource
      ami_id = ""ami-08716b70cac884aaa""

      tags = [
        {
          key                 = ""orc8r-node-type""
          value               = ""orc8r-worker-node""
          propagate_at_launch = true
        },
      ]
    },
    {
      name                 = ""wg-metrics""
      instance_type        = ""t2.xlarge""
      asg_desired_capacity = 1
      key_name             = var.key_name

      # we put the metrics nodes into 1 specific subnet because EBS volumes
      # can only be mounted into the same AZ
      subnets = [module.vpc.public_subnets[0]]

      # TODO: custom userdata to claim and mount the EBS volume
      # for now, you'll have to mount the volume to the node manually

      # Have to specify this here otherwise it forces a new resource
      ami_id = ""ami-08716b70cac884aaa""

      tags = [
        {
          key                 = ""orc8r-node-type""
          value               = ""orc8r-prometheus-node""
          propagate_at_launch = true
        },
      ]
    },
  ]

  map_users = var.map_users

  write_kubeconfig      = true
  write_aws_auth_config = true
}
",module,"module ""eks"" {
  source       = ""terraform-aws-modules/eks/aws""
  cluster_name = var.cluster_name
  vpc_id       = module.vpc.vpc_id
  subnets      = module.vpc.public_subnets

  worker_additional_security_group_ids = [aws_security_group.default.id]
  workers_additional_policies          = [""${aws_iam_policy.worker_node_policy.arn}""]

  # asg max capacity is 3
  # 1 worker group for orc8r (3 boxes total)
  # 1 worker group for metrics (1 box)
  worker_groups = [
    {
      name                 = ""wg-1""
      instance_type        = ""m4.xlarge""
      asg_desired_capacity = 3
      key_name             = var.key_name

      # Have to specify this here otherwise it forces a new resource
      ami_id = ""ami-08716b70cac884aaa""

      tags = [
        {
          key                 = ""orc8r-node-type""
          value               = ""orc8r-worker-node""
          propagate_at_launch = true
        },
      ]
    },
    {
      name                 = ""wg-metrics""
      instance_type        = ""t2.xlarge""
      asg_desired_capacity = 1
      key_name             = var.key_name

      # we put the metrics nodes into 1 specific subnet because EBS volumes
      # can only be mounted into the same AZ
      subnets = [module.vpc.public_subnets[0]]

      additional_userdata  = ""${data.template_file.metrics_userdata.rendered}""

      # Have to specify this here otherwise it forces a new resource
      ami_id = ""ami-08716b70cac884aaa""

      tags = [
        {
          key                 = ""orc8r-node-type""
          value               = ""orc8r-prometheus-node""
          propagate_at_launch = true
        },
      ]
    },
  ]

  map_users = var.map_users

  write_kubeconfig      = true
  write_aws_auth_config = true
}
",module,73,,3f09622541c736c040f6f9b91841cdbf40b7680a,fb12ebd0afc6e7cfb1e8a1339eb39b9fbdce6b63,https://github.com/magma/magma/blob/3f09622541c736c040f6f9b91841cdbf40b7680a/orc8r/cloud/deploy/terraform/main.tf#L73,https://github.com/magma/magma/blob/fb12ebd0afc6e7cfb1e8a1339eb39b9fbdce6b63/orc8r/cloud/deploy/terraform/main.tf,2019-06-28 14:42:33-07:00,2019-06-28 18:17:17-07:00,2,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1401,blueprints/data-solutions/vertex-mlops/outputs.tf,blueprints/data-solutions/vertex-mlops/outputs.tf,0,# todo,# TODO(): proper outputs,"/** 
 * Copyright 2022 Google LLC 
 * 
 * Licensed under the Apache License, Version 2.0 (the ""License""); 
 * you may not use this file except in compliance with the License. 
 * You may obtain a copy of the License at 
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, software 
 * distributed under the License is distributed on an ""AS IS"" BASIS, 
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 * See the License for the specific language governing permissions and 
 * limitations under the License. 
 */  
 # TODO(): proper outputs  ","locals {
  docker_split = try(split(""/"", module.artifact_registry.id), null)
  docker_repo  = try(""${local.docker_split[3]}-docker.pkg.dev/${local.docker_split[1]}/${local.docker_split[5]}"", null)
  gh_config = {
    WORKLOAD_ID_PROVIDER = try(google_iam_workload_identity_pool_provider.github_provider[0].name, null)
    SERVICE_ACCOUNT      = try(module.service-account-github.email, null)
    PROJECT_ID           = module.project.project_id
    DOCKER_REPO          = local.docker_repo
    SA_MLOPS             = module.service-account-mlops.email
    SUBNETWORK           = local.subnet
  }
}
",locals,"locals {
  docker_split = try(split(""/"", module.artifact_registry.id), null)
  docker_repo  = try(""${local.docker_split[3]}-docker.pkg.dev/${local.docker_split[1]}/${local.docker_split[5]}"", null)
  gh_config = {
    WORKLOAD_ID_PROVIDER = try(google_iam_workload_identity_pool_provider.github_provider[0].name, null)
    SERVICE_ACCOUNT      = try(module.service-account-github.email, null)
    PROJECT_ID           = module.project.project_id
    DOCKER_REPO          = local.docker_repo
    SA_MLOPS             = module.service-account-mlops.email
    SUBNETWORK           = local.subnet
  }
}
",locals,17,,ce1f86d20b6f90a5d688e24813fd9552af67ddd6,edf67fc5d040acfcd852a24a62d04acd2e4038f2,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/ce1f86d20b6f90a5d688e24813fd9552af67ddd6/blueprints/data-solutions/vertex-mlops/outputs.tf#L17,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/edf67fc5d040acfcd852a24a62d04acd2e4038f2/blueprints/data-solutions/vertex-mlops/outputs.tf,2023-02-02 19:13:13+01:00,2023-04-18 17:32:15+02:00,2,1,0,1,0,0,0,0,0,0
https://github.com/alphagov/govuk-aws,942,terraform/projects/app-licensify-backend/main.tf,terraform/projects/app-licensify-backend/main.tf,0,xxx,"# XXX not sure if passing a map literal works; if not, try making it a local","# XXX not sure if passing a map literal works; if not, try making it a local 
 # and assigning it here using string interpolation.","module ""licensify-backend"" {
  source = ""../../modules/aws/node_group""
  name   = ""${var.stackname}-licensify-backend""

  # XXX not sure if passing a map literal works; if not, try making it a local
  # and assigning it here using string interpolation.
  default_tags = {
    Project         = ""${var.stackname}""
    aws_stackname   = ""${var.stackname}""
    aws_environment = ""${var.aws_environment}""
    aws_migration   = ""licensing_backend""
    aws_hostname    = ""licensify-backend-1""
  }

  instance_subnet_ids               = ""${data.terraform_remote_state.infra_networking.private_subnet_ids}""
  instance_security_group_ids       = [""${data.terraform_remote_state.infra_security_groups.sg_licensify-backend_id}"", ""${data.terraform_remote_state.infra_security_groups.sg_management_id}""]
  instance_type                     = ""${var.instance_type}""
  instance_additional_user_data     = ""${join(""\n"", null_resource.user_data.*.triggers.snippet)}""
  instance_target_group_arns_length = ""1""
  instance_target_group_arns        = [""${module.internal_lb.target_group_arns[0]}""]
  instance_ami_filter_name          = ""${var.instance_ami_filter_name}""
  asg_max_size                      = ""${var.asg_size}""
  asg_min_size                      = ""${var.asg_size}""
  asg_desired_capacity              = ""${var.asg_size}""
  asg_notification_topic_arn        = ""${data.terraform_remote_state.infra_monitoring.sns_topic_autoscaling_group_events_arn}""
}
",module,"module ""licensify-backend"" {
  source = ""../../modules/aws/node_group""
  name   = ""licensify-backend""

  default_tags = {
    Project         = ""${var.stackname}""
    aws_stackname   = ""${var.stackname}""
    aws_environment = ""${var.aws_environment}""
    aws_migration   = ""licensing_backend""
    aws_hostname    = ""licensify-backend-1""
  }

  instance_subnet_ids               = ""${data.terraform_remote_state.infra_networking.private_subnet_ids}""
  instance_security_group_ids       = [""${data.terraform_remote_state.infra_security_groups.sg_licensify-backend_id}"", ""${data.terraform_remote_state.infra_security_groups.sg_management_id}""]
  instance_type                     = ""${var.instance_type}""
  instance_additional_user_data     = ""${join(""\n"", null_resource.user_data.*.triggers.snippet)}""
  instance_target_group_arns_length = ""1""
  instance_target_group_arns        = [""${module.internal_lb.target_group_arns[0]}""]
  instance_ami_filter_name          = ""${var.instance_ami_filter_name}""
  asg_max_size                      = ""${var.asg_size}""
  asg_min_size                      = ""${var.asg_size}""
  asg_desired_capacity              = ""${var.asg_size}""
  asg_notification_topic_arn        = ""${data.terraform_remote_state.infra_monitoring.sns_topic_autoscaling_group_events_arn}""
}
",module,117,,5a72a7fddd6d14951653f13bbb93b6096b2a3a73,4337784ffc75431260bcc9f5f58563f8a04e66db,https://github.com/alphagov/govuk-aws/blob/5a72a7fddd6d14951653f13bbb93b6096b2a3a73/terraform/projects/app-licensify-backend/main.tf#L117,https://github.com/alphagov/govuk-aws/blob/4337784ffc75431260bcc9f5f58563f8a04e66db/terraform/projects/app-licensify-backend/main.tf,2019-08-08 17:17:58+01:00,2019-08-09 10:44:09+01:00,2,1,0,1,0,0,0,0,0,0
https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules,1,modules/vm-series/variables.tf,modules/vm-series/variables.tf,0,fix,# FIXME remove,"default = ""-"" # FIXME remove","variable ""sep"" {
  default = ""-"" # FIXME remove
}
",variable,the block associated got renamed or deleted,,70,,14e7264e068be5eb612e06b46b346a1e4e1ce2ad,74d261f5175126cb4f9d187bcc2c99bdf49f616a,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/14e7264e068be5eb612e06b46b346a1e4e1ce2ad/modules/vm-series/variables.tf#L70,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/74d261f5175126cb4f9d187bcc2c99bdf49f616a/modules/vm-series/variables.tf,2021-01-26 10:48:30+01:00,2021-01-26 10:48:30+01:00,3,1,0,1,0,0,0,0,0,0
https://github.com/kubernetes/k8s.io,375,infra/aws/terraform/prow-build-cluster/variables.tf,infra/aws/terraform/prow-build-cluster/variables.tf,0,# todo,# TODO: remove once applied on prod,"# TODO: remove once applied on prod 
 # This variable is required in the installation process as we cannot 
 # assume a role that is yet to be created.","variable ""assume_role"" {
  type        = bool
  description = ""Assumes role to get access to EKS cluster after provisioning.""
  default     = true
}
",variable,"variable ""assume_role"" {
  type        = bool
  description = ""Assumes role to get access to EKS cluster after provisioning.""
  default     = true
}
",variable,29,,db6b5a5b1de7cac3d8f08c4b44a6a19b36855b58,ea61b21c1c9188ea5df32e9b08d1a51fe706715a,https://github.com/kubernetes/k8s.io/blob/db6b5a5b1de7cac3d8f08c4b44a6a19b36855b58/infra/aws/terraform/prow-build-cluster/variables.tf#L29,https://github.com/kubernetes/k8s.io/blob/ea61b21c1c9188ea5df32e9b08d1a51fe706715a/infra/aws/terraform/prow-build-cluster/variables.tf,2023-04-26 13:27:36+02:00,2023-04-26 15:07:51+02:00,3,1,0,1,0,0,0,1,0,0
https://github.com/compiler-explorer/infra,73,terraform/audit.tf,terraform/audit.tf,0,fix,# until https://github.com/terraform-providers/terraform-provider-aws/pull/5448 is fixed,# until https://github.com/terraform-providers/terraform-provider-aws/pull/5448 is fixed,"resource ""aws_cloudtrail"" ""audit"" {
  name                          = ""ce-audit""
  s3_bucket_name                = aws_s3_bucket.cloudtrail.id
  include_global_service_events = true
  tags                          = {
    Site = ""CompilerExplorer""
  }
  event_selector {
    include_management_events = true
    read_write_type = ""All""
  }
  is_multi_region_trail         = true
  enable_log_file_validation    = true
  depends_on = [aws_s3_bucket_policy.cloudtrail-bucket-policy]
  # until https://github.com/terraform-providers/terraform-provider-aws/pull/5448 is fixed
  lifecycle {
    ignore_changes = [event_selector]
  }
}
",resource,"resource ""aws_cloudtrail"" ""audit"" {
  name                          = ""ce-audit""
  s3_bucket_name                = aws_s3_bucket.cloudtrail.id
  include_global_service_events = true
  event_selector {
    include_management_events = true
    read_write_type = ""All""
  }
  is_multi_region_trail         = true
  enable_log_file_validation    = true
  depends_on = [aws_s3_bucket_policy.cloudtrail-bucket-policy]
}
",resource,17,,65e8fa11ba2143385880fac5c502a533ecd740b8,5e1f4cf94354e86ad45b271dcfb7be3423e20a3c,https://github.com/compiler-explorer/infra/blob/65e8fa11ba2143385880fac5c502a533ecd740b8/terraform/audit.tf#L17,https://github.com/compiler-explorer/infra/blob/5e1f4cf94354e86ad45b271dcfb7be3423e20a3c/terraform/audit.tf,2020-06-22 22:30:12-05:00,2022-10-08 16:41:47-05:00,5,1,0,0,1,0,0,0,1,0
https://github.com/camptocamp/devops-stack,96,examples/eks-aws/main.tf,examples/eks/main.tf,1,todo,"/*Available only in provider hashicorp/aws >= v4.0.0 resource ""random_string"" ""admin_password"" {   length  = 25   special = false } # TODO create an output for this password  resource ""aws_cognito_user"" ""admin"" {   user_pool_id = aws_cognito_user_pool.admin.id   username = admin   password = random_string.admin_password.result    message_action = SUPRESS # Do not send welcome message since password is hardcoded and email is non-existant    attributes = {     email = ""admin@example.org""     email_verified = true     terraform = true   } }  resource ""aws_cognito_user_in_group"" ""add_admin_argocd_admin"" {   user_pool_id = aws_cognito_user_pool.admin.id   group_name   = aws_cognito_user_group.argocd_admin_group.name   username     = aws_cognito_user.admin.username }*/","/* Available only in provider hashicorp/aws >= v4.0.0 
 resource ""random_string"" ""admin_password"" { 
 length  = 25 
 special = false 
 } # TODO create an output for this password 
  
 resource ""aws_cognito_user"" ""admin"" { 
 user_pool_id = aws_cognito_user_pool.admin.id 
 username = admin 
 password = random_string.admin_password.result 
  
 message_action = SUPRESS # Do not send welcome message since password is hardcoded and email is non-existant 
  
 attributes = { 
 email = ""admin@example.org"" 
 email_verified = true 
 terraform = true 
 } 
 } 
  
 resource ""aws_cognito_user_in_group"" ""add_admin_argocd_admin"" { 
 user_pool_id = aws_cognito_user_pool.admin.id 
 group_name   = aws_cognito_user_group.argocd_admin_group.name 
 username     = aws_cognito_user.admin.username 
 } 
 */ ","module ""eks"" {
  source = ""git::https://github.com/camptocamp/devops-stack.git//modules/eks/aws?ref=v1""

  cluster_name = ""gh-v1-cluster""
  base_domain  = ""is-sandbox.camptocamp.com""
  # cluster_version = ""1.22""

  vpc_id         = module.vpc.vpc_id
  vpc_cidr_block = module.vpc.vpc_cidr_block

  private_subnet_ids = module.vpc.private_subnets
  public_subnet_ids  = module.vpc.public_subnets

  cluster_endpoint_public_access_cidrs = [""0.0.0.0/0""]

  node_groups = {
    ""${module.eks.cluster_name}-main"" = {
      instance_type     = ""m5a.large""
      min_size          = 2
      max_size          = 3
      desired_size      = 2
      target_group_arns = module.eks.nlb_target_groups
    },
  }

  create_public_nlb = true
}
",module,"module ""eks"" {
  source = ""git::https://github.com/camptocamp/devops-stack-module-cluster-eks?ref=v2.0.2""

  cluster_name       = local.cluster_name
  kubernetes_version = local.kubernetes_version
  base_domain        = local.base_domain

  vpc_id             = module.vpc.vpc_id
  private_subnet_ids = module.vpc.private_subnets
  public_subnet_ids  = module.vpc.public_subnets

  cluster_endpoint_public_access_cidrs = [""0.0.0.0/0""]

  node_groups = {
    ""${module.eks.cluster_name}-main"" = {
      instance_type     = ""m5a.large""
      min_size          = 2
      max_size          = 3
      desired_size      = 2
      target_group_arns = module.eks.nlb_target_groups
    },
  }

  create_public_nlb = true
}
",module,41,,23a76321726eca45b1852f9cbb9a5a46dd17c13e,9f09347de36de8f813051eae5c981dc8cae5c393,https://github.com/camptocamp/devops-stack/blob/23a76321726eca45b1852f9cbb9a5a46dd17c13e/examples/eks-aws/main.tf#L41,https://github.com/camptocamp/devops-stack/blob/9f09347de36de8f813051eae5c981dc8cae5c393/examples/eks/main.tf,2023-04-03 16:40:29+02:00,2023-08-18 14:48:32+02:00,4,1,1,1,0,0,0,0,0,0
https://github.com/nasa/cumulus,32,tf-modules/cumulus/archive.tf,tf-modules/cumulus/archive.tf,0,todo,# TODO We need to figure out how to make this dynamic,# TODO We need to figure out how to make this dynamic,"module ""archive"" {
  source = ""../archive""

  prefix = var.prefix

  permissions_boundary_arn = var.permissions_boundary_arn

  ecs_cluster_name = aws_ecs_cluster.default.name

  elasticsearch_domain_arn        = var.elasticsearch_domain_arn
  elasticsearch_hostname          = var.elasticsearch_hostname
  elasticsearch_security_group_id = var.elasticsearch_security_group_id

  ems_host = ""change-ems-host""

  system_bucket     = var.system_bucket
  public_buckets    = var.public_buckets
  protected_buckets = var.protected_buckets
  private_buckets   = var.private_buckets

  vpc_id            = var.vpc_id
  lambda_subnet_ids = var.lambda_subnet_ids

  cmr_client_id   = var.cmr_client_id
  cmr_environment = var.cmr_environment
  cmr_provider    = var.cmr_provider
  cmr_username    = var.cmr_username
  cmr_password    = var.cmr_password

  urs_url             = ""https://uat.urs.earthdata.nasa.gov""
  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password

  token_secret = var.token_secret

  dynamo_tables = var.dynamo_tables

  api_port = var.archive_api_port

  schedule_sf_function_arn      = data.aws_lambda_function.schedule_sf.arn
  message_consumer_function_arn = data.aws_lambda_function.message_consumer.arn
  # TODO This should eventually come from the ingest module
  kinesis_inbound_event_logger = var.kinesis_inbound_event_logger

  # TODO We need to figure out how to make this dynamic
  background_queue_name = ""backgroundProcessing""

  distribution_api_id = module.distribution.rest_api_id
  distribution_url    = module.distribution.distribution_url

  users = var.archive_api_users
}
",module,"module ""archive"" {
  source = ""../archive""

  prefix = var.prefix

  api_url = var.archive_api_url

  deploy_to_ngap = var.deploy_to_ngap

  permissions_boundary_arn = var.permissions_boundary_arn

  lambda_processing_role_arn = aws_iam_role.lambda_processing.arn

  ecs_cluster_name = aws_ecs_cluster.default.name

  elasticsearch_domain_arn        = var.elasticsearch_domain_arn
  elasticsearch_hostname          = var.elasticsearch_hostname
  elasticsearch_security_group_id = var.elasticsearch_security_group_id

  ems_host              = var.ems_host
  ems_port              = var.ems_port
  ems_path              = var.ems_path
  ems_datasource        = var.ems_datasource
  ems_private_key       = var.ems_private_key
  ems_provider          = var.ems_provider
  ems_retention_in_days = var.ems_retention_in_days
  ems_submit_report     = var.ems_submit_report
  ems_username          = var.ems_username

  es_index_shards        = var.es_index_shards
  es_request_concurrency = var.es_request_concurrency

  system_bucket     = var.system_bucket
  public_buckets    = local.public_bucket_names
  protected_buckets = local.protected_bucket_names
  private_buckets   = local.private_bucket_names

  vpc_id            = var.vpc_id
  lambda_subnet_ids = var.lambda_subnet_ids

  cmr_client_id      = var.cmr_client_id
  cmr_environment    = var.cmr_environment
  cmr_oauth_provider = var.cmr_oauth_provider
  cmr_provider       = var.cmr_provider
  cmr_username       = var.cmr_username
  cmr_password       = var.cmr_password

  launchpad_api         = var.launchpad_api
  launchpad_certificate = var.launchpad_certificate
  launchpad_passphrase  = var.launchpad_passphrase

  saml_entity_id                  = var.saml_entity_id
  saml_assertion_consumer_service = var.saml_assertion_consumer_service
  saml_idp_login                  = var.saml_idp_login
  saml_launchpad_metadata_url     = var.saml_launchpad_metadata_url

  urs_url             = var.urs_url
  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password

  token_secret = var.token_secret

  dynamo_tables = var.dynamo_tables

  api_port = var.archive_api_port
  private_archive_api_gateway = var.private_archive_api_gateway
  api_gateway_stage = var.api_gateway_stage

  schedule_sf_function_arn                         = module.ingest.schedule_sf_lambda_function_arn
  manual_consumer_function_arn                     = module.ingest.manual_consumer_lambda_function_arn
  message_consumer_function_arn                    = module.ingest.message_consumer_lambda_function_arn
  kinesis_fallback_topic_arn                       = module.ingest.kinesis_fallback_topic_arn
  kinesis_inbound_event_logger_lambda_function_arn = module.ingest.kinesis_inbound_event_logger_lambda_function_arn

  metrics_es_host     = var.metrics_es_host
  metrics_es_password = var.metrics_es_password
  metrics_es_username = var.metrics_es_username

  daily_execution_payload_cleanup_schedule_expression = var.daily_execution_payload_cleanup_schedule_expression
  complete_execution_payload_timeout_disable = var.complete_execution_payload_timeout_disable
  complete_execution_payload_timeout = var.complete_execution_payload_timeout
  non_complete_execution_payload_timeout_disable = var.non_complete_execution_payload_timeout_disable
  non_complete_execution_payload_timeout = var.non_complete_execution_payload_timeout

  background_queue_url = module.ingest.background_queue_url

  distribution_api_id = module.distribution.rest_api_id
  distribution_url    = module.distribution.distribution_url

  users = var.archive_api_users

  oauth_provider   = var.oauth_provider
  oauth_user_group = var.oauth_user_group

  log_destination_arn = var.log_destination_arn

  tags = var.tags
}
",module,55,,ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7,cb6ec4395fab63ae7c8cf3108aa4d80040810f5f,https://github.com/nasa/cumulus/blob/ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7/tf-modules/cumulus/archive.tf#L55,https://github.com/nasa/cumulus/blob/cb6ec4395fab63ae7c8cf3108aa4d80040810f5f/tf-modules/cumulus/archive.tf,2019-08-14 14:23:38-04:00,2020-08-11 16:32:14-04:00,26,1,0,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-project-factory,258,modules/core_project_factory/main.tf,modules/core_project_factory/main.tf,0,#todo,#TODO rename resource in the next breaking change.,"resource ""time_sleep"" ""wait_5_seconds"" { #TODO rename resource in the next breaking change.","resource ""time_sleep"" ""wait_5_seconds"" { #TODO rename resource in the next breaking change.
  count           = var.vpc_service_control_attach_enabled ? 1 : 0
  depends_on      = [google_access_context_manager_service_perimeter_resource.service_perimeter_attachment[0], google_project_service.enable_access_context_manager[0]]
  create_duration = var.vpc_service_control_sleep_duration
}
",resource,"resource ""time_sleep"" ""wait_5_seconds"" { #TODO rename resource in the next breaking change.
  count           = var.vpc_service_control_attach_enabled || var.vpc_service_control_attach_dry_run ? 1 : 0
  depends_on      = [google_access_context_manager_service_perimeter_resource.service_perimeter_attachment[0], google_project_service.enable_access_context_manager[0]]
  create_duration = var.vpc_service_control_sleep_duration
}
",resource,111,111.0,086210b2b4cacfe1ab0463a9cfb065da8f902f41,cfd7f3f15e0866fe09cc5ec4a2f8e94398c773d9,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/086210b2b4cacfe1ab0463a9cfb065da8f902f41/modules/core_project_factory/main.tf#L111,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/cfd7f3f15e0866fe09cc5ec4a2f8e94398c773d9/modules/core_project_factory/main.tf#L111,2022-11-15 17:28:37-06:00,2024-05-17 18:11:28+00:00,5,0,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,101,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf,0,# todo,# TODO: reconsider duplication,login_network_storage  = var.network_storage # TODO: reconsider duplication,"module ""slurm_files"" {
  source = ""github.com/SchedMD/slurm-gcp.git//terraform/slurm_cluster/modules/slurm_files?ref=6.2.0""

  project_id         = var.project_id
  slurm_cluster_name = local.slurm_cluster_name
  bucket_dir         = var.bucket_dir
  bucket_name        = local.bucket_name

  slurmdbd_conf_tpl = var.slurmdbd_conf_tpl
  slurm_conf_tpl    = var.slurm_conf_tpl
  cgroup_conf_tpl   = var.cgroup_conf_tpl
  cloud_parameters  = var.cloud_parameters
  # TODO: resolve cyclic dependency and use 
  # try(module.slurm_controller_instance.cloudsql_secret, null)
  cloudsql_secret = null

  controller_startup_scripts         = local.ghpc_startup_script_controller
  controller_startup_scripts_timeout = var.controller_startup_scripts_timeout
  compute_startup_scripts            = local.ghpc_startup_script_compute
  compute_startup_scripts_timeout    = var.compute_startup_scripts_timeout
  login_startup_scripts              = local.ghpc_startup_script_login
  login_startup_scripts_timeout      = var.login_startup_scripts_timeout

  enable_devel         = var.enable_devel
  enable_debug_logging = var.enable_debug_logging
  extra_logging_flags  = var.extra_logging_flags

  enable_bigquery_load = var.enable_bigquery_load
  epilog_scripts       = var.epilog_scripts
  prolog_scripts       = var.prolog_scripts

  disable_default_mounts = var.disable_default_mounts
  network_storage        = var.network_storage
  login_network_storage  = var.network_storage # TODO: reconsider duplication

  partitions  = values(module.slurm_partition)[*]
  nodeset     = values(module.slurm_nodeset)[*]
  nodeset_tpu = values(module.slurm_nodeset_tpu)[*]

  depends_on = [module.bucket]
}
",module,"module ""slurm_files"" {
  source = ""github.com/SchedMD/slurm-gcp.git//terraform/slurm_cluster/modules/slurm_files?ref=6.2.0""

  project_id         = var.project_id
  slurm_cluster_name = local.slurm_cluster_name
  bucket_dir         = var.bucket_dir
  bucket_name        = local.bucket_name

  slurmdbd_conf_tpl = var.slurmdbd_conf_tpl
  slurm_conf_tpl    = var.slurm_conf_tpl
  cgroup_conf_tpl   = var.cgroup_conf_tpl
  cloud_parameters  = var.cloud_parameters
  # TODO: resolve cyclic dependency and use 
  # try(module.slurm_controller_instance.cloudsql_secret, null)
  cloudsql_secret = null

  controller_startup_scripts         = local.ghpc_startup_script_controller
  controller_startup_scripts_timeout = var.controller_startup_scripts_timeout
  compute_startup_scripts            = local.ghpc_startup_script_compute
  compute_startup_scripts_timeout    = var.compute_startup_scripts_timeout
  login_startup_scripts              = local.ghpc_startup_script_login
  login_startup_scripts_timeout      = var.login_startup_scripts_timeout

  enable_devel         = var.enable_devel
  enable_debug_logging = var.enable_debug_logging
  extra_logging_flags  = var.extra_logging_flags

  enable_bigquery_load = var.enable_bigquery_load
  epilog_scripts       = var.epilog_scripts
  prolog_scripts       = var.prolog_scripts

  disable_default_mounts = var.disable_default_mounts
  network_storage        = var.network_storage
  login_network_storage  = var.login_network_storage

  partitions  = values(module.slurm_partition)[*]
  nodeset     = values(module.slurm_nodeset)[*]
  nodeset_tpu = values(module.slurm_nodeset_tpu)[*]

  depends_on = [module.bucket]
}
",module,122,,82199854e24ac10e7293605c5c4eae45748e8c99,6b26fd7c86bc829718789d9530cf861e1e130e21,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/82199854e24ac10e7293605c5c4eae45748e8c99/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf#L122,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/6b26fd7c86bc829718789d9530cf861e1e130e21/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf,2023-11-09 19:50:03+00:00,2023-11-14 01:21:30+00:00,2,1,1,1,0,0,1,0,0,0
https://github.com/alphagov/govuk-aws,321,terraform/projects/infra-security-groups/publishing-api.tf,terraform/projects/infra-security-groups/publishing-api.tf,0,# todo,# TODO: application machines need access to publishing-api - create an application,"# TODO: application machines need access to publishing-api - create an application 
 # group that needs access?","resource ""aws_security_group_rule"" ""allow_management_to_publishing-api_https"" {
  type      = ""ingress""
  from_port = 443
  to_port   = 443
  protocol  = ""tcp""

  # Which security group is the rule assigned to
  security_group_id = ""${aws_security_group.publishing-api_elb_external.id}""

  # Which security group can use this rule
  source_security_group_id = ""${aws_security_group.management.id}""
}
",resource,the block associated got renamed or deleted,,84,,b212a5508ed19a405a50c07afee2d3d66c55a60b,7fd13330f8d6238e108f76ce76a76a28a99caaaf,https://github.com/alphagov/govuk-aws/blob/b212a5508ed19a405a50c07afee2d3d66c55a60b/terraform/projects/infra-security-groups/publishing-api.tf#L84,https://github.com/alphagov/govuk-aws/blob/7fd13330f8d6238e108f76ce76a76a28a99caaaf/terraform/projects/infra-security-groups/publishing-api.tf,2017-09-15 17:07:50+01:00,2018-01-02 17:41:32+00:00,4,1,1,1,0,1,1,0,0,0
https://github.com/uyuni-project/sumaform,1418,backend_modules/libvirt/base/main.tf,backend_modules/libvirt/base/main.tf,0,fix,"# WORKAROUND: temporary fix ""cannot write to stream: No space left on device"", which is in fact caused by a 404 (https://progress.opensuse.org/issues/95512)","opensuse153-ci-pr  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.opensuse.org""}/repositories/systemsmanagement:/sumaform:/images:/libvirt/images/opensuse153-ci-pr.x86_64.qcow2"" 
 # opensuse153o = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.opensuse.org""}/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2"" 
 # WORKAROUND: temporary fix ""cannot write to stream: No space left on device"", which is in fact caused by a 404 (https://progress.opensuse.org/issues/95512)","locals {
  images_used = var.use_shared_resources ? [] : var.images
  image_urls = {
    centos6o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.centos.org""}/centos/6/images/CentOS-6-x86_64-GenericCloud.qcow2""
    centos7      = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://github.com""}/moio/sumaform-images/releases/download/4.3.0/centos7.qcow2""
    centos7o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.centos.org""}/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2""
    centos8o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.centos.org""}/centos/8/x86_64/images/CentOS-8-GenericCloud-8.2.2004-20200611.2.x86_64.qcow2""
    opensuse152-ci-pr  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.opensuse.org""}/repositories/systemsmanagement:/sumaform:/images:/libvirt/images/opensuse152-ci-pr.x86_64.qcow2""
    opensuse152o = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.opensuse.org""}/distribution/leap/15.2/appliances/openSUSE-Leap-15.2-JeOS.x86_64-OpenStack-Cloud.qcow2""
    opensuse153-ci-pr  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.opensuse.org""}/repositories/systemsmanagement:/sumaform:/images:/libvirt/images/opensuse153-ci-pr.x86_64.qcow2""
    # opensuse153o = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.opensuse.org""}/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2""
    # WORKAROUND: temporary fix ""cannot write to stream: No space left on device"", which is in fact caused by a 404 (https://progress.opensuse.org/issues/95512)
    opensuse153o = ""https://www.mirrorservice.org/sites/download.opensuse.org/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2""
    sles15       = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles15.x86_64.qcow2""
    sles15o      = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-15-JeOS-GM/SLES15-JeOS.x86_64-15.0-OpenStack-Cloud-GM.qcow2""
    sles15sp1    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles15sp1.x86_64.qcow2""
    sles15sp1o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.suse.de""}/install/SLE-15-SP1-JeOS-QU4/SLES15-SP1-JeOS.x86_64-15.1-OpenStack-Cloud-QU4.qcow2""
    sles15sp2    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles15sp2.x86_64.qcow2""
    sles15sp2o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.suse.de""}/install/SLE-15-SP2-JeOS-GM/SLES15-SP2-JeOS.x86_64-15.2-OpenStack-Cloud-GM.qcow2""
    sles15sp3o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.suse.de""}/install/SLE-15-SP3-JeOS-GM/SLES15-SP3-JeOS.x86_64-15.3-OpenStack-Cloud-GM.qcow2""
    sles11sp4    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles11sp4.x86_64.qcow2""
    sles12       = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12.x86_64.qcow2""
    sles12sp1    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp1.x86_64.qcow2""
    sles12sp2    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp2.x86_64.qcow2""
    sles12sp3    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp3.x86_64.qcow2""
    sles12sp4    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp4.x86_64.qcow2""
    sles12sp4o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-12-SP4-JeOS-GM/SLES12-SP4-JeOS.x86_64-12.4-OpenStack-Cloud-GM.qcow2""
    sles12sp5o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://download.suse.de""}/install/SLE-12-SP5-JeOS-GM/SLES12-SP5-JeOS.x86_64-12.5-OpenStack-Cloud-GM.qcow2""
    ubuntu1604o  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud-images.ubuntu.com""}/xenial/current/xenial-server-cloudimg-amd64-disk1.img""
    ubuntu1804   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://github.com""}/moio/sumaform-images/releases/download/4.4.0/ubuntu1804.qcow2""
    ubuntu1804o  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud-images.ubuntu.com""}/bionic/current/bionic-server-cloudimg-amd64.img""
    ubuntu2004o  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud-images.ubuntu.com""}/focal/current/focal-server-cloudimg-amd64.img""
    debian9o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.debian.org""}/images/cloud/OpenStack/current-9/debian-9-openstack-amd64.qcow2""
    debian10o    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.debian.org""}/images/cloud/OpenStack/current-10/debian-10-openstack-amd64.qcow2""
  }
  pool               = lookup(var.provider_settings, ""pool"", ""default"")
  network_name       = lookup(var.provider_settings, ""network_name"", ""default"")
  bridge             = lookup(var.provider_settings, ""bridge"", null)
  additional_network = lookup(var.provider_settings, ""additional_network"", null)
}
",locals,"locals {
  images_used = var.use_shared_resources ? [] : var.images
  image_urls = {
    centos6o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.centos.org""}/centos/6/images/CentOS-6-x86_64-GenericCloud.qcow2""
    centos7      = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://github.com""}/moio/sumaform-images/releases/download/4.3.0/centos7.qcow2""
    centos7o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.centos.org""}/centos/7/images/CentOS-7-x86_64-GenericCloud.qcow2""
    centos8o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.centos.org""}/centos/8/x86_64/images/CentOS-8-GenericCloud-8.2.2004-20200611.2.x86_64.qcow2""
    opensuse152-ci-pr  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.opensuse.org""}/repositories/systemsmanagement:/sumaform:/images:/libvirt/images/opensuse152-ci-pr.x86_64.qcow2""
    opensuse152o = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.opensuse.org""}/distribution/leap/15.2/appliances/openSUSE-Leap-15.2-JeOS.x86_64-OpenStack-Cloud.qcow2""
    opensuse153-ci-pr  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.opensuse.org""}/repositories/systemsmanagement:/sumaform:/images:/libvirt/images/opensuse153-ci-pr.x86_64.qcow2""
    opensuse153o = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.opensuse.org""}/distribution/leap/15.3/appliances/openSUSE-Leap-15.3-JeOS.x86_64-OpenStack-Cloud.qcow2""
    sles15       = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles15.x86_64.qcow2""
    sles15o      = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-15-JeOS-GM/SLES15-JeOS.x86_64-15.0-OpenStack-Cloud-GM.qcow2""
    sles15sp1    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles15sp1.x86_64.qcow2""
    sles15sp1o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-15-SP1-JeOS-QU4/SLES15-SP1-JeOS.x86_64-15.1-OpenStack-Cloud-QU4.qcow2""
    sles15sp2    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles15sp2.x86_64.qcow2""
    sles15sp2o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-15-SP2-JeOS-GM/SLES15-SP2-JeOS.x86_64-15.2-OpenStack-Cloud-GM.qcow2""
    sles15sp3o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-15-SP3-JeOS-GM/SLES15-SP3-JeOS.x86_64-15.3-OpenStack-Cloud-GM.qcow2""
    sles11sp4    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles11sp4.x86_64.qcow2""
    sles12       = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12.x86_64.qcow2""
    sles12sp1    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp1.x86_64.qcow2""
    sles12sp2    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp2.x86_64.qcow2""
    sles12sp3    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp3.x86_64.qcow2""
    sles12sp4    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/ibs/Devel:/Galaxy:/Terraform:/Images/images/sles12sp4.x86_64.qcow2""
    sles12sp4o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-12-SP4-JeOS-GM/SLES12-SP4-JeOS.x86_64-12.4-OpenStack-Cloud-GM.qcow2""
    sles12sp5o   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""http://download.suse.de""}/install/SLE-12-SP5-JeOS-GM/SLES12-SP5-JeOS.x86_64-12.5-OpenStack-Cloud-GM.qcow2""
    ubuntu1604o  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud-images.ubuntu.com""}/xenial/current/xenial-server-cloudimg-amd64-disk1.img""
    ubuntu1804   = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://github.com""}/moio/sumaform-images/releases/download/4.4.0/ubuntu1804.qcow2""
    ubuntu1804o  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud-images.ubuntu.com""}/bionic/current/bionic-server-cloudimg-amd64.img""
    ubuntu2004o  = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud-images.ubuntu.com""}/focal/current/focal-server-cloudimg-amd64.img""
    debian9o     = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.debian.org""}/images/cloud/OpenStack/current-9/debian-9-openstack-amd64.qcow2""
    debian10o    = ""${var.use_mirror_images ? ""http://${var.mirror}"" : ""https://cloud.debian.org""}/images/cloud/OpenStack/current-10/debian-10-openstack-amd64.qcow2""
  }
  pool               = lookup(var.provider_settings, ""pool"", ""default"")
  network_name       = lookup(var.provider_settings, ""network_name"", ""default"")
  bridge             = lookup(var.provider_settings, ""bridge"", null)
  additional_network = lookup(var.provider_settings, ""additional_network"", null)
}
",locals,13,,a79ce678f697c52365b7a5a485e482ef843c6bbe,5d343d967b407b5f1645e34090646066c7fe2fed,https://github.com/uyuni-project/sumaform/blob/a79ce678f697c52365b7a5a485e482ef843c6bbe/backend_modules/libvirt/base/main.tf#L13,https://github.com/uyuni-project/sumaform/blob/5d343d967b407b5f1645e34090646066c7fe2fed/backend_modules/libvirt/base/main.tf,2021-07-21 16:20:47+02:00,2021-08-10 12:11:54+02:00,2,1,1,0,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,24,main.tf,main.tf,0,#todo,"#TODO: terraform 0.12 will enable ""time_partitioning ? time_partitioning_is_required : null"" (https://github.com/hashicorp/terraform/issues/17968)","#TODO: terraform 0.12 will enable ""time_partitioning ? time_partitioning_is_required : null"" (https://github.com/hashicorp/terraform/issues/17968)","resource ""google_bigquery_table"" ""main"" {
  dataset_id = ""${google_bigquery_dataset.main.dataset_id}""
  table_id   = ""${var.table_id}""
  project    = ""${var.project_id}""

  #TODO: terraform 0.12 will enable ""time_partitioning ? time_partitioning_is_required : null"" (https://github.com/hashicorp/terraform/issues/17968)
  time_partitioning {
    type = ""${var.time_partitioning}""
  }

  labels = ""${var.table_labels}""

  schema = ""${file(""${var.schema_file}"")}""
}
",resource,"resource ""google_bigquery_table"" ""main"" {
  dataset_id = ""${google_bigquery_dataset.main.dataset_id}""
  table_id   = ""${var.table_id}""
  project    = ""${var.project_id}""

  time_partitioning {
    type = ""${var.time_partitioning}""
  }

  labels = ""${var.table_labels}""

  schema = ""${file(""${var.schema_file}"")}""
}
",resource,48,,7f922f7e9df197df38c9b09dfa6e3614d71f19f5,a7966ae4afc7bb73842fb9fd6f0f708b359cf36e,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/7f922f7e9df197df38c9b09dfa6e3614d71f19f5/main.tf#L48,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/a7966ae4afc7bb73842fb9fd6f0f708b359cf36e/main.tf,2019-01-16 18:10:54-05:00,2019-01-28 12:41:47-05:00,2,1,0,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1531,blueprints/data-solutions/vertex-mlops/vertex.tf,blueprints/data-solutions/vertex-mlops/vertex.tf,0,fix,# Remove once terraform-provider-google/issues/9164 is fixed,# Remove once terraform-provider-google/issues/9164 is fixed,"resource ""google_notebooks_instance"" ""playground"" {
  for_each     = { for k, v in var.notebooks : k => v if v.type == ""USER_MANAGED"" }
  name         = ""${var.prefix}-${each.key}""
  location     = format(""%s-%s"", var.region, ""b"")
  machine_type = var.notebooks[each.key].machine_type
  project      = module.project.project_id

  container_image {
    repository = ""gcr.io/deeplearning-platform-release/base-cpu""
    tag        = ""latest""
  }

  install_gpu_driver = true
  boot_disk_type     = ""PD_SSD""
  boot_disk_size_gb  = 110
  disk_encryption    = try(local.service_encryption_keys.notebooks != null, false) ? ""CMEK"" : null
  kms_key            = try(local.service_encryption_keys.notebooks, null)

  no_public_ip    = var.notebooks[each.key].internal_ip_only
  no_proxy_access = false

  network = local.vpc
  subnet  = local.subnet

  instance_owners = try(tolist(var.notebooks[each.key].owner), null)
  service_account = module.service-account-notebook.email

  metadata = {
    notebook-disable-nbconvert = ""false""
    notebook-disable-downloads = ""false""
    notebook-disable-terminal  = ""false""
    notebook-disable-root      = ""true""
  }

  # Remove once terraform-provider-google/issues/9164 is fixed
  lifecycle {
    ignore_changes = [disk_encryption, kms_key]
  }

  #TODO Uncomment once terraform-provider-google/issues/9273 is fixed
  # tags = [""ssh""]
  depends_on = [
    google_project_iam_member.shared_vpc,
  ]
}
",resource,"resource ""google_notebooks_instance"" ""playground"" {
  for_each     = { for k, v in var.notebooks : k => v if v.type == ""USER_MANAGED"" }
  name         = ""${var.prefix}-${each.key}""
  location     = ""${var.region}-b""
  machine_type = var.notebooks[each.key].machine_type
  project      = module.project.project_id

  container_image {
    repository = ""gcr.io/deeplearning-platform-release/base-cpu""
    tag        = ""latest""
  }

  install_gpu_driver = true
  boot_disk_type     = ""PD_SSD""
  boot_disk_size_gb  = 110
  disk_encryption    = var.service_encryption_keys.notebooks != null ? ""CMEK"" : null
  kms_key            = var.service_encryption_keys.notebooks

  no_public_ip    = var.notebooks[each.key].internal_ip_only
  no_proxy_access = false

  network = local.vpc
  subnet  = local.subnet

  instance_owners = try(tolist(var.notebooks[each.key].owner), null)
  service_account = module.service-account-notebook.email
  service_account_scopes = [
    ""https://www.googleapis.com/auth/cloud-platform"",
    ""https://www.googleapis.com/auth/userinfo.email"",
  ]


  metadata = {
    notebook-disable-nbconvert = ""false""
    notebook-disable-downloads = ""false""
    notebook-disable-terminal  = ""false""
    notebook-disable-root      = ""true""
  }

  # Remove once terraform-provider-google/issues/9164 is fixed
  lifecycle {
    ignore_changes = [disk_encryption, kms_key]
  }

  #TODO Uncomment once terraform-provider-google/issues/9273 is fixed
  # tags = [""ssh""]
  depends_on = [
    google_project_iam_member.shared_vpc,
  ]
}
",resource,117,121.0,edf67fc5d040acfcd852a24a62d04acd2e4038f2,b902b1dab98fbe1d3cedc0b341838e827de5897b,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/edf67fc5d040acfcd852a24a62d04acd2e4038f2/blueprints/data-solutions/vertex-mlops/vertex.tf#L117,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b902b1dab98fbe1d3cedc0b341838e827de5897b/blueprints/data-solutions/vertex-mlops/vertex.tf#L121,2023-04-18 17:32:15+02:00,2024-02-13 07:40:31+01:00,3,0,1,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,614,examples/data-solutions/data-platform-foundations/variables.tf,examples/data-solutions/data-platform-foundations/variables.tf,0,#todo,#TODO hardcoded,#TODO hardcoded,"variable ""composer_config"" {
  type = object({
    node_count = number
    #TODO Move to network
    ip_range_cloudsql   = string
    ip_range_gke_master = string
    ip_range_web_server = string
    #TODO hardcoded
    project_policy_boolean = map(bool)
    region                 = string
    ip_allocation_policy = object({
      use_ip_aliases                = string
      cluster_secondary_range_name  = string
      services_secondary_range_name = string
    })
    #TODO Add Env variables, Airflow version
  })
  default = {
    node_count             = 3
    ip_range_cloudsql      = ""10.20.10.0/24""
    ip_range_gke_master    = ""10.20.11.0/28""
    ip_range_web_server    = ""10.20.11.16/28""
    project_policy_boolean = null
    region                 = ""europe-west1""
    ip_allocation_policy = {
      use_ip_aliases                = ""true""
      cluster_secondary_range_name  = ""pods""
      services_secondary_range_name = ""services""
    }
  }
}
",variable,"variable ""composer_config"" {
  type = object({
    node_count      = number
    airflow_version = string
    env_variables   = map(string)
  })
  default = {
    node_count      = 3
    airflow_version = ""composer-1.17.5-airflow-2.1.4""
    env_variables   = {}
  }
}
",variable,37,,2e560407c118e7b7abc32f8ac1788a3f48563f21,d8bad5779036aa31639e4611e4935287fc79a4bc,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2e560407c118e7b7abc32f8ac1788a3f48563f21/examples/data-solutions/data-platform-foundations/variables.tf#L37,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d8bad5779036aa31639e4611e4935287fc79a4bc/examples/data-solutions/data-platform-foundations/variables.tf,2022-02-07 17:51:06+01:00,2022-02-07 21:28:54+01:00,2,1,0,1,0,1,0,0,0,0
https://github.com/alphagov/govuk-aws,1064,terraform/projects/app-backend/main.tf,terraform/projects/app-backend/main.tf,0,#todo,#TODO: create new security group with alb name,#TODO: create new security group with alb name,"module ""backend_internal_alb"" {
  source                           = ""../../modules/aws/lb""
  name                             = ""${var.stackname}-backend-internal""
  internal                         = true
  vpc_id                           = ""${data.terraform_remote_state.infra_vpc.vpc_id}""
  access_logs_bucket_name          = ""${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}""
  access_logs_bucket_prefix        = ""elb/${var.stackname}-backend-internal-alb""
  listener_certificate_domain_name = ""${var.elb_internal_certname}""
  listener_action                  = ""${map(""HTTPS:443"", ""HTTP:80"")}""
  subnets                          = [""${data.terraform_remote_state.infra_networking.private_subnet_ids}""]

  #TODO: create new security group with alb name
  security_groups = [""${data.terraform_remote_state.infra_security_groups.sg_backend_elb_internal_id}""]
  alarm_actions   = [""${data.terraform_remote_state.infra_monitoring.sns_topic_cloudwatch_alarms_arn}""]
  default_tags    = ""${map(""Project"", var.stackname, ""aws_migration"", ""backend"", ""aws_environment"", var.aws_environment)}""
}
",module,,,131,0.0,75a0ad6f9093136bed47e62609c049e1bc944505,241558af7d6786415e64eea48e193f23518625c2,https://github.com/alphagov/govuk-aws/blob/75a0ad6f9093136bed47e62609c049e1bc944505/terraform/projects/app-backend/main.tf#L131,https://github.com/alphagov/govuk-aws/blob/241558af7d6786415e64eea48e193f23518625c2/terraform/projects/app-backend/main.tf#L0,2020-05-06 17:19:09+01:00,2023-05-10 15:30:22+01:00,9,2,0,1,0,1,1,0,0,0
https://github.com/kubernetes/k8s.io,2,clusters/k8s-infra-dev-cluster-turnup/test-us/10-cluster-configuration.tf,infra/gcp/clusters/_example/test-us/10-cluster-configuration.tf,1,// todo,// TODO: we should turn this on,enabled = false // TODO: we should turn this on,"resource ""google_container_cluster"" ""cluster"" {
  name     = local.cluster_name
  location = local.cluster_location

  provider = google-beta
  project  = data.google_project.project.id

  // GKE clusters are critical objects and should not be destroyed
  // IMPORTANT: should be true on production cluster
  lifecycle {
    prevent_destroy = false
  }

  // Network config
  network = ""default""
  ip_allocation_policy {
    use_ip_aliases    = true
    create_subnetwork = true
  }

  // Start with a single node, because we're going to delete the default pool
  initial_node_count = 1

  // Removes the default node pool, so we can custom create them as separate
  // objects
  remove_default_node_pool = true

  // Disable local and certificate auth
  master_auth {
    username = """"
    password = """"

    client_certificate_config {
      issue_client_certificate = false
    }
  }

  // Enable google-groups for RBAC
  authenticator_groups_config {
    security_group = ""gke-security-groups@kubernetes.io""
  }

  // Enable workload identity for GCP IAM
  workload_identity_config {
    identity_namespace = ""${data.google_project.project.id}.svc.id.goog""
  }

  // Enable Stackdriver Kubernetes Monitoring
  logging_service    = ""logging.googleapis.com/kubernetes""
  monitoring_service = ""monitoring.googleapis.com/kubernetes""

  // Set maintenance time
  maintenance_policy {
    daily_maintenance_window {
      start_time = ""11:00"" // (in UTC), 03:00 PST
    }
  }

  // Restrict master to Google IP space; use Cloud Shell to access
  master_authorized_networks_config {
  }

  // Enable GKE Usage Metering
  resource_usage_export_config {
    enable_network_egress_metering = true
    bigquery_destination {
      dataset_id = google_bigquery_dataset.usage_metering.dataset_id
    }
  }

  // Enable GKE Network Policy
  network_policy {
    enabled  = true
    provider = ""CALICO""
  }

  // Configure cluster addons
  addons_config {
    horizontal_pod_autoscaling {
      disabled = false
    }
    http_load_balancing {
      disabled = false
    }
    network_policy_config {
      disabled = false
    }
  }

  // Enable PodSecurityPolicy enforcement
  pod_security_policy_config {
    enabled = false // TODO: we should turn this on
  }

  // Enable VPA
  vertical_pod_autoscaling {
    enabled = true
  }
}
",resource,,,153,0.0,7ccb2fdde41974545511928d0eefc85a4403628c,883efd41e26032afd09a8adcd87769ffb5f06e21,https://github.com/kubernetes/k8s.io/blob/7ccb2fdde41974545511928d0eefc85a4403628c/clusters/k8s-infra-dev-cluster-turnup/test-us/10-cluster-configuration.tf#L153,https://github.com/kubernetes/k8s.io/blob/883efd41e26032afd09a8adcd87769ffb5f06e21/infra/gcp/clusters/_example/test-us/10-cluster-configuration.tf#L0,2019-09-11 10:40:20-07:00,2021-05-07 13:12:04-04:00,2,2,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,972,examples/data-solutions/data-playground/main.tf,blueprints/data-solutions/data-playground/main.tf,1,# todo,# TODO: Add encryption_key to Vertex AI notebooks as well,"############################################################################### 
 #                         Vertex AI Notebook                                   # 
 ############################################################################### 
 # TODO: Add encryption_key to Vertex AI notebooks as well 
 # TODO: Add shared VPC support","resource ""google_notebooks_instance"" ""playground"" {
  name         = ""data-play-notebook""
  location     = format(""%s-%s"", var.region, ""b"")
  machine_type = ""e2-medium""
  project      = module.project.project_id

  container_image {
    repository = ""gcr.io/deeplearning-platform-release/base-cpu""
    tag        = ""latest""
  }

  install_gpu_driver = true
  boot_disk_type     = ""PD_SSD""
  boot_disk_size_gb  = 110

  no_public_ip    = false
  no_proxy_access = false

  network = module.vpc.network.id
  subnet  = module.vpc.subnets[format(""%s/%s"", var.region, var.vpc_config.subnet_name)].id
}
",resource,"resource ""google_notebooks_instance"" ""playground"" {
  name         = ""${var.prefix}-notebook""
  location     = format(""%s-%s"", var.region, ""b"")
  machine_type = ""e2-medium""
  project      = module.project.project_id

  container_image {
    repository = ""gcr.io/deeplearning-platform-release/base-cpu""
    tag        = ""latest""
  }

  install_gpu_driver = true
  boot_disk_type     = ""PD_SSD""
  boot_disk_size_gb  = 110
  disk_encryption    = try(local.service_encryption_keys.compute != null, false) ? ""CMEK"" : null
  kms_key            = try(local.service_encryption_keys.compute, null)

  no_public_ip    = true
  no_proxy_access = false

  network = local.vpc
  subnet  = local.subnet

  service_account = module.service-account-notebook.email

  #TODO Uncomment once terraform-provider-google/issues/9273 is fixed
  # tags = [""ssh""]
  depends_on = [
    google_project_iam_member.shared_vpc,
  ]
}
",resource,91,,54d805dac04ba4d43b189d22b0751e45ca69f366,07a7be29e3bd898dfdfb8ce39dadf7a663fa2fcf,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/54d805dac04ba4d43b189d22b0751e45ca69f366/examples/data-solutions/data-playground/main.tf#L91,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/07a7be29e3bd898dfdfb8ce39dadf7a663fa2fcf/blueprints/data-solutions/data-playground/main.tf,2022-07-10 09:27:18+02:00,2023-01-19 00:33:31+01:00,10,1,1,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-bootstrap,1,modules/tf_cloudbuild_builder/cb.tf,modules/tf_cloudbuild_builder/cb.tf,0,# todo,# todo(bharathkkb): switch to yaml after https://github.com/hashicorp/terraform-provider-google/issues/9818,# todo(bharathkkb): switch to yaml after https://github.com/hashicorp/terraform-provider-google/issues/9818,"resource ""google_cloudbuild_trigger"" ""build_trigger"" {
  project     = var.project_id
  name        = var.trigger_name
  description = ""Builds a Terraform runner image. Managed by Terraform.""
  source_to_build {
    uri       = var.dockerfile_repo_uri
    ref       = var.dockerfile_repo_ref
    repo_type = var.dockerfile_repo_type
  }

  # todo(bharathkkb): switch to yaml after https://github.com/hashicorp/terraform-provider-google/issues/9818
  build {
    step {
      name = ""gcr.io/cloud-builders/docker""
      args = concat(
        [""build""],
        [for img_tag in local.img_tags_subst : ""--tag=${img_tag}""],
        [""--build-arg=TERRAFORM_VERSION=$${_TERRAFORM_FULL_VERSION}"", "".""]
      )
      dir = var.dockerfile_repo_dir != """" ? var.dockerfile_repo_dir : null
    }
    step {
      name = ""${local.gar_uri}:v$${_TERRAFORM_FULL_VERSION}""
      args = [""version""]
    }
    images      = local.img_tags_subst
    logs_bucket = module.bucket.bucket.url
  }

  substitutions   = local.tags_subst
  service_account = local.cloudbuild_sa

  depends_on = [
    google_artifact_registry_repository_iam_member.push_images,
    google_project_iam_member.logs_writer
  ]
}
",resource,"resource ""google_cloudbuild_trigger"" ""build_trigger"" {
  project     = var.project_id
  location    = var.trigger_location
  name        = var.trigger_name
  description = ""Builds a Terraform runner image. Managed by Terraform.""
  source_to_build {
    uri       = var.dockerfile_repo_uri
    ref       = var.dockerfile_repo_ref
    repo_type = var.dockerfile_repo_type
  }

  # todo(bharathkkb): switch to yaml after https://github.com/hashicorp/terraform-provider-google/issues/9818
  build {
    timeout = var.build_timeout
    step {
      name = ""gcr.io/cloud-builders/docker""
      args = concat(
        [""build""],
        [for img_tag in local.img_tags_subst : ""--tag=${img_tag}""],
        [""--build-arg=TERRAFORM_VERSION=$${_TERRAFORM_FULL_VERSION}"", "".""]
      )
      dir = var.dockerfile_repo_dir != """" ? var.dockerfile_repo_dir : null
    }
    step {
      name = ""${local.gar_uri}:v$${_TERRAFORM_FULL_VERSION}""
      args = [""version""]
    }
    images      = local.img_tags_subst
    logs_bucket = module.bucket.bucket.url

    dynamic ""options"" {
      for_each = var.enable_worker_pool ? [""worker_pool""] : []
      content {
        worker_pool = var.worker_pool_id
      }
    }
  }

  substitutions   = local.tags_subst
  service_account = local.cloudbuild_sa

  depends_on = [
    google_artifact_registry_repository_iam_member.push_images,
    google_project_iam_member.logs_writer
  ]
}
",resource,47,54.0,34120e579528dfb72dddace0485d38efaf9202bd,7c8477bd6137745176d27a4e092c997b0da64149,https://github.com/terraform-google-modules/terraform-google-bootstrap/blob/34120e579528dfb72dddace0485d38efaf9202bd/modules/tf_cloudbuild_builder/cb.tf#L47,https://github.com/terraform-google-modules/terraform-google-bootstrap/blob/7c8477bd6137745176d27a4e092c997b0da64149/modules/tf_cloudbuild_builder/cb.tf#L54,2022-05-27 17:27:37-05:00,2024-05-20 17:22:47-05:00,10,0,0,1,1,0,0,1,0,0
https://github.com/Azure/terraform-azurerm-caf-enterprise-scale,64,resources.role_assignments.tf,resources.role_assignments.tf,0,fix,# This was implemented to fix issue:,"# The following module is used to generate the Role 
 # Assignments for Policy Assignments as needed. 
 # This was implemented to fix issue: 
 # https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/issues/266","module ""role_assignments_for_policy"" {
  for_each = local.es_role_assignments_by_policy_assignment
  source   = ""./modules/role_assignments_for_policy""

  # Mandatory resource attributes
  policy_assignment_id = each.key
  scope_id             = azurerm_management_group_policy_assignment.enterprise_scale[each.key].management_group_id
  principal_id         = azurerm_management_group_policy_assignment.enterprise_scale[each.key].identity[0].principal_id
  role_definition_ids  = each.value

  # Optional resource attributes
  additional_scope_ids = local.empty_list

  # Set explicit dependency on Management Group, Policy Definition, Policy Set Definition, and Policy Assignment deployments
  depends_on = [
    time_sleep.after_azurerm_management_group,
    time_sleep.after_azurerm_policy_definition,
    time_sleep.after_azurerm_policy_set_definition,
    time_sleep.after_azurerm_policy_assignment,
    azurerm_role_assignment.policy_assignment,
  ]

}
",module,"module ""role_assignments_for_policy"" {
  for_each = local.es_role_assignments_by_policy_assignment
  source   = ""./modules/role_assignments_for_policy""

  # Mandatory resource attributes
  policy_assignment_id = each.key
  scope_id             = azurerm_management_group_policy_assignment.enterprise_scale[each.key].management_group_id
  principal_id = (
    lookup(azurerm_management_group_policy_assignment.enterprise_scale[each.key].identity[0], ""type"") == ""UserAssigned""
    ? jsondecode(data.azapi_resource.user_msi[each.key].output).properties.principalId # workarround as azurerm_management_group_policy_assignment does not export the principal_id when using UserAssigned identity
    : azurerm_management_group_policy_assignment.enterprise_scale[each.key].identity[0].principal_id
  )
  role_definition_ids = each.value

  # Optional resource attributes
  additional_scope_ids = local.empty_list

  # Set explicit dependency on Management Group, Policy Definition, Policy Set Definition, and Policy Assignment deployments
  depends_on = [
    time_sleep.after_azurerm_management_group,
    time_sleep.after_azurerm_policy_definition,
    time_sleep.after_azurerm_policy_set_definition,
    time_sleep.after_azurerm_policy_assignment,
    azurerm_role_assignment.policy_assignment,
  ]

}
",module,26,26.0,19c124a47efdbed7c92c8e4f1235885520dfe474,1ac8cb891565c233eb4cc7edb8cf75c8b0d82cf2,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/19c124a47efdbed7c92c8e4f1235885520dfe474/resources.role_assignments.tf#L26,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/1ac8cb891565c233eb4cc7edb8cf75c8b0d82cf2/resources.role_assignments.tf#L26,2022-03-31 16:23:03+01:00,2024-04-11 10:48:07+01:00,5,0,0,0,1,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,591,examples/data-solutions/dp-foundation/main.tf,examples/data-solutions/data-platform-foundations/main.tf,1,fix,#TODO Fix Network name logic,#TODO Fix Network name logic,"locals {
  _networks = {
    load = {
      #TODO Fix Network name logic
      network_name = element(split(""/"", var.network_config.network != null ? var.network_config.network : module.lod-vpc[0].self_link), length(split(""/"", var.network_config.network != null ? var.network_config.network : module.lod-vpc[0].self_link)) - 1)
      network      = var.network_config.network != null ? var.network_config.network : module.lod-vpc[0].self_link
      subnet       = var.network_config.network != null ? var.network_config.vpc_subnet_self_link.load : module.lod-vpc[0].subnet_self_links[""${var.composer_config.region}/${local.prefix_lod}-subnet""]
      subnet_range = var.network_config.vpc_subnet_range.load
    }
    orchestration = {
      #TODO Fix Network name logic
      network_name = element(split(""/"", var.network_config.network != null ? var.network_config.network : module.orc-vpc[0].self_link), length(split(""/"", var.network_config.network != null ? var.network_config.network : module.orc-vpc[0].self_link)) - 1)
      network      = var.network_config.network != null ? var.network_config.network : module.orc-vpc[0].self_link
      subnet       = var.network_config.network != null ? var.network_config.vpc_subnet_self_link.orchestration : module.orc-vpc[0].subnet_self_links[""${var.composer_config.region}/${local.prefix_orc}-subnet""]
      subnet_range = var.network_config.vpc_subnet_range.orchestration
    }
    transformation = {
      #TODO Fix Network name logic
      network_name = element(split(""/"", var.network_config.network != null ? var.network_config.network : module.trf-vpc[0].self_link), length(split(""/"", var.network_config.network != null ? var.network_config.network : module.trf-vpc[0].self_link)) - 1)
      network      = var.network_config.network != null ? var.network_config.network : module.trf-vpc[0].self_link
      subnet       = var.network_config.network != null ? var.network_config.vpc_subnet_self_link.transformation : module.trf-vpc[0].subnet_self_links[""${var.composer_config.region}/${local.prefix_trf}-subnet""]
      subnet_range = var.network_config.vpc_subnet_range.transformation
    }
  }

  _shared_vpc_service_config = var.network_config.network != null ? {
    attach       = true
    host_project = var.network_config.host_project
  } : null

  groups                  = { for k, v in var.groups : k => ""${v}@${var.organization.domain}"" }
  groups_iam              = { for k, v in local.groups : k => ""group:${v}"" }
  service_encryption_keys = var.service_encryption_keys

  # Uncomment this section and assigne comment the previous line

  # service_encryption_keys = {
  #   bq       = module.sec-kms-1.key_ids.bq
  #   composer = module.sec-kms-2.key_ids.composer
  #   dataflow = module.sec-kms-2.key_ids.dataflow
  #   storage  = module.sec-kms-1.key_ids.storage
  #   pubsub   = module.sec-kms-0.key_ids.pubsub
  # }
}
",locals,"locals {
  _networks = {
    load = {
      network_name = coalesce(local._shared_vpc_network, module.lod-vpc[0].name)
      network      = coalesce(var.network_config.network_self_link, module.lod-vpc[0].self_link)
      subnet       = try(var.network_config.subnet_self_links.load, module.lod-vpc[0].subnet_self_links[""${var.location_config.region}/${local.prefix_lod}-subnet""])
    }
    orchestration = {
      network_name = coalesce(local._shared_vpc_network, module.orc-vpc[0].name)
      network      = coalesce(var.network_config.network_self_link, module.orc-vpc[0].self_link)
      subnet       = try(var.network_config.subnet_self_links.orchestration, module.orc-vpc[0].subnet_self_links[""${var.location_config.region}/${local.prefix_orc}-subnet""])
    }
    transformation = {
      network_name = coalesce(local._shared_vpc_network, module.trf-vpc[0].name)
      network      = coalesce(var.network_config.network_self_link, module.trf-vpc[0].self_link)
      subnet       = try(var.network_config.subnet_self_links.transformation, module.trf-vpc[0].subnet_self_links[""${var.location_config.region}/${local.prefix_trf}-subnet""])
    }
  }
  _shared_vpc_network = try(regex(""[a-z]([-a-z0-9]*[a-z0-9])?$"", var.network_config.network_self_link), null)
  _shared_vpc_project = try(regex(""projects/([a-z0-9-]{6,30})"", var.network_config.network_self_link)[0], null)
  _shared_vpc_service_config = var.network_config.network_self_link != null ? {
    attach       = true
    host_project = local._shared_vpc_project
  } : null

  groups                  = { for k, v in var.groups : k => ""${v}@${var.organization.domain}"" }
  groups_iam              = { for k, v in local.groups : k => ""group:${v}"" }
  service_encryption_keys = var.service_encryption_keys

  # To create KMS keys in the common projet: uncomment assignement below and comment assignement above

  # service_encryption_keys = {
  #   bq       = module.sec-kms-1.key_ids.bq
  #   composer = module.sec-kms-2.key_ids.composer
  #   dataflow = module.sec-kms-2.key_ids.dataflow
  #   storage  = module.sec-kms-1.key_ids.storage
  #   pubsub   = module.sec-kms-0.key_ids.pubsub
  # }
}
",locals,27,,b3238969dfef485c4b1d1112242a06d1c3cca3c5,e270adf516d391a1c603b78002fcc931f902b001,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b3238969dfef485c4b1d1112242a06d1c3cca3c5/examples/data-solutions/dp-foundation/main.tf#L27,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/e270adf516d391a1c603b78002fcc931f902b001/examples/data-solutions/data-platform-foundations/main.tf,2022-02-03 14:25:46+01:00,2022-02-08 17:46:34+01:00,6,1,0,1,0,0,1,0,0,0
https://github.com/pingcap/tidb-operator,2,deploy/aws/main.tf,deploy/aws/main.tf,0,hack,# HACK: depends_on for the helm and kubernetes provider,"# HACK: depends_on for the helm and kubernetes provider 
 # Passing provider configuration value via a local_file","resource ""local_file"" ""kubeconfig"" {
  # HACK: depends_on for the helm and kubernetes provider
  # Passing provider configuration value via a local_file
  depends_on = [""module.eks""]
  sensitive_content = ""${module.eks.kubeconfig}""
  filename = ""${path.module}/credentials/kubeconfig_${var.cluster_name}""
}
",resource,the block associated got renamed or deleted,,157,,13d859cd8db08b594a35bda795516a174c6dc6c1,58095783adeb0ca30ecdf094a7e8db7f17fb5919,https://github.com/pingcap/tidb-operator/blob/13d859cd8db08b594a35bda795516a174c6dc6c1/deploy/aws/main.tf#L157,https://github.com/pingcap/tidb-operator/blob/58095783adeb0ca30ecdf094a7e8db7f17fb5919/deploy/aws/main.tf,2019-05-01 21:36:07-07:00,2019-07-03 11:55:46+08:00,4,1,1,1,1,0,0,0,0,0
https://github.com/alphagov/govuk-aws,136,terraform/projects/infra-security-groups/calculators-frontend.tf,terraform/projects/infra-security-groups/calculators-frontend.tf,0,# todo,# TODO: replace this with ingress from the frontend LBs when we build them.,# TODO: replace this with ingress from the frontend LBs when we build them.,"resource ""aws_security_group_rule"" ""allow_management_to_calculators-frontend_elb"" {
  type      = ""ingress""
  from_port = 443
  to_port   = 443
  protocol  = ""tcp""

  security_group_id        = ""${aws_security_group.calculators-frontend_elb.id}""
  source_security_group_id = ""${aws_security_group.management.id}""
}
",resource,the block associated got renamed or deleted,,47,,ffc6c7e1ab1ea8954c750636cab89d66e6f3f213,7fd13330f8d6238e108f76ce76a76a28a99caaaf,https://github.com/alphagov/govuk-aws/blob/ffc6c7e1ab1ea8954c750636cab89d66e6f3f213/terraform/projects/infra-security-groups/calculators-frontend.tf#L47,https://github.com/alphagov/govuk-aws/blob/7fd13330f8d6238e108f76ce76a76a28a99caaaf/terraform/projects/infra-security-groups/calculators-frontend.tf,2017-07-20 14:43:28+01:00,2018-01-02 17:41:32+00:00,3,1,0,1,0,1,1,0,0,0
https://github.com/kubernetes/k8s.io,280,infra/gcp/terraform/k8s-infra-oci-proxy-prod/network.tf,infra/gcp/terraform/modules/oci-proxy/network.tf,1,#todo,#TODO(ameukam): current the TF resource google_compute_global_address don't have,"#TODO(ameukam): current the TF resource google_compute_global_address don't have 
 #the value of IP in his attribute. But it's accessible with the data source: 
 #https://registry.terraform.io/providers/hashicorp/google/latest/docs/data-sources/compute_global_address","module ""lb-http"" {
  source  = ""GoogleCloudPlatform/lb-http/google//modules/serverless_negs""
  version = ""~> 6.2.0""

  project = google_project.project.project_id
  name    = local.project_id

  # ...
  backends = {
    default = {
      description = null
      groups = [
        for neg in google_compute_region_network_endpoint_group.oci-proxy :
        {
          group = neg.id
        }
      ]
      enable_cdn              = true
      security_policy         = null
      custom_request_headers  = null
      custom_response_headers = null

      iap_config = {
        enable               = false
        oauth2_client_id     = """"
        oauth2_client_secret = """"
      }

      log_config = {
        enable      = true
        sample_rate = ""1.0""
      }
    }
  }

  create_address      = false
  create_ipv6_address = false
  enable_ipv6         = true
  https_redirect      = true
  #TODO(ameukam): current the TF resource google_compute_global_address don't have
  #the value of IP in his attribute. But it's accessible with the data source:
  #https://registry.terraform.io/providers/hashicorp/google/latest/docs/data-sources/compute_global_address
  address      = data.google_compute_global_address.default_ipv4.address
  ipv6_address = data.google_compute_global_address.default_ipv6.address
  managed_ssl_certificate_domains = [
    local.domain
  ]
  random_certificate_suffix = true
  ssl                       = true
  use_ssl_certificates      = false
  security_policy = google_compute_security_policy.cloud-armor.self_link
}
",module,the block associated got renamed or deleted,,101,,5b8b742861f36e67e787ab5dc1f1df1fead4cb60,ee945e07553d74e12218d1057477e5d2f3fee8f7,https://github.com/kubernetes/k8s.io/blob/5b8b742861f36e67e787ab5dc1f1df1fead4cb60/infra/gcp/terraform/k8s-infra-oci-proxy-prod/network.tf#L101,https://github.com/kubernetes/k8s.io/blob/ee945e07553d74e12218d1057477e5d2f3fee8f7/infra/gcp/terraform/modules/oci-proxy/network.tf,2022-06-15 23:24:49-07:00,2023-05-09 22:52:37-07:00,9,1,0,1,0,0,1,0,0,0
https://github.com/alphagov/govuk-aws,312,terraform/projects/infra-security-groups/backend.tf,terraform/projects/infra-security-groups/backend.tf,0,# todo,# TODO: replace this with specific routes from application servers,# TODO: replace this with specific routes from application servers,"resource ""aws_security_group_rule"" ""allow_management_to_backend_elb_internal"" {
  type      = ""ingress""
  from_port = 443
  to_port   = 443
  protocol  = ""tcp""

  security_group_id        = ""${aws_security_group.backend_elb_internal.id}""
  source_security_group_id = ""${aws_security_group.management.id}""
}
",resource,the block associated got renamed or deleted,,61,,923e481addb08a4badef9a4ac1d531d4bbfea4b5,7fd13330f8d6238e108f76ce76a76a28a99caaaf,https://github.com/alphagov/govuk-aws/blob/923e481addb08a4badef9a4ac1d531d4bbfea4b5/terraform/projects/infra-security-groups/backend.tf#L61,https://github.com/alphagov/govuk-aws/blob/7fd13330f8d6238e108f76ce76a76a28a99caaaf/terraform/projects/infra-security-groups/backend.tf,2017-09-15 17:02:09+01:00,2018-01-02 17:41:32+00:00,2,1,0,1,0,1,1,0,0,0
https://github.com/kubernetes/k8s.io,276,infra/gcp/terraform/k8s-infra-oci-proxy/oci-proxy-sandbox.tf,infra/gcp/terraform/k8s-infra-oci-proxy/oci-proxy-sandbox.tf,0,# todo,# TODO(ameukam): adjust for production.,"container_concurrency = 5 # TODO(ameukam): adjust for production. 
 // 30 seconds less than cloud scheduler maximum.","resource ""google_cloud_run_service"" ""regions"" {
  project  = google_project.project.project_id
  for_each = toset(var.cloud_run_regions)
  name     = ""${local.project_id}-${each.key}""
  location = each.key

  template {
    metadata {
      annotations = {
        ""autoscaling.knative.dev/maxScale"" = ""3"" // Control costs.
        ""run.googleapis.com/launch-stage""  = ""BETA""
      }
    }
    spec {
      service_account_name = google_service_account.oci-proxy.email
      containers {
        image = local.image
      }
      container_concurrency = 5 # TODO(ameukam): adjust for production.
      // 30 seconds less than cloud scheduler maximum.
      timeout_seconds = 570
    }
  }

  traffic {
    percent         = 100
    latest_revision = true
  }

  depends_on = [
    google_project_service.project[""run.googleapis.com""]
  ]

  lifecycle {
    ignore_changes = [
      // This gets added by the Cloud Run API post deploy and causes diffs, can be ignored...
      template[0].metadata[0].annotations[""run.googleapis.com/sandbox""],
    ]
  }
}
",resource,"resource ""google_cloud_run_service"" ""regions"" {
  project  = google_project.project.project_id
  for_each = toset(var.cloud_run_regions)
  name     = ""${local.project_id}-${each.key}""
  location = each.key

  template {
    metadata {
      annotations = {
        ""autoscaling.knative.dev/maxScale"" = ""3"" // Control costs.
        ""run.googleapis.com/launch-stage""  = ""BETA""
      }
    }
    spec {
      service_account_name = google_service_account.oci-proxy.email
      containers {
        image = local.image
      }
      container_concurrency = 5
      // 30 seconds less than cloud scheduler maximum.
      timeout_seconds = 570
    }
  }

  traffic {
    percent         = 100
    latest_revision = true
  }

  depends_on = [
    google_project_service.project[""run.googleapis.com""]
  ]

  lifecycle {
    ignore_changes = [
      // This gets added by the Cloud Run API post deploy and causes diffs, can be ignored...
      template[0].metadata[0].annotations[""client.knative.dev/user-image""],
      template[0].metadata[0].annotations[""run.googleapis.com/sandbox""],
      template[0].metadata[0].annotations[""run.googleapis.com/client-name""],
      template[0].metadata[0].annotations[""run.googleapis.com/client-version""],
      // Ignore changes done on the container specification
      // since this cloud run service is handled by https://github.com/kubernetes-sigs/oci-proxy/blob/main/hack/make-rules/deploy.sh
      template[0].spec[0].containers[0],
    ]
  }
}
",resource,112,,6b67b4c61dcd8f7111cc8e608538ca87f108a105,8219f980d204345cc624987ba6022780fefb4db5,https://github.com/kubernetes/k8s.io/blob/6b67b4c61dcd8f7111cc8e608538ca87f108a105/infra/gcp/terraform/k8s-infra-oci-proxy/oci-proxy-sandbox.tf#L112,https://github.com/kubernetes/k8s.io/blob/8219f980d204345cc624987ba6022780fefb4db5/infra/gcp/terraform/k8s-infra-oci-proxy/oci-proxy-sandbox.tf,2022-02-07 22:14:33+01:00,2022-08-05 22:03:15+02:00,3,1,1,1,0,0,0,1,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,332,examples/eks-cluster-with-new-vpc/main.tf,examples/eks-cluster-with-new-vpc/main.tf,0,todo,# TODO - requires dependency on `cert-manager` for namespace,"# TODO - requires dependency on `cert-manager` for namespace 
 # enable_cert_manager_csi_driver = true ","module ""eks_blueprints_kubernetes_addons"" {
  source = ""../../modules/kubernetes-addons""

  eks_cluster_id       = module.eks_blueprints.eks_cluster_id
  eks_cluster_endpoint = module.eks_blueprints.eks_cluster_endpoint
  eks_oidc_provider    = module.eks_blueprints.oidc_provider
  eks_cluster_version  = module.eks_blueprints.eks_cluster_version

  # EKS Managed Add-ons
  enable_amazon_eks_vpc_cni            = true
  enable_amazon_eks_coredns            = true
  enable_amazon_eks_kube_proxy         = true
  enable_amazon_eks_aws_ebs_csi_driver = true

  # Add-ons
  enable_aws_load_balancer_controller = true
  enable_metrics_server               = true
  enable_aws_cloudwatch_metrics       = true
  enable_kubecost                     = true
  enable_gatekeeper                   = true

  enable_cluster_autoscaler = true
  cluster_autoscaler_helm_config = {
    set = [
      {
        name  = ""podLabels.prometheus\\.io/scrape"",
        value = ""true"",
        type  = ""string"",
      }
    ]
  }

  enable_cert_manager = true
  cert_manager_helm_config = {
    set_values = [
      {
        name  = ""extraArgs[0]""
        value = ""--enable-certificate-owner-ref=false""
      },
    ]
  }
  # TODO - requires dependency on `cert-manager` for namespace
  # enable_cert_manager_csi_driver = true

  tags = local.tags
}
",module,,,108,0.0,37be09b5b05ec8588daf7b8e1eb8ef26a1a8507d,243ae23284c132753957508de3f724c0af73ebf0,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/37be09b5b05ec8588daf7b8e1eb8ef26a1a8507d/examples/eks-cluster-with-new-vpc/main.tf#L108,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/243ae23284c132753957508de3f724c0af73ebf0/examples/eks-cluster-with-new-vpc/main.tf#L0,2022-10-10 22:30:52-07:00,2023-02-14 16:25:25-05:00,4,2,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd,22,modules/secure-cd/main.tf,modules/secure-cd/main.tf,0,// todo,// TODO: customer config,"    cluster                 = ""${var.deploy_branch_clusters[qa].location}.${var.deploy_branch_clusters[qa].cluster}"" // TODO: customer config","resource ""google_binary_authorization_policy"" ""deployment_policy"" {
  project = var.project_id
  
  admission_whitelist_patterns {
    name_pattern = ""gcr.io/google_containers/*""
  }

  default_admission_rule {
    evaluation_mode  = ""ALWAYS_DENY""
    enforcement_mode = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
  }

  global_policy_evaluation_mode = ""ENABLE""

  // Prod Cluster Policy
  cluster_admission_rules {
    cluster                 = ""${var.deploy_branch_clusters[prod].location}.${var.deploy_branch_clusters[prod].cluster}"" // TODO: customer config
    evaluation_mode         = ""REQUIRE_ATTESTATION""
    enforcement_mode        = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
    require_attestations_by = [""security-attestor"", ""quality-attestor"", ""build-attestor""] //TODO
  }

  // QA Cluster Policy
  cluster_admission_rules {
    cluster                 = ""${var.deploy_branch_clusters[qa].location}.${var.deploy_branch_clusters[qa].cluster}"" // TODO: customer config
    evaluation_mode         = ""REQUIRE_ATTESTATION""
    enforcement_mode        = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
    require_attestations_by = [""security-attestor"", ""build-attestor""] //TODO
  }

  // Dev Cluster Policy
  cluster_admission_rules {
    cluster                 = ""${var.deploy_branch_clusters[dev].location}.${var.deploy_branch_clusters[dev].cluster}"" // TODO: customer config
    evaluation_mode         = ""REQUIRE_ATTESTATION""
    enforcement_mode        = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
    require_attestations_by = [""security-attestor""] //TODO
  }
}",resource,"resource ""google_binary_authorization_policy"" ""deployment_policy"" {
  for_each = var.deploy_branch_clusters
  project  = each.value.project_id
  
  admission_whitelist_patterns {
    name_pattern = ""gcr.io/google_containers/*""
  }

  default_admission_rule {
    evaluation_mode  = ""ALWAYS_DENY""
    enforcement_mode = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
  }

  global_policy_evaluation_mode = ""ENABLE""

  cluster_admission_rules {
    cluster                 = ""${each.value.location}.${each.value.cluster}"" // TODO: customer config
    evaluation_mode         = ""REQUIRE_ATTESTATION""
    enforcement_mode        = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
    require_attestations_by = each.value.attestations //TODO?
  }
}
",resource,73,,f9a05b119561ffabc9659fcfae59ac85a250d10e,6249c4ca90692e593bc0c7bc6d603580150ff255,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/f9a05b119561ffabc9659fcfae59ac85a250d10e/modules/secure-cd/main.tf#L73,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/6249c4ca90692e593bc0c7bc6d603580150ff255/modules/secure-cd/main.tf,2021-10-15 12:55:01-07:00,2021-10-26 17:18:47-05:00,3,1,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1588,modules/compute-vm/variables.tf,modules/compute-vm/variables.tf,0,# todo,# TODO: size can be null when source_type is attach,# TODO: size can be null when source_type is attach,"variable ""attached_disks"" {
  description = ""Additional disks, if options is null defaults will be used in its place. Source type is one of 'image' (zonal disks in vms and template), 'snapshot' (vm), 'existing', and null.""
  type = list(object({
    name        = string
    device_name = optional(string)
    # TODO: size can be null when source_type is attach
    size              = string
    snapshot_schedule = optional(string)
    source            = optional(string)
    source_type       = optional(string)
    options = optional(
      object({
        auto_delete  = optional(bool, false)
        mode         = optional(string, ""READ_WRITE"")
        replica_zone = optional(string)
        type         = optional(string, ""pd-balanced"")
      }),
      {
        auto_delete  = true
        mode         = ""READ_WRITE""
        replica_zone = null
        type         = ""pd-balanced""
      }
    )
  }))
  default = []
  validation {
    condition = length([
      for d in var.attached_disks : d if(
        d.source_type == null
        ||
        contains([""image"", ""snapshot"", ""attach""], coalesce(d.source_type, ""1""))
      )
    ]) == length(var.attached_disks)
    error_message = ""Source type must be one of 'image', 'snapshot', 'attach', null.""
  }

  validation {
    condition = length([
      for d in var.attached_disks : d if d.options == null ||
      d.options.mode == ""READ_WRITE"" || !d.options.auto_delete
    ]) == length(var.attached_disks)
    error_message = ""auto_delete can only be specified on READ_WRITE disks.""
  }
}
",variable,"variable ""attached_disks"" {
  description = ""Additional disks, if options is null defaults will be used in its place. Source type is one of 'image' (zonal disks in vms and template), 'snapshot' (vm), 'existing', and null.""
  type = list(object({
    name        = string
    device_name = optional(string)
    # TODO: size can be null when source_type is attach
    size              = string
    snapshot_schedule = optional(string)
    source            = optional(string)
    source_type       = optional(string)
    options = optional(
      object({
        auto_delete  = optional(bool, false)
        mode         = optional(string, ""READ_WRITE"")
        replica_zone = optional(string)
        type         = optional(string, ""pd-balanced"")
      }),
      {
        auto_delete  = true
        mode         = ""READ_WRITE""
        replica_zone = null
        type         = ""pd-balanced""
      }
    )
  }))
  default = []
  validation {
    condition = length([
      for d in var.attached_disks : d if(
        d.source_type == null
        ||
        contains([""image"", ""snapshot"", ""attach""], coalesce(d.source_type, ""1""))
      )
    ]) == length(var.attached_disks)
    error_message = ""Source type must be one of 'image', 'snapshot', 'attach', null.""
  }

  validation {
    condition = length([
      for d in var.attached_disks : d if d.options == null ||
      d.options.mode == ""READ_WRITE"" || !d.options.auto_delete
    ]) == length(var.attached_disks)
    error_message = ""auto_delete can only be specified on READ_WRITE disks.""
  }
}
",variable,42,42.0,b1679ad21aebd6de752526c19d8ba1ff2579bc98,da68d3cfc4a0a90ea7d3358bc32f7d3bad27e98e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b1679ad21aebd6de752526c19d8ba1ff2579bc98/modules/compute-vm/variables.tf#L42,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/da68d3cfc4a0a90ea7d3358bc32f7d3bad27e98e/modules/compute-vm/variables.tf#L42,2023-08-11 15:25:17+00:00,2024-03-04 10:12:11+01:00,7,0,1,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-sql-db,1,examples/mysql-backup-create-service-account/main.tf,examples/mysql-backup-create-service-account/main.tf,0,# todo,# TODO: don't use force_destroy for production this is just required for testing,# TODO: don't use force_destroy for production this is just required for testing,"resource ""google_storage_bucket"" ""backup"" {
  name     = ""${module.mysql.instance_name}-backup""
  location = ""us-central1""
  # TODO: don't use force_destroy for production this is just required for testing
  force_destroy = true
  project       = var.project_id
}
",resource,"resource ""google_storage_bucket"" ""backup"" {
  name     = ""${module.mysql.instance_name}-backup""
  location = ""us-central1""
  # TODO: don't use force_destroy for production this is just required for testing
  force_destroy = true
  project       = var.project_id
}
",resource,39,41.0,c51bf296e392fca246aae1c9ba4299a5a97ef274,af54237e8c6a00c73052615abf1d82df65d115f4,https://github.com/terraform-google-modules/terraform-google-sql-db/blob/c51bf296e392fca246aae1c9ba4299a5a97ef274/examples/mysql-backup-create-service-account/main.tf#L39,https://github.com/terraform-google-modules/terraform-google-sql-db/blob/af54237e8c6a00c73052615abf1d82df65d115f4/examples/mysql-backup-create-service-account/main.tf#L41,2022-05-13 10:59:53-05:00,2024-04-25 10:17:30-07:00,5,0,1,0,0,0,0,1,1,1
https://github.com/ministryofjustice/cloud-platform-infrastructure,168,terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf,terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf,0,workaround,"# Out of the box you can't specify groups to map, just users. Some people did some workarounds","# Out of the box you can't specify groups to map, just users. Some people did some workarounds 
 # we can explore later: https://ygrene.tech/mapping-iam-groups-to-eks-user-access-66fd745a6b77","module ""eks"" {
  source  = ""terraform-aws-modules/eks/aws""
  version = ""v12.2.0""

  cluster_name     = local.cluster_name
  subnets          = concat(tolist(data.aws_subnet_ids.private.ids), tolist(data.aws_subnet_ids.public.ids))
  vpc_id           = data.aws_vpc.selected.id
  write_kubeconfig = false
  cluster_version  = ""1.17""
  enable_irsa      = true

  node_groups = {
    default_ng = {
      desired_capacity = var.cluster_node_count
      max_capacity     = 30
      min_capacity     = 1
      subnets          = data.aws_subnet_ids.private.ids

      instance_type = var.worker_node_machine_type
      k8s_labels = {
        Terraform = ""true""
        Cluster   = local.cluster_name
        Domain    = local.cluster_base_domain_name
      }
      additional_tags = {
        default_ng = ""true""
      }
    }
  }

  # Out of the box you can't specify groups to map, just users. Some people did some workarounds
  # we can explore later: https://ygrene.tech/mapping-iam-groups-to-eks-user-access-66fd745a6b77
  map_users = [
    {
      userarn  = ""arn:aws:iam::754256621582:user/AlejandroGarrido""
      username = ""AlejandroGarrido""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/PoornimaKrishnasamy""
      username = ""PoornimaKrishnasamy""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/paulWyborn""
      username = ""paulWyborn""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/SabluMiah""
      username = ""SabluMiah""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/jasonBirchall""
      username = ""jasonBirchall""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/RazvanCosma""
      username = ""RazvanCosma""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/cloud-platform/manager-concourse""
      username = ""manager-concourse""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/SteveMarshall""
      username = ""SteveMarshall""
      groups   = [""system:masters""]
    }


  ]

  tags = {
    Terraform = ""true""
    Cluster   = local.cluster_name
    Domain    = local.cluster_base_domain_name
  }
}
",module,"module ""eks"" {
  source  = ""terraform-aws-modules/eks/aws""
  version = ""18.31.2""


  cluster_name              = terraform.workspace
  subnet_ids                = concat(tolist(data.aws_subnets.private.ids), tolist(data.aws_subnets.public.ids))
  vpc_id                    = data.aws_vpc.selected.id
  cluster_version           = lookup(local.cluster_version, terraform.workspace, local.cluster_version[""default""])
  enable_irsa               = true
  cluster_enabled_log_types = var.cluster_enabled_log_types

  cloudwatch_log_group_retention_in_days = var.cluster_log_retention_in_days
  cluster_security_group_description     = ""EKS cluster security group.""
  cluster_security_group_name            = terraform.workspace

  create_node_security_group = false
  node_security_group_id     = aws_security_group.node.id

  iam_role_name    = terraform.workspace
  prefix_separator = """"

  eks_managed_node_groups = {
    default_ng_12_12_23    = local.default_ng_12_12_23
    monitoring_ng_12_12_23 = local.monitoring_ng_12_12_23
  }

  iam_role_additional_policies = [""arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore""]
  # Out of the box you can't specify groups to map, just users. Some people did some workarounds
  # we can explore later: https://ygrene.tech/mapping-iam-groups-to-eks-user-access-66fd745a6b77
  manage_aws_auth_configmap = true
  aws_auth_users = [
    {
      userarn  = ""arn:aws:iam::754256621582:user/PoornimaKrishnasamy""
      username = ""PoornimaKrishnasamy""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/SabluMiah""
      username = ""SabluMiah""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/SteveMarshall""
      username = ""SteveMarshall""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/JackStockley""
      username = ""JackStockley""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/SteveWilliams""
      username = ""SteveWilliams""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/JaskaranSarkaria""
      username = ""JaskaranSarkaria""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/TomSmith""
      username = ""TomSmith""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/cloud-platform/manager-concourse""
      username = ""manager-concourse""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/KyTruong""
      username = ""KyTruong""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/MikeBell""
      username = ""MikeBell""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/DavidElliott""
      username = ""DavidElliott""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/TariqMahmood""
      username = ""TariqMahmood""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/TimCheung""
      username = ""TimCheung""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/FolarinOyenuga""
      username = ""FolarinOyenuga""
      groups   = [""system:masters""]
    },
    {
      userarn  = ""arn:aws:iam::754256621582:user/AafreenAnsari""
      username = ""AafreenAnsari""
      groups   = [""system:masters""]
    }
  ]

  tags = local.tags
}
",module,51,192.0,71503ea4248810176ad4883eca86b74a213ac9c9,41c026574305e60538a0740ad17a4f884e51a89b,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/71503ea4248810176ad4883eca86b74a213ac9c9/terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf#L51,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/41c026574305e60538a0740ad17a4f884e51a89b/terraform/aws-accounts/cloud-platform-aws/vpc/eks/cluster.tf#L192,2021-03-30 09:50:19+01:00,2024-05-09 14:36:48+01:00,141,0,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,616,examples/data-solutions/data-platform-foundations/variables.tf,examples/data-solutions/data-platform-foundations/variables.tf,0,#todo,#TODO hardcoded Cloud NAT,#TODO hardcoded Cloud NAT,"variable ""network_config"" {
  description = ""Network configurations to use. Specify a shared VPC to use, if null networks will be created in projects.""
  type = object({
    #TODO hardcoded Cloud NAT
    network_self_link = string
    #TODO hardcoded VPC ranges
    subnet_self_links = object({
      load           = string
      transformation = string
      orchestration  = string
    })
    composer_ip_ranges = object({
      cloudsql   = string
      gke_master = string
      web_server = string
    })
    composer_secondary_ranges = object({
      pods     = string
      services = string
    })
  })
  default = {
    enable_cloud_nat = false
    host_project     = null
    network          = null

    vpc_subnet = {
      load = {
        range           = ""10.10.0.0/24""
        secondary_range = null
      }
      transformation = {
        range           = ""10.10.0.0/24""
        secondary_range = null
      }
      orchestration = {
        range = ""10.10.0.0/24""
        secondary_range = {
          pods     = ""10.10.8.0/22""
          services = ""10.10.12.0/24""
        }
      }
    }
    vpc_subnet_self_link = null
  }
}
",variable,"variable ""network_config"" {
  description = ""Shared VPC network configurations to use. If null networks will be created in projects with preconfigured values.""
  type = object({
    network_self_link = string
    subnet_self_links = object({
      load           = string
      transformation = string
      orchestration  = string
    })
    composer_ip_ranges = object({
      cloudsql   = string
      gke_master = string
      web_server = string
    })
    composer_secondary_ranges = object({
      pods     = string
      services = string
    })
  })

  default = {
    network_self_link         = null
    subnet_self_links         = null
    composer_ip_ranges        = null
    composer_secondary_ranges = null
  }
}
",variable,81,,2e560407c118e7b7abc32f8ac1788a3f48563f21,d8bad5779036aa31639e4611e4935287fc79a4bc,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2e560407c118e7b7abc32f8ac1788a3f48563f21/examples/data-solutions/data-platform-foundations/variables.tf#L81,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d8bad5779036aa31639e4611e4935287fc79a4bc/examples/data-solutions/data-platform-foundations/variables.tf,2022-02-07 17:51:06+01:00,2022-02-07 21:28:54+01:00,2,1,0,1,0,0,1,0,0,0
https://github.com/uyuni-project/sumaform,9,libvirt_host/main.tf,libvirt_host/main.tf,0,hack,// HACK: hostname is taken from VM metadata in order to,"// HACK: hostname is taken from VM metadata in order to 
 // establish dependencies with other modules","output ""hostname"" {
    // HACK: hostname is taken from VM metadata in order to
    // establish dependencies with other modules
    value = ""${libvirt_domain.domain.metadata}""
}
",output,"output ""hostname"" {
    // HACK: this output artificially depends on the domain id
    // any resource using this output will have to wait until domain is fully up
    value = ""${coalesce(""${var.name}.${var.domain}"", libvirt_domain.domain.id)}""
}
",output,68,,0cade995634cfcf24a1c0535c858f0711427ff3b,7b44a14d4d6a74a6e0387ceb7238c62df9765b92,https://github.com/uyuni-project/sumaform/blob/0cade995634cfcf24a1c0535c858f0711427ff3b/libvirt_host/main.tf#L68,https://github.com/uyuni-project/sumaform/blob/7b44a14d4d6a74a6e0387ceb7238c62df9765b92/libvirt_host/main.tf,2016-06-30 10:33:47+02:00,2016-09-05 14:18:52+02:00,11,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,804,fast/stages/00-bootstrap/cicd.tf,fast/stages/00-bootstrap/cicd.tf,0,# todo,# TODO: map null provider to Cloud Build once we add support for it,# TODO: map null provider to Cloud Build once we add support for it,"locals {
  # TODO: map null provider to Cloud Build once we add support for it
  cicd_repositories = {
    for k, v in coalesce(var.cicd_repositories, {}) : k => v
    if(
      v != null
      &&
      contains(keys(local.identity_providers), v.identity_provider)
      &&
      fileexists(""${path.module}/templates/workflow-${v.type}.yaml"")
    )
  }
  cicd_service_accounts = {
    for k, v in module.automation-tf-cicd-sa :
    k => v.iam_email
  }
}
",locals,"locals {
  cicd_repositories = {
    for k, v in coalesce(var.cicd_repositories, {}) : k => v
    if(
      v != null
      &&
      (
        v.type == ""sourcerepo""
        ||
        contains(keys(local.identity_providers), coalesce(v.identity_provider, "":""))
      )
      &&
      fileexists(""${path.module}/templates/workflow-${v.type}.yaml"")
    )
  }
  cicd_workflow_providers = {
    bootstrap = ""00-bootstrap-providers.tf""
    resman    = ""01-resman-providers.tf""
  }
  cicd_workflow_var_files = {
    bootstrap = []
    resman = [
      ""00-bootstrap.auto.tfvars.json"",
      ""globals.auto.tfvars.json""
    ]
  }
}
",locals,20,,725f7effce7bdb69522b8ad004b78cda31dcb7ce,44ae2671b0d1e8bbf8aca6ca815c68b56080a8cb,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/725f7effce7bdb69522b8ad004b78cda31dcb7ce/fast/stages/00-bootstrap/cicd.tf#L20,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/44ae2671b0d1e8bbf8aca6ca815c68b56080a8cb/fast/stages/00-bootstrap/cicd.tf,2022-04-12 08:17:27+02:00,2022-06-08 11:34:08+02:00,2,1,0,1,0,0,0,1,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,747,fast/stages/03-data-platform/dev/main.tf,fast/stages/3-data-platform/dev/main.tf,1,# todo,# TODO: align example variable,# TODO: align example variable,"module ""data-platform"" {
  source             = ""../../../../examples/data-solutions/data-platform-foundations""
  billing_account_id = var.billing_account.id
  composer_config    = var.composer_config
  data_force_destroy = var.data_force_destroy
  folder_id          = var.folder_ids.data-platform
  groups             = var.groups
  network_config = {
    host_project      = var.host_project_ids.dev-spoke-0
    network_self_link = var.vpc_self_links.dev-spoke-0
    subnet_self_links = {
      load           = var.subnet_self_links.dev-spoke-0[""europe-west1/dev-dataplatform-ew1""]
      transformation = var.subnet_self_links.dev-spoke-0[""europe-west1/dev-dataplatform-ew1""]
      orchestration  = var.subnet_self_links.dev-spoke-0[""europe-west1/dev-dataplatform-ew1""]
    }
    # TODO: align example variable
    composer_ip_ranges = {
      cloudsql   = var.network_config_composer.cloudsql_range
      gke_master = var.network_config_composer.gke_master_range
      web_server = var.network_config_composer.web_server_range
    }
    composer_secondary_ranges = {
      pods     = var.network_config_composer.gke_pods_name
      services = var.network_config_composer.gke_services_name
    }
  }
  organization_domain     = var.organization.domain
  prefix                  = var.prefix
  project_services        = var.project_services
  region                  = var.region
  service_encryption_keys = var.service_encryption_keys
}
",module,"module ""data-platform"" {
  source              = ""../../../../blueprints/data-solutions/data-platform-foundations""
  composer_config     = var.composer_config
  deletion_protection = var.deletion_protection
  data_catalog_tags   = var.data_catalog_tags
  project_config = {
    billing_account_id = var.billing_account.id
    project_create     = var.project_config.project_create
    parent             = var.folder_ids.data-platform-dev
    project_ids        = var.project_config.project_ids
  }
  groups   = var.groups_dp
  location = var.location
  network_config = {
    host_project      = var.host_project_ids.dev-spoke-0
    network_self_link = var.vpc_self_links.dev-spoke-0
    subnet_self_links = {
      load           = var.subnet_self_links.dev-spoke-0[""europe-west1/dev-dataplatform-ew1""]
      transformation = var.subnet_self_links.dev-spoke-0[""europe-west1/dev-dataplatform-ew1""]
      orchestration  = var.subnet_self_links.dev-spoke-0[""europe-west1/dev-dataplatform-ew1""]
    }
    # TODO: align example variable
    composer_ip_ranges = {
      cloudsql   = var.network_config_composer.cloudsql_range
      gke_master = var.network_config_composer.gke_master_range
    }
    composer_secondary_ranges = {
      pods     = var.network_config_composer.gke_pods_name
      services = var.network_config_composer.gke_services_name
    }
  }
  organization_domain     = var.organization.domain
  prefix                  = ""${var.prefix}-dev-dp""
  project_services        = var.project_services
  project_suffix          = var.project_suffix
  region                  = var.region
  service_encryption_keys = var.service_encryption_keys
}
",module,34,40.0,c5fa5d62e4a87fdc631e6e9426d6395373d81cf1,208902c8da28d7f013234dd508bea1975931cf13,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c5fa5d62e4a87fdc631e6e9426d6395373d81cf1/fast/stages/03-data-platform/dev/main.tf#L34,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/208902c8da28d7f013234dd508bea1975931cf13/fast/stages/3-data-platform/dev/main.tf#L40,2022-02-16 14:12:39+01:00,2024-01-20 08:49:46+01:00,13,0,0,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1862,modules/project-factory/factory-budgets.tf,modules/project-factory/factory-budgets.tf,0,implement,# reimplement the billing account factory here to interpolate projects,# reimplement the billing account factory here to interpolate projects,"locals {
  # reimplement the billing account factory here to interpolate projects
  _budget_path = try(pathexpand(var.factories_config.budgets.budgets_data_path), null)
  _budgets = (
    {
      for f in try(fileset(local._budget_path, ""**/*.yaml""), []) :
      trimsuffix(f, "".yaml"") => yamldecode(file(""${local._budget_path}/${f}""))
    }
  )
  budgets = {
    for k, v in local._budgets : k => merge(v, {
      amount = merge(
        {
          currency_code   = null
          nanos           = null
          units           = null
          use_last_period = null
        },
        try(v.amount, {})
      )
      display_name = try(v.display_name, null)
      filter = try(v.filter, null) == null ? null : {
        credit_types_treatment = (
          try(v.filter.credit_types_treatment, null) == null
          ? null
          : merge(
            { exclude_all = null, include_specified = null },
            v.filter.credit_types_treatment
          )
        )
        label = try(v.filter.label, null)
        projects = concat(
          try(v.projects, []),
          [
            for p in lookup(local.project_budgets, k, []) :
            ""projects/${module.projects[p].project_id}""
          ]
        )
        resource_ancestors = try(v.filter.resource_ancestors, null)
        services           = try(v.filter.services, null)
        subaccounts        = try(v.filter.subaccounts, null)
      }
      threshold_rules = [
        for vv in try(v.threshold_rules, []) : merge({
          percent          = null
          forecasted_spend = null
        }, vv)
      ]
      update_rules = {
        for kk, vv in try(v.update_rules, {}) : kk => merge({
          disable_default_iam_recipients   = null
          monitoring_notification_channels = null
          pubsub_topic                     = null
        }, vv)
      }
    })
  }
}
",locals,"locals {
  # reimplement the billing account factory here to interpolate projects
  _budget_path = try(pathexpand(var.factories_config.budgets.budgets_data_path), null)
  _budgets = (
    {
      for f in try(fileset(local._budget_path, ""**/*.yaml""), []) :
      trimsuffix(f, "".yaml"") => yamldecode(file(""${local._budget_path}/${f}""))
    }
  )
  budgets = {
    for k, v in local._budgets : k => merge(v, {
      amount = merge(
        {
          currency_code   = null
          nanos           = null
          units           = null
          use_last_period = null
        },
        try(v.amount, {})
      )
      display_name = try(v.display_name, null)
      filter = try(v.filter, null) == null ? null : {
        credit_types_treatment = (
          try(v.filter.credit_types_treatment, null) == null
          ? null
          : merge(
            { exclude_all = null, include_specified = null },
            v.filter.credit_types_treatment
          )
        )
        label = try(v.filter.label, null)
        projects = concat(
          try(v.projects, []),
          [
            for p in lookup(local.project_budgets, k, []) :
            ""projects/${module.projects[p].project_id}""
          ]
        )
        resource_ancestors = try(v.filter.resource_ancestors, null)
        services           = try(v.filter.services, null)
        subaccounts        = try(v.filter.subaccounts, null)
      }
      threshold_rules = [
        for vv in try(v.threshold_rules, []) : merge({
          percent          = null
          forecasted_spend = null
        }, vv)
      ]
      update_rules = {
        for kk, vv in try(v.update_rules, {}) : kk => merge({
          disable_default_iam_recipients   = null
          monitoring_notification_channels = null
          pubsub_topic                     = null
        }, vv)
      }
    })
  }
}
",locals,18,20.0,dbabfb9ae0c7fe7395ea51ae6fa50aaa6c652c32,7f8d2834b3e0fea6c56a6af1122534c41d12ee91,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/dbabfb9ae0c7fe7395ea51ae6fa50aaa6c652c32/modules/project-factory/factory-budgets.tf#L18,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/7f8d2834b3e0fea6c56a6af1122534c41d12ee91/modules/project-factory/factory-budgets.tf#L20,2024-02-27 18:13:49+00:00,2024-03-19 15:50:06+00:00,2,0,0,1,0,1,0,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,354,azure/image.tf,azure/image.tf,0,// todo,// TODO: this is an hack: replace iscsi name later,// TODO: this is an hack: replace iscsi name later,"resource ""azurerm_image"" ""monitoring"" {
  count               = var.iscsi_srv_uri != """" ? 1 : 0
  name                = ""monitoringSrvImg""
  location            = var.az_region
  resource_group_name = azurerm_resource_group.myrg.name

  os_disk {
    os_type  = ""Linux""
    os_state = ""Generalized""
    blob_uri = var.iscsi_srv_uri
    size_gb  = ""32""
  }

  tags = {
    workspace = terraform.workspace
  }
}
",resource,"resource ""azurerm_image"" ""monitoring"" {
  count               = var.monitoring_uri != """" ? 1 : 0
  name                = ""monitoringSrvImg""
  location            = var.az_region
  resource_group_name = azurerm_resource_group.myrg.name

  os_disk {
    os_type  = ""Linux""
    os_state = ""Generalized""
    blob_uri = var.monitoring_uri
    size_gb  = ""32""
  }

  tags = {
    workspace = terraform.workspace
  }
}
",resource,93,,5c0d2ffe158dbf6db6c58414cdbc56fa872b4197,9adcf152486838f9f5b600ead5e4ef373918a786,https://github.com/SUSE/ha-sap-terraform-deployments/blob/5c0d2ffe158dbf6db6c58414cdbc56fa872b4197/azure/image.tf#L93,https://github.com/SUSE/ha-sap-terraform-deployments/blob/9adcf152486838f9f5b600ead5e4ef373918a786/azure/image.tf,2019-09-05 00:01:31+02:00,2019-09-05 18:08:13+02:00,3,1,1,1,0,0,0,0,0,0
https://github.com/alphagov/govuk-aws,58,terraform/projects/govuk-security-groups/logging.tf,terraform/projects/infra-security-groups/logging.tf,1,# todo,# TODO: add rule for kibana from backends once those are created.,# TODO: add rule for kibana from backends once those are created.,"resource ""aws_security_group"" ""logging_elb"" {
  name        = ""${var.stackname}_logging_elb_access""
  vpc_id      = ""${data.terraform_remote_state.govuk_vpc.vpc_id}""
  description = ""Access the logging ELB""

  tags {
    Name = ""${var.stackname}_logging_elb_access""
  }
}
",resource,,,38,0.0,00346bc2ba10d116f3e41951070ef0ebe0d9085d,0527d5b230dc008ee687667c45d1280de00b40bc,https://github.com/alphagov/govuk-aws/blob/00346bc2ba10d116f3e41951070ef0ebe0d9085d/terraform/projects/govuk-security-groups/logging.tf#L38,https://github.com/alphagov/govuk-aws/blob/0527d5b230dc008ee687667c45d1280de00b40bc/terraform/projects/infra-security-groups/logging.tf#L0,2017-07-13 14:53:36+01:00,2017-09-19 20:14:08+01:00,3,2,0,1,0,1,0,0,1,0
https://github.com/wireapp/wire-server-deploy,28,terraform/modules/aws-vpc-security-groups/main.tf,terraform/modules/aws-vpc-security-groups/main.tf,0,hack,# HACK: running out of security groups per instance.,"# HACK: running out of security groups per instance. 
 #       adding this here since the admin node needs to talk to S3. 
 # S3","resource ""aws_security_group"" ""talk_to_k8s"" {
  name        = ""talk_to_k8s""
  description = ""hosts that are allowed to speak to kubernetes.""
  vpc_id      = var.vpc_id

  # HACK: running out of security groups per instance.
  #       adding this here since the admin node needs to talk to S3.
  # S3
  egress {
    description = """"
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = var.s3_CIDRs
  }

  # kubectl
  egress {
    description = """"
    from_port   = 6443
    to_port     = 6443
    protocol    = ""tcp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  # the application itsself.
  egress {
    description = """"
    from_port   = 31772
    to_port     = 31773
    protocol    = ""tcp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  tags = {
    Name = ""talk_to_k8s""
  }
}
",resource,"resource ""aws_security_group"" ""talk_to_k8s"" {
  name        = ""talk_to_k8s""
  description = ""hosts that are allowed to speak to kubernetes.""
  vpc_id      = var.vpc_id

  # kubectl
  egress {
    description = """"
    from_port   = 6443
    to_port     = 6443
    protocol    = ""tcp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  # the application itsself.
  egress {
    description = """"
    from_port   = 31772
    to_port     = 31773
    protocol    = ""tcp""
    cidr_blocks = [""172.17.0.0/20""]
  }

  tags = {
    Name = ""talk_to_k8s""
  }
}
",resource,187,,f239eeced44a73fb235171e9af52b1776e6ed6fc,e7f081ca744de60bf9d10cb037828b995883d2aa,https://github.com/wireapp/wire-server-deploy/blob/f239eeced44a73fb235171e9af52b1776e6ed6fc/terraform/modules/aws-vpc-security-groups/main.tf#L187,https://github.com/wireapp/wire-server-deploy/blob/e7f081ca744de60bf9d10cb037828b995883d2aa/terraform/modules/aws-vpc-security-groups/main.tf,2020-07-13 19:15:38+01:00,2020-08-26 16:29:39+02:00,2,1,1,1,0,1,0,0,0,0
https://github.com/awslabs/data-on-eks,40,streaming/spark-streaming/terraform/variables.tf,streaming/spark-streaming/terraform/variables.tf,0,implementation,# There is no need for Yunikorn in this implementation,"description = ""Enable Apache YuniKorn Scheduler"" # There is no need for Yunikorn in this implementation","variable ""enable_yunikorn"" {
  default     = false
  description = ""Enable Apache YuniKorn Scheduler"" # There is no need for Yunikorn in this implementation
  type        = bool
}
",variable,"variable ""enable_yunikorn"" {
  default     = false
  description = ""Enable Apache YuniKorn Scheduler"" # There is no need for Yunikorn in this implementation
  type        = bool
}
",variable,71,71.0,6fa0ca6fb449ccd3842dda6405b7cb0d06388665,6fa0ca6fb449ccd3842dda6405b7cb0d06388665,https://github.com/awslabs/data-on-eks/blob/6fa0ca6fb449ccd3842dda6405b7cb0d06388665/streaming/spark-streaming/terraform/variables.tf#L71,https://github.com/awslabs/data-on-eks/blob/6fa0ca6fb449ccd3842dda6405b7cb0d06388665/streaming/spark-streaming/terraform/variables.tf#L71,2024-05-16 10:57:54-07:00,2024-05-16 10:57:54-07:00,1,0,0,1,0,0,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,15,eks.tf,eks.tf,0,#todo,#TODO Refer to internal AWS-IA module,"manage_aws_auth = false # Replaced by the auth.tf file  
 #TODO Refer to internal AWS-IA module","module ""eks"" {
  create_eks      = var.create_eks
  manage_aws_auth = false # Replaced by the auth.tf file

  #TODO Refer to internal AWS-IA module
  source  = ""terraform-aws-modules/eks/aws""
  version = ""17.1.0""

  cluster_name    = module.eks-label.id
  cluster_version = var.kubernetes_version

  # NETWORK CONFIG
  vpc_id  = var.create_vpc == false ? var.vpc_id : module.vpc.vpc_id
  subnets = var.create_vpc == false ? var.private_subnet_ids : module.vpc.private_subnets

  cluster_endpoint_private_access = var.endpoint_private_access
  cluster_endpoint_public_access  = var.endpoint_public_access

  # IRSA
  enable_irsa            = var.enable_irsa
  kubeconfig_output_path = ""./kubeconfig/""

  # TAGS
  tags = module.eks-label.tags

  # CLUSTER LOGGING
  cluster_enabled_log_types = var.enabled_cluster_log_types

  # CLUSTER ENCRYPTION
  cluster_encryption_config = [
    {
      provider_key_arn = aws_kms_key.eks.arn
      resources = [
      ""secrets""]
    }
  ]
}
",module,"module ""eks"" {
  source = ""git@github.com:maheshr-amzn/terraform-aws-eks_cluster?ref=feature/aws-eks_cluster-development""
  # version = ???

  create_eks      = var.create_eks
  manage_aws_auth = false     # Replaced by the auth.tf file

  eks_cluster_name    = module.eks-label.id
  eks_cluster_version = var.kubernetes_version

  # NETWORK CONFIG
  vpc_id  = var.create_vpc == false ? var.vpc_id : module.vpc.vpc_id
  subnets = var.create_vpc == false ? var.private_subnet_ids : module.vpc.private_subnets

  eks_cluster_endpoint_private_access = var.endpoint_private_access
  eks_cluster_endpoint_public_access  = var.endpoint_public_access

  # IRSA
  enable_irsa            = var.enable_irsa
//  kubeconfig_output_path = ""./kubeconfig/""

  # TAGS
  tags = module.eks-label.tags

  # CLUSTER LOGGING
  enabled_cluster_log_types = var.enabled_cluster_log_types

  # CLUSTER ENCRYPTION
  cluster_encryption_config = [
    {
      provider_key_arn = aws_kms_key.eks.arn
      resources = [
      ""secrets""]
    }
  ]
}
",module,43,,125390ed86df57dc9d8064df92b36e14cc8eb3e2,35137a9a2e6959bc4ce5ffb977c913914757bf1a,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/125390ed86df57dc9d8064df92b36e14cc8eb3e2/eks.tf#L43,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/35137a9a2e6959bc4ce5ffb977c913914757bf1a/eks.tf,2021-09-13 14:12:34+01:00,2021-09-21 10:41:45+01:00,2,1,1,1,0,0,0,0,0,0
https://github.com/Azure/terraform-azurerm-caf-enterprise-scale,4,archetypes/locals.role_definitions.tf,modules/archetypes/locals.role_definitions.tf,1,implemented,# Logic implemented to determine whether Role Definitions,"# Generate the Role Definition configurations for the specified archetype. 
 # Logic implemented to determine whether Role Definitions 
 # need to be loaded to save on compute and memory resources 
 # when none defined in archetype definition.","locals {
  archetype_role_definitions_list      = local.archetype_definition.role_definitions
  archetype_role_definitions_specified = try(length(local.archetype_role_definitions_list) > 0, false)
}
",locals,"locals {
  archetype_role_definitions_list      = local.archetype_definition.role_definitions
  archetype_role_definitions_specified = try(length(local.archetype_role_definitions_list) > 0, false)
}
",locals,2,2.0,a05be92e463f90e577936a84395ffb88cfd045e7,1b41f7062f286e790be9b2e16a74562936f691df,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/a05be92e463f90e577936a84395ffb88cfd045e7/archetypes/locals.role_definitions.tf#L2,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/1b41f7062f286e790be9b2e16a74562936f691df/modules/archetypes/locals.role_definitions.tf#L2,2020-09-25 20:39:19+01:00,2022-01-27 09:57:26+00:00,7,0,0,0,0,1,0,0,0,0
https://github.com/claranet/terraform-datadog-monitors,26,databases/elasticsearch/monitors-elasticsearch.tf,database/elasticsearch/monitors-elasticsearch.tf,1,todo,// TODO add tags to filter by node type and do not apply this monitor on non-data nodes,// TODO add tags to filter by node type and do not apply this monitor on non-data nodes,"resource ""datadog_monitor"" ""flush_latency"" {
  name    = ""[${var.environment}] Elasticsearch average index flushing to disk latency {{#is_alert}}{{{comparator}}} {{threshold}}ms ({{value}}ms){{/is_alert}}{{#is_warning}}{{{comparator}}} {{warn_threshold}}ms ({{value}}ms){{/is_warning}}""
  message = ""${coalesce(var.flush_latency_message, var.message)}""

  type = ""query alert""

  // TODO add tags to filter by node type and do not apply this monitor on non-data nodes
  query = <<EOF
  ${var.flush_latency_time_aggregator}(${var.flush_latency_timeframe}):
    avg:elasticsearch.flush.total.time{${data.template_file.filter.rendered}} by {node_name} / avg:elasticsearch.flush.total{${data.template_file.filter.rendered}} by {node_name} * 1000
  > ${var.flush_latency_threshold_critical}
EOF

  thresholds {
    warning  = ""${var.flush_latency_threshold_warning}""
    critical = ""${var.flush_latency_threshold_critical}""
  }

  notify_audit        = false
  locked              = false
  include_tags        = true
  require_full_window = true
  notify_no_data      = true

  evaluation_delay = ""${var.evaluation_delay}""

  silenced = ""${var.flush_latency_silenced}""

  tags = [
    ""resource:elasticsearch"",
    ""env:${var.environment}"",
    ""created_by:terraform"",
    ""${var.flush_latency_extra_tags}"",
  ]
}
",resource,"resource ""datadog_monitor"" ""flush_latency"" {
  count   = var.flush_latency_enabled == ""true"" ? 1 : 0
  name    = ""${var.prefix_slug == """" ? """" : ""[${var.prefix_slug}]""}[${var.environment}] Elasticsearch average index flushing to disk latency {{#is_alert}}{{{comparator}}} {{threshold}}ms ({{value}}ms){{/is_alert}}{{#is_warning}}{{{comparator}}} {{warn_threshold}}ms ({{value}}ms){{/is_warning}}""
  message = coalesce(var.flush_latency_message, var.message)
  type    = ""query alert""

  // TODO add tags to filter by node type and do not apply this monitor on non-data nodes
  query = <<EOQ
  ${var.flush_latency_time_aggregator}(${var.flush_latency_timeframe}):
    default(
      diff(avg:elasticsearch.flush.total.time${module.filter-tags.query_alert} by {node_name}) /
      diff(avg:elasticsearch.flush.total${module.filter-tags.query_alert} by {node_name})
    * 1000, 0)
  > ${var.flush_latency_threshold_critical}
EOQ

  monitor_thresholds {
    warning  = var.flush_latency_threshold_warning
    critical = var.flush_latency_threshold_critical
  }

  evaluation_delay    = var.evaluation_delay
  new_host_delay      = var.new_host_delay
  new_group_delay     = var.new_group_delay
  notify_audit        = false
  include_tags        = true
  require_full_window = true
  notify_no_data      = false

  tags = concat(local.common_tags, var.tags, var.flush_latency_extra_tags)
}
",resource,438,397.0,0500de1330cba0c19e4b4e39187cb8affe96452b,a449a1360a67ebf9abddf3e4e6ff6fa17c50b12a,https://github.com/claranet/terraform-datadog-monitors/blob/0500de1330cba0c19e4b4e39187cb8affe96452b/databases/elasticsearch/monitors-elasticsearch.tf#L438,https://github.com/claranet/terraform-datadog-monitors/blob/a449a1360a67ebf9abddf3e4e6ff6fa17c50b12a/database/elasticsearch/monitors-elasticsearch.tf#L397,2018-08-30 16:19:34+02:00,2023-12-12 16:00:34+01:00,31,0,0,1,0,0,0,0,1,0
https://github.com/cloudfoundry/bosh-bootloader,2,plan-patches/cfcr-azure/terraform/cfcr_sg_override.tf,plan-patches/cfcr-azure/terraform/cfcr_sg_override.tf,0,# todo,"# TODO: after the new cpi release, switch to cfcr resource group.","# TODO: after the new cpi release, switch to cfcr resource group.","resource ""azurerm_network_security_group"" ""cfcr-worker"" {
  name                = ""${var.env_id}-cfcr-worker-sg""
  location            = ""${var.region}""
  # TODO: after the new cpi release, switch to cfcr resource group.
  resource_group_name          = ""${azurerm_resource_group.bosh.name}""
}",resource,the block associated got renamed or deleted,,25,,5169911b36b56c3d7da1815cfc1b4b5e2c9e46eb,f098a6c6ca76caf96d25a1469be65f7cb7d5c514,https://github.com/cloudfoundry/bosh-bootloader/blob/5169911b36b56c3d7da1815cfc1b4b5e2c9e46eb/plan-patches/cfcr-azure/terraform/cfcr_sg_override.tf#L25,https://github.com/cloudfoundry/bosh-bootloader/blob/f098a6c6ca76caf96d25a1469be65f7cb7d5c514/plan-patches/cfcr-azure/terraform/cfcr_sg_override.tf,2018-10-09 15:43:19-07:00,2018-10-29 09:34:36-07:00,2,1,0,1,0,1,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,850,fast/stages/03-gke-multitenant/prod/gke-clusters.tf,blueprints/gke/multitenant-fleet/gke-clusters.tf,1,# todo,"# TODO: the attributes below are ""primed"" from project-level defaults","# resource_usage_export_config = { 
 #   enabled = true 
 #   dataset = module.gke-dataset-resource-usage.id 
 # } 
 # TODO: the attributes below are ""primed"" from project-level defaults 
 #       in locals, merge defaults with cluster-level stuff 
 # TODO(jccb): change fabric module","module ""gke-cluster"" {
  source                   = ""../../../../modules/gke-cluster""
  for_each                 = local.clusters
  project_id               = module.gke-project-0.project_id
  name                     = each.key
  description              = each.value.description
  location                 = each.value.location
  network                  = each.value.net.vpc
  subnetwork               = each.value.net.subnet
  secondary_range_pods     = each.value.net.pods
  secondary_range_services = each.value.net.services
  labels                   = each.value.labels
  addons = {
    cloudrun_config                       = each.value.overrides.cloudrun_config
    dns_cache_config                      = true
    http_load_balancing                   = true
    gce_persistent_disk_csi_driver_config = true
    horizontal_pod_autoscaling            = true
    config_connector_config               = true
    kalm_config                           = false
    # enable only if enable_dataplane_v2 is changed to false below
    network_policy_config = false
    istio_config = {
      enabled = false
      tls     = false
    }
  }
  # change these here for all clusters if absolutely needed
  # authenticator_security_group = var.authenticator_security_group
  enable_dataplane_v2         = true
  enable_l4_ilb_subsetting    = false
  enable_intranode_visibility = true
  enable_shielded_nodes       = true
  workload_identity           = true
  private_cluster_config = {
    enable_private_nodes    = true
    enable_private_endpoint = true
    master_ipv4_cidr_block  = each.value.net.master_range
    master_global_access    = true
  }
  dns_config = each.value.dns_domain == null ? null : {
    cluster_dns        = ""CLOUD_DNS""
    cluster_dns_scope  = ""VPC_SCOPE""
    cluster_dns_domain = ""${each.key}.${var.dns_domain}""
  }
  logging_config    = [""SYSTEM_COMPONENTS"", ""WORKLOADS""]
  monitoring_config = [""SYSTEM_COMPONENTS"", ""WORKLOADS""]

  # if you don't have compute.networks.updatePeering in the host
  # project, comment out the next line and ask your network admin to
  # create the peering for you
  peering_config = {
    export_routes = true
    import_routes = false
    project_id    = var.vpc_host_project
  }
  # resource_usage_export_config = {
  #   enabled = true
  #   dataset = module.gke-dataset-resource-usage.id
  # }
  # TODO: the attributes below are ""primed"" from project-level defaults
  #       in locals, merge defaults with cluster-level stuff
  # TODO(jccb): change fabric module
  database_encryption = (
    each.value.overrides.database_encryption_key == null ? {
      enabled  = false
      state    = null
      key_name = null
      } : {
      enabled  = true
      state    = ""ENCRYPTED""
      key_name = each.value.overrides.database_encryption_key
    }
  )
  default_max_pods_per_node   = each.value.overrides.max_pods_per_node
  enable_binary_authorization = each.value.overrides.enable_binary_authorization
  master_authorized_ranges    = each.value.overrides.master_authorized_ranges
  pod_security_policy         = each.value.overrides.pod_security_policy
  release_channel             = each.value.overrides.release_channel
  vertical_pod_autoscaling    = each.value.overrides.vertical_pod_autoscaling
  # dynamic ""cluster_autoscaling"" {
  #   for_each = each.value.cluster_autoscaling == null ? {} : { 1 = 1 }
  #   content {
  #     enabled    = true
  #     cpu_min    = each.value.cluster_autoscaling.cpu_min
  #     cpu_max    = each.value.cluster_autoscaling.cpu_max
  #     memory_min = each.value.cluster_autoscaling.memory_min
  #     memory_max = each.value.cluster_autoscaling.memory_max
  #   }
  # }

  depends_on = [
    google_project_iam_member.host_project_bindings
  ]
}
",module,"module ""gke-cluster"" {
  source      = ""../../../modules/gke-cluster""
  for_each    = local.clusters
  name        = each.key
  project_id  = module.gke-project-0.project_id
  description = each.value.description
  location    = each.value.location
  vpc_config = {
    network    = var.vpc_config.vpc_self_link
    subnetwork = each.value.net.subnet
    secondary_range_names = {
      pods     = each.value.net.pods
      services = each.value.net.services
    }
    master_authorized_ranges = each.value.overrides.master_authorized_ranges
  }
  labels = each.value.labels
  enable_addons = {
    cloudrun                       = each.value.overrides.cloudrun_config
    config_connector               = true
    dns_cache                      = true
    gce_persistent_disk_csi_driver = true
    gcp_filestore_csi_driver       = each.value.overrides.gcp_filestore_csi_driver_config
    gke_backup_agent               = false
    horizontal_pod_autoscaling     = true
    http_load_balancing            = true
  }
  enable_features = {
    cloud_dns = var.dns_domain == null ? null : {
      cluster_dns        = ""CLOUD_DNS""
      cluster_dns_scope  = ""VPC_SCOPE""
      cluster_dns_domain = ""${each.key}.${var.dns_domain}""
    }
    database_encryption = (
      each.value.overrides.database_encryption_key == null
      ? null
      : {
        state    = ""ENCRYPTED""
        key_name = each.value.overrides.database_encryption_key
      }
    )
    dataplane_v2         = true
    groups_for_rbac      = var.authenticator_security_group
    intranode_visibility = true
    pod_security_policy  = each.value.overrides.pod_security_policy
    resource_usage_export = {
      dataset = module.gke-dataset-resource-usage.dataset_id
    }
    shielded_nodes           = true
    vertical_pod_autoscaling = each.value.overrides.vertical_pod_autoscaling
    workload_identity        = true
  }
  private_cluster_config = {
    enable_private_endpoint = true
    master_ipv4_cidr_block  = each.value.net.master_range
    master_global_access    = true
    peering_config = var.peering_config == null ? null : {
      export_routes = var.peering_config.export_routes
      import_routes = var.peering_config.import_routes
      project_id    = var.vpc_config.host_project_id
    }
  }
  logging_config    = [""SYSTEM_COMPONENTS"", ""WORKLOADS""]
  monitoring_config = [""SYSTEM_COMPONENTS"", ""WORKLOADS""]
  max_pods_per_node = each.value.overrides.max_pods_per_node
  release_channel   = each.value.overrides.release_channel
}
",module,86,,f3f9a4a88cedd64f6fc91b64666046aa6726a2f3,16822e94ab70d75099214b9db786affcb231fbf6,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/f3f9a4a88cedd64f6fc91b64666046aa6726a2f3/fast/stages/03-gke-multitenant/prod/gke-clusters.tf#L86,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/16822e94ab70d75099214b9db786affcb231fbf6/blueprints/gke/multitenant-fleet/gke-clusters.tf,2022-06-08 11:41:50+02:00,2022-10-10 09:38:21+02:00,15,1,1,1,0,0,0,0,0,0
https://github.com/jenkins-x/terraform-google-jx,3,main.tf,main.tf,0,// todo,// TODO: replace with the following when parent_domain deprecations is complete: apex_domain  = var.apex_domain,// TODO: replace with the following when parent_domain deprecations is complete: apex_domain  = var.apex_domain,"locals {
  interpolated_content = templatefile(""${path.module}/modules/jx-requirements.yml.tpl"", {
    gcp_project                 = var.gcp_project
    zone                        = var.cluster_location
    cluster_name                = local.cluster_name
    git_owner_requirement_repos = var.git_owner_requirement_repos
    dev_env_approvers           = var.dev_env_approvers
    lets_encrypt_production     = var.lets_encrypt_production
    // Storage buckets
    log_storage_url        = module.cluster.log_storage_url
    report_storage_url     = module.cluster.report_storage_url
    repository_storage_url = module.cluster.repository_storage_url
    backup_bucket_url      = module.backup.backup_bucket_url
    // Vault
    external_vault = local.external_vault
    vault_bucket   = length(module.vault) > 0 ? module.vault[0].vault_bucket_name : """"
    vault_key      = length(module.vault) > 0 ? module.vault[0].vault_key : """"
    vault_keyring  = length(module.vault) > 0 ? module.vault[0].vault_keyring : """"
    vault_name     = length(module.vault) > 0 ? module.vault[0].vault_name : """"
    vault_sa       = length(module.vault) > 0 ? module.vault[0].vault_sa : """"
    vault_url      = var.vault_url
    // Velero
    enable_backup    = var.enable_backup
    velero_sa        = module.backup.velero_sa
    velero_namespace = module.backup.backup_bucket_url != """" ? var.velero_namespace : """"
    velero_schedule  = var.velero_schedule
    velero_ttl       = var.velero_ttl
    // DNS
    // TODO: remove parent_domain when its deprecations is complete: domain_enabled = var.apex_domain != """" ? true : false
    domain_enabled = var.apex_domain != """" ? true : (var.parent_domain != """" ? true : false)
    // TODO: replace with the following when parent_domain deprecations is complete: apex_domain  = var.apex_domain
    apex_domain    = var.apex_domain != """" ? var.apex_domain : var.parent_domain
    subdomain      = var.subdomain
    tls_email      = var.tls_email

    version_stream_ref = var.version_stream_ref
    version_stream_url = var.version_stream_url
    webhook            = var.webhook
  })

  split_content   = split(""\n"", local.interpolated_content)
  compact_content = compact(local.split_content)
  content         = join(""\n"", local.compact_content)
}
",locals,"locals {
  requirements_file = var.jx2 ? ""${path.module}/modules/jx-requirements.yml.tpl"" : ""${path.module}/modules/jx-requirements-v3.yml.tpl""
  interpolated_content = templatefile(local.requirements_file, {
    gcp_project                 = var.gcp_project
    zone                        = var.cluster_location
    cluster_name                = local.cluster_name
    git_owner_requirement_repos = var.git_owner_requirement_repos
    dev_env_approvers           = var.dev_env_approvers
    lets_encrypt_production     = var.lets_encrypt_production
    // GCP Artifact
    enable_artifact        = var.artifact_enable
    registry               = module.cluster.artifact_registry_repository
    docker_registry_org    = module.cluster.artifact_registry_repository_name
    // Storage buckets
    log_storage_url        = module.cluster.log_storage_url
    report_storage_url     = module.cluster.report_storage_url
    repository_storage_url = module.cluster.repository_storage_url
    backup_bucket_url      = module.backup.backup_bucket_url
    // Vault
    external_vault  = local.external_vault
    vault_bucket    = length(module.vault) > 0 ? module.vault[0].vault_bucket_name : """"
    vault_key       = length(module.vault) > 0 ? module.vault[0].vault_key : """"
    vault_keyring   = length(module.vault) > 0 ? module.vault[0].vault_keyring : """"
    vault_name      = length(module.vault) > 0 ? module.vault[0].vault_name : """"
    vault_sa        = length(module.vault) > 0 ? module.vault[0].vault_sa : """"
    vault_url       = var.vault_url
    vault_installed = !var.gsm ? true : false
    // Velero
    enable_backup    = var.enable_backup
    velero_sa        = module.backup.velero_sa
    velero_namespace = module.backup.backup_bucket_url != """" ? var.velero_namespace : """"
    velero_schedule  = var.velero_schedule
    velero_ttl       = var.velero_ttl
    // DNS
    // TODO: remove parent_domain when its deprecations is complete: domain_enabled = var.apex_domain != """" ? true : false
    domain_enabled = var.apex_domain != """" ? true : (var.parent_domain != """" ? true : false)
    // TODO: replace with the following when parent_domain deprecations is complete: apex_domain  = var.apex_domain
    apex_domain = var.apex_domain != """" ? var.apex_domain : var.parent_domain
    subdomain   = var.subdomain
    tls_email   = var.tls_email
    // Kuberhealthy
    kuberhealthy = var.kuberhealthy

    version_stream_ref = var.version_stream_ref
    version_stream_url = var.version_stream_url
    webhook            = var.webhook
  })

  split_content   = split(""\n"", local.interpolated_content)
  compact_content = compact(local.split_content)
  content         = join(""\n"", local.compact_content)
}
",locals,281,306.0,ffc0b68673f1e9341ef89e88f6af157631a9086d,6d61937b9d1a81a879246040a6f6e11ed5626a4e,https://github.com/jenkins-x/terraform-google-jx/blob/ffc0b68673f1e9341ef89e88f6af157631a9086d/main.tf#L281,https://github.com/jenkins-x/terraform-google-jx/blob/6d61937b9d1a81a879246040a6f6e11ed5626a4e/main.tf#L306,2020-12-09 11:35:14+10:00,2024-04-25 13:02:59+02:00,30,0,0,1,0,0,1,0,0,0
https://github.com/kubernetes/k8s.io,325,infra/aws/terraform/prow-build-cluster/eks.tf,infra/aws/terraform/prow-build-cluster/eks.tf,0,todo,# TODO - remove this policy once AWS releases a managed version similar to AmazonEKS_CNI_Policy (IPv4),"# We are using the IRSA created below for permissions 
 # However, we have to deploy with the policy attached FIRST (when creating a fresh cluster) 
 # and then turn this off after the cluster/node group is created. Without this initial policy, 
 # the VPC CNI fails to assign IPs and nodes cannot join the cluster 
 # See https://github.com/aws/containers-roadmap/issues/1666 for more context 
 # TODO - remove this policy once AWS releases a managed version similar to AmazonEKS_CNI_Policy (IPv4)","module ""eks"" {
  source  = ""terraform-aws-modules/eks/aws""
  version = ""19.10.0""

  # General cluster properties.
  cluster_name                   = var.cluster_name
  cluster_version                = var.cluster_version
  cluster_endpoint_public_access = true

  # Manage aws-auth ConfigMap.
  manage_aws_auth_configmap = true

  # We use IPv6 because we require a large number of nodes and pods.
  # With IPv4, we can use only /16 VPC, which is a blocker for running
  # 100+ nodes.
  cluster_ip_family = ""ipv6""

  # We are using the IRSA created below for permissions
  # However, we have to deploy with the policy attached FIRST (when creating a fresh cluster)
  # and then turn this off after the cluster/node group is created. Without this initial policy,
  # the VPC CNI fails to assign IPs and nodes cannot join the cluster
  # See https://github.com/aws/containers-roadmap/issues/1666 for more context
  # TODO - remove this policy once AWS releases a managed version similar to AmazonEKS_CNI_Policy (IPv4)
  create_cni_ipv6_iam_policy = true

  vpc_id                   = module.vpc.vpc_id
  subnet_ids               = module.vpc.private_subnets
  control_plane_subnet_ids = module.vpc.intra_subnets

  cluster_addons = {
    coredns = {
      most_recent = true
    }
    kube-proxy = {
      most_recent = true
    }
    vpc-cni = {
      most_recent              = true
      service_account_role_arn = module.vpc_cni_irsa.iam_role_arn
    }
    aws-ebs-csi-driver = {
      most_recent              = true
      service_account_role_arn = module.ebs_csi_irsa.iam_role_arn
    }
  }

  eks_managed_node_group_defaults = {
    ami_id                     = var.node_ami
    enable_bootstrap_user_data = true
    instance_types             = var.node_instance_types

    # We are using the IRSA created below for permissions
    # However, we have to deploy with the policy attached FIRST (when creating a fresh cluster)
    # and then turn this off after the cluster/node group is created. Without this initial policy,
    # the VPC CNI fails to assign IPs and nodes cannot join the cluster
    # See https://github.com/aws/containers-roadmap/issues/1666 for more context
    iam_role_attach_cni_policy = false
  }

  eks_managed_node_groups = {
    # Build cluster node group.
    build = {
      name            = ""build-managed""
      description     = ""EKS managed node group used for build nodes""
      use_name_prefix = true

      subnet_ids = module.vpc.private_subnets

      min_size     = var.node_min_size
      max_size     = var.node_max_size
      desired_size = var.node_desired_size

      ami_id                     = var.node_ami
      enable_bootstrap_user_data = true

      # Force version update if existing pods are unable to be drained due to a PodDisruptionBudget issue.
      force_update_version = true
      update_config = {
        max_unavailable_percentage = 33
      }

      # Required to ensure Prow works well.
      pre_bootstrap_user_data = <<-EOT
        sysctl -w fs.inotify.max_user_watches=524288
      EOT

      capacity_type  = ""ON_DEMAND""
      instance_types = var.node_instance_types

      ebs_optimized     = true
      enable_monitoring = true

      block_device_mappings = {
        xvda = {
          device_name = ""/dev/xvda""
          ebs = {
            volume_size           = var.node_volume_size
            volume_type           = ""gp3""
            iops                  = 16000 # Maximum for gp3 volume.
            throughput            = 1000  # Maximum for gp3 volume.
            encrypted             = false
            delete_on_termination = true
          }
        }
      }

      enclave_options = {
        enabled = true
      }

      tags = local.node_group_tags
    }
  }
}
",module,"module ""eks"" {
  source  = ""terraform-aws-modules/eks/aws""
  version = ""19.10.0""

  # General cluster properties.
  cluster_name                   = var.cluster_name
  cluster_version                = var.cluster_version
  cluster_endpoint_public_access = true

  # Manage aws-auth ConfigMap.
  manage_aws_auth_configmap = true

  # We use IPv4 for the best compatibility with the existing setup.
  # Additionally, Ubuntu EKS optimized AMI doesn't support IPv6 well.
  cluster_ip_family = ""ipv4""

  vpc_id                   = module.vpc.vpc_id
  subnet_ids               = module.vpc.private_subnets
  control_plane_subnet_ids = module.vpc.intra_subnets

  cluster_addons = {
    coredns = {
      most_recent = true
    }
    kube-proxy = {
      most_recent = true
    }
    vpc-cni = {
      most_recent              = true
      service_account_role_arn = module.vpc_cni_irsa.iam_role_arn
    }
    aws-ebs-csi-driver = {
      most_recent              = true
      service_account_role_arn = module.ebs_csi_irsa.iam_role_arn
    }
  }

  eks_managed_node_group_defaults = {
    ami_id                     = var.node_ami
    enable_bootstrap_user_data = true
    instance_types             = var.node_instance_types

    # We are using the IRSA created below for permissions
    # However, we have to deploy with the policy attached FIRST (when creating a fresh cluster)
    # and then turn this off after the cluster/node group is created. Without this initial policy,
    # the VPC CNI fails to assign IPs and nodes cannot join the cluster
    # See https://github.com/aws/containers-roadmap/issues/1666 for more context
    iam_role_attach_cni_policy = true
  }

  eks_managed_node_groups = {
    # Build cluster node group.
    build = {
      name            = ""build-managed""
      description     = ""EKS managed node group used for build nodes""
      use_name_prefix = true

      subnet_ids = module.vpc.private_subnets

      min_size     = var.node_min_size
      max_size     = var.node_max_size
      desired_size = var.node_desired_size

      ami_id                     = var.node_ami
      enable_bootstrap_user_data = true

      # Force version update if existing pods are unable to be drained due to a PodDisruptionBudget issue.
      force_update_version = true
      update_config = {
        max_unavailable_percentage = var.node_max_unavailable_percentage
      }

      # Required to ensure Prow works well.
      pre_bootstrap_user_data = <<-EOT
        sysctl -w fs.inotify.max_user_watches=524288
      EOT

      capacity_type  = ""ON_DEMAND""
      instance_types = var.node_instance_types

      ebs_optimized     = true
      enable_monitoring = true

      block_device_mappings = {
        xvda = {
          device_name = ""/dev/xvda""
          ebs = {
            volume_size           = var.node_volume_size
            volume_type           = ""gp3""
            iops                  = 16000 # Maximum for gp3 volume.
            throughput            = 1000  # Maximum for gp3 volume.
            encrypted             = false
            delete_on_termination = true
          }
        }
      }

      enclave_options = {
        enabled = true
      }

      tags = local.node_group_tags
    }
  }
}
",module,43,,fae0a7eda405a007e05cf1366e820cb8603816d9,d02931c774caf517e6ff91fe4193fdfc4dcb724a,https://github.com/kubernetes/k8s.io/blob/fae0a7eda405a007e05cf1366e820cb8603816d9/infra/aws/terraform/prow-build-cluster/eks.tf#L43,https://github.com/kubernetes/k8s.io/blob/d02931c774caf517e6ff91fe4193fdfc4dcb724a/infra/aws/terraform/prow-build-cluster/eks.tf,2023-03-01 10:38:47+01:00,2023-03-02 17:03:17+01:00,2,1,1,1,1,1,0,0,0,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,294,kube.example.tf,kube.example.tf,0,fix,"#  If you are going to use Rancher addons for instance, it's always a good idea to fix the kube version to latest - 0.01,","### The following values are entirely optional (and can be removed from this if unused)  
 # You can refine a base domain name to be use in this form of nodename.base_domain for setting the reserve dns inside Hetzner 
 # base_domain = ""mycluster.example.com""  
 # Cluster Autoscaler 
 # Providing at least one map for the array enables the cluster autoscaler feature, default is disabled 
 # Please note that the autoscaler should not be used with initial_k3s_channel < ""v1.25"". So ideally lock it to ""v1.25"". 
 # * Example below: 
 # autoscaler_nodepools = [ 
 #   { 
 #     name        = ""autoscaler"" 
 #     server_type = ""cpx21"" # must be same or better than the control_plane server type (regarding disk size)! 
 #     location    = ""fsn1"" 
 #     min_nodes   = 0 
 #     max_nodes   = 5 
 #   } 
 # ]  
 # Enable etcd snapshot backups to S3 storage. 
 # Just provide a map with the needed settings (according to your S3 storage provider) and backups to S3 will 
 # be enabled (with the default settings for etcd snapshots). 
 # Cloudflare's R2 offers 10GB, 10 million reads and 1 million writes per month for free. 
 # For proper context, have a look at https://docs.k3s.io/backup-restore. 
 # etcd_s3_backup = { 
 #   etcd-s3-endpoint        = ""xxxx.r2.cloudflarestorage.com"" 
 #   etcd-s3-access-key      = ""<access-key>"" 
 #   etcd-s3-secret-key      = ""<secret-key>"" 
 #   etcd-s3-bucket          = ""k3s-etcd-snapshots"" 
 # }  
 # To use local storage on the nodes, you can enable Longhorn, default is ""false"". 
 # See a full recap on how to configure agent nodepools for longhorn here https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/discussions/373#discussioncomment-3983159 
 # enable_longhorn = true  
 # By default, longhorn is pulled from https://charts.longhorn.io. 
 # If you need a version of longhorn which assures compatibility with rancher you can set this variable to https://charts.rancher.io. 
 # longhorn_repository = ""https://charts.rancher.io""  
 # The namespace for longhorn deployment, default is ""longhorn-system"". 
 # longhorn_namespace = ""longhorn-system""  
 # The file system type for Longhorn, if enabled (ext4 is the default, otherwise you can choose xfs). 
 # longhorn_fstype = ""xfs""  
 # how many replica volumes should longhorn create (default is 3). 
 # longhorn_replica_count = 1  
 # When you enable Longhorn, you can go with the default settings and just modify the above two variables OR you can add a longhorn_values variable 
 # with all needed helm values, see towards the end of the file in the advanced section. 
 # If that file is present, the system will use it during the deploy, if not it will use the default values with the two variable above that can be customized. 
 # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.  
 # Also, you can choose to use a Hetzner volume with Longhorn. By default, it will use the nodes own storage space, but if you add an attribute of 
 # longhorn_volume_size ( not a variable, just a possible agent nodepool attribute) with a value between 10 and 10000 GB to your agent nodepool definition, it will create and use the volume in question. 
 # See the agent nodepool section for an example of how to do that.  
 # To disable Hetzner CSI storage, you can set the following to ""true"", default is ""false"". 
 # disable_hetzner_csi = true  
 # If you want to use a specific Hetzner CCM and CSI version, set them below; otherwise, leave them as-is for the latest versions. 
 # hetzner_ccm_version = """" 
 # hetzner_csi_version = """"  
 # If you want to specify the Kured version, set it below - otherwise it'll use the latest version available. 
 # kured_version = """"  
 # If you want to specify what time Kured starts restarting nodes, set it below. Default is 3AM. 
 # kured_start_time = """"  
 # If you want to specify what time Kured stops restarting nodes, set it below. Default is 8AM. 
 # kured_end_time = """"  
 # If you want to specify what timezone Kured uses, set it below. Default is Local. 
 # kured_time_zone = """"  
 # If you want to enable the Nginx ingress controller (https://kubernetes.github.io/ingress-nginx/) instead of Traefik, you can set this to ""nginx"". Default is ""traefik"". 
 # By the default we load optimal Traefik and Nginx ingress controller config for Hetzner, however you may need to tweak it to your needs, so to do, 
 # we allow you to add a traefik_values and nginx_values, see towards the end of this file in the advanced section. 
 # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration. 
 # If you want to disable both controllers set this to ""none"" 
 # ingress_controller = ""nginx""  
 # You can change the number of replicas for selected ingress controller here. The default 0 means autoselecting based on number of agent nodes (1 node = 1 replica, 2 nodes = 2 replicas, 3+ nodes = 3 replicas) 
 # ingress_replica_count = 1  
 # Use the klipperLB (similar to metalLB), instead of the default Hetzner one, that has an advantage of dropping the cost of the setup. 
 # Automatically ""true"" in the case of single node cluster (as it does not make sense to use the Hetzner LB in that situation). 
 # It can work with any ingress controller that you choose to deploy. 
 # Please note that because the klipperLB points to all nodes, we automatically allow scheduling on the control plane when it is active. 
 # enable_klipper_metal_lb = ""true""  
 # If you want to configure additional arguments for traefik, enter them here as a list and in the form of traefik CLI arguments; see https://doc.traefik.io/traefik/reference/static-configuration/cli/ 
 # They are the options that go into the additionalArguments section of the Traefik helm values file. 
 # Example: traefik_additional_options = [""--log.level=DEBUG"", ""--tracing=true""] 
 # traefik_additional_options = []  
 # By default traefik is configured to redirect http traffic to https, you can set this to ""false"" to disable the redirection. 
 # traefik_redirect_to_https = false  
 # If you want to disable the metric server set this to ""false"". Default is ""true"". 
 # enable_metrics_server = false  
 # If you want to allow non-control-plane workloads to run on the control-plane nodes, set this to ""true"". The default is ""false"". 
 # True by default for single node clusters, and when enable_klipper_metal_lb is true. In those cases, the value below will be ignored. 
 # allow_scheduling_on_control_plane = true  
 # If you want to disable the automatic upgrade of k3s, you can set below to ""false"". 
 # Ideally, keep it on, to always have the latest Kubernetes version, but lock the initial_k3s_channel to a kube major version, 
 # of your choice, like v1.25 or v1.26. That way you get the best of both worlds without the breaking changes risk. 
 # For production use, always use an HA setup with at least 3 control-plane nodes and 2 agents, and keep this on for maximum security.  
 # The default is ""true"" (in HA setup i.e. at least 3 control plane nodes & 2 agents, just keep it enabled since it works flawlessly). 
 # automatically_upgrade_k3s = false  
 # The default is ""true"" (in HA setup it works wonderfully well, with automatic roll-back to the previous snapshot in case of an issue). 
 # IMPORTANT! For non-HA clusters i.e. when the number of control-plane nodes is < 3, you have to turn it off. 
 # automatically_upgrade_os = false  
 # If you need more control over kured and the reboot behaviour, you can pass additional options to kured. 
 # For example limiting reboots to certain timeframes. For all options see: https://kured.dev/docs/configuration/ 
 # The default options are: `--reboot-command=/usr/bin/systemctl reboot --pre-reboot-node-labels=kured=rebooting --post-reboot-node-labels=kured=done --period=5m` 
 # Defaults can be overridden by using the same key. 
 # kured_options = { 
 #   ""reboot-days"": ""su"" 
 #   ""start-time"": ""3am"" 
 #   ""end-time"": ""8am"" 
 # }  
 # Allows you to specify either stable, latest, testing or supported minor versions. 
 # see https://rancher.com/docs/k3s/latest/en/upgrades/basic/ and https://update.k3s.io/v1-release/channels 
 #  If you are going to use Rancher addons for instance, it's always a good idea to fix the kube version to latest - 0.01, 
 # at the time of writing the latest is v1.26, so setting the value below to ""v1.25"" will insure maximum compatibility with Rancher, Longhorn and so on! 
 # The default is ""v1.25"". 
 # initial_k3s_channel = ""stable""  
 # The cluster name, by default ""k3s"" 
 # cluster_name = """"  
 # Whether to use the cluster name in the node name, in the form of {cluster_name}-{nodepool_name}, the default is ""true"". 
 # use_cluster_name_in_node_name = false  
 # Extra k3s registries. This is useful if you have private registries and you 
 # want to pull images without additional secrets. 
 # registries.yaml file docs: https://docs.k3s.io/installation/private-registry 
 /* k3s_registries = <<-EOT 
 mirrors: 
 hub.my_registry.com: 
 endpoint: 
 - ""hub.my_registry.com"" 
 configs: 
 hub.my_registry.com: 
 auth: 
 username: username 
 password: password 
 EOT */  
 # If you want to allow all outbound traffic you can set this to ""false"". Default is ""true"". 
 # restrict_outbound_traffic = false  
 # Adding extra firewall rules, like opening a port 
 # More info on the format here https://registry.terraform.io/providers/hetznercloud/hcloud/latest/docs/resources/firewall 
 # extra_firewall_rules = [ 
 #   # For Postgres 
 #   { 
 #     direction       = ""in"" 
 #     protocol        = ""tcp"" 
 #     port            = ""5432"" 
 #     source_ips      = [""0.0.0.0/0"", ""::/0""] 
 #     destination_ips = [] # Won't be used for this rule 
 #   }, 
 #   # To Allow ArgoCD access to resources via SSH 
 #   { 
 #     direction       = ""out"" 
 #     protocol        = ""tcp"" 
 #     port            = ""22"" 
 #     source_ips      = [] # Won't be used for this rule 
 #     destination_ips = [""0.0.0.0/0"", ""::/0""] 
 #   } 
 # ]  
 # If you want to configure a different CNI for k3s, use this flag 
 # possible values: flannel (Default), calico, and cilium 
 # As for Cilium, we allow infinite configurations via helm values, please check the CNI section of the readme over at https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/#cni. 
 # Also, see the cilium_values at towards the end of this file, in the advanced section. 
 # cni_plugin = ""cilium""  
 # If you want to disable the k3s default network policy controller, use this flag! 
 # Both Calico and Ciliun cni_plugin values override this value to true automatically, the default is ""false"". 
 # disable_network_policy = true  
 # If you want to disable the automatic use of placement group ""spread"". See https://docs.hetzner.com/cloud/placement-groups/overview/ 
 # That may be useful if you need to deploy more than 500 nodes! The default is ""false"". 
 # placement_group_disable = true  
 # By default, we allow ICMP ping in to the nodes, to check for liveness for instance. If you do not want to allow that, you can. Just set this flag to true (false by default). 
 # block_icmp_ping_in = true  
 # You can enable cert-manager (installed by Helm behind the scenes) with the following flag, the default is ""false"". 
 # enable_cert_manager = true  
 # By default we use a known good mirror to download the needed OpenSUSE, but for instance, it may not be available in US-East location, in which case, 
 # you can find a working mirror for you at https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2.mirrorlist, 
 # Or use the value below to automatically select an optimal mirror (by default we have it fixed to one german mirror that we know works for most locations) 
 # opensuse_microos_mirror_link = ""https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2""  
 # IP Addresses to use for the DNS Servers, set to an empty list to use the ones provided by Hetzner, defaults to [""1.1.1.1"", "" 1.0.0.1"", ""8.8.8.8""]. 
 # For rancher installs, best to leave it as default. 
 # dns_servers = []  
 # When this is enabled, rather than the first node, all external traffic will be routed via a control-plane loadbalancer, allowing for high availability. 
 # The default is false. 
 # use_control_plane_lb = true  
 # Let's say you are not using the control plane LB solution above, and still want to have one hostname point to all your control-plane nodes. 
 # You could create multiple A records of to let's say cp.cluster.my.org pointing to all of your control-plane nodes ips. 
 # In which case, you need to define that hostname in the k3s TLS-SANs config to allow connection through it. It can be hostnames or IP addresses. 
 # additional_tls_sans = [""cp.cluster.my.org""]  
 # Oftentimes, you need to communicate to the cluster from inside the cluster itself, in which case it is important to set this value, as it will configure the hostname 
 # at the load balancer level, and will save you from many slows downs when initiating communications from inside. Later on, you can point your DNS to the IP given 
 # to the LB. And if you have other services pointing to it, you are also free to create CNAMES to point to it, or whatever you see fit. 
 # If set, it will apply to either ingress controllers, Traefik or Ingress-Nginx. 
 # lb_hostname = """"  
 # You can enable Rancher (installed by Helm behind the scenes) with the following flag, the default is ""false"". 
 # When Rancher is enabled, it automatically installs cert-manager too, and it uses rancher's own self-signed certificates. 
 # See for options https://rancher.com/docs/rancher/v2.0-v2.4/en/installation/resources/advanced/helm2/helm-rancher/#choose-your-ssl-configuration 
 # The easiest thing is to leave everything as is (using the default rancher self-signed certificate) and put Cloudflare in front of it. 
 # As for the number of replicas, by default it is set to the numbe of control plane nodes. 
 # You can customized all of the above by adding a rancher_values variable see at the end of this file in the advanced section. 
 # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration. 
 # IMPORTANT: Rancher's install is quite memory intensive, you will require at least 4GB if RAM, meaning cx21 server type (for your control plane). 
 # ALSO, in order for Rancher to successfully deploy, you have to set the ""rancher_hostname"". 
 # enable_rancher = true  
 # If using Rancher you can set the Rancher hostname, it must be unique hostname even if you do not use it. 
 # If not pointing the DNS, you can just port-forward locally via kubectl to get access to the dashboard. 
 # If you already set the lb_hostname above and are using a Hetzner LB, you do not need to set this one, as it will be used by default. 
 # But if you set this one explicitly, it will have preference over the lb_hostname in rancher settings. 
 # rancher_hostname = ""rancher.xyz.dev""  
 # When Rancher is deployed, by default is uses the ""latest"" channel. But this can be customized. 
 # The allowed values are ""stable"" or ""latest"". 
 # rancher_install_channel = ""stable""  
 # Finally, you can specify a bootstrap-password for your rancher instance. Minimum 48 characters long! 
 # If you leave empty, one will be generated for you. 
 # (Can be used by another rancher2 provider to continue setup of rancher outside this module.) 
 # rancher_bootstrap_password = """"  
 # Separate from the above Rancher config (only use one or the other). You can import this cluster directly on an 
 # an already active Rancher install. By clicking ""import cluster"" choosing ""generic"", giving it a name and pasting 
 # the cluster registration url below. However, you can also ignore that and apply the url via kubectl as instructed 
 # by Rancher in the wizard, and that would register your cluster too. 
 # More information about the registration can be found here https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/registered-clusters/ 
 # rancher_registration_manifest_url = ""https://rancher.xyz.dev/v3/import/xxxxxxxxxxxxxxxxxxYYYYYYYYYYYYYYYYYYYzzzzzzzzzzzzzzzzzzzzz.yaml""  
 # Extra values that will be passed to the `extra-manifests/kustomization.yaml.tpl` if its present. 
 # extra_kustomize_parameters={}  
 # It is best practice to turn this off, but for backwards compatibility it is set to ""true"" by default. 
 # See https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/issues/349 
 # When ""false"". The kubeconfig file can instead be created by executing: ""terraform output --raw kubeconfig > cluster_kubeconfig.yaml"" 
 # Always be careful to not commit this file! 
 # create_kubeconfig = false  
 # Don't create the kustomize backup. This can be helpful for automation. 
 # create_kustomization = false  
 ### ADVANCED - Custom helm values for packages above (search _values if you want to located where those are mentioned upper in this file) 
 #  Inside the _values variable below are examples, up to you to find out the best helm values possible, we do not provide support for customized helm values. 
 # Please understand that the indentation is very important, inside the EOTs, as those are proper yaml helm values. 
 # We advise you to use the default values, and only change them if you know what you are doing!  
 # Cilium, all Cilium helm values can be found at https://github.com/cilium/cilium/blob/master/install/kubernetes/cilium/values.yaml 
 # The following is an example, please note that the current indentation inside the EOT is important. 
 /*   cilium_values = <<EOT 
 ipam: 
 mode: kubernetes 
 devices: ""eth1"" 
 k8s: 
 requireIPv4PodCIDR: true 
 kubeProxyReplacement: strict 
 EOT */  
 # Cert manager, all cert-manager helm values can be found at https://github.com/cert-manager/cert-manager/blob/master/deploy/charts/cert-manager/values.yaml 
 # The following is an example, please note that the current indentation inside the EOT is important. 
 /*   cert_manager_values = <<EOT 
 installCRDs: true 
 replicaCount: 3 
 webhook: 
 replicaCount: 3 
 cainjector: 
 replicaCount: 3 
 EOT */  
 # Longhorn, all Longhorn helm values can be found at https://github.com/longhorn/longhorn/blob/master/chart/values.yaml 
 # The following is an example, please note that the current indentation inside the EOT is important. 
 /*   longhorn_values = <<EOT 
 defaultSettings: 
 defaultDataPath: /var/longhorn 
 persistence: 
 defaultFsType: ext4 
 defaultClassReplicaCount: 3 
 defaultClass: true 
 EOT */  
 # Nginx, all Nginx helm values can be found at https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml 
 # You can also have a look at https://kubernetes.github.io/ingress-nginx/, to understand how it works, and all the options at your disposal. 
 # The following is an example, please note that the current indentation inside the EOT is important. 
 /*   nginx_ingress_values = <<EOT 
 controller: 
 watchIngressWithoutClass: ""true"" 
 kind: ""DaemonSet"" 
 config: 
 ""use-forwarded-headers"": ""true"" 
 ""compute-full-forwarded-for"": ""true"" 
 ""use-proxy-protocol"": ""true"" 
 service: 
 annotations: 
 ""load-balancer.hetzner.cloud/name"": ""k3s"" 
 ""load-balancer.hetzner.cloud/use-private-ip"": ""true"" 
 ""load-balancer.hetzner.cloud/disable-private-ingress"": ""true"" 
 ""load-balancer.hetzner.cloud/location"": ""nbg1"" 
 ""load-balancer.hetzner.cloud/type"": ""lb11"" 
 ""load-balancer.hetzner.cloud/uses-proxyprotocol"": ""true"" 
 EOT */  
 # Rancher, all Rancher helm values can be found at https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/chart-options/ 
 # The following is an example, please note that the current indentation inside the EOT is important. 
 /*   rancher_values = <<EOT 
 ingress: 
 tls: 
 source: ""rancher"" 
 hostname: ""rancher.example.com"" 
 replicas: 1 
 bootstrapPassword: ""supermario"" 
 EOT */ ","module ""kube-hetzner"" {
  providers = {
    hcloud = hcloud
  }
  hcloud_token = local.hcloud_token

  # Then fill or edit the below values. Only the first values starting with a * are obligatory; the rest can remain with their default values, or you
  # could adapt them to your needs.

  # * For local dev, path to the git repo
  # source = ""../../kube-hetzner/""
  # If you want to use the latest master branch
  # source = ""github.com/kube-hetzner/terraform-hcloud-kube-hetzner""
  # For normal use, this is the path to the terraform registry
  source = ""kube-hetzner/kube-hetzner/hcloud""

  # You can optionally specify a version number
  # version = ""1.2.0""

  # Note that some values, notably ""location"" and ""public_key"" have no effect after initializing the cluster.
  # This is to keep Terraform from re-provisioning all nodes at once, which would lose data. If you want to update
  # those, you should instead change the value here and manually re-provision each node. Grep for ""lifecycle"".

  # Customize the SSH port (by default 22)
  # ssh_port = 2222

  # * Your ssh public key
  ssh_public_key = file(""~/.ssh/id_rsa.pub"")
  # * Your private key must be ""ssh_private_key = null"" when you want to use ssh-agent for a Yubikey-like device authentification or an SSH key-pair with a passphrase.
  # For more details on SSH see https://github.com/kube-hetzner/kube-hetzner/blob/master/docs/ssh.md
  ssh_private_key = file(""~/.ssh/id_rsa"")
  # You can add additional SSH public Keys to grant other team members root access to your cluster nodes.
  # ssh_additional_public_keys = []

  # You can also add additional SSH public Keys which are saved in the hetzner cloud by a label.
  # See https://docs.hetzner.cloud/#label-selector
  # ssh_hcloud_key_label = ""role=admin""

  # If you want to use an ssh key that is already registered within hetzner cloud, you can pass its id.
  # If no id is passed, a new ssh key will be registered within hetzner cloud.
  # It is important that exactly this key is passed via `ssh_public_key` & `ssh_private_key` vars.
  # hcloud_ssh_key_id = """"

  # These can be customized, or left with the default values
  # * For Hetzner locations see https://docs.hetzner.com/general/others/data-centers-and-connection/
  network_region = ""eu-central"" # change to `us-east` if location is ash

  # For the control planes, at least three nodes are the minimum for HA. Otherwise, you need to turn off the automatic upgrades (see README).
  # **It must always be an ODD number, never even!** Search the internet for ""splitbrain problem with etcd"" or see https://rancher.com/docs/k3s/latest/en/installation/ha-embedded/
  # For instance, one is ok (non-HA), two is not ok, and three is ok (becomes HA). It does not matter if they are in the same nodepool or not! So they can be in different locations and of various types.

  # Of course, you can choose any number of nodepools you want, with the location you want. The only constraint on the location is that you need to stay in the same network region, Europe, or the US.
  # For the server type, the minimum instance supported is cpx11 (just a few cents more than cx11); see https://www.hetzner.com/cloud.

  # IMPORTANT: Before you create your cluster, you can do anything you want with the nodepools, but you need at least one of each, control plane and agent.
  # Once the cluster is up and running, you can change nodepool count and even set it to 0 (in the case of the first control-plane nodepool, the minimum is 1).
  # You can also rename it (if the count is 0), but do not remove a nodepool from the list.

  # The only nodepools that are safe to remove from the list are at the end. That is due to how subnets and IPs get allocated (FILO).
  # You can, however, freely add other nodepools at the end of each list if you want. The maximum number of nodepools you can create combined for both lists is 255.
  # Also, before decreasing the count of any nodepools to 0, it's essential to drain and cordon the nodes in question. Otherwise, it will leave your cluster in a bad state.

  # Before initializing the cluster, you can change all parameters and add or remove any nodepools. You need at least one nodepool of each kind, control plane, and agent.
  # The nodepool names are entirely arbitrary, you can choose whatever you want, but no special characters or underscore, and they must be unique; only alphanumeric characters and dashes are allowed.

  # If you want to have a single node cluster, have one control plane nodepools with a count of 1, and one agent nodepool with a count of 0.

  # Please note that changing labels and taints after the first run will have no effect. If needed, you can do that through Kubernetes directly.

  # * Example below:

  control_plane_nodepools = [
    {
      name        = ""control-plane-fsn1"",
      server_type = ""cpx11"",
      location    = ""fsn1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""control-plane-nbg1"",
      server_type = ""cpx11"",
      location    = ""nbg1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""control-plane-hel1"",
      server_type = ""cpx11"",
      location    = ""hel1"",
      labels      = [],
      taints      = [],
      count       = 1
    }
  ]

  agent_nodepools = [
    {
      name        = ""agent-small"",
      server_type = ""cpx11"",
      location    = ""fsn1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""agent-large"",
      server_type = ""cpx21"",
      location    = ""nbg1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""storage"",
      server_type = ""cpx21"",
      location    = ""fsn1"",
      # Fully optional, just a demo.
      labels      = [
        ""node.kubernetes.io/server-usage=storage""
      ],
      taints      = [],
      count       = 1
      # In the case of using Longhorn, you can use Hetzner volumes instead of using the node's own storage by specifying a value from 10 to 10000 (in GB)
      # It will create one volume per node in the nodepool, and configure Longhorn to use them.
      # Something worth noting is that Volume storage is slower than node storage, which is achieved by not mentioning longhorn_volume_size or setting it to 0.
      # So for something like DBs, you definitely want node storage, for other things like backups, volume storage is fine, and cheaper.
      # longhorn_volume_size = 20
    }
  ]
  # Add custom control plane configuration options here.
  # E.g to enable monitoring for etcd, proxy etc:
  # control_planes_custom_config = {
  #  etcd-expose-metrics = true,
  #  kube-controller-manager-arg = ""bind-address=0.0.0.0"",
  #  kube-proxy-arg =""metrics-bind-address=0.0.0.0"",
  #  kube-scheduler-arg = ""bind-address=0.0.0.0"",
  # }

  # * LB location and type, the latter will depend on how much load you want it to handle, see https://www.hetzner.com/cloud/load-balancer
  load_balancer_type     = ""lb11""
  load_balancer_location = ""fsn1""

  ### The following values are entirely optional (and can be removed from this if unused)

  # You can refine a base domain name to be use in this form of nodename.base_domain for setting the reserve dns inside Hetzner
  # base_domain = ""mycluster.example.com""

  # Cluster Autoscaler
  # Providing at least one map for the array enables the cluster autoscaler feature, default is disabled
  # Please note that the autoscaler should not be used with initial_k3s_channel < ""v1.25"". So ideally lock it to ""v1.25"".
  # * Example below:
  # autoscaler_nodepools = [
  #   {
  #     name        = ""autoscaler""
  #     server_type = ""cpx21"" # must be same or better than the control_plane server type (regarding disk size)!
  #     location    = ""fsn1""
  #     min_nodes   = 0
  #     max_nodes   = 5
  #   }
  # ]

  # Enable etcd snapshot backups to S3 storage.
  # Just provide a map with the needed settings (according to your S3 storage provider) and backups to S3 will
  # be enabled (with the default settings for etcd snapshots).
  # Cloudflare's R2 offers 10GB, 10 million reads and 1 million writes per month for free.
  # For proper context, have a look at https://docs.k3s.io/backup-restore.
  # etcd_s3_backup = {
  #   etcd-s3-endpoint        = ""xxxx.r2.cloudflarestorage.com""
  #   etcd-s3-access-key      = ""<access-key>""
  #   etcd-s3-secret-key      = ""<secret-key>""
  #   etcd-s3-bucket          = ""k3s-etcd-snapshots""
  # }

  # To use local storage on the nodes, you can enable Longhorn, default is ""false"".
  # See a full recap on how to configure agent nodepools for longhorn here https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/discussions/373#discussioncomment-3983159
  # enable_longhorn = true

  # By default, longhorn is pulled from https://charts.longhorn.io.
  # If you need a version of longhorn which assures compatibility with rancher you can set this variable to https://charts.rancher.io. 
  # longhorn_repository = ""https://charts.rancher.io""

  # The namespace for longhorn deployment, default is ""longhorn-system"".
  # longhorn_namespace = ""longhorn-system""

  # The file system type for Longhorn, if enabled (ext4 is the default, otherwise you can choose xfs).
  # longhorn_fstype = ""xfs""

  # how many replica volumes should longhorn create (default is 3).
  # longhorn_replica_count = 1

  # When you enable Longhorn, you can go with the default settings and just modify the above two variables OR you can add a longhorn_values variable
  # with all needed helm values, see towards the end of the file in the advanced section.
  # If that file is present, the system will use it during the deploy, if not it will use the default values with the two variable above that can be customized.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.

  # Also, you can choose to use a Hetzner volume with Longhorn. By default, it will use the nodes own storage space, but if you add an attribute of
  # longhorn_volume_size ( not a variable, just a possible agent nodepool attribute) with a value between 10 and 10000 GB to your agent nodepool definition, it will create and use the volume in question.
  # See the agent nodepool section for an example of how to do that.

  # To disable Hetzner CSI storage, you can set the following to ""true"", default is ""false"".
  # disable_hetzner_csi = true

  # If you want to use a specific Hetzner CCM and CSI version, set them below; otherwise, leave them as-is for the latest versions.
  # hetzner_ccm_version = """"
  # hetzner_csi_version = """"

  # If you want to specify the Kured version, set it below - otherwise it'll use the latest version available.
  # kured_version = """"

  # If you want to specify what time Kured starts restarting nodes, set it below. Default is 3AM.
  # kured_start_time = """"

  # If you want to specify what time Kured stops restarting nodes, set it below. Default is 8AM.
  # kured_end_time = """"

  # If you want to specify what timezone Kured uses, set it below. Default is Local.
  # kured_time_zone = """"

  # If you want to enable the Nginx ingress controller (https://kubernetes.github.io/ingress-nginx/) instead of Traefik, you can set this to ""nginx"". Default is ""traefik"".
  # By the default we load optimal Traefik and Nginx ingress controller config for Hetzner, however you may need to tweak it to your needs, so to do,
  # we allow you to add a traefik_values and nginx_values, see towards the end of this file in the advanced section.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.
  # If you want to disable both controllers set this to ""none""
  # ingress_controller = ""nginx""

  # You can change the number of replicas for selected ingress controller here. The default 0 means autoselecting based on number of agent nodes (1 node = 1 replica, 2 nodes = 2 replicas, 3+ nodes = 3 replicas)
  # ingress_replica_count = 1

  # Use the klipperLB (similar to metalLB), instead of the default Hetzner one, that has an advantage of dropping the cost of the setup.
  # Automatically ""true"" in the case of single node cluster (as it does not make sense to use the Hetzner LB in that situation).
  # It can work with any ingress controller that you choose to deploy.
  # Please note that because the klipperLB points to all nodes, we automatically allow scheduling on the control plane when it is active.
  # enable_klipper_metal_lb = ""true""

  # If you want to configure additional arguments for traefik, enter them here as a list and in the form of traefik CLI arguments; see https://doc.traefik.io/traefik/reference/static-configuration/cli/
  # They are the options that go into the additionalArguments section of the Traefik helm values file.
  # Example: traefik_additional_options = [""--log.level=DEBUG"", ""--tracing=true""]
  # traefik_additional_options = []

  # By default traefik is configured to redirect http traffic to https, you can set this to ""false"" to disable the redirection.
  # traefik_redirect_to_https = false

  # If you want to disable the metric server set this to ""false"". Default is ""true"".
  # enable_metrics_server = false

  # If you want to allow non-control-plane workloads to run on the control-plane nodes, set this to ""true"". The default is ""false"".
  # True by default for single node clusters, and when enable_klipper_metal_lb is true. In those cases, the value below will be ignored.
  # allow_scheduling_on_control_plane = true

  # If you want to disable the automatic upgrade of k3s, you can set below to ""false"".
  # Ideally, keep it on, to always have the latest Kubernetes version, but lock the initial_k3s_channel to a kube major version,
  # of your choice, like v1.25 or v1.26. That way you get the best of both worlds without the breaking changes risk.
  # For production use, always use an HA setup with at least 3 control-plane nodes and 2 agents, and keep this on for maximum security.

  # The default is ""true"" (in HA setup i.e. at least 3 control plane nodes & 2 agents, just keep it enabled since it works flawlessly).
  # automatically_upgrade_k3s = false

  # The default is ""true"" (in HA setup it works wonderfully well, with automatic roll-back to the previous snapshot in case of an issue).
  # IMPORTANT! For non-HA clusters i.e. when the number of control-plane nodes is < 3, you have to turn it off.
  # automatically_upgrade_os = false

  # If you need more control over kured and the reboot behaviour, you can pass additional options to kured.
  # For example limiting reboots to certain timeframes. For all options see: https://kured.dev/docs/configuration/
  # The default options are: `--reboot-command=/usr/bin/systemctl reboot --pre-reboot-node-labels=kured=rebooting --post-reboot-node-labels=kured=done --period=5m`
  # Defaults can be overridden by using the same key.
  # kured_options = {
  #   ""reboot-days"": ""su""
  #   ""start-time"": ""3am""
  #   ""end-time"": ""8am""
  # }

  # Allows you to specify either stable, latest, testing or supported minor versions.
  # see https://rancher.com/docs/k3s/latest/en/upgrades/basic/ and https://update.k3s.io/v1-release/channels
  #  If you are going to use Rancher addons for instance, it's always a good idea to fix the kube version to latest - 0.01,
  # at the time of writing the latest is v1.26, so setting the value below to ""v1.25"" will insure maximum compatibility with Rancher, Longhorn and so on!
  # The default is ""v1.25"".
  # initial_k3s_channel = ""stable""

  # The cluster name, by default ""k3s""
  # cluster_name = """"

  # Whether to use the cluster name in the node name, in the form of {cluster_name}-{nodepool_name}, the default is ""true"".
  # use_cluster_name_in_node_name = false

  # Extra k3s registries. This is useful if you have private registries and you
  # want to pull images without additional secrets.
  # registries.yaml file docs: https://docs.k3s.io/installation/private-registry
  /* k3s_registries = <<-EOT
    mirrors:
      hub.my_registry.com:
        endpoint:
          - ""hub.my_registry.com""
    configs:
      hub.my_registry.com:
        auth:
          username: username
          password: password
  EOT */

  # If you want to allow all outbound traffic you can set this to ""false"". Default is ""true"".
  # restrict_outbound_traffic = false

  # Adding extra firewall rules, like opening a port
  # More info on the format here https://registry.terraform.io/providers/hetznercloud/hcloud/latest/docs/resources/firewall
  # extra_firewall_rules = [
  #   # For Postgres
  #   {
  #     direction       = ""in""
  #     protocol        = ""tcp""
  #     port            = ""5432""
  #     source_ips      = [""0.0.0.0/0"", ""::/0""]
  #     destination_ips = [] # Won't be used for this rule
  #   },
  #   # To Allow ArgoCD access to resources via SSH
  #   {
  #     direction       = ""out""
  #     protocol        = ""tcp""
  #     port            = ""22""
  #     source_ips      = [] # Won't be used for this rule
  #     destination_ips = [""0.0.0.0/0"", ""::/0""]
  #   }
  # ]

  # If you want to configure a different CNI for k3s, use this flag
  # possible values: flannel (Default), calico, and cilium
  # As for Cilium, we allow infinite configurations via helm values, please check the CNI section of the readme over at https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/#cni.
  # Also, see the cilium_values at towards the end of this file, in the advanced section.
  # cni_plugin = ""cilium""

  # If you want to disable the k3s default network policy controller, use this flag!
  # Both Calico and Ciliun cni_plugin values override this value to true automatically, the default is ""false"".
  # disable_network_policy = true

  # If you want to disable the automatic use of placement group ""spread"". See https://docs.hetzner.com/cloud/placement-groups/overview/
  # That may be useful if you need to deploy more than 500 nodes! The default is ""false"".
  # placement_group_disable = true

  # By default, we allow ICMP ping in to the nodes, to check for liveness for instance. If you do not want to allow that, you can. Just set this flag to true (false by default).
  # block_icmp_ping_in = true

  # You can enable cert-manager (installed by Helm behind the scenes) with the following flag, the default is ""false"".
  # enable_cert_manager = true

  # By default we use a known good mirror to download the needed OpenSUSE, but for instance, it may not be available in US-East location, in which case,
  # you can find a working mirror for you at https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2.mirrorlist,
  # Or use the value below to automatically select an optimal mirror (by default we have it fixed to one german mirror that we know works for most locations)
  # opensuse_microos_mirror_link = ""https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2""

  # IP Addresses to use for the DNS Servers, set to an empty list to use the ones provided by Hetzner, defaults to [""1.1.1.1"", "" 1.0.0.1"", ""8.8.8.8""].
  # For rancher installs, best to leave it as default.
  # dns_servers = []

  # When this is enabled, rather than the first node, all external traffic will be routed via a control-plane loadbalancer, allowing for high availability.
  # The default is false.
  # use_control_plane_lb = true

  # Let's say you are not using the control plane LB solution above, and still want to have one hostname point to all your control-plane nodes.
  # You could create multiple A records of to let's say cp.cluster.my.org pointing to all of your control-plane nodes ips.
  # In which case, you need to define that hostname in the k3s TLS-SANs config to allow connection through it. It can be hostnames or IP addresses.
  # additional_tls_sans = [""cp.cluster.my.org""]

  # Oftentimes, you need to communicate to the cluster from inside the cluster itself, in which case it is important to set this value, as it will configure the hostname
  # at the load balancer level, and will save you from many slows downs when initiating communications from inside. Later on, you can point your DNS to the IP given
  # to the LB. And if you have other services pointing to it, you are also free to create CNAMES to point to it, or whatever you see fit.
  # If set, it will apply to either ingress controllers, Traefik or Ingress-Nginx.
  # lb_hostname = """"

  # You can enable Rancher (installed by Helm behind the scenes) with the following flag, the default is ""false"".
  # When Rancher is enabled, it automatically installs cert-manager too, and it uses rancher's own self-signed certificates.
  # See for options https://rancher.com/docs/rancher/v2.0-v2.4/en/installation/resources/advanced/helm2/helm-rancher/#choose-your-ssl-configuration
  # The easiest thing is to leave everything as is (using the default rancher self-signed certificate) and put Cloudflare in front of it.
  # As for the number of replicas, by default it is set to the numbe of control plane nodes.
  # You can customized all of the above by adding a rancher_values variable see at the end of this file in the advanced section.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.
  # IMPORTANT: Rancher's install is quite memory intensive, you will require at least 4GB if RAM, meaning cx21 server type (for your control plane).
  # ALSO, in order for Rancher to successfully deploy, you have to set the ""rancher_hostname"".
  # enable_rancher = true

  # If using Rancher you can set the Rancher hostname, it must be unique hostname even if you do not use it.
  # If not pointing the DNS, you can just port-forward locally via kubectl to get access to the dashboard.
  # If you already set the lb_hostname above and are using a Hetzner LB, you do not need to set this one, as it will be used by default.
  # But if you set this one explicitly, it will have preference over the lb_hostname in rancher settings.
  # rancher_hostname = ""rancher.xyz.dev""

  # When Rancher is deployed, by default is uses the ""latest"" channel. But this can be customized.
  # The allowed values are ""stable"" or ""latest"".
  # rancher_install_channel = ""stable""

  # Finally, you can specify a bootstrap-password for your rancher instance. Minimum 48 characters long!
  # If you leave empty, one will be generated for you.
  # (Can be used by another rancher2 provider to continue setup of rancher outside this module.)
  # rancher_bootstrap_password = """"

  # Separate from the above Rancher config (only use one or the other). You can import this cluster directly on an
  # an already active Rancher install. By clicking ""import cluster"" choosing ""generic"", giving it a name and pasting
  # the cluster registration url below. However, you can also ignore that and apply the url via kubectl as instructed
  # by Rancher in the wizard, and that would register your cluster too.
  # More information about the registration can be found here https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/registered-clusters/
  # rancher_registration_manifest_url = ""https://rancher.xyz.dev/v3/import/xxxxxxxxxxxxxxxxxxYYYYYYYYYYYYYYYYYYYzzzzzzzzzzzzzzzzzzzzz.yaml""

  # Extra values that will be passed to the `extra-manifests/kustomization.yaml.tpl` if its present.
  # extra_kustomize_parameters={}

  # It is best practice to turn this off, but for backwards compatibility it is set to ""true"" by default.
  # See https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/issues/349
  # When ""false"". The kubeconfig file can instead be created by executing: ""terraform output --raw kubeconfig > cluster_kubeconfig.yaml""
  # Always be careful to not commit this file!
  # create_kubeconfig = false

  # Don't create the kustomize backup. This can be helpful for automation.
  # create_kustomization = false

  ### ADVANCED - Custom helm values for packages above (search _values if you want to located where those are mentioned upper in this file)
  #  Inside the _values variable below are examples, up to you to find out the best helm values possible, we do not provide support for customized helm values.
  # Please understand that the indentation is very important, inside the EOTs, as those are proper yaml helm values.
  # We advise you to use the default values, and only change them if you know what you are doing!

  # Cilium, all Cilium helm values can be found at https://github.com/cilium/cilium/blob/master/install/kubernetes/cilium/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   cilium_values = <<EOT
ipam:
  mode: kubernetes
devices: ""eth1""
k8s:
  requireIPv4PodCIDR: true
kubeProxyReplacement: strict
  EOT */

  # Cert manager, all cert-manager helm values can be found at https://github.com/cert-manager/cert-manager/blob/master/deploy/charts/cert-manager/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   cert_manager_values = <<EOT
installCRDs: true
replicaCount: 3
webhook:
  replicaCount: 3
cainjector:
  replicaCount: 3
  EOT */

  # Longhorn, all Longhorn helm values can be found at https://github.com/longhorn/longhorn/blob/master/chart/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   longhorn_values = <<EOT
defaultSettings:
  defaultDataPath: /var/longhorn
persistence:
  defaultFsType: ext4
  defaultClassReplicaCount: 3
  defaultClass: true
  EOT */

  # Nginx, all Nginx helm values can be found at https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml
  # You can also have a look at https://kubernetes.github.io/ingress-nginx/, to understand how it works, and all the options at your disposal.
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   nginx_ingress_values = <<EOT
controller:
  watchIngressWithoutClass: ""true""
  kind: ""DaemonSet""
  config:
    ""use-forwarded-headers"": ""true""
    ""compute-full-forwarded-for"": ""true""
    ""use-proxy-protocol"": ""true""
  service:
    annotations:
      ""load-balancer.hetzner.cloud/name"": ""k3s""
      ""load-balancer.hetzner.cloud/use-private-ip"": ""true""
      ""load-balancer.hetzner.cloud/disable-private-ingress"": ""true""
      ""load-balancer.hetzner.cloud/location"": ""nbg1""
      ""load-balancer.hetzner.cloud/type"": ""lb11""
      ""load-balancer.hetzner.cloud/uses-proxyprotocol"": ""true""
  EOT */

  # Rancher, all Rancher helm values can be found at https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/chart-options/
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   rancher_values = <<EOT
ingress:
  tls:
    source: ""rancher""
hostname: ""rancher.example.com""
replicas: 1
bootstrapPassword: ""supermario""
  EOT */

}
",module,"module ""kube-hetzner"" {
  providers = {
    hcloud = hcloud
  }
  hcloud_token = local.hcloud_token

  # Then fill or edit the below values. Only the first values starting with a * are obligatory; the rest can remain with their default values, or you
  # could adapt them to your needs.

  # * For local dev, path to the git repo
  # source = ""../../kube-hetzner/""
  # If you want to use the latest master branch
  # source = ""github.com/kube-hetzner/terraform-hcloud-kube-hetzner""
  # For normal use, this is the path to the terraform registry
  source = ""kube-hetzner/kube-hetzner/hcloud""

  # You can optionally specify a version number
  # version = ""1.2.0""

  # Note that some values, notably ""location"" and ""public_key"" have no effect after initializing the cluster.
  # This is to keep Terraform from re-provisioning all nodes at once, which would lose data. If you want to update
  # those, you should instead change the value here and manually re-provision each node. Grep for ""lifecycle"".

  # Customize the SSH port (by default 22)
  # ssh_port = 2222

  # * Your ssh public key
  ssh_public_key = file(""~/.ssh/id_rsa.pub"")
  # * Your private key must be ""ssh_private_key = null"" when you want to use ssh-agent for a Yubikey-like device authentification or an SSH key-pair with a passphrase.
  # For more details on SSH see https://github.com/kube-hetzner/kube-hetzner/blob/master/docs/ssh.md
  ssh_private_key = file(""~/.ssh/id_rsa"")
  # You can add additional SSH public Keys to grant other team members root access to your cluster nodes.
  # ssh_additional_public_keys = []

  # You can also add additional SSH public Keys which are saved in the hetzner cloud by a label.
  # See https://docs.hetzner.cloud/#label-selector
  # ssh_hcloud_key_label = ""role=admin""

  # If you want to use an ssh key that is already registered within hetzner cloud, you can pass its id.
  # If no id is passed, a new ssh key will be registered within hetzner cloud.
  # It is important that exactly this key is passed via `ssh_public_key` & `ssh_private_key` vars.
  # hcloud_ssh_key_id = """"

  # These can be customized, or left with the default values
  # * For Hetzner locations see https://docs.hetzner.com/general/others/data-centers-and-connection/
  network_region = ""eu-central"" # change to `us-east` if location is ash

  # For the control planes, at least three nodes are the minimum for HA. Otherwise, you need to turn off the automatic upgrades (see README).
  # **It must always be an ODD number, never even!** Search the internet for ""splitbrain problem with etcd"" or see https://rancher.com/docs/k3s/latest/en/installation/ha-embedded/
  # For instance, one is ok (non-HA), two is not ok, and three is ok (becomes HA). It does not matter if they are in the same nodepool or not! So they can be in different locations and of various types.

  # Of course, you can choose any number of nodepools you want, with the location you want. The only constraint on the location is that you need to stay in the same network region, Europe, or the US.
  # For the server type, the minimum instance supported is cpx11 (just a few cents more than cx11); see https://www.hetzner.com/cloud.

  # IMPORTANT: Before you create your cluster, you can do anything you want with the nodepools, but you need at least one of each, control plane and agent.
  # Once the cluster is up and running, you can change nodepool count and even set it to 0 (in the case of the first control-plane nodepool, the minimum is 1).
  # You can also rename it (if the count is 0), but do not remove a nodepool from the list.

  # The only nodepools that are safe to remove from the list are at the end. That is due to how subnets and IPs get allocated (FILO).
  # You can, however, freely add other nodepools at the end of each list if you want. The maximum number of nodepools you can create combined for both lists is 255.
  # Also, before decreasing the count of any nodepools to 0, it's essential to drain and cordon the nodes in question. Otherwise, it will leave your cluster in a bad state.

  # Before initializing the cluster, you can change all parameters and add or remove any nodepools. You need at least one nodepool of each kind, control plane, and agent.
  # The nodepool names are entirely arbitrary, you can choose whatever you want, but no special characters or underscore, and they must be unique; only alphanumeric characters and dashes are allowed.

  # If you want to have a single node cluster, have one control plane nodepools with a count of 1, and one agent nodepool with a count of 0.

  # Please note that changing labels and taints after the first run will have no effect. If needed, you can do that through Kubernetes directly.

  # * Example below:

  control_plane_nodepools = [
    {
      name        = ""control-plane-fsn1"",
      server_type = ""cpx11"",
      location    = ""fsn1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""control-plane-nbg1"",
      server_type = ""cpx11"",
      location    = ""nbg1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""control-plane-hel1"",
      server_type = ""cpx11"",
      location    = ""hel1"",
      labels      = [],
      taints      = [],
      count       = 1
    }
  ]

  agent_nodepools = [
    {
      name        = ""agent-small"",
      server_type = ""cpx11"",
      location    = ""fsn1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""agent-large"",
      server_type = ""cpx21"",
      location    = ""nbg1"",
      labels      = [],
      taints      = [],
      count       = 1
    },
    {
      name        = ""storage"",
      server_type = ""cpx21"",
      location    = ""fsn1"",
      # Fully optional, just a demo.
      labels      = [
        ""node.kubernetes.io/server-usage=storage""
      ],
      taints      = [],
      count       = 1
      # In the case of using Longhorn, you can use Hetzner volumes instead of using the node's own storage by specifying a value from 10 to 10000 (in GB)
      # It will create one volume per node in the nodepool, and configure Longhorn to use them.
      # Something worth noting is that Volume storage is slower than node storage, which is achieved by not mentioning longhorn_volume_size or setting it to 0.
      # So for something like DBs, you definitely want node storage, for other things like backups, volume storage is fine, and cheaper.
      # longhorn_volume_size = 20
    }
  ]
  # Add custom control plane configuration options here.
  # E.g to enable monitoring for etcd, proxy etc:
  # control_planes_custom_config = {
  #  etcd-expose-metrics = true,
  #  kube-controller-manager-arg = ""bind-address=0.0.0.0"",
  #  kube-proxy-arg =""metrics-bind-address=0.0.0.0"",
  #  kube-scheduler-arg = ""bind-address=0.0.0.0"",
  # }

  # * LB location and type, the latter will depend on how much load you want it to handle, see https://www.hetzner.com/cloud/load-balancer
  load_balancer_type     = ""lb11""
  load_balancer_location = ""fsn1""

  ### The following values are entirely optional (and can be removed from this if unused)

  # You can refine a base domain name to be use in this form of nodename.base_domain for setting the reserve dns inside Hetzner
  # base_domain = ""mycluster.example.com""

  # Cluster Autoscaler
  # Providing at least one map for the array enables the cluster autoscaler feature, default is disabled
  # Please note that the autoscaler should not be used with initial_k3s_channel < ""v1.25"". So ideally lock it to ""v1.25"".
  # * Example below:
  # autoscaler_nodepools = [
  #   {
  #     name        = ""autoscaler""
  #     server_type = ""cpx21"" # must be same or better than the control_plane server type (regarding disk size)!
  #     location    = ""fsn1""
  #     min_nodes   = 0
  #     max_nodes   = 5
  #   }
  # ]

  # Enable etcd snapshot backups to S3 storage.
  # Just provide a map with the needed settings (according to your S3 storage provider) and backups to S3 will
  # be enabled (with the default settings for etcd snapshots).
  # Cloudflare's R2 offers 10GB, 10 million reads and 1 million writes per month for free.
  # For proper context, have a look at https://docs.k3s.io/backup-restore.
  # etcd_s3_backup = {
  #   etcd-s3-endpoint        = ""xxxx.r2.cloudflarestorage.com""
  #   etcd-s3-access-key      = ""<access-key>""
  #   etcd-s3-secret-key      = ""<secret-key>""
  #   etcd-s3-bucket          = ""k3s-etcd-snapshots""
  # }

  # To use local storage on the nodes, you can enable Longhorn, default is ""false"".
  # See a full recap on how to configure agent nodepools for longhorn here https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/discussions/373#discussioncomment-3983159
  # enable_longhorn = true

  # By default, longhorn is pulled from https://charts.longhorn.io.
  # If you need a version of longhorn which assures compatibility with rancher you can set this variable to https://charts.rancher.io. 
  # longhorn_repository = ""https://charts.rancher.io""

  # The namespace for longhorn deployment, default is ""longhorn-system"".
  # longhorn_namespace = ""longhorn-system""

  # The file system type for Longhorn, if enabled (ext4 is the default, otherwise you can choose xfs).
  # longhorn_fstype = ""xfs""

  # how many replica volumes should longhorn create (default is 3).
  # longhorn_replica_count = 1

  # When you enable Longhorn, you can go with the default settings and just modify the above two variables OR you can add a longhorn_values variable
  # with all needed helm values, see towards the end of the file in the advanced section.
  # If that file is present, the system will use it during the deploy, if not it will use the default values with the two variable above that can be customized.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.

  # Also, you can choose to use a Hetzner volume with Longhorn. By default, it will use the nodes own storage space, but if you add an attribute of
  # longhorn_volume_size ( not a variable, just a possible agent nodepool attribute) with a value between 10 and 10000 GB to your agent nodepool definition, it will create and use the volume in question.
  # See the agent nodepool section for an example of how to do that.

  # To disable Hetzner CSI storage, you can set the following to ""true"", default is ""false"".
  # disable_hetzner_csi = true

  # If you want to use a specific Hetzner CCM and CSI version, set them below; otherwise, leave them as-is for the latest versions.
  # hetzner_ccm_version = """"
  # hetzner_csi_version = """"

  # If you want to specify the Kured version, set it below - otherwise it'll use the latest version available.
  # kured_version = """"

  # If you want to specify what time Kured starts restarting nodes, set it below. Default is 3AM.
  # kured_start_time = """"

  # If you want to specify what time Kured stops restarting nodes, set it below. Default is 8AM.
  # kured_end_time = """"

  # If you want to specify what timezone Kured uses, set it below. Default is Local.
  # kured_time_zone = """"

  # If you want to enable the Nginx ingress controller (https://kubernetes.github.io/ingress-nginx/) instead of Traefik, you can set this to ""nginx"". Default is ""traefik"".
  # By the default we load optimal Traefik and Nginx ingress controller config for Hetzner, however you may need to tweak it to your needs, so to do,
  # we allow you to add a traefik_values and nginx_values, see towards the end of this file in the advanced section.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.
  # If you want to disable both controllers set this to ""none""
  # ingress_controller = ""nginx""

  # You can change the number of replicas for selected ingress controller here. The default 0 means autoselecting based on number of agent nodes (1 node = 1 replica, 2 nodes = 2 replicas, 3+ nodes = 3 replicas)
  # ingress_replica_count = 1

  # Use the klipperLB (similar to metalLB), instead of the default Hetzner one, that has an advantage of dropping the cost of the setup.
  # Automatically ""true"" in the case of single node cluster (as it does not make sense to use the Hetzner LB in that situation).
  # It can work with any ingress controller that you choose to deploy.
  # Please note that because the klipperLB points to all nodes, we automatically allow scheduling on the control plane when it is active.
  # enable_klipper_metal_lb = ""true""

  # If you want to configure additional arguments for traefik, enter them here as a list and in the form of traefik CLI arguments; see https://doc.traefik.io/traefik/reference/static-configuration/cli/
  # They are the options that go into the additionalArguments section of the Traefik helm values file.
  # Example: traefik_additional_options = [""--log.level=DEBUG"", ""--tracing=true""]
  # traefik_additional_options = []

  # By default traefik is configured to redirect http traffic to https, you can set this to ""false"" to disable the redirection.
  # traefik_redirect_to_https = false

  # If you want to disable the metric server set this to ""false"". Default is ""true"".
  # enable_metrics_server = false

  # If you want to allow non-control-plane workloads to run on the control-plane nodes, set this to ""true"". The default is ""false"".
  # True by default for single node clusters, and when enable_klipper_metal_lb is true. In those cases, the value below will be ignored.
  # allow_scheduling_on_control_plane = true

  # If you want to disable the automatic upgrade of k3s, you can set below to ""false"".
  # Ideally, keep it on, to always have the latest Kubernetes version, but lock the initial_k3s_channel to a kube major version,
  # of your choice, like v1.25 or v1.26. That way you get the best of both worlds without the breaking changes risk.
  # For production use, always use an HA setup with at least 3 control-plane nodes and 2 agents, and keep this on for maximum security.

  # The default is ""true"" (in HA setup i.e. at least 3 control plane nodes & 2 agents, just keep it enabled since it works flawlessly).
  # automatically_upgrade_k3s = false

  # The default is ""true"" (in HA setup it works wonderfully well, with automatic roll-back to the previous snapshot in case of an issue).
  # IMPORTANT! For non-HA clusters i.e. when the number of control-plane nodes is < 3, you have to turn it off.
  # automatically_upgrade_os = false

  # If you need more control over kured and the reboot behaviour, you can pass additional options to kured.
  # For example limiting reboots to certain timeframes. For all options see: https://kured.dev/docs/configuration/
  # The default options are: `--reboot-command=/usr/bin/systemctl reboot --pre-reboot-node-labels=kured=rebooting --post-reboot-node-labels=kured=done --period=5m`
  # Defaults can be overridden by using the same key.
  # kured_options = {
  #   ""reboot-days"": ""su""
  #   ""start-time"": ""3am""
  #   ""end-time"": ""8am""
  # }

  # Allows you to specify either stable, latest, testing or supported minor versions.
  # see https://rancher.com/docs/k3s/latest/en/upgrades/basic/ and https://update.k3s.io/v1-release/channels
  #  If you are going to use Rancher addons for instance, it's always a good idea to fix the kube version to latest - 0.01,
  # at the time of writing the latest is v1.26, so setting the value below to ""v1.25"" will insure maximum compatibility with Rancher, Longhorn and so on!
  # The default is ""v1.25"".
  # initial_k3s_channel = ""stable""

  # The cluster name, by default ""k3s""
  # cluster_name = """"

  # Whether to use the cluster name in the node name, in the form of {cluster_name}-{nodepool_name}, the default is ""true"".
  # use_cluster_name_in_node_name = false

  # Extra k3s registries. This is useful if you have private registries and you
  # want to pull images without additional secrets.
  # registries.yaml file docs: https://docs.k3s.io/installation/private-registry
  /* k3s_registries = <<-EOT
    mirrors:
      hub.my_registry.com:
        endpoint:
          - ""hub.my_registry.com""
    configs:
      hub.my_registry.com:
        auth:
          username: username
          password: password
  EOT */

  # If you want to allow all outbound traffic you can set this to ""false"". Default is ""true"".
  # restrict_outbound_traffic = false

  # Adding extra firewall rules, like opening a port
  # More info on the format here https://registry.terraform.io/providers/hetznercloud/hcloud/latest/docs/resources/firewall
  # extra_firewall_rules = [
  #   # For Postgres
  #   {
  #     direction       = ""in""
  #     protocol        = ""tcp""
  #     port            = ""5432""
  #     source_ips      = [""0.0.0.0/0"", ""::/0""]
  #     destination_ips = [] # Won't be used for this rule
  #   },
  #   # To Allow ArgoCD access to resources via SSH
  #   {
  #     direction       = ""out""
  #     protocol        = ""tcp""
  #     port            = ""22""
  #     source_ips      = [] # Won't be used for this rule
  #     destination_ips = [""0.0.0.0/0"", ""::/0""]
  #   }
  # ]

  # If you want to configure a different CNI for k3s, use this flag
  # possible values: flannel (Default), calico, and cilium
  # As for Cilium, we allow infinite configurations via helm values, please check the CNI section of the readme over at https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/#cni.
  # Also, see the cilium_values at towards the end of this file, in the advanced section.
  # cni_plugin = ""cilium""

  # If you want to disable the k3s default network policy controller, use this flag!
  # Both Calico and Ciliun cni_plugin values override this value to true automatically, the default is ""false"".
  # disable_network_policy = true

  # If you want to disable the automatic use of placement group ""spread"". See https://docs.hetzner.com/cloud/placement-groups/overview/
  # That may be useful if you need to deploy more than 500 nodes! The default is ""false"".
  # placement_group_disable = true

  # By default, we allow ICMP ping in to the nodes, to check for liveness for instance. If you do not want to allow that, you can. Just set this flag to true (false by default).
  # block_icmp_ping_in = true

  # You can enable cert-manager (installed by Helm behind the scenes) with the following flag, the default is ""false"".
  # enable_cert_manager = true

  # By default we use a known good mirror to download the needed OpenSUSE, but for instance, it may not be available in US-East location, in which case,
  # you can find a working mirror for you at https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2.mirrorlist,
  # Or use the value below to automatically select an optimal mirror (by default we have it fixed to one german mirror that we know works for most locations)
  # opensuse_microos_mirror_link = ""https://download.opensuse.org/tumbleweed/appliances/openSUSE-MicroOS.x86_64-OpenStack-Cloud.qcow2""

  # IP Addresses to use for the DNS Servers, set to an empty list to use the ones provided by Hetzner, defaults to [""1.1.1.1"", "" 1.0.0.1"", ""8.8.8.8""].
  # For rancher installs, best to leave it as default.
  # dns_servers = []

  # When this is enabled, rather than the first node, all external traffic will be routed via a control-plane loadbalancer, allowing for high availability.
  # The default is false.
  # use_control_plane_lb = true

  # Let's say you are not using the control plane LB solution above, and still want to have one hostname point to all your control-plane nodes.
  # You could create multiple A records of to let's say cp.cluster.my.org pointing to all of your control-plane nodes ips.
  # In which case, you need to define that hostname in the k3s TLS-SANs config to allow connection through it. It can be hostnames or IP addresses.
  # additional_tls_sans = [""cp.cluster.my.org""]

  # Oftentimes, you need to communicate to the cluster from inside the cluster itself, in which case it is important to set this value, as it will configure the hostname
  # at the load balancer level, and will save you from many slows downs when initiating communications from inside. Later on, you can point your DNS to the IP given
  # to the LB. And if you have other services pointing to it, you are also free to create CNAMES to point to it, or whatever you see fit.
  # If set, it will apply to either ingress controllers, Traefik or Ingress-Nginx.
  # lb_hostname = """"

  # You can enable Rancher (installed by Helm behind the scenes) with the following flag, the default is ""false"".
  # When Rancher is enabled, it automatically installs cert-manager too, and it uses rancher's own self-signed certificates.
  # See for options https://rancher.com/docs/rancher/v2.0-v2.4/en/installation/resources/advanced/helm2/helm-rancher/#choose-your-ssl-configuration
  # The easiest thing is to leave everything as is (using the default rancher self-signed certificate) and put Cloudflare in front of it.
  # As for the number of replicas, by default it is set to the numbe of control plane nodes.
  # You can customized all of the above by adding a rancher_values variable see at the end of this file in the advanced section.
  # After the cluster is deployed, you can always use HelmChartConfig definition to tweak the configuration.
  # IMPORTANT: Rancher's install is quite memory intensive, you will require at least 4GB if RAM, meaning cx21 server type (for your control plane).
  # ALSO, in order for Rancher to successfully deploy, you have to set the ""rancher_hostname"".
  # enable_rancher = true

  # If using Rancher you can set the Rancher hostname, it must be unique hostname even if you do not use it.
  # If not pointing the DNS, you can just port-forward locally via kubectl to get access to the dashboard.
  # If you already set the lb_hostname above and are using a Hetzner LB, you do not need to set this one, as it will be used by default.
  # But if you set this one explicitly, it will have preference over the lb_hostname in rancher settings.
  # rancher_hostname = ""rancher.xyz.dev""

  # When Rancher is deployed, by default is uses the ""latest"" channel. But this can be customized.
  # The allowed values are ""stable"" or ""latest"".
  # rancher_install_channel = ""stable""

  # Finally, you can specify a bootstrap-password for your rancher instance. Minimum 48 characters long!
  # If you leave empty, one will be generated for you.
  # (Can be used by another rancher2 provider to continue setup of rancher outside this module.)
  # rancher_bootstrap_password = """"

  # Separate from the above Rancher config (only use one or the other). You can import this cluster directly on an
  # an already active Rancher install. By clicking ""import cluster"" choosing ""generic"", giving it a name and pasting
  # the cluster registration url below. However, you can also ignore that and apply the url via kubectl as instructed
  # by Rancher in the wizard, and that would register your cluster too.
  # More information about the registration can be found here https://rancher.com/docs/rancher/v2.6/en/cluster-provisioning/registered-clusters/
  # rancher_registration_manifest_url = ""https://rancher.xyz.dev/v3/import/xxxxxxxxxxxxxxxxxxYYYYYYYYYYYYYYYYYYYzzzzzzzzzzzzzzzzzzzzz.yaml""

  # Extra values that will be passed to the `extra-manifests/kustomization.yaml.tpl` if its present.
  # extra_kustomize_parameters={}

  # It is best practice to turn this off, but for backwards compatibility it is set to ""true"" by default.
  # See https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/issues/349
  # When ""false"". The kubeconfig file can instead be created by executing: ""terraform output --raw kubeconfig > cluster_kubeconfig.yaml""
  # Always be careful to not commit this file!
  # create_kubeconfig = false

  # Don't create the kustomize backup. This can be helpful for automation.
  # create_kustomization = false

  ### ADVANCED - Custom helm values for packages above (search _values if you want to located where those are mentioned upper in this file)
  #  Inside the _values variable below are examples, up to you to find out the best helm values possible, we do not provide support for customized helm values.
  # Please understand that the indentation is very important, inside the EOTs, as those are proper yaml helm values.
  # We advise you to use the default values, and only change them if you know what you are doing!

  # Cilium, all Cilium helm values can be found at https://github.com/cilium/cilium/blob/master/install/kubernetes/cilium/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   cilium_values = <<EOT
ipam:
  mode: kubernetes
devices: ""eth1""
k8s:
  requireIPv4PodCIDR: true
kubeProxyReplacement: strict
  EOT */

  # Cert manager, all cert-manager helm values can be found at https://github.com/cert-manager/cert-manager/blob/master/deploy/charts/cert-manager/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   cert_manager_values = <<EOT
installCRDs: true
replicaCount: 3
webhook:
  replicaCount: 3
cainjector:
  replicaCount: 3
  EOT */

  # Longhorn, all Longhorn helm values can be found at https://github.com/longhorn/longhorn/blob/master/chart/values.yaml
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   longhorn_values = <<EOT
defaultSettings:
  defaultDataPath: /var/longhorn
persistence:
  defaultFsType: ext4
  defaultClassReplicaCount: 3
  defaultClass: true
  EOT */

  # Nginx, all Nginx helm values can be found at https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml
  # You can also have a look at https://kubernetes.github.io/ingress-nginx/, to understand how it works, and all the options at your disposal.
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   nginx_ingress_values = <<EOT
controller:
  watchIngressWithoutClass: ""true""
  kind: ""DaemonSet""
  config:
    ""use-forwarded-headers"": ""true""
    ""compute-full-forwarded-for"": ""true""
    ""use-proxy-protocol"": ""true""
  service:
    annotations:
      ""load-balancer.hetzner.cloud/name"": ""k3s""
      ""load-balancer.hetzner.cloud/use-private-ip"": ""true""
      ""load-balancer.hetzner.cloud/disable-private-ingress"": ""true""
      ""load-balancer.hetzner.cloud/location"": ""nbg1""
      ""load-balancer.hetzner.cloud/type"": ""lb11""
      ""load-balancer.hetzner.cloud/uses-proxyprotocol"": ""true""
  EOT */

  # Rancher, all Rancher helm values can be found at https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/chart-options/
  # The following is an example, please note that the current indentation inside the EOT is important.
  /*   rancher_values = <<EOT
ingress:
  tls:
    source: ""rancher""
hostname: ""rancher.example.com""
replicas: 1
bootstrapPassword: ""supermario""
  EOT */

}
",module,282,282.0,1e07f152504e14250afb8fe82fed2545baec642f,1e07f152504e14250afb8fe82fed2545baec642f,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/1e07f152504e14250afb8fe82fed2545baec642f/kube.example.tf#L282,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/1e07f152504e14250afb8fe82fed2545baec642f/kube.example.tf#L282,2023-01-21 12:53:45+01:00,2023-01-21 12:53:45+01:00,1,0,1,1,0,0,0,0,0,0
https://github.com/CDCgov/prime-simplereport,4,ops/services/alerts/app_service_metrics/exceptions.tf,ops/services/alerts/app_service_metrics/exceptions.tf,0,workaround,"#   than a day, and we only want to alert 1x/week. As a workaround, this hardcodes the alert period to Thursdays","# - Collect all requests that were exceptions in the week preceeding today 
 # - Do the same for today 
 # - leftanti join the two result sets to return only results from today that were not found in the week preceeding today 
 # - Note the first 'where' clause - the azurerm_monitor_scheduled_query_rules_alert resource doesn't allow intervals longer 
 #   than a day, and we only want to alert 1x/week. As a workaround, this hardcodes the alert period to Thursdays 
 #   at 16:00 UTC (12:00 EDT). The alert runs hourly, so this guarantees that it will fire once and only once per week.","resource ""azurerm_monitor_scheduled_query_rules_alert"" ""first_error_in_a_week"" {
  name                = ""${var.env}-first-error-in-a-week""
  description         = ""${local.env_title} alert when an error is seen for the first time in a week""
  location            = data.azurerm_resource_group.app.location
  resource_group_name = var.rg_name

  action {
    action_group = var.action_group_ids
  }

  data_source_id = var.app_insights_id
  enabled        = contains(var.disabled_alerts, ""first_error_in_a_week"") ? false : true

  # - Collect all requests that were exceptions in the week preceeding today
  # - Do the same for today
  # - leftanti join the two result sets to return only results from today that were not found in the week preceeding today
  # - Note the first 'where' clause - the azurerm_monitor_scheduled_query_rules_alert resource doesn't allow intervals longer
  #   than a day, and we only want to alert 1x/week. As a workaround, this hardcodes the alert period to Thursdays
  #   at 16:00 UTC (12:00 EDT). The alert runs hourly, so this guarantees that it will fire once and only once per week.
  query = <<-QUERY
requests
| where dayofweek(now()) == time(4) and hourofday(now()) == 16
| where timestamp <= now() and timestamp > now(-1d) and success == false
| join kind= inner (
    exceptions
    | where timestamp <= now() and timestamp > now(-1d)
    )
    on operation_Id
| project stackTrace = details[0].rawStack, exceptionType = type, failedMethod = method, requestName = name, combinedErrorString = strcat(type, method, name), timestamp
| join kind= leftanti (
    requests
    | where timestamp <= now(-1d) and timestamp > now(-8d) and success == false
    | join kind= inner (
        exceptions
        | where timestamp <= now(-1d) and timestamp > now(-8d)
        )
        on operation_Id
    | project stackTrace = details[0].rawStack, exceptionType = type, failedMethod = method, requestName = name, combinedErrorString = strcat(type, method, name)
    )
    on combinedErrorString
| summarize stackTrace = any(stackTrace) by failedMethod, requestName, exceptionType, timestamp
| sort by timestamp
  QUERY

  severity    = 2
  frequency   = 60 // Run hourly
  time_window = 1440
  trigger {
    operator  = ""GreaterThan""
    threshold = 0
  }
}
",resource,"resource ""azurerm_monitor_scheduled_query_rules_alert"" ""first_error_in_a_week"" {
  name                = ""${var.env}-first-error-in-a-week""
  description         = ""${local.env_title} alert when an error is seen for the first time in a week""
  location            = data.azurerm_resource_group.app.location
  resource_group_name = var.rg_name

  action {
    action_group = var.action_group_ids
  }

  data_source_id = var.app_insights_id
  enabled        = false

  # - Collect all requests that were exceptions in the week preceeding today
  # - Do the same for today
  # - leftanti join the two result sets to return only results from today that were not found in the week preceeding today
  query = <<-QUERY
requests
| where timestamp <= now() and timestamp > now(-1d) and success == false
| join kind= inner (
    exceptions
    | where timestamp <= now() and timestamp > now(-1d)
    )
    on operation_Id
| project stackTrace = details[0].rawStack, exceptionType = type, failedMethod = method, requestName = name, combinedErrorString = strcat(type, method, name), timestamp
| join kind= leftanti (
    requests
    | where timestamp <= now(-1d) and timestamp > now(-8d) and success == false
    | join kind= inner (
        exceptions
        | where timestamp <= now(-1d) and timestamp > now(-8d)
        )
        on operation_Id
    | project stackTrace = details[0].rawStack, exceptionType = type, failedMethod = method, requestName = name, combinedErrorString = strcat(type, method, name)
    )
    on combinedErrorString
| summarize stackTrace = any(stackTrace) by failedMethod, requestName, exceptionType, timestamp
| sort by timestamp
  QUERY

  severity    = 2
  frequency   = 60 // Run hourly
  time_window = 1440
  trigger {
    operator  = ""GreaterThan""
    threshold = 0
  }
}
",resource,76,,d7d55375850dcf95aa027c27e93bd71fd629cd0b,f2feac3dc838381985a2e69d62d79c011a8f0e44,https://github.com/CDCgov/prime-simplereport/blob/d7d55375850dcf95aa027c27e93bd71fd629cd0b/ops/services/alerts/app_service_metrics/exceptions.tf#L76,https://github.com/CDCgov/prime-simplereport/blob/f2feac3dc838381985a2e69d62d79c011a8f0e44/ops/services/alerts/app_service_metrics/exceptions.tf,2021-08-04 12:28:21-04:00,2021-08-05 16:43:08-04:00,2,1,0,1,0,0,0,0,1,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,245,modules/network/subnets.tf,modules/network/subnets.tf,0,todo,"# TODO enumerate worker pools for public/private overrides, conditional subnets for both
","# Map of subnets for standard components with additional configuration derived 
 # TODO enumerate worker pools for public/private overrides, conditional subnets for both","locals {
  # VCN subnet configuration
  # See https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengnetworkconfig.htm#vcnconfig
  vcn_cidr = element(var.vcn_cidrs, 1)

  # Filter configured subnets eligible for resource creation
  new_subnet_cidrs = {
    for k, v in var.subnets : k => v
    if lookup(v, ""id"", null) == null && lookup(v, ""create"", ""auto"") != ""never""
  }

  # Generate CIDR ranges for subnets to be created
  subnet_cidrs = {
    for k, v in local.new_subnet_cidrs :
    k => cidrsubnet(local.vcn_cidr, lookup(v, ""newbits""), lookup(v, ""netnum""))
  }

  # Map of subnets for standard components with additional configuration derived
  # TODO enumerate worker pools for public/private overrides, conditional subnets for both
  subnet_info = {
    bastion  = { create = var.create_bastion, is_public = var.bastion_is_public }
    cp       = { is_public = var.control_plane_is_public }
    workers  = { is_public = var.worker_is_public }
    pods     = { create = var.cni_type == ""npn"" }
    operator = { create = var.create_operator }
    fss      = { create = var.create_fss }
    int_lb = {
      create         = var.load_balancers == ""internal"" || var.load_balancers == ""both"",
      create_seclist = true, dns_label = ""ilb"",
    }
    pub_lb = {
      create         = var.load_balancers == ""public"" || var.load_balancers == ""both"",
      create_seclist = true, is_public = true, dns_label = ""plb"",
    }
  }
}
",locals,"locals {
  # VCN subnet configuration
  # See https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengnetworkconfig.htm#vcnconfig
  # May be undefined when VCN is neither created nor required, e.g. when creating only workers for
  # an existing cluster. Fallback value is unused.
  vcn_cidr = length(var.vcn_cidrs) > 0 ? element(var.vcn_cidrs, 0) : ""0.0.0.0/16""

  # Filter configured subnets eligible for resource creation
  subnet_cidrs_new = {
    for k, v in var.subnets : k => merge(v, {
      ""type"" = (lookup(v, ""netnum"", null) == null && lookup(v, ""newbits"", null) != null ? ""newbits""
        : (lookup(v, ""netnum"", null) != null && lookup(v, ""newbits"", null) != null ? ""netnum""
          : (lookup(v, ""cidr"", null) != null ? ""cidr""
            : (lookup(v, ""id"", null) != null ? ""id""
      : ""invalid""))))
    })
  }

  # Handle subnets configured with provided CIDRs
  subnet_cidrs_cidr_input = {
    for k, v in local.subnet_cidrs_new : k => lookup(v, ""cidr"") if v.type == ""cidr""
  }

  # Handle subnets configured with only newbits for sizing
  subnet_cidrs_newbits_input = {
    for k, v in local.subnet_cidrs_new : k => lookup(v, ""newbits"") if v.type == ""newbits""
  }

  # Generate CIDR ranges for subnets to be created
  subnet_cidrs_newbits_ranges = cidrsubnets(local.vcn_cidr, values(local.subnet_cidrs_newbits_input)...)
  subnet_cidrs_newbits_resolved = length(local.vcn_cidr) > 0 ? {
    for k, v in local.subnet_cidrs_newbits_input : k => element(local.subnet_cidrs_newbits_ranges, index(keys(local.subnet_cidrs_newbits_input), k))
  } : {}

  # Handle subnets configured with netnum + newbits for sizing
  subnet_cidrs_netnum_newbits_ranges = {
    for k, v in local.subnet_cidrs_new : k => cidrsubnet(local.vcn_cidr, lookup(v, ""newbits""), lookup(v, ""netnum""))
    if v.type == ""netnum""
  }

  // Combine provided and calculated subnet CIDRs
  subnet_cidrs_all = merge(local.subnet_cidrs_cidr_input, local.subnet_cidrs_newbits_resolved, local.subnet_cidrs_netnum_newbits_ranges)

  # Map of subnets for standard components with additional configuration derived
  # TODO enumerate worker pools for public/private overrides, conditional subnets for both
  subnet_info = {
    bastion  = { create = var.create_bastion, is_public = var.bastion_is_public }
    cp       = { create = var.create_cluster, is_public = var.control_plane_is_public }
    workers  = { create = var.create_cluster, is_public = var.worker_is_public }
    pods     = { create = var.create_cluster && var.cni_type == ""npn"" }
    operator = { create = var.create_operator }
    fss      = { create = var.create_fss }
    int_lb = {
      create         = var.create_cluster && contains([""both"", ""internal""], var.load_balancers),
      create_seclist = true, dns_label = ""ilb"",
    }
    pub_lb = {
      create         = var.create_cluster && contains([""both"", ""public""], var.load_balancers),
      create_seclist = true, is_public = true, dns_label = ""plb"",
    }
  }

  # Create subnets if when all are true:
  # - Associated component is enabled OR configured with create == 'always'
  # - Subnet is configured with newbits and/or netnum/cidr
  # - Not configured with create == 'never'
  # - Not configured with an existing 'id'
  subnets_to_create = length(var.vcn_cidrs) > 0 ? merge(
    { for k, v in local.subnet_info : k =>
      # Override `create = true` if configured with ""always""
      merge(v, lookup(lookup(var.subnets, k, {}), ""create"", ""auto"") == ""always"" ? { ""create"" = true } : {})
      if alltrue([                                                       # Filter disabled subnets from output
        contains(keys(local.subnet_cidrs_all), k),                       # has a calculated CIDR range (not id input)
        lookup(lookup(var.subnets, k, {}), ""create"", ""auto"") != ""never"", # not disabled
        anytrue([
          tobool(lookup(v, ""create"", true)),                               # automatically enabled
          lookup(lookup(var.subnets, k, {}), ""create"", ""auto"") == ""always"" # force enabled
        ]),
      ])
    }
  ) : {}

  subnet_output = { for k, v in var.subnets :
    k => lookup(v, ""id"", null) != null ? v.id : lookup(lookup(oci_core_subnet.oke, k, {}), ""id"", null)
  }
}
",locals,22,,c3f1db06105d2c72b3e28ba10475c6c9f019204c,f49f1da39d79cf260d80dcb10ee8e399828e6e1c,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/c3f1db06105d2c72b3e28ba10475c6c9f019204c/modules/network/subnets.tf#L22,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/f49f1da39d79cf260d80dcb10ee8e399828e6e1c/modules/network/subnets.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,9,1,0,1,0,0,1,0,0,0
https://github.com/kubernetes/k8s.io,243,infra/gcp/terraform/k8s-infra-kubernetes-io/main.tf,infra/gcp/terraform/k8s-infra-kubernetes-io/main.tf,0,# todo,"# calendar_period = ""MONTH"" # TODO: terraform doesn't support this?","# calendar_period = ""MONTH"" # TODO: terraform doesn't support this? 
 # exclude promotions, which is where our credits come from, since that zeros out cost","resource ""google_billing_budget"" ""k8s_infra"" {
  billing_account = data.google_billing_account.account.billing_account
  display_name = ""k8s-infra-monthly""
  budget_filter {
    # calendar_period = ""MONTH"" # TODO: terraform doesn't support this?
    # exclude promotions, which is where our credits come from, since that zeros out cost
    credit_types_treatment = ""INCLUDE_SPECIFIED_CREDITS""
    credit_types           = [
        ""SUSTAINED_USAGE_DISCOUNT"",
        ""DISCOUNT"",
        ""COMMITTED_USAGE_DISCOUNT"",
        ""FREE_TIER"",
        ""COMMITTED_USAGE_DISCOUNT_DOLLAR_BASE"",
        ""SUBSCRIPTION_BENEFIT"",
    ]
  }
  amount {
    specified_amount {
      currency_code = ""USD""
      units = ""250000"" # 3M/yr / 12mo
    }
  }
  all_updates_rule {
    # Don't send to users with Billing Account Administrators and
    # Billing Account Users IAM roles for the billing account
    disable_default_iam_recipients = true
    monitoring_notification_channels = [
      data.google_monitoring_notification_channel.sig_k8s_infra_leads.name
    ]
  }
  dynamic ""threshold_rules"" {
    for_each = toset([0.9, 1.0])
    content {
      threshold_percent = threshold_rules.value
    }
  }
}
",resource,the block associated got renamed or deleted,,107,,3d4fe9a599e378fe1ba8518f0517b5ca4126b181,348b73a1f3749e3becf6bc2433e0bfea6fda807d,https://github.com/kubernetes/k8s.io/blob/3d4fe9a599e378fe1ba8518f0517b5ca4126b181/infra/gcp/terraform/k8s-infra-kubernetes-io/main.tf#L107,https://github.com/kubernetes/k8s.io/blob/348b73a1f3749e3becf6bc2433e0bfea6fda807d/infra/gcp/terraform/k8s-infra-kubernetes-io/main.tf,2021-10-14 12:00:26-07:00,2021-10-14 15:18:11-07:00,2,1,0,1,1,0,0,0,0,0
https://github.com/pingcap/tidb-operator,45,deploy/aws/tidb-operator/main.tf,deploy/modules/aws/tidb-operator/main.tf,1,hack,# we have the following hack,"# kubernetes and helm providers rely on EKS, but terraform provider doesn't support depends_on 
 # follow this link https://github.com/hashicorp/terraform/issues/2430#issuecomment-370685911 
 # we have the following hack","resource ""local_file"" ""kubeconfig"" {
  depends_on        = [module.eks]
  sensitive_content = module.eks.kubeconfig
  filename          = module.eks.kubeconfig_filename
}
",resource,"resource ""local_file"" ""kubeconfig"" {
  depends_on        = [module.eks]
  sensitive_content = module.eks.kubeconfig
  filename          = module.eks.kubeconfig_filename
}
",resource,28,33.0,58095783adeb0ca30ecdf094a7e8db7f17fb5919,1a6d49b25efccbb6fe03c3ec7caa6258f76f685a,https://github.com/pingcap/tidb-operator/blob/58095783adeb0ca30ecdf094a7e8db7f17fb5919/deploy/aws/tidb-operator/main.tf#L28,https://github.com/pingcap/tidb-operator/blob/1a6d49b25efccbb6fe03c3ec7caa6258f76f685a/deploy/modules/aws/tidb-operator/main.tf#L33,2019-07-03 11:55:46+08:00,2020-05-11 11:29:58+08:00,6,0,1,1,1,0,0,0,0,0
https://github.com/wireapp/wire-server-deploy,71,terraform/examples/wire-server-deploy-offline-hetzner/main.tf,terraform/examples/wire-server-deploy-offline-hetzner/main.tf,0,# todo,# TODO: IPv6,# TODO: IPv6,"locals {
  rfc1918_cidr        = ""10.0.0.0/8""
  kubenode_count      = 3
  minio_count         = 2
  elasticsearch_count = 2
  cassandra_count     = 3
  restund_count       = 2
  ssh_keys            = [hcloud_ssh_key.adminhost.name]

  # TODO: IPv6
  disable_network_cfg = <<-EOF
  #cloud-config
  runcmd:

    # Allow DNS
    - iptables -A OUTPUT -o eth0 -p udp --dport 53  -j ACCEPT
    - ip6tables -A OUTPUT -o eth0 -p udp --dport 53  -j ACCEPT

    # Allow NTP
    - iptables -A OUTPUT -o eth0 -p udp --dport 123 -j ACCEPT
    - ip6tables -A OUTPUT -o eth0 -p udp --dport 123 -j ACCEPT

    # Drop all other traffic
    - iptables -A OUTPUT -o eth0 -j DROP
    - ip6tables -A OUTPUT -o eth0 -j DROP

  EOF
}
",locals,"locals {
  rfc1918_cidr        = ""10.0.0.0/8""
  kubenode_count      = 3
  minio_count         = 2
  elasticsearch_count = 2
  cassandra_count     = 3
  restund_count       = 2
  ssh_keys            = [hcloud_ssh_key.adminhost.name]

  # TODO: IPv6
  disable_network_cfg = <<-EOF
  #cloud-config
  runcmd:

    # Allow DNS
    - iptables -A OUTPUT -o eth0 -p udp --dport 53  -j ACCEPT
    - ip6tables -A OUTPUT -o eth0 -p udp --dport 53  -j ACCEPT

    # Allow NTP
    - iptables -A OUTPUT -o eth0 -p udp --dport 123 -j ACCEPT
    - ip6tables -A OUTPUT -o eth0 -p udp --dport 123 -j ACCEPT

    # Drop all other traffic
    - iptables -A OUTPUT -o eth0 -j DROP
    - ip6tables -A OUTPUT -o eth0 -j DROP

  EOF
}
",locals,10,10.0,b7ab89140674e4ed5db56509e60b2b6c60d88900,ecea454794c466c81e910883a6a2cf67dda457ce,https://github.com/wireapp/wire-server-deploy/blob/b7ab89140674e4ed5db56509e60b2b6c60d88900/terraform/examples/wire-server-deploy-offline-hetzner/main.tf#L10,https://github.com/wireapp/wire-server-deploy/blob/ecea454794c466c81e910883a6a2cf67dda457ce/terraform/examples/wire-server-deploy-offline-hetzner/main.tf#L10,2021-04-20 13:14:17+02:00,2023-08-02 08:32:44+01:00,4,0,0,0,0,0,1,0,0,0
https://github.com/Worklytics/psoxy,121,infra/modules/aws-psoxy-instance/main.tf,infra/modules/aws-psoxy-instance/main.tf,0,# todo,# TODO: limit to SSM parameters in question,"# TODO: limit to SSM parameters in question 
 # ""Resource"": ""arn:aws:ssm:us-east-2:123456789012:parameter/prod-*""","resource ""aws_iam_policy"" ""policy"" {
  name        = ""${var.function_name}_ssmGetParameters""
  description = ""Allow lambda function role to read SSM parameters""

  policy = jsonencode(
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": [
        ""ssm:GetParameter*""
      ],
      ""Effect"": ""Allow"",
      ""Resource"": ""*""
      # TODO: limit to SSM parameters in question
      # ""Resource"": ""arn:aws:ssm:us-east-2:123456789012:parameter/prod-*""
    }
  ]
})

}
",resource,"resource ""aws_iam_policy"" ""policy"" {
  name        = ""${var.function_name}_ssmGetParameters""
  description = ""Allow lambda function role to read SSM parameters""

  policy = jsonencode(
    {
      ""Version"" : ""2012-10-17"",
      ""Statement"" : [
        {
          ""Action"" : [
            ""ssm:GetParameter*""
          ],
          ""Effect"" : ""Allow"",
          ""Resource"" : ""arn:aws:ssm:${var.region}:${var.aws_account_id}:parameter/*""
        }
      ]
  })
}
",resource,101,,3f3e920ccb3225be2ee885e6f3b9d10902ccc8a4,117af0cb45dab7c468732cc2219e354c4929e460,https://github.com/Worklytics/psoxy/blob/3f3e920ccb3225be2ee885e6f3b9d10902ccc8a4/infra/modules/aws-psoxy-instance/main.tf#L101,https://github.com/Worklytics/psoxy/blob/117af0cb45dab7c468732cc2219e354c4929e460/infra/modules/aws-psoxy-instance/main.tf,2022-01-11 16:22:58-08:00,2022-05-09 17:19:18-07:00,38,1,0,0,0,1,0,0,0,0
https://github.com/uyuni-project/sumaform,1389,backend_modules/azure/network/main.tf,backend_modules/azure/network/main.tf,0,/*todo,/*TODO: add tags*/,"/* 
 This module sets up a class B VPC sliced into two subnets, one public and one private. 
 The private network has no Internet access. 
 The public network has an Internet Gateway and accepts SSH connections from a whitelist of trusted IPs. 
 */ 
 /*TODO: add tags*/","locals{

}
",locals,"resource ""azurerm_resource_group"" ""suma-rg"" {
  name     = ""${var.name_prefix}-resources""
  location = ""${var.location}""
}
",resource,6,6.0,1990c31dba05892a132fd5aaa936ff1b1940f4b5,4b92532e86215e984ef5a1d2983372ab470d1c84,https://github.com/uyuni-project/sumaform/blob/1990c31dba05892a132fd5aaa936ff1b1940f4b5/backend_modules/azure/network/main.tf#L6,https://github.com/uyuni-project/sumaform/blob/4b92532e86215e984ef5a1d2983372ab470d1c84/backend_modules/azure/network/main.tf#L6,2021-04-14 11:47:58+01:00,2023-09-05 15:07:37+02:00,13,0,0,1,0,0,0,0,0,0
https://github.com/CDCgov/prime-simplereport,39,ops/services/postgres_db/main.tf,ops/services/postgres_db/main.tf,0,// todo,// TODO: replace with commented-out line below when removing old DB config,// TODO: replace with commented-out line below when removing old DB config,"resource ""azurerm_postgresql_flexible_server"" ""db"" {
  name                = ""simple-report-${var.env}-flexible-db""
  location            = var.rg_location
  resource_group_name = var.rg_name
  sku_name            = var.env == ""prod"" ? ""MO_Standard_E8ds_v4"" : ""MO_Standard_E4ds_v4""
  version             = ""13""
  delegated_subnet_id = var.subnet_id
  private_dns_zone_id = var.dns_zone_id

  // TODO: replace with commented-out line below when removing old DB config
  administrator_login = ""simple_report""
  //administrator_login    = var.administrator_login
  administrator_password = data.azurerm_key_vault_secret.db_password.value

  storage_mb                   = 524288 // 512 GB
  backup_retention_days        = 7
  geo_redundant_backup_enabled = false

  tags = var.tags

  // Time is Eastern
  maintenance_window {
    day_of_week  = 0
    start_hour   = 0
    start_minute = 0
  }

  # See note at https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/postgresql_flexible_server#high_availability
  lifecycle {
    ignore_changes = [
      zone,
      high_availability.0.standby_availability_zone
    ]
  }
}
",resource,"resource ""azurerm_postgresql_flexible_server"" ""db"" {
  name                = ""simple-report-${var.env}-flexible-db""
  location            = var.rg_location
  resource_group_name = var.rg_name
  sku_name            = var.env == ""prod"" ? ""MO_Standard_E8ds_v4"" : ""MO_Standard_E4ds_v4""
  version             = ""13""
  delegated_subnet_id = var.subnet_id


  administrator_login    = var.administrator_login
  administrator_password = data.azurerm_key_vault_secret.db_password.value

  storage_mb                   = 524288 // 512 GB
  backup_retention_days        = 7
  geo_redundant_backup_enabled = false

  tags = var.tags

  // Time is Eastern
  maintenance_window {
    day_of_week  = 0
    start_hour   = 0
    start_minute = 0
  }

  # See note at https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/postgresql_flexible_server#high_availability
  lifecycle {
    ignore_changes = [
      zone,
      high_availability.0.standby_availability_zone
    ]
  }
}
",resource,77,,3e4185fa549937cff9213c325bc0fee780e41eb0,fb9a18eb71f31044ca7a58958a25dea489263ca7,https://github.com/CDCgov/prime-simplereport/blob/3e4185fa549937cff9213c325bc0fee780e41eb0/ops/services/postgres_db/main.tf#L77,https://github.com/CDCgov/prime-simplereport/blob/fb9a18eb71f31044ca7a58958a25dea489263ca7/ops/services/postgres_db/main.tf,2022-02-16 12:59:39-05:00,2022-04-28 23:27:57-04:00,2,1,1,1,0,1,0,0,0,0
https://github.com/compiler-explorer/infra,116,terraform/ec2.tf,terraform/ec2.tf,0,todo,// TODO make 4xlarge or similar,// TODO make 4xlarge or similar,"resource ""aws_instance"" ""BuilderNode"" {
  ami                         = local.builder_image_id
  // TODO bring into the fold
  iam_instance_profile        = ""GccBuilder""
  ebs_optimized               = true
  // TODO make 4xlarge or similar
  instance_type               = ""c5d.large""
  monitoring                  = false
  key_name                    = ""mattgodbolt""
  subnet_id                   = aws_subnet.ce-1a.id
  // TODO reconsider, make an SG specifically for builder
  vpc_security_group_ids      = [aws_security_group.AdminNode.id]
  associate_public_ip_address = true
  source_dest_check           = false

  root_block_device {
    volume_type           = ""gp2""
    volume_size           = 24
    delete_on_termination = true
  }

  tags = {
    Name = ""Builder-New""
  }
}
",resource,"resource ""aws_instance"" ""BuilderNode"" {
  ami                         = local.builder_image_id
  // TODO bring into the fold
  iam_instance_profile        = ""GccBuilder""
  ebs_optimized               = true
  instance_type               = ""c5d.4xlarge""
  monitoring                  = false
  key_name                    = ""mattgodbolt""
  subnet_id                   = aws_subnet.ce-1a.id
  // TODO reconsider, make an SG specifically for builder
  vpc_security_group_ids      = [aws_security_group.AdminNode.id]
  associate_public_ip_address = true
  source_dest_check           = false

  root_block_device {
    volume_type           = ""gp2""
    volume_size           = 24
    delete_on_termination = true
  }


  tags = {
    Name = ""Builder""
  }
}
",resource,74,,b419a94ffc423c637e8722d79fb4f42770acf05e,0a4123656290c2ced43d947a97ad52ed6599be49,https://github.com/compiler-explorer/infra/blob/b419a94ffc423c637e8722d79fb4f42770acf05e/terraform/ec2.tf#L74,https://github.com/compiler-explorer/infra/blob/0a4123656290c2ced43d947a97ad52ed6599be49/terraform/ec2.tf,2021-08-30 22:47:49-05:00,2021-09-01 07:52:53-05:00,2,1,1,1,0,0,0,0,0,0
https://github.com/Azure/terraform-azurerm-caf-enterprise-scale,7,modules/terraform-azurerm-enterprise-scale-archetypes/locals.policy_assignments.tf,modules/archetypes/locals.policy_assignments.tf,1,implemented,# Logic implemented to determine whether Policy Assignments,"# Generate the Policy Assignment configurations for the specified archetype. 
 # Logic implemented to determine whether Policy Assignments 
 # need to be loaded to save on compute and memory resources 
 # when none defined in archetype definition.","locals {
  archetype_policy_assignments_list      = local.archetype_definition.policy_assignments
  archetype_policy_assignments_specified = try(length(local.archetype_policy_assignments_list) > 0, false)
}
",locals,"locals {
  archetype_policy_assignments_list      = local.archetype_definition.policy_assignments
  archetype_policy_assignments_specified = try(length(local.archetype_policy_assignments_list) > 0, false)
}
",locals,2,2.0,d5f8cbdd0d83352502d8fda485af2901e58464f5,644e61aedf55b4a033383669e70fb54096ee6b45,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/d5f8cbdd0d83352502d8fda485af2901e58464f5/modules/terraform-azurerm-enterprise-scale-archetypes/locals.policy_assignments.tf#L2,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/644e61aedf55b4a033383669e70fb54096ee6b45/modules/archetypes/locals.policy_assignments.tf#L2,2020-10-09 13:19:33+01:00,2023-07-21 14:05:10+01:00,12,0,0,0,0,1,0,0,0,0
https://github.com/alphagov/govuk-aws,845,terraform/projects/app-elasticsearch6/main.tf,terraform/projects/app-elasticsearch6/main.tf,0,# todo,# TODO: change the default to 3 after the first deploy,# TODO: change the default to 3 after the first deploy,"variable ""elasticsearch6_master_instance_count"" {
  type        = ""string""
  description = ""Number of dedicated master nodes in the cluster""
  default     = ""2""
}
",variable,the block associated got renamed or deleted,,63,,cc6d4e4b85c01b2f75e322ff70b212c45ff29834,4357f2d981e7f59c9ba3dc7858196714b4851628,https://github.com/alphagov/govuk-aws/blob/cc6d4e4b85c01b2f75e322ff70b212c45ff29834/terraform/projects/app-elasticsearch6/main.tf#L63,https://github.com/alphagov/govuk-aws/blob/4357f2d981e7f59c9ba3dc7858196714b4851628/terraform/projects/app-elasticsearch6/main.tf,2019-05-31 12:49:39+01:00,2023-08-08 15:01:01+01:00,13,1,1,1,0,0,0,1,0,0
https://github.com/compiler-explorer/infra,153,terraform/network.tf,terraform/network.tf,0,hack,// Hackily we inject this into the module instead of reading it out. Mainly due to frustration with a foreach(),// Hackily we inject this into the module instead of reading it out. Mainly due to frustration with a foreach(),"locals {
  // Hackily we inject this into the module instead of reading it out. Mainly due to frustration with a foreach()
  subnet_mappings = {
    ""1a"": ""0"",
    ""1b"": ""1"",
    ""1c"": ""4"",
    ""1d"": ""2"",
    ""1e"": ""6"",
    ""1f"": ""5"",
  }
  // All the subnet IDs, but not available until after planning, so can't be used in foreach. If you need this in
  // foreach, then you need to foreach over subnet_mappings and grab the ids from that.
  all_subnet_ids  = [for subnet, _ in local.subnet_mappings: module.ce_network.subnet[subnet].id]
}
",locals,"locals {
  // Hackily we inject this into the module instead of reading it out. Mainly due to frustration with a foreach()
  subnet_mappings = {
    ""1a"" : ""0"",
    ""1b"" : ""1"",
    ""1c"" : ""4"",
    ""1d"" : ""2"",
    ""1f"" : ""5"",
  }
  // All the subnet IDs, but not available until after planning, so can't be used in foreach. If you need this in
  // foreach, then you need to foreach over subnet_mappings and grab the ids from that.
  all_subnet_ids = [for subnet, _ in local.subnet_mappings : module.ce_network.subnet[subnet].id]
}
",locals,8,8.0,1d77db2c63d45cc8899aa9f1713ab6098e92595a,4b1e506ff9f009823bdf11e8f3dff9877c8527d8,https://github.com/compiler-explorer/infra/blob/1d77db2c63d45cc8899aa9f1713ab6098e92595a/terraform/network.tf#L8,https://github.com/compiler-explorer/infra/blob/4b1e506ff9f009823bdf11e8f3dff9877c8527d8/terraform/network.tf#L8,2021-09-20 07:44:56-05:00,2022-10-08 19:09:27-05:00,3,0,0,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,827,infra/examples-dev/aws/variables.tf,infra/examples-dev/aws/variables.tf,0,# todo,# TODO: rethink this schema before we publish this,# TODO: rethink this schema before we publish this,"variable ""lookup_table_builders"" {
  type = map(object({
    input_connector_id            = string
    sanitized_accessor_role_names = list(string)
    rules = object({
      pseudonymFormat       = string
      columnsToRedact       = list(string)
      columnsToInclude      = list(string)
      columnsToPseudonymize = list(string)
      columnsToDuplicate    = map(string)
      columnsToRename       = map(string)
    })
  }))
  default = {
    #    ""hris-lookup"" = {
    #      input_connector_id = ""hris"",
    #      sanitized_accessor_role_names = [
    #        # ADD LIST OF NAMES OF YOUR AWS ROLES WHICH CAN READ LOOKUP TABLE
    #      ],
    #      rules       = {
    #        pseudonym_format = ""URL_SAFE_TOKEN""
    #        columnsToRedact       = [
    #          ""employee_email"",
    #          ""manager_id"",
    #          ""manager_email"",
    #        ]
    #        columnsToPseudonymize = [
    #          ""employee_id"", # primary key
    #        ]
    #        columnsToDuplicate   = {
    #          ""employee_id"" = ""employee_id_orig""
    #        }
    #        columnsToRename      = {}
    #        columnsToInclude     = null
    #      }
    #
    #    }
  }
}
",variable,"variable ""lookup_table_builders"" {
  type = map(object({
    input_connector_id            = string
    sanitized_accessor_role_names = list(string)
    rules = object({
      pseudonymFormat       = optional(string, ""URL_SAFE_TOKEN"")
      columnsToRedact       = optional(list(string))
      columnsToInclude      = optional(list(string))
      columnsToPseudonymize = optional(list(string))
      columnsToDuplicate    = optional(map(string))
      columnsToRename       = optional(map(string))
    })
  }))
  default = {
    #    ""hris-lookup"" = {
    #      input_connector_id = ""hris"",
    #      sanitized_accessor_role_names = [
    #        # ADD LIST OF NAMES OF YOUR AWS ROLES WHICH CAN READ LOOKUP TABLE
    #      ],
    #      rules       = {
    #        pseudonym_format = ""URL_SAFE_TOKEN""
    #        columnsToRedact       = [
    #          ""employee_email"",
    #          ""manager_id"",
    #          ""manager_email"",
    #        ]
    #        columnsToPseudonymize = [
    #          ""employee_id"", # primary key
    #        ]
    #        columnsToDuplicate   = {
    #          ""employee_id"" = ""employee_id_orig""
    #        }
    #        columnsToRename      = {}
    #        columnsToInclude     = null
    #      }
    #
    #    }
  }
}
",variable,186,256.0,674bb3cc40b9675953558a8fe9f432c2298ba3a0,338ff46f49075efb1124868a2e98d9b71dbcf03b,https://github.com/Worklytics/psoxy/blob/674bb3cc40b9675953558a8fe9f432c2298ba3a0/infra/examples-dev/aws/variables.tf#L186,https://github.com/Worklytics/psoxy/blob/338ff46f49075efb1124868a2e98d9b71dbcf03b/infra/examples-dev/aws/variables.tf#L256,2023-03-22 16:22:58-07:00,2023-12-29 21:19:18-08:00,15,0,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1571,modules/artifact-registry/main.tf,modules/artifact-registry/main.tf,0,# todo,# TODO: open a bug on the provider for this permadiff,# TODO: open a bug on the provider for this permadiff,"resource ""google_artifact_registry_repository"" ""registry"" {
  project       = var.project_id
  location      = var.location
  description   = var.description
  format        = upper(local.format_string)
  labels        = var.labels
  repository_id = var.name
  mode          = ""${upper(local.mode_string)}_REPOSITORY""
  kms_key_name  = var.encryption_key

  dynamic ""docker_config"" {
    # TODO: open a bug on the provider for this permadiff
    for_each = (
      local.format_string == ""docker"" && var.format.docker.immutable_tags == true
      ? [""""]
      : []
    )
    content {
      immutable_tags = var.format.docker.immutable_tags
    }
  }

  dynamic ""maven_config"" {
    for_each = local.format_string == ""maven"" ? [""""] : []
    content {
      allow_snapshot_overwrites = var.format.maven.allow_snapshot_overwrites
      version_policy            = var.format.maven.version_policy
    }
  }

  dynamic ""remote_repository_config"" {
    for_each = local.mode_string == ""remote"" ? [""""] : []
    content {
      dynamic ""docker_repository"" {
        for_each = local.format_string == ""docker"" ? [""""] : []
        content {
          public_repository = ""DOCKER_HUB""
        }
      }
      dynamic ""maven_repository"" {
        for_each = local.format_string == ""maven"" ? [""""] : []
        content {
          public_repository = ""MAVEN_CENTRAL""
        }
      }
      dynamic ""npm_repository"" {
        for_each = local.format_string == ""npm"" ? [""""] : []
        content {
          public_repository = ""NPMJS""
        }
      }
      dynamic ""python_repository"" {
        for_each = local.format_string == ""python"" ? [""""] : []
        content {
          public_repository = ""PYPI""
        }
      }
    }
  }

  dynamic ""virtual_repository_config"" {
    for_each = local.mode_string == ""virtual"" ? [""""] : []
    content {
      dynamic ""upstream_policies"" {
        for_each = var.mode.virtual
        content {
          id         = upstream_policies.key
          repository = upstream_policies.value.repository
          priority   = upstream_policies.value.priority
        }
      }
    }
  }

  lifecycle {
    precondition {
      condition = local.mode_string != ""remote"" || contains(
        [""docker"", ""maven"", ""npm"", ""python""], local.format_string
      )
      error_message = ""Invalid format for remote repository.""
    }
  }

}
",resource,"resource ""google_artifact_registry_repository"" ""registry"" {
  provider      = google-beta
  project       = var.project_id
  location      = var.location
  description   = var.description
  format        = upper(local.format_string)
  labels        = var.labels
  repository_id = var.name
  mode          = ""${upper(local.mode_string)}_REPOSITORY""
  kms_key_name  = var.encryption_key

  cleanup_policy_dry_run = var.cleanup_policy_dry_run
  dynamic ""cleanup_policies"" {
    for_each = var.cleanup_policies == null ? {} : var.cleanup_policies
    content {
      id     = cleanup_policies.key
      action = cleanup_policies.value.action

      dynamic ""condition"" {
        for_each = (cleanup_policies.value.condition != null) ? [""""] : []
        content {
          tag_state             = cleanup_policies.value.condition.tag_state
          tag_prefixes          = cleanup_policies.value.condition.tag_prefixes
          version_name_prefixes = cleanup_policies.value.condition.version_name_prefixes
          package_name_prefixes = cleanup_policies.value.condition.package_name_prefixes
          newer_than            = cleanup_policies.value.condition.newer_than
          older_than            = cleanup_policies.value.condition.older_than
        }
      }

      dynamic ""most_recent_versions"" {
        for_each = (cleanup_policies.value.most_recent_versions != null) ? [""""] : []
        content {
          package_name_prefixes = cleanup_policies.value.most_recent_versions.package_name_prefixes
          keep_count            = cleanup_policies.value.most_recent_versions.keep_count
        }
      }
    }
  }

  dynamic ""docker_config"" {
    # TODO: open a bug on the provider for this permadiff
    for_each = (
      local.format_string == ""docker"" && try(var.format.docker.immutable_tags, null) == true
      ? [""""]
      : []
    )
    content {
      immutable_tags = var.format.docker.immutable_tags
    }
  }

  dynamic ""maven_config"" {
    for_each = local.format_string == ""maven"" ? [""""] : []
    content {
      allow_snapshot_overwrites = var.format.maven.allow_snapshot_overwrites
      version_policy            = var.format.maven.version_policy
    }
  }

  dynamic ""remote_repository_config"" {
    for_each = local.mode_string == ""remote"" ? [""""] : []
    content {
      dynamic ""docker_repository"" {
        for_each = local.format_string == ""docker"" ? [""""] : []
        content {
          public_repository = ""DOCKER_HUB""
        }
      }
      dynamic ""maven_repository"" {
        for_each = local.format_string == ""maven"" ? [""""] : []
        content {
          public_repository = ""MAVEN_CENTRAL""
        }
      }
      dynamic ""npm_repository"" {
        for_each = local.format_string == ""npm"" ? [""""] : []
        content {
          public_repository = ""NPMJS""
        }
      }
      dynamic ""python_repository"" {
        for_each = local.format_string == ""python"" ? [""""] : []
        content {
          public_repository = ""PYPI""
        }
      }
    }
  }

  dynamic ""virtual_repository_config"" {
    for_each = local.mode_string == ""virtual"" ? [""""] : []
    content {
      dynamic ""upstream_policies"" {
        for_each = var.mode.virtual
        content {
          id         = upstream_policies.key
          repository = upstream_policies.value.repository
          priority   = upstream_policies.value.priority
        }
      }
    }
  }

  lifecycle {
    precondition {
      condition = local.mode_string != ""remote"" || contains(
        [""docker"", ""maven"", ""npm"", ""python""], local.format_string
      )
      error_message = ""Invalid format for remote repository.""
    }
  }

}
",resource,33,63.0,3df98c8feba71fc46212b6096c23738041d5b103,3a2484843c6825f133ba8e05cfdb10de34d6ee07,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3df98c8feba71fc46212b6096c23738041d5b103/modules/artifact-registry/main.tf#L33,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3a2484843c6825f133ba8e05cfdb10de34d6ee07/modules/artifact-registry/main.tf#L63,2023-07-31 18:04:07+02:00,2023-12-01 10:33:02+00:00,3,0,1,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-vpc-service-controls,3,examples/simple_example/variables.tf,examples/simple_example/variables.tf,0,# todo,## TODO: remove after test,## TODO: remove after test,"variable ""policy_name"" {
  description = ""The policy's name.""

  ## TODO: remove after test
  default = """"
}
",variable,"variable ""policy_name"" {
  description = ""The policy's name.""
}
",variable,27,,07ef41e99d7389c0b4a31c4a67a12a7ab02627bd,79819996404c4ad171ea8759e10b83a6cfd36df0,https://github.com/terraform-google-modules/terraform-google-vpc-service-controls/blob/07ef41e99d7389c0b4a31c4a67a12a7ab02627bd/examples/simple_example/variables.tf#L27,https://github.com/terraform-google-modules/terraform-google-vpc-service-controls/blob/79819996404c4ad171ea8759e10b83a6cfd36df0/examples/simple_example/variables.tf,2019-04-26 00:29:28+00:00,2019-04-26 14:30:45+00:00,2,1,0,1,0,1,0,0,0,1
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1514,modules/gke-cluster/main.tf,modules/gke-cluster-standard/main.tf,1,#todo,#TODO add support for configs,#TODO add support for configs,"resource ""google_gke_backup_backup_plan"" ""backup_plan"" {
  for_each =  try(var.backup_configs.enable_backup_agent, false) ? var.backup_configs.backup_plans : null
  name = each.key
  cluster = google_container_cluster.cluster.id
  location = each.value.region
  project = var.project_id
  retention_policy {
    backup_delete_lock_days = try(each.value.retention_policy_delete_lock_days)
    backup_retain_days      = try(each.value.retention_policy_days)
    locked                  = try(each.value.retention_policy_lock)
  }
  backup_schedule {
    cron_schedule = each.value.schedule
  }
  #TODO add support for configs
  backup_config {
    include_volume_data = true
    include_secrets = true
    all_namespaces = true
  }
}
",resource,"resource ""google_gke_backup_backup_plan"" ""backup_plan"" {
  for_each = var.backup_configs.enable_backup_agent ? var.backup_configs.backup_plans : {}
  name     = each.key
  cluster  = google_container_cluster.cluster.id
  location = each.value.region
  project  = var.project_id
  retention_policy {
    backup_delete_lock_days = try(each.value.retention_policy_delete_lock_days)
    backup_retain_days      = try(each.value.retention_policy_days)
    locked                  = try(each.value.retention_policy_lock)
  }
  backup_schedule {
    cron_schedule = each.value.schedule
  }

  backup_config {
    include_volume_data = each.value.include_volume_data
    include_secrets     = each.value.include_secrets

    dynamic ""encryption_key"" {
      for_each = each.value.encryption_key != null ? [""""] : []
      content {
        gcp_kms_encryption_key = each.value.encryption_key
      }
    }

    all_namespaces = lookup(each.value, ""namespaces"", null) != null ? null : true
    dynamic ""selected_namespaces"" {
      for_each = each.value.namespaces != null ? [""""] : []
      content {
        namespaces = each.value.namespaces
      }
    }
  }
}
",resource,403,,64a9952656b49bafb620ec219d6472bb7eb4e590,5763eb53d427a79450d4302863cfa4eaca593154,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/64a9952656b49bafb620ec219d6472bb7eb4e590/modules/gke-cluster/main.tf#L403,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/5763eb53d427a79450d4302863cfa4eaca593154/modules/gke-cluster-standard/main.tf,2023-03-30 12:47:39+02:00,2023-05-02 14:59:12+00:00,6,1,1,1,0,0,0,0,0,0
https://github.com/alphagov/govuk-aws,1050,terraform/projects/infra-security-groups/cache.tf,terraform/projects/infra-security-groups/cache.tf,0,crap,# Allow the prometheus instances to scrape router's metrics,# Allow the prometheus instances to scrape router's metrics,"resource ""aws_security_group_rule"" ""cache_ingress_prometheus_router"" {
  type      = ""ingress""
  from_port = 3055
  to_port   = 3055
  protocol  = ""tcp""

  # Which security group is the rule assigned to
  security_group_id = ""${aws_security_group.cache.id}""

  # Which security group can use this rule
  source_security_group_id = ""${aws_security_group.prometheus.id}""
}
",resource,,,177,0.0,2f98d8ddca4f860b5ffa0cd1fc2ec36cc35b7b9d,c56537748dc97bd5618840bb62b795aebd3bf4cf,https://github.com/alphagov/govuk-aws/blob/2f98d8ddca4f860b5ffa0cd1fc2ec36cc35b7b9d/terraform/projects/infra-security-groups/cache.tf#L177,https://github.com/alphagov/govuk-aws/blob/c56537748dc97bd5618840bb62b795aebd3bf4cf/terraform/projects/infra-security-groups/cache.tf#L0,2020-03-05 15:48:39+00:00,2023-05-12 17:01:25+01:00,5,2,0,0,0,1,0,0,1,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1665,fast/stages/0-bootstrap/organization-iam.tf,fast/stages/0-bootstrap/organization-iam.tf,0,# todo,# TODO: align additive roles with the README,# TODO: align additive roles with the README,"locals {
  # IAM roles in the org to reset (remove principals)
  iam_delete_roles = [
    ""roles/billing.creator""
  ]
  # domain IAM bindings
  iam_domain_bindings = {
    ""domain:${var.organization.domain}"" = {
      authoritative = [""roles/browser""]
      additive      = []
    }
  }
  # human (groups) IAM bindings
  iam_group_bindings = {
    (local.groups.gcp-billing-admins) = {
      authoritative = []
      additive = (
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin"",
          ""roles/billing.costsManager""
        ]
      )
    }
    (local.groups.gcp-network-admins) = {
      authoritative = [
        ""roles/cloudasset.owner"",
        ""roles/cloudsupport.techSupportEditor"",
      ]
      additive = [
        ""roles/compute.orgFirewallPolicyAdmin"",
        ""roles/compute.xpnAdmin""
      ]
    }
    (local.groups.gcp-organization-admins) = {
      authoritative = [
        ""roles/cloudasset.owner"",
        ""roles/cloudsupport.admin"",
        ""roles/compute.osAdminLogin"",
        ""roles/compute.osLoginExternalUser"",
        ""roles/owner"",
        ""roles/resourcemanager.folderAdmin"",
        ""roles/resourcemanager.organizationAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.tagAdmin""
      ]
      additive = concat(
        [
          ""roles/orgpolicy.policyAdmin""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin"",
          ""roles/billing.costsManager""
        ]
      )
    }
    (local.groups.gcp-security-admins) = {
      authoritative = [
        ""roles/cloudasset.owner"",
        ""roles/cloudsupport.techSupportEditor"",
        ""roles/iam.securityReviewer"",
        ""roles/logging.admin"",
        ""roles/securitycenter.admin"",
      ]
      additive = [
        ""roles/accesscontextmanager.policyAdmin"",
        ""roles/iam.organizationRoleAdmin"",
        ""roles/orgpolicy.policyAdmin""
      ]
    }
    (local.groups.gcp-support) = {
      authoritative = [
        ""roles/cloudsupport.techSupportEditor"",
        ""roles/logging.viewer"",
        ""roles/monitoring.viewer"",
      ]
      additive = []
    }
  }
  # machine (service accounts) IAM bindings, in logical format
  # the service account module's ""magic"" outputs allow us to use dynamic values
  iam_sa_bindings = {
    (module.automation-tf-bootstrap-sa.iam_email) = {
      authoritative = [
        ""roles/logging.admin"",
        ""roles/resourcemanager.organizationAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.projectMover"",
        ""roles/resourcemanager.tagAdmin""
      ]
      additive = concat(
        [
          ""roles/iam.organizationRoleAdmin"",
          ""roles/orgpolicy.policyAdmin""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin"",
          ""roles/billing.costsManager""
        ]
      )
    }
    (module.automation-tf-resman-sa.iam_email) = {
      authoritative = [
        ""roles/logging.admin"",
        ""roles/resourcemanager.folderAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.tagAdmin"",
        ""roles/resourcemanager.tagUser""
      ]
      additive = concat(
        [
          ""roles/orgpolicy.policyAdmin""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin"",
          ""roles/billing.costsManager""
        ]
      )
    }
  }
  # bootstrap user bindings
  iam_user_bootstrap_bindings = var.bootstrap_user == null ? {} : {
    ""user:${var.bootstrap_user}"" = {
      authoritative = [
        ""roles/logging.admin"",
        ""roles/owner"",
        ""roles/resourcemanager.organizationAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.tagAdmin""
      ]
      # TODO: align additive roles with the README
      additive = (
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin"",
          ""roles/billing.costsManager""
        ]
      )
    }
  }
}
",locals,"locals {
  # IAM roles in the org to reset (remove principals)
  iam_delete_roles = [
    ""roles/billing.creator""
  ]
  # domain IAM bindings
  iam_domain_bindings = var.organization.domain == null ? {} : {
    ""domain:${var.organization.domain}"" = {
      authoritative = [""roles/browser""]
      additive      = []
    }
  }
  # human (groups) IAM bindings
  iam_principal_bindings = {
    (local.principals.gcp-billing-admins) = {
      authoritative = []
      additive = (
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin""
        ]
      )
    }
    (local.principals.gcp-network-admins) = {
      authoritative = [
        ""roles/cloudasset.owner"",
        ""roles/cloudsupport.techSupportEditor"",
      ]
      additive = [
        ""roles/compute.orgFirewallPolicyAdmin"",
        ""roles/compute.xpnAdmin""
      ]
    }
    (local.principals.gcp-organization-admins) = {
      authoritative = [
        ""roles/cloudasset.owner"",
        ""roles/cloudsupport.admin"",
        ""roles/compute.osAdminLogin"",
        ""roles/compute.osLoginExternalUser"",
        ""roles/owner"",
        ""roles/resourcemanager.folderAdmin"",
        ""roles/resourcemanager.organizationAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.tagAdmin"",
        ""roles/iam.workforcePoolAdmin""
      ]
      additive = concat(
        [
          ""roles/orgpolicy.policyAdmin""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin""
        ]
      )
    }
    (local.principals.gcp-security-admins) = {
      authoritative = [
        ""roles/cloudasset.owner"",
        ""roles/cloudsupport.techSupportEditor"",
        ""roles/iam.securityReviewer"",
        ""roles/logging.admin"",
        ""roles/securitycenter.admin"",
      ]
      additive = [
        ""roles/accesscontextmanager.policyAdmin"",
        ""roles/iam.organizationRoleAdmin"",
        ""roles/orgpolicy.policyAdmin""
      ]
    }
    (local.principals.gcp-support) = {
      authoritative = [
        ""roles/cloudsupport.techSupportEditor"",
        ""roles/logging.viewer"",
        ""roles/monitoring.viewer"",
      ]
      additive = []
    }
  }
  # machine (service accounts) IAM bindings, in logical format
  # the service account module's ""magic"" outputs allow us to use dynamic values
  iam_sa_bindings = {
    (module.automation-tf-bootstrap-sa.iam_email) = {
      authoritative = [
        ""roles/essentialcontacts.admin"",
        ""roles/iam.workforcePoolAdmin"",
        ""roles/logging.admin"",
        ""roles/resourcemanager.organizationAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.projectMover"",
        ""roles/resourcemanager.tagAdmin""
      ]
      additive = concat(
        [
          ""roles/iam.organizationRoleAdmin"",
          ""roles/orgpolicy.policyAdmin""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin""
        ]
      )
    }
    (module.automation-tf-bootstrap-r-sa.iam_email) = {
      authoritative = [
        ""roles/essentialcontacts.viewer"",
        ""roles/logging.viewer"",
        ""roles/resourcemanager.folderViewer"",
        ""roles/resourcemanager.tagViewer""
      ]
      additive = concat(
        [
          # the organizationAdminViewer custom role is granted via the SA module
          ""roles/iam.organizationRoleViewer"",
          ""roles/iam.workforcePoolViewer"",
          ""roles/orgpolicy.policyViewer""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.viewer""
        ]
      )
    }
    (module.automation-tf-resman-sa.iam_email) = {
      authoritative = [
        ""roles/essentialcontacts.admin"",
        ""roles/logging.admin"",
        ""roles/resourcemanager.folderAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.tagAdmin"",
        ""roles/resourcemanager.tagUser""
      ]
      additive = concat(
        [
          ""roles/accesscontextmanager.policyAdmin"",
          ""roles/orgpolicy.policyAdmin""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin""
        ]
      )
    }
    (module.automation-tf-resman-r-sa.iam_email) = {
      authoritative = [
        ""roles/accesscontextmanager.policyReader"",
        ""roles/essentialcontacts.viewer"",
        ""roles/logging.viewer"",
        ""roles/resourcemanager.folderViewer"",
        ""roles/resourcemanager.tagViewer"",
        ""roles/serviceusage.serviceUsageViewer""
      ]
      additive = concat(
        [
          # the organizationAdminViewer custom role is granted via the SA module
          ""roles/orgpolicy.policyViewer""
        ],
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.viewer""
        ]
      )
    }
  }
  # bootstrap user bindings
  iam_user_bootstrap_bindings = var.bootstrap_user == null ? {} : {
    ""user:${var.bootstrap_user}"" = {
      authoritative = [
        ""roles/logging.admin"",
        ""roles/owner"",
        ""roles/resourcemanager.organizationAdmin"",
        ""roles/resourcemanager.projectCreator"",
        ""roles/resourcemanager.tagAdmin""
      ]
      # TODO: align additive roles with the README
      additive = (
        local.billing_mode != ""org"" ? [] : [
          ""roles/billing.admin""
        ]
      )
    }
  }
}
",locals,148,187.0,b2d27b5f12a55e696f955c39530e29f27478bea4,be9214f99a0718eb8698e9d539e2ad93cb442ac7,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b2d27b5f12a55e696f955c39530e29f27478bea4/fast/stages/0-bootstrap/organization-iam.tf#L148,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/be9214f99a0718eb8698e9d539e2ad93cb442ac7/fast/stages/0-bootstrap/organization-iam.tf#L187,2023-09-28 11:41:56+02:00,2024-05-21 10:39:47+02:00,10,0,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1670,modules/secret-manager/main.tf,modules/secret-manager/main.tf,0,# todo,# TODO(jccb): support custom keys inside auto,# TODO(jccb): support custom keys inside auto,"resource ""google_secret_manager_secret"" ""default"" {
  for_each  = var.secrets
  project   = var.project_id
  secret_id = each.key
  labels    = lookup(var.labels, each.key, null)

  dynamic ""replication"" {
    for_each = each.value == null ? [""""] : []
    content {
      # TODO(jccb): support custom keys inside auto
      auto {}
    }
  }

  dynamic ""replication"" {
    for_each = each.value == null ? [] : [each.value]
    iterator = locations
    content {
      user_managed {
        dynamic ""replicas"" {
          for_each = locations.value
          iterator = location
          content {
            location = location.value
            dynamic ""customer_managed_encryption"" {
              for_each = try(var.encryption_key[location.value] != null ? [""""] : [], [])
              content {
                kms_key_name = var.encryption_key[location.value]
              }
            }
          }
        }
      }
    }
  }
}
",resource,"resource ""google_secret_manager_secret"" ""default"" {
  for_each  = var.secrets
  project   = var.project_id
  secret_id = each.key
  labels    = lookup(var.labels, each.key, null)

  dynamic ""replication"" {
    for_each = each.value.locations == null ? [""""] : []
    content {
      auto {
        dynamic ""customer_managed_encryption"" {
          for_each = try(lookup(each.value.keys, ""global"", null) == null ? [] : [""""], [])
          content {
            kms_key_name = each.value.keys[""global""]
          }
        }
      }
    }
  }

  dynamic ""replication"" {
    for_each = each.value.locations == null ? [] : [""""]
    content {
      user_managed {
        dynamic ""replicas"" {
          for_each = each.value.locations
          iterator = location
          content {
            location = location.value
            dynamic ""customer_managed_encryption"" {
              for_each = try(lookup(each.value.keys, location.value, null) == null ? [] : [""""], [])
              content {
                kms_key_name = each.value.keys[location.value]
              }
            }
          }
        }
      }
    }
  }
}
",resource,47,,789328ff5a1f71892a90d2174bc327aeaff08d31,d07f8fd33d0a5fc8eb9118a0258e3158949d95f9,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/789328ff5a1f71892a90d2174bc327aeaff08d31/modules/secret-manager/main.tf#L47,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d07f8fd33d0a5fc8eb9118a0258e3158949d95f9/modules/secret-manager/main.tf,2023-10-03 12:15:36+00:00,2023-11-10 16:45:47+01:00,2,1,0,1,0,1,0,0,0,0
https://github.com/aws-observability/terraform-aws-observability-accelerator,8,locals.tf,locals.tf,0,todo,# TODO when tf resource for AMG api keys are supported,"# TODO when tf resource for AMG api keys are supported 
 # create a short-lived api key on the fly if api_key is not provided","locals {
  eks_oidc_issuer_url  = replace(data.aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer, ""https://"", """")
  eks_cluster_endpoint = data.aws_eks_cluster.eks_cluster.endpoint
  eks_cluster_version  = data.aws_eks_cluster.eks_cluster.version

  # if region is not passed, we assume the current one
  amp_ws_region   = coalesce(var.managed_prometheus_region, data.aws_region.current.name)
  amp_ws_id       = var.enable_managed_prometheus ? aws_prometheus_workspace.this[0].id : var.managed_prometheus_id
  amp_ws_endpoint = ""https://aps-workspaces.${local.amp_ws_region}.amazonaws.com/workspaces/${local.amp_ws_id}/""

  # if region is not passed, we assume the current one
  amg_ws_region = coalesce(var.managed_grafana_region, data.aws_region.current.name)

  # if grafana_workspace_id is supplied, we infer the endpoint from
  # computed region, else we create a new workspace
  amg_ws_endpoint = var.enable_managed_grafana ? ""https://${module.managed_grafana[0].workspace_endpoint}"" : ""https://${var.managed_grafana_workspace_id}.grafana-workspace.${local.amg_ws_region}.amazonaws.com""

  # TODO when tf resource for AMG api keys are supported
  # create a short-lived api key on the fly if api_key is not provided
  amg_api_key = var.grafana_api_key

  context = {
    aws_caller_identity_account_id = data.aws_caller_identity.current.account_id
    aws_caller_identity_arn        = data.aws_caller_identity.current.arn
    aws_eks_cluster_endpoint       = local.eks_cluster_endpoint
    aws_partition_id               = data.aws_partition.current.partition
    aws_region_name                = data.aws_region.current.name
    eks_cluster_id                 = var.eks_cluster_id
    eks_oidc_issuer_url            = local.eks_oidc_issuer_url
    eks_oidc_provider_arn          = ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/${local.eks_oidc_issuer_url}""
    tags                           = var.tags
    irsa_iam_role_path             = var.irsa_iam_role_path
    irsa_iam_permissions_boundary  = var.irsa_iam_permissions_boundary
  }

  name = ""aws-observability-accelerator""
}
",locals,"locals {
  eks_oidc_issuer_url  = replace(data.aws_eks_cluster.eks_cluster.identity[0].oidc[0].issuer, ""https://"", """")
  eks_cluster_endpoint = data.aws_eks_cluster.eks_cluster.endpoint
  eks_cluster_version  = data.aws_eks_cluster.eks_cluster.version

  # if region is not passed, we assume the current one
  amp_ws_region   = coalesce(var.managed_prometheus_workspace_region, data.aws_region.current.name)
  amp_ws_id       = var.enable_managed_prometheus ? aws_prometheus_workspace.this[0].id : var.managed_prometheus_workspace_id
  amp_ws_endpoint = ""https://aps-workspaces.${local.amp_ws_region}.amazonaws.com/workspaces/${local.amp_ws_id}/""

  # if region is not passed, we assume the current one
  amg_ws_region = coalesce(var.managed_grafana_region, data.aws_region.current.name)

  # if grafana_workspace_id is supplied, we infer the endpoint from
  # computed region, else we create a new workspace
  amg_ws_endpoint = var.managed_grafana_workspace_id == """" ? ""https://${module.managed_grafana[0].workspace_endpoint}"" : ""https://${data.aws_grafana_workspace.this[0].endpoint}""
  amg_ws_id       = var.managed_grafana_workspace_id == """" ? module.managed_grafana[0].workspace_ : data.aws_grafana_workspace.this[0].endpoint

  amg_api_key = var.grafana_api_key

  context = {
    aws_caller_identity_account_id = data.aws_caller_identity.current.account_id
    aws_caller_identity_arn        = data.aws_caller_identity.current.arn
    aws_eks_cluster_endpoint       = local.eks_cluster_endpoint
    aws_partition_id               = data.aws_partition.current.partition
    aws_region_name                = data.aws_region.current.name
    eks_cluster_id                 = var.eks_cluster_id
    eks_oidc_issuer_url            = local.eks_oidc_issuer_url
    eks_oidc_provider_arn          = ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/${local.eks_oidc_issuer_url}""
    tags                           = var.tags
    irsa_iam_role_path             = var.irsa_iam_role_path
    irsa_iam_permissions_boundary  = var.irsa_iam_permissions_boundary
  }

  name = ""aws-observability-accelerator""
}
",locals,52,,2a5564607491389ad1a87c16add9542d44d9ac20,05511992e9cc5b9fdd5c3ba59e30704d78fadda3,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/2a5564607491389ad1a87c16add9542d44d9ac20/locals.tf#L52,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/05511992e9cc5b9fdd5c3ba59e30704d78fadda3/locals.tf,2022-08-26 17:30:03+02:00,2022-08-30 19:59:34+02:00,5,1,0,1,1,1,1,0,1,0
https://github.com/kubernetes/k8s.io,163,infra/gcp/terraform/kubernetes-public/k8s-triage.tf,infra/gcp/terraform/kubernetes-public/k8s-triage.tf,0,// todo,// TODO(spiffxp): remove legacy serviceaccount when migration completed,// TODO(spiffxp): remove legacy serviceaccount when migration completed,"locals {
  // TODO(spiffxp): remove legacy serviceaccount when migration completed
  triage_legacy_sa_email = ""triage@k8s-gubernator.iam.gserviceaccount.com""
  triage_dataset               = ""k8s-triage""
  prow_owners                  = ""k8s-infra-prow-oncall@kubernetes.io""
}
",locals,"locals {
  // TODO(spiffxp): remove legacy serviceaccount when migration completed
  triage_legacy_sa_email = ""triage@k8s-gubernator.iam.gserviceaccount.com""
}
",locals,9,25.0,51dbc9a02e4c70248e2a7ea71fc6e32293a0666b,c6f1ce0ddb827eca6f416c363f96a1f948a5e33e,https://github.com/kubernetes/k8s.io/blob/51dbc9a02e4c70248e2a7ea71fc6e32293a0666b/infra/gcp/terraform/kubernetes-public/k8s-triage.tf#L9,https://github.com/kubernetes/k8s.io/blob/c6f1ce0ddb827eca6f416c363f96a1f948a5e33e/infra/gcp/terraform/kubernetes-public/k8s-triage.tf#L25,2021-08-12 11:12:49-07:00,2021-10-08 12:33:34+02:00,7,0,0,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-sql-db,6,modules/backup/variables.tf,modules/backup/variables.tf,0,#todo,#TODO: gleichda change default to true on next major release (#336),#TODO: gleichda change default to true on next major release (#336),"variable ""compress_export"" {
  description = ""Whether or not to compress the export when storing in the bucket; Only valid for MySQL and PostgreSQL""
  type        = bool
  default     = false
}
",variable,"variable ""compress_export"" {
  description = ""Whether or not to compress the export when storing in the bucket; Only valid for MySQL and PostgreSQL""
  type        = bool
  default     = true
}
",variable,91,,b1ef34d0ee1a84ef2c0be4141cb83448052264da,ff3724429a04b54d25bfea6eb3db68d78d1128bb,https://github.com/terraform-google-modules/terraform-google-sql-db/blob/b1ef34d0ee1a84ef2c0be4141cb83448052264da/modules/backup/variables.tf#L91,https://github.com/terraform-google-modules/terraform-google-sql-db/blob/ff3724429a04b54d25bfea6eb3db68d78d1128bb/modules/backup/variables.tf,2022-08-09 09:38:28-05:00,2022-10-24 10:03:17-07:00,2,1,0,1,0,0,0,0,0,0
https://github.com/chanzuckerberg/cztack,121,aws-s3-account-public-access-block/variables.tf,aws-s3-account-public-access-block/variables.tf,0,# todo,# # TODO(el): on tf 0.13,"# # TODO(el): on tf 0.13 
 #  validation { 
 #    condition { 
 #       (var.restrict == ""all"" || var.restrict == ""new"" || var.restrict == ""none"") 
 #    } 
 #    error_message = ""restrict must be one of (all, new, none)"" 
 #  }","variable restrict {
  type        = string
  default     = ""all""
  description = <<EOF
  How restrictive should the account-wide access block be. Accepted values are `all`, `new`, `none`.
  `all` blocks public access to all buckets in account.
  `new` prevents you from granting public access to any more buckets, existing public buckets remain public.
  `none` restricts no access.
  EOF

  # # TODO(el): on tf 0.13
  #  validation {
  #    condition {
  #       (var.restrict == ""all"" || var.restrict == ""new"" || var.restrict == ""none"")
  #    }
  #    error_message = ""restrict must be one of (all, new, none)""
  #  }
}
",variable,"variable ""restrict"" {
  type        = string
  default     = ""all""
  description = <<EOF
  How restrictive should the account-wide access block be. Accepted values are `all`, `new`, `none`.
  `all` blocks public access to all buckets in account.
  `new` prevents you from granting public access to any more buckets, existing public buckets remain public.
  `none` restricts no access.
  EOF

  # # TODO(el): on tf 0.13
  #  validation {
  #    condition {
  #       (var.restrict == ""all"" || var.restrict == ""new"" || var.restrict == ""none"")
  #    }
  #    error_message = ""restrict must be one of (all, new, none)""
  #  }
}
",variable,11,11.0,67584526b30ead327b536d614e568998558345ea,9df439500dee7468643ca03a844cf7a5b1e1b313,https://github.com/chanzuckerberg/cztack/blob/67584526b30ead327b536d614e568998558345ea/aws-s3-account-public-access-block/variables.tf#L11,https://github.com/chanzuckerberg/cztack/blob/9df439500dee7468643ca03a844cf7a5b1e1b313/aws-s3-account-public-access-block/variables.tf#L11,2020-10-30 16:45:46-04:00,2021-04-13 14:52:04-04:00,2,0,0,1,1,0,0,0,0,0
https://github.com/Worklytics/psoxy,1533,infra/modules/worklytics-connectors/main.tf,infra/modules/worklytics-connectors/main.tf,0,# todo,# TODO: deal w/ adding the OAUTH_REFRESH_TOKEN_STUFF from above,# TODO: deal w/ adding the OAUTH_REFRESH_TOKEN_STUFF from above,"locals {
  # TODO: deal w/ adding the OAUTH_REFRESH_TOKEN_STUFF from above
  enabled_api_connectors  = module.worklytics_connector_specs.enabled_oauth_long_access_connectors
  enabled_bulk_connectors = module.worklytics_connector_specs.enabled_bulk_connectors
}
",locals,"locals {
  enabled_api_connectors  = module.worklytics_connector_specs.enabled_oauth_long_access_connectors
  enabled_bulk_connectors = module.worklytics_connector_specs.enabled_bulk_connectors
}
",locals,21,,f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e,7a2ae9aca88a5f018b9671f325f71db8e9285358,https://github.com/Worklytics/psoxy/blob/f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e/infra/modules/worklytics-connectors/main.tf#L21,https://github.com/Worklytics/psoxy/blob/7a2ae9aca88a5f018b9671f325f71db8e9285358/infra/modules/worklytics-connectors/main.tf,2023-06-16 14:08:45-07:00,2023-07-11 15:51:07-07:00,2,1,0,1,0,1,0,0,0,0
https://github.com/chanzuckerberg/cztack,169,databricks-cluster-policy/main.tf,databricks-cluster-policy/main.tf,0,workaround,# Workaround for looping over grantees and setting resource count,# Workaround for looping over grantees and setting resource count,"locals {
  # default policy attributes that can be overridden but are otherwise 
  # included for each policy
  default_policy = {
    ""custom_tags.Cluster_Policy"" : {
      ""type"" : ""fixed"",
      ""value"" : var.policy_name
    },
    ""custom_tags.Databricks_Workspace_Id"" : {
      ""type"" : ""fixed"",
      ""value"" : var.databricks_workspace_id
    },
    ""custom_tags.Databricks_Host"" : {
      ""type"" : ""fixed"",
      ""value"" : var.databricks_host
    },
  }

  # Workaround for looping over grantees and setting resource count
  inherited_cluster_policy_grantees = toset([for grantee in var.grantees : grantee if var.policy_family_id != null])
  custom_cluster_policy_grantees    = toset([for grantee in var.grantees : grantee if var.policy_family_id == null])
}
",locals,"locals {
  # default policy attributes that can be overridden but are otherwise 
  # included for each policy
  default_policy = {
    ""custom_tags.Cluster_Policy"" : {
      ""type"" : ""fixed"",
      ""value"" : var.policy_name
    },
    ""custom_tags.Databricks_Workspace_Id"" : {
      ""type"" : ""fixed"",
      ""value"" : var.databricks_workspace_id
    },
    ""custom_tags.Databricks_Host"" : {
      ""type"" : ""fixed"",
      ""value"" : var.databricks_host
    },
  }

  # Workaround for looping over grantees and setting resource count
  inherited_cluster_policy_grantees = toset([for grantee in var.grantees : grantee if var.policy_family_id != null])
  custom_cluster_policy_grantees    = toset([for grantee in var.grantees : grantee if var.policy_family_id == null])
}
",locals,19,19.0,5f42e9bbb2eafdbde5a3afbc0d0fc1aa6d4093b9,0ab051aab7c11e550fcab20c4eaeef562c4d3e39,https://github.com/chanzuckerberg/cztack/blob/5f42e9bbb2eafdbde5a3afbc0d0fc1aa6d4093b9/databricks-cluster-policy/main.tf#L19,https://github.com/chanzuckerberg/cztack/blob/0ab051aab7c11e550fcab20c4eaeef562c4d3e39/databricks-cluster-policy/main.tf#L19,2023-10-31 13:13:06-07:00,2024-03-06 16:58:57-08:00,3,0,0,1,0,1,0,0,0,0
https://github.com/ministryofjustice/aws-root-account,6,terraform/organizations-accounts.tf,terraform/organizations-accounts.tf,0,hack,# This is a slightly hacky way to get email addresses for already configured accounts.,"# This is a slightly hacky way to get email addresses for already configured accounts. 
 # We should store all (including new) account email addresses in AWS Secrets Manager, 
 # rather than rely on this in the future.","data ""aws_organizations_organization"" ""root"" {}
",data,the block associated got renamed or deleted,,1,,5e53a40e86403ec063a74b9f9bd68864ea3c195a,9d8fb3bcfae2288a98498d3cc34a310bf918a287,https://github.com/ministryofjustice/aws-root-account/blob/5e53a40e86403ec063a74b9f9bd68864ea3c195a/terraform/organizations-accounts.tf#L1,https://github.com/ministryofjustice/aws-root-account/blob/9d8fb3bcfae2288a98498d3cc34a310bf918a287/terraform/organizations-accounts.tf,2020-11-26 12:48:39+00:00,2020-12-18 12:46:23+00:00,9,1,0,1,0,1,0,0,0,0
https://github.com/uyuni-project/sumaform,1584,modules/server/variables.tf,modules/server/variables.tf,0,workaround,"# WORKAROUND: this is causing problems in the testsuite, disable it for now","# WORKAROUND: this is causing problems in the testsuite, disable it for now","variable ""c3p0_connection_timeout"" {
  description = ""c3p0 connections will be closed after this timeout""
  # WORKAROUND: this is causing problems in the testsuite, disable it for now
  default     = false
}
",variable,"variable ""c3p0_connection_timeout"" {
  description = ""c3p0 connections will be closed after this timeout""
  # WORKAROUND: this is causing problems in the testsuite, disable it for now
  default     = false
}
",variable,279,315.0,c7a1ca80f5f3194c87d28ae1506791b150e496dc,9ddd0e0dd8d873e0a44e1b64214d41718f700ff8,https://github.com/uyuni-project/sumaform/blob/c7a1ca80f5f3194c87d28ae1506791b150e496dc/modules/server/variables.tf#L279,https://github.com/uyuni-project/sumaform/blob/9ddd0e0dd8d873e0a44e1b64214d41718f700ff8/modules/server/variables.tf#L315,2023-06-12 11:59:16+02:00,2024-04-08 10:06:08+02:00,17,0,0,1,0,0,0,0,0,1
https://github.com/nasa/cumulus,145,tf-modules/archive/ingest-reporting.tf,tf-modules/archive/ingest-reporting.tf,0,fix,# TODO Re-enable once IAM permissions have been fixed,"# TODO Re-enable once IAM permissions have been fixed 
 # tags                 = local.default_tags","resource ""aws_iam_role"" ""report_granules_lambda_role"" {
  name                 = ""${var.prefix}-ReportGranulesLambda""
  assume_role_policy   = data.aws_iam_policy_document.lambda_assume_role_policy.json
  permissions_boundary = var.permissions_boundary_arn
  # TODO Re-enable once IAM permissions have been fixed
  # tags                 = local.default_tags
}
",resource,the block associated got renamed or deleted,,102,,60cef06b6a3c52c2993b0cb4da153e49f70c1c8c,c225c172d951fa95f466ed8c9ad82ff0a54e7542,https://github.com/nasa/cumulus/blob/60cef06b6a3c52c2993b0cb4da153e49f70c1c8c/tf-modules/archive/ingest-reporting.tf#L102,https://github.com/nasa/cumulus/blob/c225c172d951fa95f466ed8c9ad82ff0a54e7542/tf-modules/archive/ingest-reporting.tf,2019-10-10 13:45:06-04:00,2020-02-21 12:05:54-05:00,6,1,0,1,0,1,0,0,0,0
https://github.com/nasa/cumulus,15,variables.tf,variables.tf,0,todo,# TODO Add descriptions,"# TODO Add descriptions  
 # Required variables ","variable ""prefix"" {
  type = string
}
",variable,"variable ""prefix"" {
  type        = string
  description = ""Resource prefix unique to this deployment""
}
",variable,1,,d02dddacfe2488210b9a79990db5bbd8d20c39f8,0ea14e208a63ef4bdf1033d09f547ba89d3cecbe,https://github.com/nasa/cumulus/blob/d02dddacfe2488210b9a79990db5bbd8d20c39f8/variables.tf#L1,https://github.com/nasa/cumulus/blob/0ea14e208a63ef4bdf1033d09f547ba89d3cecbe/variables.tf,2019-07-22 14:25:58-04:00,2019-07-25 10:55:47-04:00,3,1,0,1,0,0,0,0,0,0
https://github.com/alphagov/govuk-aws,854,terraform/modules/aws/lb_listener_rules/main.tf,terraform/modules/aws/lb_listener_rules/main.tf,0,implement,"/** * ## Modules: aws/lb_listener_rules * * This module creates Load Balancer listener rules based on Host header and target groups for * an existing listener resource. * * If the parameter `autoscaling_group_name` is non empty, the module also creates an attachment * from each target group to the ASG with the specified name. * * Limitations: *  - The target group deregistration_delay, health_check_interval and health_check_timeout * values can be configured with variables, but will be the same for all the target groups *  - With Terraform we can't provide a 'count' or list for listener_rule condition blocks, * so at the moment only one condition can be specified per rule *  - At the moment this module only implements Host Header based rules*/","/** 
 * ## Modules: aws/lb_listener_rules 
 * 
 * This module creates Load Balancer listener rules based on Host header and target groups for 
 * an existing listener resource. 
 * 
 * If the parameter `autoscaling_group_name` is non empty, the module also creates an attachment 
 * from each target group to the ASG with the specified name. 
 * 
 * Limitations: 
 *  - The target group deregistration_delay, health_check_interval and health_check_timeout 
 * values can be configured with variables, but will be the same for all the target groups 
 *  - With Terraform we can't provide a 'count' or list for listener_rule condition blocks, 
 * so at the moment only one condition can be specified per rule 
 *  - At the moment this module only implements Host Header based rules 
 */ ","variable ""default_tags"" {
  type        = ""map""
  description = ""Additional resource tags""
  default     = {}
}
",variable,"variable ""default_tags"" {
  type        = map(string)
  description = ""Additional resource tags""
  default     = {}
}
",variable,1,1.0,a272a6f68c811aa9bb956809fd9e2caec8fa8089,7a0cb9b14717825fe20ec66dde2591851d2de47b,https://github.com/alphagov/govuk-aws/blob/a272a6f68c811aa9bb956809fd9e2caec8fa8089/terraform/modules/aws/lb_listener_rules/main.tf#L1,https://github.com/alphagov/govuk-aws/blob/7a0cb9b14717825fe20ec66dde2591851d2de47b/terraform/modules/aws/lb_listener_rules/main.tf#L1,2019-06-12 17:18:39+01:00,2023-12-01 16:46:15+00:00,10,0,0,1,1,0,1,0,0,0
https://github.com/chanzuckerberg/cztack,94,aws-cloudfront-logs-bucket/outputs.tf,aws-cloudfront-logs-bucket/outputs.tf,0,hack,// HACK(el): we do this to hint TF dependency graph since modules can't depend_on,// HACK(el): we do this to hint TF dependency graph since modules can't depend_on,"output ""name"" {
  value = module.aws-cloudfront-logs-bucket.name
}
",output,"output ""name"" {
  value = module.aws-cloudfront-logs-bucket.name
}
",output,1,1.0,d87b0071e7729697680297d70284c649a0cf13cb,ec0aea5167b9ae64529cdac2441a59b67424f040,https://github.com/chanzuckerberg/cztack/blob/d87b0071e7729697680297d70284c649a0cf13cb/aws-cloudfront-logs-bucket/outputs.tf#L1,https://github.com/chanzuckerberg/cztack/blob/ec0aea5167b9ae64529cdac2441a59b67424f040/aws-cloudfront-logs-bucket/outputs.tf#L1,2020-07-08 10:19:24-07:00,2024-03-27 14:09:21-04:00,2,0,0,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1639,fast/stages/3-gke-multitenant/dev/variables.tf,fast/stages/3-gke-multitenant/dev/variables.tf,0,todo,# TODO add kube state metrics,"# TODO add kube state metrics  
 # Google Cloud Managed Service for Prometheus","variable ""clusters"" {
  description = ""Clusters configuration. Refer to the gke-cluster-standard module for type details.""
  type = map(object({
    cluster_autoscaling = optional(any)
    description         = optional(string)
    enable_addons = optional(any, {
      horizontal_pod_autoscaling = true, http_load_balancing = true
    })
    enable_features = optional(any, {
      workload_identity = true
    })
    issue_client_certificate = optional(bool, false)
    labels                   = optional(map(string))
    location                 = string
    logging_config = optional(object({
      enable_system_logs             = optional(bool, true)
      enable_workloads_logs          = optional(bool, true)
      enable_api_server_logs         = optional(bool, false)
      enable_scheduler_logs          = optional(bool, false)
      enable_controller_manager_logs = optional(bool, false)
    }), {})
    maintenance_config = optional(any, {
      daily_window_start_time = ""03:00""
      recurring_window        = null
      maintenance_exclusion   = []
    })
    max_pods_per_node  = optional(number, 110)
    min_master_version = optional(string)
    monitoring_config = optional(object({
      enable_system_metrics = optional(bool, true)

      # Control plane metrics
      enable_api_server_metrics         = optional(bool, false)
      enable_controller_manager_metrics = optional(bool, false)
      enable_scheduler_metrics          = optional(bool, false)

      # TODO add kube state metrics

      # Google Cloud Managed Service for Prometheus
      enable_managed_prometheus = optional(bool, true)
    }), {})
    node_locations         = optional(list(string))
    private_cluster_config = optional(any)
    release_channel        = optional(string)
    vpc_config = object({
      subnetwork = string
      network    = optional(string)
      secondary_range_blocks = optional(object({
        pods     = string
        services = string
      }))
      secondary_range_names = optional(object({
        pods     = optional(string, ""pods"")
        services = optional(string, ""services"")
      }))
      master_authorized_ranges = optional(map(string))
      master_ipv4_cidr_block   = optional(string)
    })
  }))
  default  = {}
  nullable = false
}
",variable,"variable ""clusters"" {
  description = ""Clusters configuration. Refer to the gke-cluster-standard module for type details.""
  type = map(object({
    cluster_autoscaling = optional(any)
    description         = optional(string)
    enable_addons = optional(any, {
      horizontal_pod_autoscaling = true, http_load_balancing = true
    })
    enable_features = optional(any, {
      workload_identity = true
    })
    issue_client_certificate = optional(bool, false)
    labels                   = optional(map(string))
    location                 = string
    logging_config = optional(object({
      enable_system_logs             = optional(bool, true)
      enable_workloads_logs          = optional(bool, true)
      enable_api_server_logs         = optional(bool, false)
      enable_scheduler_logs          = optional(bool, false)
      enable_controller_manager_logs = optional(bool, false)
    }), {})
    maintenance_config = optional(any, {
      daily_window_start_time = ""03:00""
      recurring_window        = null
      maintenance_exclusion   = []
    })
    max_pods_per_node  = optional(number, 110)
    min_master_version = optional(string)
    monitoring_config = optional(object({
      enable_system_metrics = optional(bool, true)

      # (Optional) control plane metrics
      enable_api_server_metrics         = optional(bool, false)
      enable_controller_manager_metrics = optional(bool, false)
      enable_scheduler_metrics          = optional(bool, false)

      # (Optional) kube state metrics
      enable_daemonset_metrics   = optional(bool, false)
      enable_deployment_metrics  = optional(bool, false)
      enable_hpa_metrics         = optional(bool, false)
      enable_pod_metrics         = optional(bool, false)
      enable_statefulset_metrics = optional(bool, false)
      enable_storage_metrics     = optional(bool, false)

      # Google Cloud Managed Service for Prometheus
      enable_managed_prometheus = optional(bool, true)
    }), {})
    node_locations         = optional(list(string))
    private_cluster_config = optional(any)
    release_channel        = optional(string)
    vpc_config = object({
      subnetwork = string
      network    = optional(string)
      secondary_range_blocks = optional(object({
        pods     = string
        services = string
      }))
      secondary_range_names = optional(object({
        pods     = optional(string, ""pods"")
        services = optional(string, ""services"")
      }))
      master_authorized_ranges = optional(map(string))
      master_ipv4_cidr_block   = optional(string)
    })
  }))
  default  = {}
  nullable = false
}
",variable,78,,b3dc91b5cd6375d9e589f149d98985a895fffbc2,6eb862a7754e9796cc26386227ea763c579edcdc,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b3dc91b5cd6375d9e589f149d98985a895fffbc2/fast/stages/3-gke-multitenant/dev/variables.tf#L78,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/6eb862a7754e9796cc26386227ea763c579edcdc/fast/stages/3-gke-multitenant/dev/variables.tf,2023-09-14 23:25:57+01:00,2023-09-15 12:18:45+01:00,2,1,1,1,0,0,0,0,1,0
https://github.com/kubernetes/k8s.io,369,infra/aws/terraform/prow-build-cluster/variables.tf,infra/aws/terraform/prow-build-cluster/variables.tf,0,# todo,# TODO(xmudrii): This is a temporary variable. To be deleted after making canary cluster a build cluster.,"# TODO(xmudrii): This is a temporary variable. To be deleted after making canary cluster a build cluster. 
 # This variable defines if this cluster is used as a Prow build cluster.","variable ""prow_build_cluster"" {
  type        = bool
  description = ""Provision this cluster as a Prow build cluster.""
  default     = true
  nullable    = false
}
",variable,the block associated got renamed or deleted,,40,,77dd998553934196b4d573be7a2fb8bcf7d8b56f,3cf0ef275d51659e041a4663921016a73d4eb7b8,https://github.com/kubernetes/k8s.io/blob/77dd998553934196b4d573be7a2fb8bcf7d8b56f/infra/aws/terraform/prow-build-cluster/variables.tf#L40,https://github.com/kubernetes/k8s.io/blob/3cf0ef275d51659e041a4663921016a73d4eb7b8/infra/aws/terraform/prow-build-cluster/variables.tf,2023-04-25 13:38:58+02:00,2023-04-26 14:58:43+02:00,4,1,1,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-slo,14,modules/slo-pipeline/main.tf,modules/slo-pipeline/main.tf,0,todo,# TODO update version once event-function is released with new functionality,# TODO update version once event-function is released with new functionality,"module ""event_function"" {
  # TODO update version once event-function is released with new functionality
  source = ""github.com/taylorludwig/terraform-google-event-function?ref=feature%2F37-terraform-created-files-in-archive""
  # source  = ""terraform-google-modules/event-function/google""
  # version = ""~> 1.1""

  description            = ""SLO Exporter to BigQuery or Stackdriver Monitoring""
  name                   = var.function_name
  available_memory_mb    = var.function_memory
  project_id             = var.project_id
  region                 = var.region
  service_account_email  = local.service_account_email
  source_directory       = local.function_source_directory
  source_dependent_files = [local_file.exporters]
  bucket_name            = local.bucket_name
  runtime                = ""python37""
  timeout_s              = ""60""
  entry_point            = ""main""

  event_trigger = {
    event_type = ""providers/cloud.pubsub/eventTypes/topic.publish""
    resource   = ""projects/${var.project_id}/topics/${google_pubsub_topic.stream.name}""
  }
}
",module,"module ""event_function"" {
  source  = ""terraform-google-modules/event-function/google""
  version = ""~> 1.2""

  description            = ""SLO Exporter to BigQuery or Stackdriver Monitoring""
  name                   = var.function_name
  available_memory_mb    = var.function_memory
  project_id             = var.project_id
  region                 = var.region
  service_account_email  = local.service_account_email
  source_directory       = local.function_source_directory
  source_dependent_files = [local_file.exporters]
  bucket_name            = local.bucket_name
  runtime                = ""python37""
  timeout_s              = ""60""
  entry_point            = ""main""

  event_trigger = {
    event_type = ""providers/cloud.pubsub/eventTypes/topic.publish""
    resource   = ""projects/${var.project_id}/topics/${google_pubsub_topic.stream.name}""
  }
}
",module,49,,33d08bd605f74f3168020d48535152adda5dedd6,bc2231a36af0a89bf505e1da2fda8cc5210b8912,https://github.com/terraform-google-modules/terraform-google-slo/blob/33d08bd605f74f3168020d48535152adda5dedd6/modules/slo-pipeline/main.tf#L49,https://github.com/terraform-google-modules/terraform-google-slo/blob/bc2231a36af0a89bf505e1da2fda8cc5210b8912/modules/slo-pipeline/main.tf,2019-12-13 16:06:10-08:00,2019-12-18 10:43:18-08:00,2,1,1,1,1,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-kubernetes-engine,11,modules/safer-cluster/variables.tf,modules/safer-cluster/variables.tf,0,// todo,// TODO(mmontan): allow specifying which project to use,"// TODO(mmontan): allow specifying which project to use 
 // for reading images. ","variable ""service_account"" {
  type        = string
  description = ""The service account to run nodes as if not overridden in `node_pools`. The create_service_account variable default value (true) will cause a cluster-specific service account to be created.""
  default     = """"
}
",variable,"variable ""service_account"" {
  type        = string
  description = ""The service account to run nodes as if not overridden in `node_pools`. The create_service_account variable default value (true) will cause a cluster-specific service account to be created.""
  default     = """"
}
",variable,211,,e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5,5a194719faa144ad0a7ee578663d336358f5073c,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5/modules/safer-cluster/variables.tf#L211,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/5a194719faa144ad0a7ee578663d336358f5073c/modules/safer-cluster/variables.tf,2019-10-04 15:21:33-07:00,2019-11-13 15:10:12-06:00,6,1,1,0,0,1,0,0,0,0
https://github.com/alphagov/govuk-aws,1089,terraform/projects/infra-database-backups-bucket/main.tf,terraform/projects/infra-database-backups-bucket/main.tf,0,# todo,# TODO: make staging use the same rules as integration. We don't need to,"# Production/Staging lifecycle rules. 
 # 
 # TODO: make staging use the same rules as integration. We don't need to 
 # retain backups of staging for very long. ","resource ""aws_s3_bucket"" ""database_backups"" {
  bucket = ""govuk-${var.aws_environment}-database-backups""
  region = ""${var.aws_region}""

  tags {
    Name            = ""govuk-${var.aws_environment}-database-backups""
    aws_environment = ""${var.aws_environment}""
  }

  logging {
    target_bucket = ""${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}""
    target_prefix = ""s3/govuk-${var.aws_environment}-database-backups/""
  }

  # Production/Staging lifecycle rules.
  #
  # TODO: make staging use the same rules as integration. We don't need to
  # retain backups of staging for very long.

  lifecycle_rule {
    id      = ""mysql_lifecycle_rule""
    prefix  = ""mysql/""
    enabled = ""${var.aws_environment != ""integration""}""

    transition {
      storage_class = ""STANDARD_IA""
      days          = ""${var.standard_s3_storage_time}""
    }

    transition {
      storage_class = ""GLACIER""
      days          = ""${var.glacier_storage_time}""
    }

    expiration {
      days = 120
    }
  }
  lifecycle_rule {
    id      = ""postgres_lifecycle_rule""
    prefix  = ""postgres/""
    enabled = ""${var.aws_environment != ""integration""}""

    transition {
      storage_class = ""STANDARD_IA""
      days          = ""${var.standard_s3_storage_time}""
    }

    transition {
      storage_class = ""GLACIER""
      days          = ""${var.glacier_storage_time}""
    }

    expiration {
      days = 120
    }
  }
  lifecycle_rule {
    id      = ""mongo_daily_lifecycle_rule""
    prefix  = ""mongodb/daily""
    enabled = ""${var.aws_environment != ""integration""}""

    transition {
      storage_class = ""STANDARD_IA""
      days          = ""${var.standard_s3_storage_time}""
    }

    transition {
      storage_class = ""GLACIER""
      days          = ""${var.glacier_storage_time}""
    }

    expiration {
      days = 120
    }
  }
  lifecycle_rule {
    id      = ""mongo_regular_lifecycle_rule""
    prefix  = ""mongodb/regular""
    enabled = true

    expiration {
      days = ""${var.expiration_time_whisper_mongo}""
    }
  }
  lifecycle_rule {
    id      = ""whisper_lifecycle_rule""
    prefix  = ""whisper/""
    enabled = true

    expiration {
      days = ""${var.expiration_time_whisper_mongo}""
    }
  }

  # Integration-specific lifecycle rules. These rules are created in all
  # environments but are only enabled in Integration.
  #
  # TODO: create these only in environments where they're needed, instead of
  # creating them everywhere and leaving them disabled.
  #
  # TODO: these are all set to the same var.expiration_time so just replace
  # them with one rule. Similarly for the prod ones above.

  lifecycle_rule {
    id      = ""mysql_lifecycle_rule_integration""
    prefix  = ""mysql/""
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }
  }
  lifecycle_rule {
    id      = ""postgres_lifecycle_rule_integration""
    prefix  = ""postgres/""
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }
  }
  lifecycle_rule {
    id      = ""mongo_daily_lifecycle_rule_integration""
    prefix  = ""mongodb/daily""
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }
  }
  lifecycle_rule {
    id      = ""whole_bucket_lifecycle_rule_integration""
    prefix  = """"
    enabled = ""${var.aws_environment == ""integration""}""

    expiration {
      days = ""${var.expiration_time}""
    }

    noncurrent_version_expiration {
      days = ""1""
    }
  }

  # End of Integration-specific lifecycle rules.


  # Lifecycle rule for coronavirus find support backup

  lifecycle_rule {
    id      = ""coronavirus_find_support_lifecycle_rule""
    prefix  = ""coronavirus-find-support/production.sql.gzip""
    enabled = true

    expiration {
      days = 365
    }
  }

  # Lifecycle rule for coronavirus business volunteer form backup

  lifecycle_rule {
    id      = ""coronavirus_business_volunteer_form_lifecycle_rule""
    prefix  = ""coronavirus-business-volunteer-form/production.sql.gzip""
    enabled = true

    expiration {
      days = 365
    }
  }
  versioning {
    enabled = true
  }
  replication_configuration {
    role = ""${aws_iam_role.backup_replication_role.arn}""

    rules {
      id     = ""main_replication_rule""
      prefix = """"
      status = ""${var.replication_setting}""

      destination {
        bucket        = ""${aws_s3_bucket.database_backups_replica.arn}""
        storage_class = ""STANDARD""
      }
    }
  }
}
",resource,"resource ""aws_s3_bucket"" ""database_backups"" {
  bucket = ""govuk-${var.aws_environment}-database-backups""
  region = ""${var.aws_region}""

  tags {
    Name            = ""govuk-${var.aws_environment}-database-backups""
    aws_environment = ""${var.aws_environment}""
  }

  logging {
    target_bucket = ""${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}""
    target_prefix = ""s3/govuk-${var.aws_environment}-database-backups/""
  }

  versioning {
    # It's not entirely clear if versioning is useful on this bucket  but it was previously configured this way,
    # so we've decided not to change it. Whilst it helps protect against accidental deletion, it doesn't protect
    # against malicious actors, so shouldn't be considered a security feature.
    enabled = true
  }

  lifecycle_rule {
    # Use a long retention period in production
    id      = ""long_retention_period""
    enabled = ""${var.aws_environment == ""production""}""

    # Ideally everything would go in the Standard (Infrequent Access) storage class when created.
    # But newly created objects always go into Standard, and can only move into IA after at least 30 days.
    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html
    transition {
      storage_class = ""STANDARD_IA""
      days          = 30
    }

    # Likewise, we have to wait at least another 30 days before we can move objects into Glacier storage.
    transition {
      storage_class = ""GLACIER""
      days          = 60
    }

    # Versioning is enabled on this bucket, so this rule will 'soft delete' objects.
    # In AWS lingo, this means a 'delete marker' will be set on the current version of the object.
    # More info on how expiration rules apply to versioned buckets here:
    # https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#intro-lifecycle-rules-actions
    expiration {
      days = 120
    }

    # This rule will 'hard delete' objects 1 day after they were 'soft deleted'.
    # In other words: old database backups will be permanently deleted 1 day after they've expired.
    noncurrent_version_expiration {
      days = ""1""
    }
  }

  lifecycle_rule {
    # Use a short retention period in integration and staging
    id      = ""short_retention_period""
    enabled = ""${var.aws_environment != ""production""}""

    expiration {
      days = ""3""
    }

    noncurrent_version_expiration {
      days = ""1""
    }
  }

  replication_configuration {
    role = ""${aws_iam_role.backup_replication_role.arn}""

    rules {
      id     = ""main_replication_rule""
      status = ""Enabled""

      destination {
        bucket        = ""${aws_s3_bucket.database_backups_replica.arn}""
        storage_class = ""STANDARD_IA""
      }
    }
  }
}
",resource,116,,d1fcb45657475a7de489503eae548845ec8e4296,78cc3f5ce73fb5a8f6d5126ff694ac87e5a0eb79,https://github.com/alphagov/govuk-aws/blob/d1fcb45657475a7de489503eae548845ec8e4296/terraform/projects/infra-database-backups-bucket/main.tf#L116,https://github.com/alphagov/govuk-aws/blob/78cc3f5ce73fb5a8f6d5126ff694ac87e5a0eb79/terraform/projects/infra-database-backups-bucket/main.tf,2020-11-24 17:53:04+00:00,2022-01-31 17:30:52+00:00,4,1,1,1,0,0,0,1,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,989,modules/project/service-accounts.tf,modules/project/service-accounts.tf,0,# todo,# TODO: jit?,# TODO: jit?,"locals {
  _service_accounts_cmek_service_dependencies = {
    ""composer"" : [
      ""composer"",
      ""artifactregistry"", ""container-engine"", ""compute"", ""pubsub"", ""storage""
    ]
    ""dataflow"" : [""dataflow"", ""compute""]
  }
  _service_accounts_robot_services = {
    artifactregistry  = ""service-%s@gcp-sa-artifactregistry""
    bq                = ""bq-%s@bigquery-encryption""
    cloudasset        = ""service-%s@gcp-sa-cloudasset""
    cloudbuild        = ""service-%s@gcp-sa-cloudbuild""
    cloudfunctions    = ""service-%s@gcf-admin-robot""
    cloudrun          = ""service-%s@serverless-robot-prod""
    composer          = ""service-%s@cloudcomposer-accounts""
    compute           = ""service-%s@compute-system""
    container-engine  = ""service-%s@container-engine-robot""
    containerregistry = ""service-%s@containerregistry""
    dataflow          = ""service-%s@dataflow-service-producer-prod""
    dataproc          = ""service-%s@dataproc-accounts""
    fleet             = ""service-%s@gcp-sa-gkehub""
    gae-flex          = ""service-%s@gae-api-prod""
    # TODO: deprecate gcf
    gcf = ""service-%s@gcf-admin-robot""
    # TODO: jit?
    gke-mcs                  = ""service-%s@gcp-sa-mcsd""
    monitoring-notifications = ""service-%s@gcp-sa-monitoring-notification""
    pubsub                   = ""service-%s@gcp-sa-pubsub""
    secretmanager            = ""service-%s@gcp-sa-secretmanager""
    sql                      = ""service-%s@gcp-sa-cloud-sql""
    sqladmin                 = ""service-%s@gcp-sa-cloud-sql""
    storage                  = ""service-%s@gs-project-accounts""
  }
  service_accounts_default = {
    compute = ""${local.project.number}-compute@developer.gserviceaccount.com""
    gae     = ""${local.project.project_id}@appspot.gserviceaccount.com""
  }
  service_account_cloud_services = (
    ""${local.project.number}@cloudservices.gserviceaccount.com""
  )
  service_accounts_robots = merge(
    {
      for k, v in local._service_accounts_robot_services :
      k => ""${format(v, local.project.number)}.iam.gserviceaccount.com""
    },
    {
      gke-mcs-importer = ""${local.project.project_id}.svc.id.goog[gke-mcs/gke-mcs-importer]""
    }
  )
  service_accounts_jit_services = [
    ""cloudasset.googleapis.com"",
    ""gkehub.googleapis.com"",
    ""pubsub.googleapis.com"",
    ""secretmanager.googleapis.com"",
    ""sqladmin.googleapis.com""
  ]
  service_accounts_cmek_service_keys = distinct(flatten([
    for s in keys(var.service_encryption_key_ids) : [
      for ss in try(local._service_accounts_cmek_service_dependencies[s], [s]) : [
        for key in var.service_encryption_key_ids[s] : {
          service = ss
          key     = key
        } if key != null
      ]
    ]
  ]))
}
",locals,"locals {
  _service_accounts_cmek_service_dependencies = {
    ""composer"" : [
      ""composer"",
      ""artifactregistry"", ""container-engine"", ""compute"", ""pubsub"", ""storage""
    ]
    ""dataflow"" : [""dataflow"", ""compute""]
  }
  _service_agents_data = yamldecode(file(""${path.module}/service-agents.yaml""))
  service_accounts_default = {
    compute      = ""${local.project.number}-compute@developer.gserviceaccount.com""
    gae          = ""${local.project.project_id}@appspot.gserviceaccount.com""
    workstations = ""service-${local.project.number}@gcp-sa-workstationsvm.iam.gserviceaccount.com""
  }
  service_account_cloud_services = (
    ""${local.project.number}@cloudservices.gserviceaccount.com""
  )
  service_accounts_robots = merge(
    {
      for agent in local._service_agents_data :
      agent.name => format(agent.service_agent, local.project.number)
    },
    {
      for agent in local._service_agents_data :
      agent.alias => format(agent.service_agent, local.project.number)
      if lookup(agent, ""alias"", null) != null
    },
    {
      gke-mcs-importer = ""${local.project.project_id}.svc.id.goog[gke-mcs/gke-mcs-importer]""
    }
  )
  service_accounts_jit_services = [
    for agent in local._service_agents_data :
    ""${agent.name}.googleapis.com""
    if lookup(agent, ""jit"", false)
  ]
  service_accounts_cmek_service_keys = distinct(flatten([
    for s in keys(var.service_encryption_key_ids) : [
      for ss in try(local._service_accounts_cmek_service_dependencies[s], [s]) : [
        for key in var.service_encryption_key_ids[s] : {
          service = ss
          key     = key
        } if key != null
      ]
    ]
  ]))
}
",locals,44,,133fd078232ef202140450d921bb8018b60e700f,b503bde544670d9acdd584a9798613dc84c0c0d5,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/133fd078232ef202140450d921bb8018b60e700f/modules/project/service-accounts.tf#L44,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b503bde544670d9acdd584a9798613dc84c0c0d5/modules/project/service-accounts.tf,2022-07-29 11:31:34+02:00,2023-03-30 09:36:14+03:00,16,1,0,1,0,0,0,0,0,0
https://github.com/Azure/sap-automation,5,deploy/terraform/run/sap_system/module.tf,deploy/terraform/run/sap_system/module.tf,0,workaround,// Workaround to create dependency from anchor to db to app,anchor_vm                                    = module.common_infrastructure.anchor_vm // Workaround to create dependency from anchor to db to app,"module ""anydb_node"" {
  source = ""../../terraform-units/modules/sap_system/anydb_node""
  providers = {
    azurerm.main     = azurerm
    azurerm.deployer = azurerm.deployer
  }
  depends_on = [module.common_infrastructure]
  order_deployment = local.db_zonal_deployment ? (
    module.app_tier.scs_vm_ids[0]
  ) : (null)
  databases                                    = local.databases
  infrastructure                               = local.infrastructure
  options                                      = local.options
  resource_group                               = module.common_infrastructure.resource_group
  storage_bootdiag_endpoint                    = module.common_infrastructure.storage_bootdiag_endpoint
  ppg                                          = module.common_infrastructure.ppg
  sid_kv_user_id                               = module.common_infrastructure.sid_kv_user_id
  naming                                       = module.sap_namegenerator.naming
  custom_disk_sizes_filename                   = var.db_disk_sizes_filename
  admin_subnet                                 = module.common_infrastructure.admin_subnet
  db_subnet                                    = module.common_infrastructure.db_subnet
  anchor_vm                                    = module.common_infrastructure.anchor_vm // Workaround to create dependency from anchor to db to app
  sid_password                                 = module.common_infrastructure.sid_password
  sid_username                                 = module.common_infrastructure.sid_username
  sdu_public_key                               = module.common_infrastructure.sdu_public_key
  sap_sid                                      = local.sap_sid
  db_asg_id                                    = module.common_infrastructure.db_asg_id
  terraform_template_version                   = var.terraform_template_version
  deployment                                   = var.deployment
  cloudinit_growpart_config                    = null # This needs more consideration module.common_infrastructure.cloudinit_growpart_config
  license_type                                 = var.license_type
  use_loadbalancers_for_standalone_deployments = var.use_loadbalancers_for_standalone_deployments
  database_vm_names                            = var.database_vm_names
  database_vm_db_nic_ips                       = var.database_vm_db_nic_ips
  database_vm_admin_nic_ips                    = var.database_vm_admin_nic_ips
  database_vm_storage_nic_ips                  = var.database_vm_storage_nic_ips
  database_server_count = upper(try(local.databases[0].platform, ""HANA"")) == ""HANA"" ? (
    0) : (
    local.databases[0].high_availability ? 2 * var.database_server_count : var.database_server_count
  )
}
",module,"module ""anydb_node"" {
  source                                        = ""../../terraform-units/modules/sap_system/anydb_node""
  providers                                     = {
                                                    azurerm.deployer       = azurerm
                                                    azurerm.main           = azurerm.system
                                                    azurerm.dnsmanagement  = azurerm.dnsmanagement
                                                    # azapi.api                                 = azapi.api
                                                  }

  depends_on                                    = [module.common_infrastructure]

  admin_subnet                                  = try(module.common_infrastructure.admin_subnet, null)
  anchor_vm                                     = module.common_infrastructure.anchor_vm // Workaround to create dependency from anchor to db to app
  cloudinit_growpart_config                     = null # This needs more consideration module.common_infrastructure.cloudinit_growpart_config
  custom_disk_sizes_filename                    = try(coalesce(var.custom_disk_sizes_filename, var.db_disk_sizes_filename), """")
  database                                      = local.database
  database_vm_db_nic_ips                        = var.database_vm_db_nic_ips
  database_vm_db_nic_secondary_ips              = var.database_vm_db_nic_secondary_ips
  database_vm_admin_nic_ips                     = var.database_vm_admin_nic_ips
  database_server_count                         = upper(try(local.database.platform, ""HANA"")) == ""HANA"" ? (
                                                  0) : (
                                                    local.database.high_availability ? 2 * var.database_server_count : var.database_server_count
                                                  )
  db_asg_id                                     = module.common_infrastructure.db_asg_id
  db_subnet                                     = module.common_infrastructure.db_subnet
  deploy_application_security_groups            = var.deploy_application_security_groups
  deployment                                    = var.deployment
  fencing_role_name                             = var.fencing_role_name
  infrastructure                                = local.infrastructure
  landscape_tfstate                             = data.terraform_remote_state.landscape.outputs
  license_type                                  = var.license_type
  management_dns_resourcegroup_name             = try(data.terraform_remote_state.landscape.outputs.management_dns_resourcegroup_name, local.saplib_resource_group_name)
  management_dns_subscription_id                = try(data.terraform_remote_state.landscape.outputs.management_dns_subscription_id, null)
  naming                                        = length(var.name_override_file) > 0 ? local.custom_names : module.sap_namegenerator.naming
  options                                       = local.options
  order_deployment                              = local.enable_db_deployment ? (
                                                    local.db_zonal_deployment && local.application_tier.enable_deployment ? (
                                                      try(module.app_tier.scs_vm_ids[0], null)
                                                    ) : (null)
                                                  ) : (null)
  ppg                                           = module.common_infrastructure.ppg
  register_virtual_network_to_dns               = try(data.terraform_remote_state.landscape.outputs.register_virtual_network_to_dns, true)
  register_endpoints_with_dns                   = var.register_endpoints_with_dns
  resource_group                                = module.common_infrastructure.resource_group
  sap_sid                                       = local.sap_sid
  scale_set_id                                  = try(module.common_infrastructure.scale_set_id, null)
  sdu_public_key                                = module.common_infrastructure.sdu_public_key
  sid_keyvault_user_id                          = module.common_infrastructure.sid_keyvault_user_id
  sid_password                                  = module.common_infrastructure.sid_password
  sid_username                                  = module.common_infrastructure.sid_username
  storage_bootdiag_endpoint                     = module.common_infrastructure.storage_bootdiag_endpoint
  tags                                          = var.tags
  terraform_template_version                    = var.terraform_template_version
  use_custom_dns_a_registration                 = data.terraform_remote_state.landscape.outputs.use_custom_dns_a_registration
  use_loadbalancers_for_standalone_deployments  = var.use_loadbalancers_for_standalone_deployments
  use_msi_for_clusters                          = var.use_msi_for_clusters
  use_observer                                  = var.use_observer
  use_scalesets_for_deployment                  = var.use_scalesets_for_deployment
  use_secondary_ips                             = var.use_secondary_ips
}
",module,164,242.0,6ff0b891114c36d3aeccb850d830b698cd1fe52a,df063c58945a9efa2cb2ba303762c43f0b9c1d8f,https://github.com/Azure/sap-automation/blob/6ff0b891114c36d3aeccb850d830b698cd1fe52a/deploy/terraform/run/sap_system/module.tf#L164,https://github.com/Azure/sap-automation/blob/df063c58945a9efa2cb2ba303762c43f0b9c1d8f/deploy/terraform/run/sap_system/module.tf#L242,2021-11-17 19:29:07+02:00,2024-05-17 12:37:17+03:00,110,0,1,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,2058,infra/modules/aws/main.tf,infra/modules/aws/main.tf,0,# todo,# TODO: take version from somewhere else here this isn't *necessary* the version if local build or remote artifact,# TODO: take version from somewhere else here this isn't *necessary* the version if local build or remote artifact,"module ""test_tool"" {
  count = var.install_test_tool ? 1 : 0

  source = ""../psoxy-test-tool""

  path_to_tools = ""${var.psoxy_base_dir}tools""
  # TODO: take version from somewhere else here; this isn't *necessary* the version if local build or remote artifact
  psoxy_version = module.psoxy_package.version
}
",module,"module ""test_tool"" {
  count = var.install_test_tool ? 1 : 0

  source = ""../psoxy-test-tool""

  path_to_tools = ""${var.psoxy_base_dir}tools""
  # TODO: take version from somewhere else here; this isn't *necessary* the version if local build or remote artifact
  psoxy_version = module.psoxy_package.version
}
",module,160,224.0,43055bd38ca21063c5d534c45a35c5f3c82f048c,e07a69ceca80240af2462aa09465535cc795d0b6,https://github.com/Worklytics/psoxy/blob/43055bd38ca21063c5d534c45a35c5f3c82f048c/infra/modules/aws/main.tf#L160,https://github.com/Worklytics/psoxy/blob/e07a69ceca80240af2462aa09465535cc795d0b6/infra/modules/aws/main.tf#L224,2023-07-07 15:05:16+00:00,2024-03-05 12:38:07-08:00,5,0,0,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,622,infra/modules/worklytics-psoxy-connection-generic/variables.tf,infra/modules/worklytics-psoxy-connection-generic/variables.tf,0,# todo,# TODO: rename to `proxy_instance_id` in future versions avoid coupling to brand name,# TODO: rename to `proxy_instance_id` in future versions avoid coupling to brand name,"variable ""psoxy_instance_id"" {
  type        = string
  description = ""friendly unique-id for Psoxy instance""
  default     = null
}
",variable,"variable ""psoxy_instance_id"" {
  type        = string
  description = ""friendly unique-id for Psoxy instance""
  default     = null
}
",variable,1,1.0,df24acebb5a0d8049e753a7084cbf84c34e773b3,5937e6a45055a94ff2b493f96f21863d91699825,https://github.com/Worklytics/psoxy/blob/df24acebb5a0d8049e753a7084cbf84c34e773b3/infra/modules/worklytics-psoxy-connection-generic/variables.tf#L1,https://github.com/Worklytics/psoxy/blob/5937e6a45055a94ff2b493f96f21863d91699825/infra/modules/worklytics-psoxy-connection-generic/variables.tf#L1,2023-01-16 09:59:11-08:00,2024-03-06 18:11:21+00:00,3,0,0,1,0,0,0,0,0,0
https://github.com/chanzuckerberg/cztack,21,aws-ecs-job/main.tf,aws-ecs-job/main.tf,0,implementation,# Defaults to a minimal hello-world implementation should be updated separately from,"# Default container definition if var.manage_task_definition == false 
 # Defaults to a minimal hello-world implementation; should be updated separately from 
 # Terraform, e.g. using ecs deploy or czecs","locals {
  template = <<TEMPLATE
[
  {
    ""name"": ""${local.container_name}"",
    ""image"": ""library/busybox:1.29"",
    ""command"": [""sh"", ""-c"", ""while true; do { echo -e 'HTTP/1.1 200 OK\r\n\nRunning stub server'; date; } | nc -l -p 8080; done""],
    ""memoryReservation"": 4
  }
]
TEMPLATE
}
",locals,"locals {
  template = <<TEMPLATE
[
  {
    ""name"": ""${local.container_name}"",
    ""image"": ""library/busybox:1.29"",
    ""command"": [""sh"", ""-c"", ""while true; do { echo -e 'HTTP/1.1 200 OK\r\n\nRunning stub server'; date; } | nc -l -p 8080; done""],
    ""memoryReservation"": 4
  }
]
TEMPLATE
}
",locals,52,68.0,6918848f1dab99c67e49d21bdc839d907ff8b647,093abc491ab7fdbc1693d8360da3a386dd81f7fe,https://github.com/chanzuckerberg/cztack/blob/6918848f1dab99c67e49d21bdc839d907ff8b647/aws-ecs-job/main.tf#L52,https://github.com/chanzuckerberg/cztack/blob/093abc491ab7fdbc1693d8360da3a386dd81f7fe/aws-ecs-job/main.tf#L68,2019-09-25 09:47:44-07:00,2019-10-08 07:57:34-07:00,3,0,1,1,1,0,0,0,0,0
https://github.com/ManagedKube/kubernetes-ops,7,terraform-modules/aws/msk_1.0.9/module/context.tf,terraform-modules/aws/msk_1.0.9/module/context.tf,0,fix,"# which was not fixed until Terraform 1.0.0,","# Note: we have to use [] instead of null for unset lists due to 
 # https://github.com/hashicorp/terraform/issues/28137 
 # which was not fixed until Terraform 1.0.0, 
 # but we want the default to be all the labels in `label_order` 
 # and we want users to be able to prevent all tag generation 
 # by setting `labels_as_tags` to `[]`, so we need 
 # a different sentinel to indicate ""default""","variable ""context"" {
  type = any
  default = {
    enabled             = true
    namespace           = null
    tenant              = null
    environment         = null
    stage               = null
    name                = null
    delimiter           = null
    attributes          = []
    tags                = {}
    additional_tag_map  = {}
    regex_replace_chars = null
    label_order         = []
    id_length_limit     = null
    label_key_case      = null
    label_value_case    = null
    descriptor_formats  = {}
    # Note: we have to use [] instead of null for unset lists due to
    # https://github.com/hashicorp/terraform/issues/28137
    # which was not fixed until Terraform 1.0.0,
    # but we want the default to be all the labels in `label_order`
    # and we want users to be able to prevent all tag generation
    # by setting `labels_as_tags` to `[]`, so we need
    # a different sentinel to indicate ""default""
    labels_as_tags = [""unset""]
  }
  description = <<-EOT
    Single object for setting entire context at once.
    See description of individual variables for details.
    Leave string and numeric variables as `null` to use default value.
    Individual variable settings (non-null) override settings in context object,
    except for attributes, tags, and additional_tag_map, which are merged.
  EOT

  validation {
    condition     = lookup(var.context, ""label_key_case"", null) == null ? true : contains([""lower"", ""title"", ""upper""], var.context[""label_key_case""])
    error_message = ""Allowed values: `lower`, `title`, `upper`.""
  }

  validation {
    condition     = lookup(var.context, ""label_value_case"", null) == null ? true : contains([""lower"", ""title"", ""upper"", ""none""], var.context[""label_value_case""])
    error_message = ""Allowed values: `lower`, `title`, `upper`, `none`.""
  }
}
",variable,"variable ""context"" {
  type = any
  default = {
    enabled             = true
    namespace           = null
    tenant              = null
    environment         = null
    stage               = null
    name                = null
    delimiter           = null
    attributes          = []
    tags                = {}
    additional_tag_map  = {}
    regex_replace_chars = null
    label_order         = []
    id_length_limit     = null
    label_key_case      = null
    label_value_case    = null
    descriptor_formats  = {}
    # Note: we have to use [] instead of null for unset lists due to
    # https://github.com/hashicorp/terraform/issues/28137
    # which was not fixed until Terraform 1.0.0,
    # but we want the default to be all the labels in `label_order`
    # and we want users to be able to prevent all tag generation
    # by setting `labels_as_tags` to `[]`, so we need
    # a different sentinel to indicate ""default""
    labels_as_tags = [""unset""]
  }
  description = <<-EOT
    Single object for setting entire context at once.
    See description of individual variables for details.
    Leave string and numeric variables as `null` to use default value.
    Individual variable settings (non-null) override settings in context object,
    except for attributes, tags, and additional_tag_map, which are merged.
  EOT

  validation {
    condition     = lookup(var.context, ""label_key_case"", null) == null ? true : contains([""lower"", ""title"", ""upper""], var.context[""label_key_case""])
    error_message = ""Allowed values: `lower`, `title`, `upper`.""
  }

  validation {
    condition     = lookup(var.context, ""label_value_case"", null) == null ? true : contains([""lower"", ""title"", ""upper"", ""none""], var.context[""label_value_case""])
    error_message = ""Allowed values: `lower`, `title`, `upper`, `none`.""
  }
}
",variable,71,71.0,c8193c7d74e2f7c624f0867337294cb66a2b9469,c8193c7d74e2f7c624f0867337294cb66a2b9469,https://github.com/ManagedKube/kubernetes-ops/blob/c8193c7d74e2f7c624f0867337294cb66a2b9469/terraform-modules/aws/msk_1.0.9/module/context.tf#L71,https://github.com/ManagedKube/kubernetes-ops/blob/c8193c7d74e2f7c624f0867337294cb66a2b9469/terraform-modules/aws/msk_1.0.9/module/context.tf#L71,2023-12-14 10:29:30-08:00,2023-12-14 10:29:30-08:00,1,0,0,1,1,0,0,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,346,azure/salt_provisioner.tf,azure/salt_provisioner.tf,0,// todo,// TODO: add or don't add this (from libvirt),"// TODO: add or don't add this (from libvirt) 
 // network_domain: ${var.network_domain}  ","resource ""null_resource"" ""monitoring_provisioner"" {
  count = var.provisioner == ""salt"" ? 1 : 0


  triggers = {
    monitoring_id = azurerm_virtual_machine.monitoring.id
  }

  connection {
    host        = data.azurerm_public_ip.monitoring.ip_address
    type        = ""ssh""
    user        = var.admin_user
    private_key = file(var.private_key_location)
  }

  provisioner ""file"" {
    source      = ""../salt""
    destination = ""/tmp""
  }

  provisioner ""file"" {
    content     = data.template_file.salt_provisioner.rendered
    destination = ""/tmp/salt_provisioner.sh""
  }

// TODO: add or don't add this (from libvirt)
// network_domain: ${var.network_domain}


  provisioner ""file"" {
    content = <<EOF
provider: azure
role: monitoring
name_prefix: ${terraform.workspace}-${var.name}
hostname: ${terraform.workspace}-${var.name}${var.monitoring_count > 1 ? ""0${count.index + 1}"" : """"}
timezone: ${var.timezone}
reg_code: ${var.reg_code}
reg_email: ${var.reg_email}
reg_additional_modules: {${join("", "",formatlist(""'%s': '%s'"",keys(var.reg_additional_modules),values(var.reg_additional_modules),),)}}
additional_repos: {${join("", "",formatlist(""'%s': '%s'"",keys(var.additional_repos),values(var.additional_repos),),)}}
additional_packages: [${join("", "", formatlist(""'%s'"", var.additional_packages))}]
authorized_keys: [${trimspace(file(var.public_key_location))},${trimspace(file(var.public_key_location))}]
host_ips: [${join("", "", formatlist(""'%s'"", [var.monitoring_srv_ip]))}]
host_ip: ${var.monitoring_srv_ip}
role: monitoring
provider: libvirt
ha_sap_deployment_repo: ${var.ha_sap_deployment_repo}
monitored_services: [${join("", "", formatlist(""'%s'"", var.monitored_services))}]
EOF

destination = ""/tmp/grains""
}

provisioner ""remote-exec"" {
  inline = [
    ""${var.background ? ""nohup"" : """"} sudo sh /tmp/salt_provisioner.sh > /tmp/provisioning.log ${var.background ? ""&"" : """"}"",
    ""return_code=$? && sleep 1 && exit $return_code"",
  ] # Workaround to let the process start in background properly
}

}
",resource,"resource ""null_resource"" ""monitoring_provisioner"" {
  count = var.provisioner == ""salt"" ? 1 : 0


  triggers = {
    monitoring_id = azurerm_virtual_machine.monitoring.id
  }

  connection {
    host        = data.azurerm_public_ip.monitoring.ip_address
    type        = ""ssh""
    user        = var.admin_user
    private_key = file(var.private_key_location)
  }

  provisioner ""file"" {
    source      = ""../salt""
    destination = ""/tmp""
  }

  provisioner ""file"" {
    content     = data.template_file.salt_provisioner.rendered
    destination = ""/tmp/salt_provisioner.sh""
  }
  provisioner ""file"" {
    content = <<EOF
provider: azure
role: monitoring
name_prefix: ${terraform.workspace}-monitoring
hostname: ${terraform.workspace}-monitoring
timezone: ${var.timezone}
reg_code: ${var.reg_code}
reg_email: ${var.reg_email}
reg_additional_modules: {${join("", "", formatlist(""'%s': '%s'"", keys(var.reg_additional_modules), values(var.reg_additional_modules), ), )}}
additional_repos: {${join("", "", formatlist(""'%s': '%s'"", keys(var.additional_repos), values(var.additional_repos), ), )}}
additional_packages: [${join("", "", formatlist(""'%s'"", var.additional_packages))}]
authorized_keys: [${trimspace(file(var.public_key_location))},${trimspace(file(var.public_key_location))}]
host_ips: [${join("", "", formatlist(""'%s'"", [var.monitoring_srv_ip]))}]
host_ip: ${var.monitoring_srv_ip}
ha_sap_deployment_repo: ${var.ha_sap_deployment_repo}
monitored_services: [${join("", "", formatlist(""'%s'"", var.monitored_services))}]
network_domain: ""tf.local""
EOF

    destination = ""/tmp/grains""
  }

  provisioner ""remote-exec"" {
    inline = [
      ""${var.background ? ""nohup"" : """"} sudo sh /tmp/salt_provisioner.sh > /tmp/provisioning.log ${var.background ? ""&"" : """"}"",
      ""return_code=$? && sleep 1 && exit $return_code"",
    ] # Workaround to let the process start in background properly
  }

}
",resource,192,,f41baea2a7a45b527e944b62bcab73612c693e02,3d715fbde7f5b9ad25c6c317103955efc32e12b3,https://github.com/SUSE/ha-sap-terraform-deployments/blob/f41baea2a7a45b527e944b62bcab73612c693e02/azure/salt_provisioner.tf#L192,https://github.com/SUSE/ha-sap-terraform-deployments/blob/3d715fbde7f5b9ad25c6c317103955efc32e12b3/azure/salt_provisioner.tf,2019-09-05 00:01:31+02:00,2019-09-05 00:01:59+02:00,3,1,0,1,0,0,1,0,0,0
https://github.com/camptocamp/devops-stack,1,modules/aks-azure/main.tf,modules/aks/azure/main.tf,1,# todo,# TODO: I'm not sure this is required,# TODO: I'm not sure this is required,"resource ""azurerm_role_assignment"" ""reader"" {
  scope                = format(""%s/resourcegroups/%s"", data.azurerm_subscription.primary.id, module.cluster.node_resource_group)
  role_definition_name = ""Reader""
  principal_id         = azurerm_user_assigned_identity.cert_manager.principal_id
}
",resource,,,157,0.0,a8d46993274079cba8165c95371bc5b385f9b815,23a76321726eca45b1852f9cbb9a5a46dd17c13e,https://github.com/camptocamp/devops-stack/blob/a8d46993274079cba8165c95371bc5b385f9b815/modules/aks-azure/main.tf#L157,https://github.com/camptocamp/devops-stack/blob/23a76321726eca45b1852f9cbb9a5a46dd17c13e/modules/aks/azure/main.tf#L0,2020-12-03 18:17:13+01:00,2023-04-03 16:40:29+02:00,48,2,0,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,7,variables.tf,variables.tf,0,#todo,#TODO: update the descriptions and change the defaults if needed,"/** 
 * Copyright 2018 Google LLC 
 * 
 * Licensed under the Apache License, Version 2.0 (the ""License""); 
 * you may not use this file except in compliance with the License. 
 * You may obtain a copy of the License at 
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, software 
 * distributed under the License is distributed on an ""AS IS"" BASIS, 
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 * See the License for the specific language governing permissions and 
 * limitations under the License. 
 */  
 #TODO: update the descriptions and change the defaults if needed","variable ""dataset_id"" {
  description = ""update""
}
",variable,"variable ""dataset_id"" {
  description = ""Unique ID for the dataset being provisioned""
}
",variable,17,,d56aa2c9a80343d60eed3e1a7d24962be31ee0b6,a7966ae4afc7bb73842fb9fd6f0f708b359cf36e,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/d56aa2c9a80343d60eed3e1a7d24962be31ee0b6/variables.tf#L17,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/a7966ae4afc7bb73842fb9fd6f0f708b359cf36e/variables.tf,2018-11-20 10:30:15-05:00,2019-01-28 12:41:47-05:00,5,1,0,1,0,0,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,357,main.tf,main.tf,0,todo,# TODO - hopefully this can be removed once the AWS endpoint is named properly in China,"# TODO - hopefully this can be removed once the AWS endpoint is named properly in China 
 # https://github.com/terraform-aws-modules/terraform-aws-eks/issues/1904","locals {
  iam_role_name     = coalesce(var.iam_role_name, ""${var.cluster_name}-cluster"")
  policy_arn_prefix = ""arn:${data.aws_partition.current.partition}:iam::aws:policy""

  # TODO - hopefully this can be removed once the AWS endpoint is named properly in China
  # https://github.com/terraform-aws-modules/terraform-aws-eks/issues/1904
  dns_suffix = coalesce(var.cluster_iam_role_dns_suffix, data.aws_partition.current.dns_suffix)
}
",locals,"locals {
  create = var.create && var.putin_khuylo

  partition = data.aws_partition.current.partition

  cluster_role = try(aws_iam_role.this[0].arn, var.iam_role_arn)

  create_outposts_local_cluster    = length(var.outpost_config) > 0
  enable_cluster_encryption_config = length(var.cluster_encryption_config) > 0 && !local.create_outposts_local_cluster
}
",locals,176,,9af0c2495a1fe7a02411ac436f48f6d9ca8b359f,6b40bdbb1d283d9259f43b03d24dca99cc1eceff,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/9af0c2495a1fe7a02411ac436f48f6d9ca8b359f/main.tf#L176,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/6b40bdbb1d283d9259f43b03d24dca99cc1eceff/main.tf,2022-03-02 18:26:20+01:00,2024-02-02 09:36:25-05:00,46,1,0,1,1,0,0,0,0,0
https://github.com/nasa/cumulus,293,tf-modules/archive/process_dead_letter_archive.tf,tf-modules/archive/process_dead_letter_archive.tf,0,todo,"// TODO - need sns topic ARNs for granules, PDRs","// TODO - need sns topic ARNs for granules, PDRs","resource ""aws_lambda_function"" ""process_dead_letter_archive"" {
  filename         = ""${path.module}/../../packages/api/dist/processDeadLetterArchive/lambda.zip""
  source_code_hash = filebase64sha256(""${path.module}/../../packages/api/dist/processDeadLetterArchive/lambda.zip"")
  function_name    = ""${var.prefix}-processDeadLetterArchive""
  role             = aws_iam_role.process_dead_letter_archive_role.arn
  handler          = ""index.handler""
  runtime          = ""nodejs12.x""
  timeout          = 300
  memory_size      = 512

  environment {
    // TODO - need sns topic ARNs for granules, PDRs
    variables = {
      acquireTimeoutMillis      = var.rds_connection_timing_configuration.acquireTimeoutMillis
      createRetryIntervalMillis = var.rds_connection_timing_configuration.createRetryIntervalMillis
      createTimeoutMillis       = var.rds_connection_timing_configuration.createTimeoutMillis
      databaseCredentialSecretArn = var.rds_user_access_secret_arn
      execution_sns_topic_arn   = aws_sns_topic.report_executions_topic.arn
      RDS_DEPLOYMENT_CUMULUS_VERSION = ""9.0.0""
      ExecutionsTable           = var.dynamo_tables.executions.name
      GranulesTable             = var.dynamo_tables.granules.name
      idleTimeoutMillis         = var.rds_connection_timing_configuration.idleTimeoutMillis
      PdrsTable                 = var.dynamo_tables.pdrs.name
      reapIntervalMillis        = var.rds_connection_timing_configuration.reapIntervalMillis
      stackName                 = var.prefix
      system_bucket             = var.system_bucket
      ES_HOST = var.elasticsearch_hostname
    }
  }

  dynamic ""vpc_config"" {
    for_each = length(var.lambda_subnet_ids) == 0 ? [] : [1]
    content {
      subnet_ids = var.lambda_subnet_ids
      security_group_ids = compact([
        aws_security_group.no_ingress_all_egress[0].id,
        var.rds_security_group
      ])
    }
  }

  tags = var.tags
}
",resource,"resource ""aws_lambda_function"" ""process_dead_letter_archive"" {
  filename         = ""${path.module}/../../packages/api/dist/processDeadLetterArchive/lambda.zip""
  source_code_hash = filebase64sha256(""${path.module}/../../packages/api/dist/processDeadLetterArchive/lambda.zip"")
  function_name    = ""${var.prefix}-processDeadLetterArchive""
  role             = aws_iam_role.process_dead_letter_archive_role.arn
  handler          = ""index.handler""
  runtime          = ""nodejs12.x""
  timeout          = 300
  memory_size      = 512

  environment {
    variables = {
      acquireTimeoutMillis           = var.rds_connection_timing_configuration.acquireTimeoutMillis
      createRetryIntervalMillis      = var.rds_connection_timing_configuration.createRetryIntervalMillis
      createTimeoutMillis            = var.rds_connection_timing_configuration.createTimeoutMillis
      databaseCredentialSecretArn    = var.rds_user_access_secret_arn
      ExecutionsTable                = var.dynamo_tables.executions.name
      execution_sns_topic_arn        = aws_sns_topic.report_executions_topic.arn
      GranulesTable                  = var.dynamo_tables.granules.name
      granule_sns_topic_arn          = aws_sns_topic.report_granules_topic.arn
      idleTimeoutMillis              = var.rds_connection_timing_configuration.idleTimeoutMillis
      PdrsTable                      = var.dynamo_tables.pdrs.name
      pdr_sns_topic_arn              = aws_sns_topic.report_pdrs_topic.arn
      reapIntervalMillis             = var.rds_connection_timing_configuration.reapIntervalMillis
      stackName                      = var.prefix
      system_bucket                  = var.system_bucket
      RDS_DEPLOYMENT_CUMULUS_VERSION = ""9.0.0""
      ES_HOST                        = var.elasticsearch_hostname
    }
  }

  dynamic ""vpc_config"" {
    for_each = length(var.lambda_subnet_ids) == 0 ? [] : [1]
    content {
      subnet_ids = var.lambda_subnet_ids
      security_group_ids = compact([
        aws_security_group.no_ingress_all_egress[0].id,
        var.rds_security_group
      ])
    }
  }

  tags = var.tags
}
",resource,93,,a40c205c230cbcfb49db60e837ad202735c8b87f,f5132068ea6e260efe90a13f49d106023930a716,https://github.com/nasa/cumulus/blob/a40c205c230cbcfb49db60e837ad202735c8b87f/tf-modules/archive/process_dead_letter_archive.tf#L93,https://github.com/nasa/cumulus/blob/f5132068ea6e260efe90a13f49d106023930a716/tf-modules/archive/process_dead_letter_archive.tf,2021-09-23 13:03:56-04:00,2021-10-14 09:38:42-07:00,3,1,1,1,0,0,0,0,0,0
https://github.com/compiler-explorer/infra,161,terraform/security.tf,terraform/security.tf,0,# todo,# TODO! hardcoded from another,"    ""CI""          = ""sg-0b6cec49789cbf1a8""  # TODO! hardcoded from another","resource ""aws_security_group_rule"" ""efs_inbound"" {
  for_each                 = {
    ""Admin""       = aws_security_group.AdminNode.id,
    ""Compilation"" = aws_security_group.CompilerExplorer.id
    ""Builder""     = aws_security_group.Builder.id
    ""CI""          = ""sg-0b6cec49789cbf1a8""  # TODO! hardcoded from another
  }
  security_group_id        = aws_security_group.efs.id
  type                     = ""ingress""
  from_port                = 0
  to_port                  = 65535
  protocol                 = ""all""
  source_security_group_id = each.value
  description              = ""${each.key} node acccess""
}
",resource,"resource ""aws_security_group_rule"" ""efs_inbound"" {
  for_each                 = {
    ""Admin""       = aws_security_group.AdminNode.id,
    ""Compilation"" = aws_security_group.CompilerExplorer.id
    ""Builder""     = aws_security_group.Builder.id
  }
  security_group_id        = aws_security_group.efs.id
  type                     = ""ingress""
  from_port                = 0
  to_port                  = 65535
  protocol                 = ""all""
  source_security_group_id = each.value
  description              = ""${each.key} node acccess""
}
",resource,419,,31bbd3c4f99d686ec26a81b00435dcb6c7538f7f,f6a403d5c5294ac511990056b94a449e34f0fbdf,https://github.com/compiler-explorer/infra/blob/31bbd3c4f99d686ec26a81b00435dcb6c7538f7f/terraform/security.tf#L419,https://github.com/compiler-explorer/infra/blob/f6a403d5c5294ac511990056b94a449e34f0fbdf/terraform/security.tf,2021-10-27 08:34:58-05:00,2021-10-27 08:55:44-05:00,2,1,0,1,0,1,0,1,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,224,locals.tf,locals.tf,0,todo,# TODO - move this into `aws-eks-teams` to avoid getting out of sync,# TODO - move this into `aws-eks-teams` to avoid getting out of sync,"locals {

  context = {
    # Data resources
    aws_region_name = data.aws_region.current.name
    # aws_caller_identity
    aws_caller_identity_account_id = data.aws_caller_identity.current.account_id
    aws_caller_identity_arn        = data.aws_caller_identity.current.arn
    # aws_partition
    aws_partition_id         = data.aws_partition.current.id
    aws_partition_dns_suffix = data.aws_partition.current.dns_suffix
    # http details
    http_endpoint               = ""enabled""
    http_tokens                 = ""required""
    http_put_response_hop_limit = 2 # Hop limit should be between 2 and 64 for IMDSv2 instance metadata services
  }

  eks_cluster_id     = module.aws_eks.cluster_id
  cluster_ca_base64  = module.aws_eks.cluster_certificate_authority_data
  cluster_endpoint   = module.aws_eks.cluster_endpoint
  vpc_id             = var.vpc_id
  private_subnet_ids = var.private_subnet_ids
  public_subnet_ids  = var.public_subnet_ids

  enable_workers            = length(var.self_managed_node_groups) > 0 || length(var.managed_node_groups) > 0 ? true : false
  worker_security_group_ids = local.enable_workers ? compact(flatten([[module.aws_eks.node_security_group_id], var.worker_additional_security_group_ids])) : []

  node_group_context = {
    # EKS Cluster Config
    eks_cluster_id    = local.eks_cluster_id
    cluster_ca_base64 = local.cluster_ca_base64
    cluster_endpoint  = local.cluster_endpoint
    cluster_version   = var.cluster_version
    # VPC Config
    vpc_id             = local.vpc_id
    private_subnet_ids = local.private_subnet_ids
    public_subnet_ids  = local.public_subnet_ids

    # Worker Security Group
    worker_security_group_ids = local.worker_security_group_ids

    # Http config
    http_endpoint               = local.context.http_endpoint
    http_tokens                 = local.context.http_tokens
    http_put_response_hop_limit = local.context.http_put_response_hop_limit

    # Data sources
    aws_partition_dns_suffix = local.context.aws_partition_dns_suffix
    aws_partition_id         = local.context.aws_partition_id

    iam_role_path                 = var.iam_role_path
    iam_role_permissions_boundary = var.iam_role_permissions_boundary

    # Service IPv4/IPv6 CIDR range
    service_ipv6_cidr = var.cluster_service_ipv6_cidr
    service_ipv4_cidr = var.cluster_service_ipv4_cidr

    tags = var.tags
  }

  fargate_context = {
    eks_cluster_id                = local.eks_cluster_id
    aws_partition_id              = local.context.aws_partition_id
    iam_role_path                 = var.iam_role_path
    iam_role_permissions_boundary = var.iam_role_permissions_boundary
    tags                          = var.tags
  }

  # Managed node IAM Roles for aws-auth
  managed_node_group_aws_auth_config_map = length(var.managed_node_groups) > 0 == true ? [
    for key, node in var.managed_node_groups : {
      rolearn : try(node.iam_role_arn, ""arn:${local.context.aws_partition_id}:iam::${local.context.aws_caller_identity_account_id}:role/${module.aws_eks.cluster_id}-${node.node_group_name}"")
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes""
      ]
    }
  ] : []

  # Self Managed node IAM Roles for aws-auth
  self_managed_node_group_aws_auth_config_map = length(var.self_managed_node_groups) > 0 ? [
    for key, node in var.self_managed_node_groups : {
      rolearn : try(node.iam_role_arn, ""arn:${local.context.aws_partition_id}:iam::${local.context.aws_caller_identity_account_id}:role/${module.aws_eks.cluster_id}-${node.node_group_name}"")
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes""
      ]
    } if node.launch_template_os != ""windows""
  ] : []

  # Self Managed Windows node IAM Roles for aws-auth
  windows_node_group_aws_auth_config_map = length(var.self_managed_node_groups) > 0 && var.enable_windows_support ? [
    for key, node in var.self_managed_node_groups : {
      rolearn : ""arn:${local.context.aws_partition_id}:iam::${local.context.aws_caller_identity_account_id}:role/${module.aws_eks.cluster_id}-${node.node_group_name}""
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes"",
        ""eks:kube-proxy-windows""
      ]
    } if node.launch_template_os == ""windows""
  ] : []

  # Fargate node IAM Roles for aws-auth
  fargate_profiles_aws_auth_config_map = length(var.fargate_profiles) > 0 ? [
    for key, node in var.fargate_profiles : {
      rolearn : try(node.iam_role_arn, ""arn:${local.context.aws_partition_id}:iam::${local.context.aws_caller_identity_account_id}:role/${module.aws_eks.cluster_id}-${node.fargate_profile_name}"")
      username : ""system:node:{{SessionName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes"",
        ""system:node-proxier""
      ]
    }
  ] : []

  # EMR on EKS IAM Roles for aws-auth
  emr_on_eks_config_map = var.enable_emr_on_eks == true ? [
    {
      rolearn : ""arn:${local.context.aws_partition_id}:iam::${local.context.aws_caller_identity_account_id}:role/AWSServiceRoleForAmazonEMRContainers""
      username : ""emr-containers""
      groups : []
    }
  ] : []

  # Teams
  partition  = local.context.aws_partition_id
  account_id = local.context.aws_caller_identity_account_id

  # TODO - move this into `aws-eks-teams` to avoid getting out of sync
  platform_teams_config_map = length(var.platform_teams) > 0 ? [
    for platform_team_name, platform_team_data in var.platform_teams : {
      rolearn : ""arn:${local.partition}:iam::${local.account_id}:role/${module.aws_eks.cluster_id}-${platform_team_name}-access""
      username : ""${platform_team_name}""
      groups : [
        ""system:masters""
      ]
    }
  ] : []

  # TODO - move this into `aws-eks-teams` to avoid getting out of sync
  application_teams_config_map = length(var.application_teams) > 0 ? [
    for team_name, team_data in var.application_teams : {
      rolearn : ""arn:${local.partition}:iam::${local.account_id}:role/${module.aws_eks.cluster_id}-${team_name}-access""
      username : ""${team_name}""
      groups : [
        ""${team_name}-group""
      ]
    }
  ] : []

  cluster_iam_role_name = var.iam_role_name == null ? ""${var.cluster_name}-cluster-role"" : var.iam_role_name
  cluster_iam_role_arn  = var.create_iam_role ? ""arn:${local.context.aws_partition_id}:iam::${local.context.aws_caller_identity_account_id}:role/${local.cluster_iam_role_name}"" : var.iam_role_arn
}
",locals,,,132,0.0,4757dd4262db9ce2c4743aebb8bfb5486e29b6a4,19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/4757dd4262db9ce2c4743aebb8bfb5486e29b6a4/locals.tf#L132,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1/locals.tf#L0,2022-05-26 19:12:05-04:00,2023-06-05 10:07:47-04:00,6,2,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,748,fast/stages/01-resman/organization.tf,fast/stages/01-resman/organization.tf,0,# todo,# TODO: implement tag-based conditions on this org role,# TODO: implement tag-based conditions on this org role,"module ""organization"" {
  source          = ""../../../modules/organization""
  organization_id = ""organizations/${var.organization.id}""
  # IAM additive bindings, granted via the restricted Organization Admin custom
  # role assigned in stage 00; they need to be additive to avoid conflicts
  iam_additive = merge(
    {
      ""roles/accesscontextmanager.policyAdmin"" = [
        module.branch-security-sa.iam_email
      ]
      ""roles/compute.orgFirewallPolicyAdmin"" = [
        module.branch-network-sa.iam_email
      ]
      ""roles/compute.xpnAdmin"" = [
        module.branch-network-sa.iam_email
      ]
      # TODO: implement tag-based conditions on this org role
      ""roles/orgpolicy.policyAdmin"" = concat(
        local.branch_teams_pf_sa_iam_emails,
        local.branch_dataplatform_sa_iam_emails,
      )
    },
    local.billing_org ? {
      ""roles/billing.costsManager"" = local.branch_teams_pf_sa_iam_emails
      ""roles/billing.user"" = concat(
        [
          module.branch-network-sa.iam_email,
          module.branch-security-sa.iam_email,
        ],
        local.branch_dataplatform_sa_iam_emails,
        # enable if individual teams can create their own projects
        # [
        #   for k, v in module.branch-teams-team-sa : v.iam_email
        # ],
        local.branch_teams_pf_sa_iam_emails
      )
    } : {}
  )
  # sample subset of useful organization policies, edit to suit requirements
  policy_boolean = {
    ""constraints/cloudfunctions.requireVPCConnector""              = true
    ""constraints/compute.disableGuestAttributesAccess""            = true
    ""constraints/compute.disableInternetNetworkEndpointGroup""     = true
    ""constraints/compute.disableNestedVirtualization""             = true
    ""constraints/compute.disableSerialPortAccess""                 = true
    ""constraints/compute.requireOsLogin""                          = true
    ""constraints/compute.restrictXpnProjectLienRemoval""           = true
    ""constraints/compute.skipDefaultNetworkCreation""              = true
    ""constraints/compute.setNewProjectDefaultToZonalDNSOnly""      = true
    ""constraints/iam.automaticIamGrantsForDefaultServiceAccounts"" = true
    ""constraints/iam.disableServiceAccountKeyCreation""            = true
    ""constraints/iam.disableServiceAccountKeyUpload""              = true
    ""constraints/sql.restrictPublicIp""                            = true
    ""constraints/sql.restrictAuthorizedNetworks""                  = true
    ""constraints/storage.uniformBucketLevelAccess""                = true
  }
  policy_list = {
    ""constraints/cloudfunctions.allowedIngressSettings"" = merge(
      local.list_allow, { values = [""is:ALLOW_INTERNAL_ONLY""] }
    )
    ""constraints/cloudfunctions.allowedVpcConnectorEgressSettings"" = merge(
      local.list_allow, { values = [""is:PRIVATE_RANGES_ONLY""] }
    )
    ""constraints/compute.restrictLoadBalancerCreationForTypes"" = merge(
      local.list_allow, { values = [""in:INTERNAL""] }
    )
    ""constraints/compute.vmExternalIpAccess"" = local.list_deny
    ""constraints/iam.allowedPolicyMemberDomains"" = merge(
      local.list_allow, {
        values = concat(
          [var.organization.customer_id],
          try(local.policy_configs.allowed_policy_member_domains, [])
        )
    })
    ""constraints/run.allowedIngress"" = merge(
      local.list_allow, { values = [""is:internal""] }
    )
    ""constraints/run.allowedVPCEgress"" = merge(
      local.list_allow, { values = [""is:private-ranges-only""] }
    )
    # ""constraints/compute.restrictCloudNATUsage""                      = local.list_deny
    # ""constraints/compute.restrictDedicatedInterconnectUsage""         = local.list_deny
    # ""constraints/compute.restrictPartnerInterconnectUsage""           = local.list_deny
    # ""constraints/compute.restrictProtocolForwardingCreationForTypes"" = local.list_deny
    # ""constraints/compute.restrictSharedVpcHostProjects""              = local.list_deny
    # ""constraints/compute.restrictSharedVpcSubnetworks""               = local.list_deny
    # ""constraints/compute.restrictVpcPeering"" = local.list_deny
    # ""constraints/compute.restrictVpnPeerIPs"" = local.list_deny
    # ""constraints/compute.vmCanIpForward""     = local.list_deny
    # ""constraints/gcp.resourceLocations"" = {
    #   inherit_from_parent = false
    #   suggested_value     = null
    #   status              = true
    #   values              = local.allowed_regions
    # }
  }
}
",module,"module ""organization"" {
  source          = ""../../../modules/organization""
  organization_id = ""organizations/${var.organization.id}""
  # IAM additive bindings, granted via the restricted Organization Admin custom
  # role assigned in stage 00; they need to be additive to avoid conflicts
  iam_additive = merge(
    {
      ""roles/accesscontextmanager.policyAdmin"" = [
        module.branch-security-sa.iam_email
      ]
      ""roles/compute.orgFirewallPolicyAdmin"" = [
        module.branch-network-sa.iam_email
      ]
      ""roles/compute.xpnAdmin"" = [
        module.branch-network-sa.iam_email
      ]
    },
    local.billing_org ? {
      ""roles/billing.costsManager"" = local.branch_teams_pf_sa_iam_emails
      ""roles/billing.user"" = concat(
        [
          module.branch-network-sa.iam_email,
          module.branch-security-sa.iam_email,
        ],
        local.branch_dataplatform_sa_iam_emails,
        # enable if individual teams can create their own projects
        # [
        #   for k, v in module.branch-teams-team-sa : v.iam_email
        # ],
        local.branch_teams_pf_sa_iam_emails
      )
    } : {}
  )
  # sample subset of useful organization policies, edit to suit requirements
  policy_boolean = {
    ""constraints/cloudfunctions.requireVPCConnector""              = true
    ""constraints/compute.disableGuestAttributesAccess""            = true
    ""constraints/compute.disableInternetNetworkEndpointGroup""     = true
    ""constraints/compute.disableNestedVirtualization""             = true
    ""constraints/compute.disableSerialPortAccess""                 = true
    ""constraints/compute.requireOsLogin""                          = true
    ""constraints/compute.restrictXpnProjectLienRemoval""           = true
    ""constraints/compute.skipDefaultNetworkCreation""              = true
    ""constraints/compute.setNewProjectDefaultToZonalDNSOnly""      = true
    ""constraints/iam.automaticIamGrantsForDefaultServiceAccounts"" = true
    ""constraints/iam.disableServiceAccountKeyCreation""            = true
    ""constraints/iam.disableServiceAccountKeyUpload""              = true
    ""constraints/sql.restrictPublicIp""                            = true
    ""constraints/sql.restrictAuthorizedNetworks""                  = true
    ""constraints/storage.uniformBucketLevelAccess""                = true
  }
  policy_list = {
    ""constraints/cloudfunctions.allowedIngressSettings"" = merge(
      local.list_allow, { values = [""is:ALLOW_INTERNAL_ONLY""] }
    )
    ""constraints/cloudfunctions.allowedVpcConnectorEgressSettings"" = merge(
      local.list_allow, { values = [""is:PRIVATE_RANGES_ONLY""] }
    )
    ""constraints/compute.restrictLoadBalancerCreationForTypes"" = merge(
      local.list_allow, { values = [""in:INTERNAL""] }
    )
    ""constraints/compute.vmExternalIpAccess"" = local.list_deny
    ""constraints/iam.allowedPolicyMemberDomains"" = merge(
      local.list_allow, {
        values = concat(
          [var.organization.customer_id],
          try(local.policy_configs.allowed_policy_member_domains, [])
        )
    })
    ""constraints/run.allowedIngress"" = merge(
      local.list_allow, { values = [""is:internal""] }
    )
    ""constraints/run.allowedVPCEgress"" = merge(
      local.list_allow, { values = [""is:private-ranges-only""] }
    )
    # ""constraints/compute.restrictCloudNATUsage""                      = local.list_deny
    # ""constraints/compute.restrictDedicatedInterconnectUsage""         = local.list_deny
    # ""constraints/compute.restrictPartnerInterconnectUsage""           = local.list_deny
    # ""constraints/compute.restrictProtocolForwardingCreationForTypes"" = local.list_deny
    # ""constraints/compute.restrictSharedVpcHostProjects""              = local.list_deny
    # ""constraints/compute.restrictSharedVpcSubnetworks""               = local.list_deny
    # ""constraints/compute.restrictVpcPeering"" = local.list_deny
    # ""constraints/compute.restrictVpnPeerIPs"" = local.list_deny
    # ""constraints/compute.vmCanIpForward""     = local.list_deny
    # ""constraints/gcp.resourceLocations"" = {
    #   inherit_from_parent = false
    #   suggested_value     = null
    #   status              = true
    #   values              = local.allowed_regions
    # }
  }
  tags = {
    context = {
      description = ""Resource management context.""
      iam         = {}
      values = {
        data       = null
        gke        = null
        networking = null
        sandbox    = null
        security   = null
        teams      = null
      }
    }
    environment = {
      description = ""Environment definition.""
      iam         = {}
      values = {
        development = null
        production  = null
      }
    }
  }
}
",module,66,,b9804d895b7debed37e41592484fbcb1dd9bd2f6,474bcbdd0e727c0974857c721a5288ff1cbf42f9,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b9804d895b7debed37e41592484fbcb1dd9bd2f6/fast/stages/01-resman/organization.tf#L66,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/474bcbdd0e727c0974857c721a5288ff1cbf42f9/fast/stages/01-resman/organization.tf,2022-02-18 14:39:33+01:00,2022-02-20 11:26:30+01:00,2,1,0,1,0,1,0,0,0,0
https://github.com/nasa/cumulus,301,tf-modules/archive/ingest-reporting.tf,tf-modules/archive/ingest-reporting.tf,0,todo,# TODO Remove this topic,# TODO Remove this topic,"resource ""aws_sns_topic"" ""report_collections_topic"" {
  name = ""${var.prefix}-report-collections-topic""
  tags = var.tags
}
",resource,"resource ""aws_sns_topic"" ""report_collections_topic"" {
  name = ""${var.prefix}-report-collections-topic""
  tags = var.tags
}
",resource,340,,413e8ef43461de1f3848756673a4a0ddd2e32506,f0fbbd070c6cf1398dbcfc02c3d62026ea9dbeba,https://github.com/nasa/cumulus/blob/413e8ef43461de1f3848756673a4a0ddd2e32506/tf-modules/archive/ingest-reporting.tf#L340,https://github.com/nasa/cumulus/blob/f0fbbd070c6cf1398dbcfc02c3d62026ea9dbeba/tf-modules/archive/ingest-reporting.tf,2023-02-20 10:55:23-05:00,2023-02-20 13:26:46-05:00,3,1,1,1,0,0,0,0,0,0
https://github.com/kubernetes/k8s.io,342,infra/gcp/terraform/modules/oci-proxy/oci-proxy.tf,infra/gcp/terraform/modules/oci-proxy/main.tf,1,// todo,// TODO: switch DEFAULT_AWS_BASE_URL to cloudfront or else refine the region mapping,"// TODO: switch DEFAULT_AWS_BASE_URL to cloudfront or else refine the region mapping 
 // GCP asia-east1 is Changhua County, Taiwan","locals {
  cloud_run_config = {
    asia-east1 = {
      // TODO: switch DEFAULT_AWS_BASE_URL to cloudfront or else refine the region mapping
      // GCP asia-east1 is Changhua County, Taiwan
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS ap-southeast-1 is Singapore
          value = ""https://prod-registry-k8s-io-ap-southeast-1.s3.dualstack.ap-southeast-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://asia-east1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP asia-northeast1 is Tokyo, Japan
    asia-northeast1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS ap-northeast-1 is Tokyo
          value = ""https://prod-registry-k8s-io-ap-northeast-1.s3.dualstack.ap-northeast-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://asia-northeast1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP asia-northeast2 is Osaka, Japan
    asia-northeast2 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS ap-northeast-1 is Tokyo
          value = ""https://prod-registry-k8s-io-ap-northeast-1.s3.dualstack.ap-northeast-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://asia-northeast2-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP asia-south1 is Mumbai, India
    asia-south1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS ap-south-1 is Mumbai
          value = ""https://prod-registry-k8s-io-ap-south-1.s3.dualstack.ap-south-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://asia-south1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP australia-southeast1 is Sydney
    australia-southeast1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS ap-southeast-1 is Singapore
          value = ""https://prod-registry-k8s-io-ap-southeast-1.s3.dualstack.ap-southeast-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://australia-southeast1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-north1 is Hamina, Finland
    europe-north1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS eu-central-1 is Frankfurt
          value = ""https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-north1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-southwest1 is Madrid, Spain
    europe-southwest1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS eu-central-1 is Frankfurt
          value = ""https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-southwest1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-west1 is St. Ghislain, Belgium
    europe-west1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS eu-central-1 is Frankfurt
          value = ""https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-west1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-west2 is London, UK
    europe-west2 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS eu-west-2 is London
          value = ""https://prod-registry-k8s-io-eu-west-2.s3.dualstack.eu-west-2.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-west2-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-west4 is Eemshaven, Netherlands
    europe-west4 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS eu-central-1 is Frankfurt
          value = ""https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-west4-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-west8 is Milan, Italy
    europe-west8 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS eu-central-1 is Frankfurt
          value = ""https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-west8-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-west9 is Paris, France
    europe-west9 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS eu-west-2 is London
          value = ""https://prod-registry-k8s-io-eu-west-2.s3.dualstack.eu-west-2.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-west9-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP southamerica-west1 is Santiago, Chile
    southamerica-west1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-east-1 is Virginia, USA
          // See: https://github.com/kubernetes/k8s.io/pull/4739/files#r1100667255
          value = ""https://prod-registry-k8s-io-us-east-1.s3.dualstack.us-east-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://southamerica-west1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP us-central1 is Iowa, USA
    us-central1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-east-2 is Ohio, USA
          value = ""https://prod-registry-k8s-io-us-east-2.s3.dualstack.us-east-2.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://us-central1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP us-east1 is South Carolina, USA
    us-east1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-east-1 is Virginia, USA
          value = ""https://prod-registry-k8s-io-us-east-1.s3.dualstack.us-east-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://us-east1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP us-east4 is Virginia, USA
    us-east4 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-east-1 is Virginia, USA
          value = ""https://prod-registry-k8s-io-us-east-1.s3.dualstack.us-east-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://us-east4-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP us-east5 is Ohio, USA
    us-east5 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-east-2 is Ohio, USA
          value = ""https://prod-registry-k8s-io-us-east-2.s3.dualstack.us-east-2.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://us-east5-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP us-south1 is Texas, USA
    us-south1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-east-2 is Ohio, USA
          value = ""https://prod-registry-k8s-io-us-east-2.s3.dualstack.us-east-2.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://us-south1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP us-west1 is Oregon, USA
    us-west1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-west-2 is Oregon, USA
          value = ""https://prod-registry-k8s-io-us-west-2.s3.dualstack.us-west-2.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://us-west1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP us-west2 is California, USA
    us-west2 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-west-1 is California, USA
          value = ""https://prod-registry-k8s-io-us-west-1.s3.dualstack.us-west-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://us-west2-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
  }
}",locals,"locals {
  cloud_run_config = {
    asia-east1 = {
      // GCP asia-east1 is Changhua County, Taiwan
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS Cloudfront
          value = ""https://d39mqg4b1dx9z1.cloudfront.net"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://asia-east1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP asia-northeast1 is Tokyo, Japan
    asia-northeast1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS ap-northeast-1 is Tokyo
          value = ""https://prod-registry-k8s-io-ap-northeast-1.s3.dualstack.ap-northeast-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://asia-northeast1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP asia-northeast2 is Osaka, Japan
    asia-northeast2 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS ap-northeast-1 is Tokyo
          value = ""https://prod-registry-k8s-io-ap-northeast-1.s3.dualstack.ap-northeast-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://asia-northeast2-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP asia-south1 is Mumbai, India
    asia-south1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS ap-south-1 is Mumbai
          value = ""https://prod-registry-k8s-io-ap-south-1.s3.dualstack.ap-south-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://asia-south1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP australia-southeast1 is Sydney
    australia-southeast1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS ap-southeast-1 is Singapore
          value = ""https://prod-registry-k8s-io-ap-southeast-1.s3.dualstack.ap-southeast-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://australia-southeast1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-north1 is Hamina, Finland
    europe-north1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS eu-central-1 is Frankfurt
          value = ""https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-north1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-southwest1 is Madrid, Spain
    europe-southwest1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS eu-central-1 is Frankfurt
          value = ""https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-southwest1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-west1 is St. Ghislain, Belgium
    europe-west1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS Cloudfront
          value = ""https://d39mqg4b1dx9z1.cloudfront.net"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-west1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-west10 is Berlin, Germany
    europe-west10 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS eu-central-1 is Frankfurt
          value = ""https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-west10-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-west2 is London, UK
    europe-west2 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS eu-west-1 is Ireland
          value = ""https://prod-registry-k8s-io-eu-west-1.s3.dualstack.eu-west-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-west2-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-west3 is Frankfurt, Germany
    europe-west3 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS eu-central-1 is Frankfurt
          value = ""https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-west3-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-west4 is Eemshaven, Netherlands
    europe-west4 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS Cloudfront
          value = ""https://d39mqg4b1dx9z1.cloudfront.net"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-west4-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-west8 is Milan, Italy
    europe-west8 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS eu-central-1 is Frankfurt
          value = ""https://prod-registry-k8s-io-eu-central-1.s3.dualstack.eu-central-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-west8-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP europe-west9 is Paris, France
    europe-west9 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS eu-west-1 is in Ireland
          value = ""https://prod-registry-k8s-io-eu-west-1.s3.dualstack.eu-west-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://europe-west9-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP southamerica-west1 is Santiago, Chile
    southamerica-west1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-east-1 is Virginia, USA
          // See: https://github.com/kubernetes/k8s.io/pull/4739/files#r1100667255
          value = ""https://prod-registry-k8s-io-us-east-1.s3.dualstack.us-east-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://southamerica-west1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP us-central1 is Iowa, USA
    us-central1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-east-2 is Ohio, USA
          value = ""https://prod-registry-k8s-io-us-east-2.s3.dualstack.us-east-2.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://us-central1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP us-east1 is South Carolina, USA
    us-east1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-east-1 is Virginia, USA
          value = ""https://prod-registry-k8s-io-us-east-1.s3.dualstack.us-east-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://us-east1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP us-east4 is Virginia, USA
    us-east4 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-east-1 is Virginia, USA
          value = ""https://prod-registry-k8s-io-us-east-1.s3.dualstack.us-east-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://us-east4-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP us-east5 is Ohio, USA
    us-east5 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-east-2 is Ohio, USA
          value = ""https://prod-registry-k8s-io-us-east-2.s3.dualstack.us-east-2.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://us-east5-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP us-south1 is Texas, USA
    us-south1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-east-2 is Ohio, USA
          value = ""https://prod-registry-k8s-io-us-east-2.s3.dualstack.us-east-2.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://us-south1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP us-west1 is Oregon, USA
    us-west1 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-west-2 is Oregon, USA
          value = ""https://prod-registry-k8s-io-us-west-2.s3.dualstack.us-west-2.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://us-west1-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
    // GCP us-west2 is California, USA
    us-west2 = {
      environment_variables = [
        {
          name = ""DEFAULT_AWS_BASE_URL"",
          // AWS us-west-1 is California, USA
          value = ""https://prod-registry-k8s-io-us-west-1.s3.dualstack.us-west-1.amazonaws.com"",
        },
        {
          name  = ""UPSTREAM_REGISTRY_ENDPOINT"",
          value = ""https://us-west2-docker.pkg.dev""
        },
        {
          name  = ""UPSTREAM_REGISTRY_PATH"",
          value = ""k8s-artifacts-prod/images""
        }
      ]
    }
  }
}
",locals,20,,ffbaa4eb3b652a9cc7520593cae83a8004ef88a3,3ca41f506262aed659a3a76877db17f1232618ba,https://github.com/kubernetes/k8s.io/blob/ffbaa4eb3b652a9cc7520593cae83a8004ef88a3/infra/gcp/terraform/modules/oci-proxy/oci-proxy.tf#L20,https://github.com/kubernetes/k8s.io/blob/3ca41f506262aed659a3a76877db17f1232618ba/infra/gcp/terraform/modules/oci-proxy/main.tf,2023-04-02 19:38:59-07:00,2024-01-30 23:09:46+01:00,7,1,0,1,0,0,1,0,0,0
https://github.com/kubernetes/k8s.io,160,infra/gcp/terraform/kubernetes-public/k8s-infra-prow.tf,infra/gcp/terraform/kubernetes-public/k8s-infra-prow.tf,0,todo,// TODO (ameukam): move hardcoded value to terraform variables,"// Allow deck (component of k8s-infra-prow) service account to use GCP SA k8s-infra-prow via workload identity 
 // TODO (ameukam): move hardcoded value to terraform variables","resource ""google_service_account_iam_member"" ""aaa_cluster_sa_iam"" {
  role               = ""roles/iam.workloadIdentityUser""
  service_account_id = google_service_account.k8s_infra_prow.name
  member             = format(""serviceAccount:%s.svc.id.goog[%s/%s]"", ""kubernetes-public"", ""prow"", ""deck"")
}
",resource,"resource ""google_service_account_iam_member"" ""aaa_cluster_sa_iam"" {
  role               = ""roles/iam.workloadIdentityUser""
  service_account_id = google_service_account.k8s_infra_prow.name
  member             = format(""serviceAccount:%s.svc.id.goog[%s/%s]"", ""kubernetes-public"", ""prow"", ""deck"")
}
",resource,46,56.0,c7aa8b1685f1b9a8ab978639b28164e6cef305a2,5e69979eb5251d9a1ad6ca6ec8856c99a5927034,https://github.com/kubernetes/k8s.io/blob/c7aa8b1685f1b9a8ab978639b28164e6cef305a2/infra/gcp/terraform/kubernetes-public/k8s-infra-prow.tf#L46,https://github.com/kubernetes/k8s.io/blob/5e69979eb5251d9a1ad6ca6ec8856c99a5927034/infra/gcp/terraform/kubernetes-public/k8s-infra-prow.tf#L56,2021-08-10 16:09:26-07:00,2024-01-03 18:16:49+00:00,6,0,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,613,examples/data-solutions/data-platform-foundations/variables.tf,examples/data-solutions/data-platform-foundations/variables.tf,0,#todo,#TODO Move to network,#TODO Move to network,"variable ""composer_config"" {
  type = object({
    node_count = number
    #TODO Move to network
    ip_range_cloudsql   = string
    ip_range_gke_master = string
    ip_range_web_server = string
    #TODO hardcoded
    project_policy_boolean = map(bool)
    region                 = string
    ip_allocation_policy = object({
      use_ip_aliases                = string
      cluster_secondary_range_name  = string
      services_secondary_range_name = string
    })
    #TODO Add Env variables, Airflow version
  })
  default = {
    node_count             = 3
    ip_range_cloudsql      = ""10.20.10.0/24""
    ip_range_gke_master    = ""10.20.11.0/28""
    ip_range_web_server    = ""10.20.11.16/28""
    project_policy_boolean = null
    region                 = ""europe-west1""
    ip_allocation_policy = {
      use_ip_aliases                = ""true""
      cluster_secondary_range_name  = ""pods""
      services_secondary_range_name = ""services""
    }
  }
}
",variable,"variable ""composer_config"" {
  type = object({
    node_count      = number
    airflow_version = string
    env_variables   = map(string)
  })
  default = {
    node_count      = 3
    airflow_version = ""composer-1.17.5-airflow-2.1.4""
    env_variables   = {}
  }
}
",variable,33,,2e560407c118e7b7abc32f8ac1788a3f48563f21,d8bad5779036aa31639e4611e4935287fc79a4bc,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2e560407c118e7b7abc32f8ac1788a3f48563f21/examples/data-solutions/data-platform-foundations/variables.tf#L33,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d8bad5779036aa31639e4611e4935287fc79a4bc/examples/data-solutions/data-platform-foundations/variables.tf,2022-02-07 17:51:06+01:00,2022-02-07 21:28:54+01:00,2,1,0,1,0,0,1,0,0,0
https://github.com/ministryofjustice/modernisation-platform,198,terraform/environments/data-platform-apps-and-tools/environment-configuration.tf,terraform/environments/data-platform-apps-and-tools/environment-configuration.tf,0,// todo,// TODO: Replace this with Obersevability Platform PRODUCTION Prometheus URL,// TODO: Replace this with Obersevability Platform PRODUCTION Prometheus URL,"locals {
  airflow_name              = ""${local.application_name}-${local.environment}""
  environment_configuration = local.environment_configurations[local.environment]
  environment_configurations = {
    development = {
      /* Route53 */
      route53_zone = ""apps-tools.development.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* SES */
      ses_domain_identity = ""apps-tools.development.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* VPC */
      vpc_cidr                   = ""10.26.128.0/21""
      vpc_private_subnets        = [""10.26.130.0/23"", ""10.26.132.0/23"", ""10.26.134.0/23""]
      vpc_public_subnets         = [""10.26.128.0/27"", ""10.26.128.32/27"", ""10.26.128.64/27""]
      vpc_database_subnets       = [""10.26.128.96/27"", ""10.26.128.128/27"", ""10.26.128.160/27""]
      vpc_enable_nat_gateway     = true
      vpc_one_nat_gateway_per_az = false

      /* EKS */
      eks_cluster_name = ""apps-tools-${local.environment}""
      eks_versions = {
        cluster                   = ""1.28""
        ami_release               = ""1.15.1-264e294c"" // [major version].[minor version].[patch version]-[first 8 chars of commit SHA]. Get the SHA from here: https://github.com/bottlerocket-os/bottlerocket/releases
        addon_coredns             = ""v1.10.1-eksbuild.4""
        addon_kube_proxy          = ""v1.28.2-eksbuild.2""
        addon_vpc_cni             = ""v1.15.1-eksbuild.1""
        addon_aws_guardduty_agent = ""v1.3.0-eksbuild.1""
        addon_ebs_csi_driver      = ""v1.23.1-eksbuild.1""
        addon_efs_csi_driver      = ""v1.7.0-eksbuild.1""
      }
      eks_sso_access_role = ""modernisation-platform-sandbox""

      /* Airflow */
      airflow_s3_bucket             = ""moj-data-platform-airflow-development20230627094128036000000001"" // This is defined in modernisation-platform-environments
      airflow_dag_s3_path           = ""dags/""                                                           // This is defined in modernisation-platform-environments
      airflow_requirements_s3_path  = ""requirements.txt""                                                // This is defined in modernisation-platform-environments
      airflow_execution_role_name   = ""${local.application_name}-${local.environment}-airflow-execution""
      airflow_name                  = ""${local.application_name}-${local.environment}""
      airflow_version               = ""2.6.3""
      airflow_environment_class     = ""mw1.medium""
      airflow_max_workers           = 2
      airflow_min_workers           = 1
      airflow_schedulers            = 2
      airflow_webserver_access_mode = ""PUBLIC_ONLY""
      airflow_configuration_options = {
        ""webserver.warn_deployment_exposure"" = 0
      }
      airflow_mail_from_address               = ""airflow""
      airflow_weekly_maintenance_window_start = ""SAT:00:00""

      /* Data Platform */
      data_platform_account_id        = local.environment_management.account_ids[""data-platform-development""]
      data_platform_openmetadata_role = ""openmetadata""

      /* Observability Platform */
      observability_platform_account_id     = local.environment_management.account_ids[""observability-platform-development""]
      observability_platform_role           = ""data-platform-apps-and-tools""
      observability_platform_prometheus_url = ""https://aps-workspaces.eu-west-2.amazonaws.com/workspaces/ws-464eea97-631a-4e5d-af22-4c5528d9e0e6/api/v1/remote_write""
    }
    production = {
      /* Route53 */
      route53_zone = ""apps-tools.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* SES */
      ses_domain_identity = ""apps-tools.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* VPC */
      vpc_cidr                   = ""10.27.128.0/21""
      vpc_private_subnets        = [""10.27.130.0/23"", ""10.27.132.0/23"", ""10.27.134.0/23""]
      vpc_public_subnets         = [""10.27.128.0/27"", ""10.27.128.32/27"", ""10.27.128.64/27""]
      vpc_database_subnets       = [""10.27.128.96/27"", ""10.27.128.128/27"", ""10.27.128.160/27""]
      vpc_enable_nat_gateway     = true
      vpc_one_nat_gateway_per_az = false

      /* EKS */
      eks_cluster_name = ""apps-tools-${local.environment}""
      eks_versions = {
        cluster                   = ""1.28""
        ami_release               = ""1.15.1-264e294c"" // [major version].[minor version].[patch version]-[first 8 chars of commit SHA]. Get the SHA from here: https://github.com/bottlerocket-os/bottlerocket/releases
        addon_coredns             = ""v1.10.1-eksbuild.4""
        addon_kube_proxy          = ""v1.28.2-eksbuild.2""
        addon_vpc_cni             = ""v1.15.1-eksbuild.1""
        addon_aws_guardduty_agent = ""v1.3.0-eksbuild.1""
        addon_ebs_csi_driver      = ""v1.23.1-eksbuild.1""
        addon_efs_csi_driver      = ""v1.7.0-eksbuild.1""
      }
      eks_sso_access_role = ""modernisation-platform-developer""

      /* Airflow */
      airflow_s3_bucket             = ""moj-data-platform-airflow-production20230908140747954800000002"" // This is defined in modernisation-platform-environments
      airflow_dag_s3_path           = ""dags/""                                                          // This is defined in modernisation-platform-environments
      airflow_requirements_s3_path  = ""requirements.txt""                                               // This is defined in modernisation-platform-environments
      airflow_execution_role_name   = ""${local.application_name}-${local.environment}-airflow-execution""
      airflow_name                  = ""${local.application_name}-${local.environment}""
      airflow_version               = ""2.6.3""
      airflow_environment_class     = ""mw1.medium""
      airflow_max_workers           = 2
      airflow_min_workers           = 1
      airflow_schedulers            = 2
      airflow_webserver_access_mode = ""PUBLIC_ONLY""
      airflow_configuration_options = {
        ""webserver.warn_deployment_exposure"" = 0
      }
      airflow_mail_from_address               = ""airflow""
      airflow_weekly_maintenance_window_start = ""SAT:00:00""

      /* Data Platform */
      data_platform_account_id        = local.environment_management.account_ids[""data-platform-production""]
      data_platform_openmetadata_role = ""openmetadata""

      /* Observability Platform */
      observability_platform_account_id = local.environment_management.account_ids[""observability-platform-production""]
      observability_platform_role       = ""data-platform-apps-and-tools""
      // TODO: Replace this with Obersevability Platform PRODUCTION Prometheus URL
      observability_platform_prometheus_url = ""https://aps-workspaces.eu-west-2.amazonaws.com/workspaces/ws-464eea97-631a-4e5d-af22-4c5528d9e0e6/api/v1/remote_write""
    }
  }
}
",locals,"locals {
  airflow_name              = ""${local.application_name}-${local.environment}""
  environment_configuration = local.environment_configurations[local.environment]
  environment_configurations = {
    development = {
      /* Route53 */
      route53_zone = ""apps-tools.development.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* SES */
      ses_domain_identity = ""apps-tools.development.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* VPC */
      vpc_cidr                   = ""10.26.128.0/21""
      vpc_private_subnets        = [""10.26.130.0/23"", ""10.26.132.0/23"", ""10.26.134.0/23""]
      vpc_public_subnets         = [""10.26.128.0/27"", ""10.26.128.32/27"", ""10.26.128.64/27""]
      vpc_database_subnets       = [""10.26.128.96/27"", ""10.26.128.128/27"", ""10.26.128.160/27""]
      vpc_enable_nat_gateway     = true
      vpc_one_nat_gateway_per_az = false

      /* EKS */
      eks_cluster_name = ""apps-tools-${local.environment}""
      eks_versions = {
        cluster                   = ""1.28""
        ami_release               = ""1.15.1-264e294c"" // [major version].[minor version].[patch version]-[first 8 chars of commit SHA]. Get the SHA from here: https://github.com/bottlerocket-os/bottlerocket/releases
        addon_coredns             = ""v1.10.1-eksbuild.4""
        addon_kube_proxy          = ""v1.28.2-eksbuild.2""
        addon_vpc_cni             = ""v1.15.1-eksbuild.1""
        addon_aws_guardduty_agent = ""v1.3.0-eksbuild.1""
        addon_ebs_csi_driver      = ""v1.23.1-eksbuild.1""
        addon_efs_csi_driver      = ""v1.7.0-eksbuild.1""
      }
      eks_sso_access_role = ""modernisation-platform-sandbox""

      /* Airflow */
      airflow_s3_bucket             = ""moj-data-platform-airflow-development20230627094128036000000001"" // This is defined in modernisation-platform-environments
      airflow_dag_s3_path           = ""dags/""                                                           // This is defined in modernisation-platform-environments
      airflow_requirements_s3_path  = ""requirements.txt""                                                // This is defined in modernisation-platform-environments
      airflow_execution_role_name   = ""${local.application_name}-${local.environment}-airflow-execution""
      airflow_name                  = ""${local.application_name}-${local.environment}""
      airflow_version               = ""2.6.3""
      airflow_environment_class     = ""mw1.medium""
      airflow_max_workers           = 2
      airflow_min_workers           = 1
      airflow_schedulers            = 2
      airflow_webserver_access_mode = ""PUBLIC_ONLY""
      airflow_configuration_options = {
        ""webserver.warn_deployment_exposure"" = 0
      }
      airflow_mail_from_address               = ""airflow""
      airflow_weekly_maintenance_window_start = ""SAT:00:00""

      /* Data Platform */
      data_platform_account_id        = local.environment_management.account_ids[""data-platform-development""]
      data_platform_openmetadata_role = ""openmetadata""

      /* Observability Platform */
      observability_platform_account_id     = local.environment_management.account_ids[""observability-platform-development""]
      observability_platform_role           = ""data-platform-apps-and-tools""
      observability_platform_prometheus_url = ""https://aps-workspaces.eu-west-2.amazonaws.com/workspaces/ws-464eea97-631a-4e5d-af22-4c5528d9e0e6/api/v1/remote_write""
    }
    production = {
      /* Route53 */
      route53_zone = ""apps-tools.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* SES */
      ses_domain_identity = ""apps-tools.data-platform.service.justice.gov.uk"" // This is defined in modernisation-platform-environments

      /* VPC */
      vpc_cidr                   = ""10.27.128.0/21""
      vpc_private_subnets        = [""10.27.130.0/23"", ""10.27.132.0/23"", ""10.27.134.0/23""]
      vpc_public_subnets         = [""10.27.128.0/27"", ""10.27.128.32/27"", ""10.27.128.64/27""]
      vpc_database_subnets       = [""10.27.128.96/27"", ""10.27.128.128/27"", ""10.27.128.160/27""]
      vpc_enable_nat_gateway     = true
      vpc_one_nat_gateway_per_az = false

      /* EKS */
      eks_cluster_name = ""apps-tools-${local.environment}""
      eks_versions = {
        cluster                   = ""1.28""
        ami_release               = ""1.15.1-264e294c"" // [major version].[minor version].[patch version]-[first 8 chars of commit SHA]. Get the SHA from here: https://github.com/bottlerocket-os/bottlerocket/releases
        addon_coredns             = ""v1.10.1-eksbuild.4""
        addon_kube_proxy          = ""v1.28.2-eksbuild.2""
        addon_vpc_cni             = ""v1.15.1-eksbuild.1""
        addon_aws_guardduty_agent = ""v1.3.0-eksbuild.1""
        addon_ebs_csi_driver      = ""v1.23.1-eksbuild.1""
        addon_efs_csi_driver      = ""v1.7.0-eksbuild.1""
      }
      eks_sso_access_role = ""modernisation-platform-developer""

      /* Airflow */
      airflow_s3_bucket             = ""moj-data-platform-airflow-production20230908140747954800000002"" // This is defined in modernisation-platform-environments
      airflow_dag_s3_path           = ""dags/""                                                          // This is defined in modernisation-platform-environments
      airflow_requirements_s3_path  = ""requirements.txt""                                               // This is defined in modernisation-platform-environments
      airflow_execution_role_name   = ""${local.application_name}-${local.environment}-airflow-execution""
      airflow_name                  = ""${local.application_name}-${local.environment}""
      airflow_version               = ""2.6.3""
      airflow_environment_class     = ""mw1.medium""
      airflow_max_workers           = 2
      airflow_min_workers           = 1
      airflow_schedulers            = 2
      airflow_webserver_access_mode = ""PUBLIC_ONLY""
      airflow_configuration_options = {
        ""webserver.warn_deployment_exposure"" = 0
      }
      airflow_mail_from_address               = ""airflow""
      airflow_weekly_maintenance_window_start = ""SAT:00:00""

      /* Data Platform */
      data_platform_account_id        = local.environment_management.account_ids[""data-platform-production""]
      data_platform_openmetadata_role = ""openmetadata""

      /* Observability Platform */
      observability_platform_account_id     = local.environment_management.account_ids[""observability-platform-production""]
      observability_platform_role           = ""data-platform-apps-and-tools""
      observability_platform_prometheus_url = ""https://aps-workspaces.eu-west-2.amazonaws.com/workspaces/ws-55a65e9b-aab9-47a0-88b4-8275c50f1ff9/api/v1/remote_write""
    }
  }
}
",locals,115,,90002b7667baf7c1eb23d0e49dcfa9d9eaee7567,72b87280173d4417efe586f9faf8583a187e65f5,https://github.com/ministryofjustice/modernisation-platform/blob/90002b7667baf7c1eb23d0e49dcfa9d9eaee7567/terraform/environments/data-platform-apps-and-tools/environment-configuration.tf#L115,https://github.com/ministryofjustice/modernisation-platform/blob/72b87280173d4417efe586f9faf8583a187e65f5/terraform/environments/data-platform-apps-and-tools/environment-configuration.tf,2023-10-18 15:43:32+01:00,2023-10-19 10:41:31+01:00,2,1,0,1,0,0,0,1,1,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1759,modules/net-lb-app-ext-regional/backend-service.tf,modules/net-lb-app-ext-regional/backend-service.tf,0,# todo,# TODO(jccb): add security_policy block,"# TODO(jccb): add security_policy block 
 # TODO(jccb): add connection_tracking_policy block ","resource ""google_compute_region_backend_service"" ""default"" {
  provider = google-beta
  for_each = var.backend_service_configs
  project = (
    each.value.project_id == null
    ? var.project_id
    : each.value.project_id
  )
  name                            = ""${var.name}-${each.key}""
  region                          = var.region
  description                     = var.description
  affinity_cookie_ttl_sec         = each.value.affinity_cookie_ttl_sec
  connection_draining_timeout_sec = each.value.connection_draining_timeout_sec
  enable_cdn                      = each.value.enable_cdn
  health_checks = length(each.value.health_checks) == 0 ? null : [
    for k in each.value.health_checks : lookup(local.hc_ids, k, k)
  ]
  # external regional load balancer is always EXTERNAL_MANAGER.
  # TODO(jccb): double check if this is true
  load_balancing_scheme = ""EXTERNAL_MANAGED""
  #TODO(jccb): add locality_lb_policy with MAGLEV and WEIGHTED_MAGLEV when scheme EXTERNAL
  port_name = (
    each.value.port_name == null
    ? lower(each.value.protocol == null ? var.protocol : each.value.protocol)
    : each.value.port_name
  )
  protocol = (
    each.value.protocol == null ? var.protocol : each.value.protocol
  )
  session_affinity = each.value.session_affinity
  timeout_sec      = each.value.timeout_sec

  dynamic ""backend"" {
    for_each = { for b in coalesce(each.value.backends, []) : b.backend => b }
    content {
      group           = lookup(local.group_ids, backend.key, backend.key)
      balancing_mode  = backend.value.balancing_mode # UTILIZATION, RATE
      capacity_scaler = backend.value.capacity_scaler
      description     = backend.value.description
      max_connections = try(
        backend.value.max_connections.per_group, null
      )
      max_connections_per_endpoint = try(
        backend.value.max_connections.per_endpoint, null
      )
      max_connections_per_instance = try(
        backend.value.max_connections.per_instance, null
      )
      max_rate = try(
        backend.value.max_rate.per_group, null
      )
      max_rate_per_endpoint = try(
        backend.value.max_rate.per_endpoint, null
      )
      max_rate_per_instance = try(
        backend.value.max_rate.per_instance, null
      )
      max_utilization = backend.value.max_utilization
    }
  }

  dynamic ""cdn_policy"" {
    for_each = (
      each.value.cdn_policy == null ? [] : [each.value.cdn_policy]
    )
    iterator = cdn
    content {
      cache_mode                   = cdn.value.cache_mode
      client_ttl                   = cdn.value.client_ttl
      default_ttl                  = cdn.value.default_ttl
      max_ttl                      = cdn.value.max_ttl
      negative_caching             = cdn.value.negative_caching
      serve_while_stale            = cdn.value.serve_while_stale
      signed_url_cache_max_age_sec = cdn.value.signed_url_cache_max_age_sec
      dynamic ""cache_key_policy"" {
        for_each = (
          cdn.value.cache_key_policy == null
          ? []
          : [cdn.value.cache_key_policy]
        )
        iterator = ck
        content {
          include_host           = ck.value.include_host
          include_named_cookies  = ck.value.include_named_cookies
          include_protocol       = ck.value.include_protocol
          include_query_string   = ck.value.include_query_string
          query_string_blacklist = ck.value.query_string_blacklist
          query_string_whitelist = ck.value.query_string_whitelist
        }
      }
      dynamic ""negative_caching_policy"" {
        for_each = (
          cdn.value.negative_caching_policy == null
          ? []
          : [cdn.value.negative_caching_policy]
        )
        iterator = nc
        content {
          code = nc.value.code
          ttl  = nc.value.ttl
        }
      }
    }
  }

  dynamic ""circuit_breakers"" {
    for_each = (
      each.value.circuit_breakers == null ? [] : [each.value.circuit_breakers]
    )
    iterator = cb
    content {
      max_connections             = cb.value.max_connections
      max_pending_requests        = cb.value.max_pending_requests
      max_requests                = cb.value.max_requests
      max_requests_per_connection = cb.value.max_requests_per_connection
      max_retries                 = cb.value.max_retries
      dynamic ""connect_timeout"" {
        for_each = (
          cb.value.connect_timeout == null ? [] : [cb.value.connect_timeout]
        )
        content {
          seconds = connect_timeout.value.seconds
          nanos   = connect_timeout.value.nanos
        }
      }
    }
  }

  dynamic ""consistent_hash"" {
    for_each = (
      each.value.consistent_hash == null ? [] : [each.value.consistent_hash]
    )
    iterator = ch
    content {
      http_header_name  = ch.value.http_header_name
      minimum_ring_size = ch.value.minimum_ring_size
      dynamic ""http_cookie"" {
        for_each = ch.value.http_cookie == null ? [] : [ch.value.http_cookie]
        content {
          name = http_cookie.value.name
          path = http_cookie.value.path
          dynamic ""ttl"" {
            for_each = (
              http_cookie.value.ttl == null ? [] : [http_cookie.value.ttl]
            )
            content {
              seconds = ttl.value.seconds
              nanos   = ttl.value.nanos
            }
          }
        }
      }
    }
  }

  dynamic ""iap"" {
    for_each = each.value.iap_config == null ? [] : [each.value.iap_config]
    content {
      oauth2_client_id            = iap.value.oauth2_client_id
      oauth2_client_secret        = iap.value.oauth2_client_secret
      oauth2_client_secret_sha256 = iap.value.oauth2_client_secret_sha256
    }
  }

  dynamic ""log_config"" {
    for_each = each.value.log_sample_rate == null ? [] : [""""]
    content {
      enable      = true
      sample_rate = each.value.log_sample_rate
    }
  }

  dynamic ""outlier_detection"" {
    for_each = (
      each.value.outlier_detection == null ? [] : [each.value.outlier_detection]
    )
    iterator = od
    content {
      consecutive_errors                    = od.value.consecutive_errors
      consecutive_gateway_failure           = od.value.consecutive_gateway_failure
      enforcing_consecutive_errors          = od.value.enforcing_consecutive_errors
      enforcing_consecutive_gateway_failure = od.value.enforcing_consecutive_gateway_failure
      enforcing_success_rate                = od.value.enforcing_success_rate
      max_ejection_percent                  = od.value.max_ejection_percent
      success_rate_minimum_hosts            = od.value.success_rate_minimum_hosts
      success_rate_request_volume           = od.value.success_rate_request_volume
      success_rate_stdev_factor             = od.value.success_rate_stdev_factor
      dynamic ""base_ejection_time"" {
        for_each = (
          od.value.base_ejection_time == null ? [] : [od.value.base_ejection_time]
        )
        content {
          seconds = base_ejection_time.value.seconds
          nanos   = base_ejection_time.value.nanos
        }
      }
      dynamic ""interval"" {
        for_each = (
          od.value.interval == null ? [] : [od.value.interval]
        )
        content {
          seconds = interval.value.seconds
          nanos   = interval.value.nanos
        }
      }
    }
  }
}
",resource,"resource ""google_compute_region_backend_service"" ""default"" {
  provider = google-beta
  for_each = var.backend_service_configs
  project = (
    each.value.project_id == null
    ? var.project_id
    : each.value.project_id
  )
  name                            = ""${var.name}-${each.key}""
  region                          = var.region
  description                     = var.description
  affinity_cookie_ttl_sec         = each.value.affinity_cookie_ttl_sec
  connection_draining_timeout_sec = each.value.connection_draining_timeout_sec
  enable_cdn                      = each.value.enable_cdn
  health_checks = length(each.value.health_checks) == 0 ? null : [
    for k in each.value.health_checks : lookup(local.hc_ids, k, k)
  ]
  # external regional load balancer is always EXTERNAL_MANAGER.
  # TODO(jccb): double check if this is true
  load_balancing_scheme = ""EXTERNAL_MANAGED""
  #TODO(jccb): add locality_lb_policy with MAGLEV and WEIGHTED_MAGLEV when scheme EXTERNAL
  port_name = (
    each.value.port_name == null
    ? lower(each.value.protocol == null ? var.protocol : each.value.protocol)
    : each.value.port_name
  )
  protocol = (
    each.value.protocol == null ? var.protocol : each.value.protocol
  )
  session_affinity = each.value.session_affinity
  timeout_sec      = each.value.timeout_sec

  dynamic ""backend"" {
    for_each = { for b in coalesce(each.value.backends, []) : b.backend => b }
    content {
      group           = lookup(local.group_ids, backend.key, backend.key)
      balancing_mode  = backend.value.balancing_mode # UTILIZATION, RATE
      capacity_scaler = backend.value.capacity_scaler
      description     = backend.value.description
      max_connections = try(
        backend.value.max_connections.per_group, null
      )
      max_connections_per_endpoint = try(
        backend.value.max_connections.per_endpoint, null
      )
      max_connections_per_instance = try(
        backend.value.max_connections.per_instance, null
      )
      max_rate = try(
        backend.value.max_rate.per_group, null
      )
      max_rate_per_endpoint = try(
        backend.value.max_rate.per_endpoint, null
      )
      max_rate_per_instance = try(
        backend.value.max_rate.per_instance, null
      )
      max_utilization = backend.value.max_utilization
    }
  }

  dynamic ""cdn_policy"" {
    for_each = (
      each.value.cdn_policy == null ? [] : [each.value.cdn_policy]
    )
    iterator = cdn
    content {
      cache_mode                   = cdn.value.cache_mode
      client_ttl                   = cdn.value.client_ttl
      default_ttl                  = cdn.value.default_ttl
      max_ttl                      = cdn.value.max_ttl
      negative_caching             = cdn.value.negative_caching
      serve_while_stale            = cdn.value.serve_while_stale
      signed_url_cache_max_age_sec = cdn.value.signed_url_cache_max_age_sec
      dynamic ""cache_key_policy"" {
        for_each = (
          cdn.value.cache_key_policy == null
          ? []
          : [cdn.value.cache_key_policy]
        )
        iterator = ck
        content {
          include_host           = ck.value.include_host
          include_named_cookies  = ck.value.include_named_cookies
          include_protocol       = ck.value.include_protocol
          include_query_string   = ck.value.include_query_string
          query_string_blacklist = ck.value.query_string_blacklist
          query_string_whitelist = ck.value.query_string_whitelist
        }
      }
      dynamic ""negative_caching_policy"" {
        for_each = (
          cdn.value.negative_caching_policy == null
          ? []
          : [cdn.value.negative_caching_policy]
        )
        iterator = nc
        content {
          code = nc.value.code
          ttl  = nc.value.ttl
        }
      }
    }
  }

  dynamic ""circuit_breakers"" {
    for_each = (
      each.value.circuit_breakers == null ? [] : [each.value.circuit_breakers]
    )
    iterator = cb
    content {
      max_connections             = cb.value.max_connections
      max_pending_requests        = cb.value.max_pending_requests
      max_requests                = cb.value.max_requests
      max_requests_per_connection = cb.value.max_requests_per_connection
      max_retries                 = cb.value.max_retries
      dynamic ""connect_timeout"" {
        for_each = (
          cb.value.connect_timeout == null ? [] : [cb.value.connect_timeout]
        )
        content {
          seconds = connect_timeout.value.seconds
          nanos   = connect_timeout.value.nanos
        }
      }
    }
  }

  dynamic ""consistent_hash"" {
    for_each = (
      each.value.consistent_hash == null ? [] : [each.value.consistent_hash]
    )
    iterator = ch
    content {
      http_header_name  = ch.value.http_header_name
      minimum_ring_size = ch.value.minimum_ring_size
      dynamic ""http_cookie"" {
        for_each = ch.value.http_cookie == null ? [] : [ch.value.http_cookie]
        content {
          name = http_cookie.value.name
          path = http_cookie.value.path
          dynamic ""ttl"" {
            for_each = (
              http_cookie.value.ttl == null ? [] : [http_cookie.value.ttl]
            )
            content {
              seconds = ttl.value.seconds
              nanos   = ttl.value.nanos
            }
          }
        }
      }
    }
  }

  dynamic ""iap"" {
    for_each = each.value.iap_config == null ? [] : [each.value.iap_config]
    content {
      oauth2_client_id            = iap.value.oauth2_client_id
      oauth2_client_secret        = iap.value.oauth2_client_secret
      oauth2_client_secret_sha256 = iap.value.oauth2_client_secret_sha256
    }
  }

  dynamic ""log_config"" {
    for_each = each.value.log_sample_rate == null ? [] : [""""]
    content {
      enable      = true
      sample_rate = each.value.log_sample_rate
    }
  }

  dynamic ""outlier_detection"" {
    for_each = (
      each.value.outlier_detection == null ? [] : [each.value.outlier_detection]
    )
    iterator = od
    content {
      consecutive_errors                    = od.value.consecutive_errors
      consecutive_gateway_failure           = od.value.consecutive_gateway_failure
      enforcing_consecutive_errors          = od.value.enforcing_consecutive_errors
      enforcing_consecutive_gateway_failure = od.value.enforcing_consecutive_gateway_failure
      enforcing_success_rate                = od.value.enforcing_success_rate
      max_ejection_percent                  = od.value.max_ejection_percent
      success_rate_minimum_hosts            = od.value.success_rate_minimum_hosts
      success_rate_request_volume           = od.value.success_rate_request_volume
      success_rate_stdev_factor             = od.value.success_rate_stdev_factor
      dynamic ""base_ejection_time"" {
        for_each = (
          od.value.base_ejection_time == null ? [] : [od.value.base_ejection_time]
        )
        content {
          seconds = base_ejection_time.value.seconds
          nanos   = base_ejection_time.value.nanos
        }
      }
      dynamic ""interval"" {
        for_each = (
          od.value.interval == null ? [] : [od.value.interval]
        )
        content {
          seconds = interval.value.seconds
          nanos   = interval.value.nanos
        }
      }
    }
  }
}
",resource,39,39.0,8beb621e070226b7f11a82807a706170ae7040ea,8beb621e070226b7f11a82807a706170ae7040ea,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/8beb621e070226b7f11a82807a706170ae7040ea/modules/net-lb-app-ext-regional/backend-service.tf#L39,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/8beb621e070226b7f11a82807a706170ae7040ea/modules/net-lb-app-ext-regional/backend-service.tf#L39,2024-01-05 16:59:27+01:00,2024-01-05 16:59:27+01:00,1,0,0,1,0,1,0,0,0,0
https://github.com/rust-lang/simpleinfra,4,terragrunt/modules/ecs-cluster/cluster.tf,terragrunt/modules/ecs-cluster/cluster.tf,0,# todo,# TODO: change this as it is deprecated,# TODO: change this as it is deprecated,"resource ""aws_ecs_cluster"" ""cluster"" {
  name = var.cluster_name
  # TODO: change this as it is deprecated
  capacity_providers = [""FARGATE"", ""FARGATE_SPOT""]

  setting {
    name  = ""containerInsights""
    value = ""enabled""
  }

  tags = {
    Name = var.cluster_name
  }
}
",resource,"resource ""aws_ecs_cluster"" ""cluster"" {
  name = var.cluster_name

  setting {
    name  = ""containerInsights""
    value = ""enabled""
  }

  tags = {
    Name = var.cluster_name
  }
}
",resource,18,,385329684454974c1801d3aecb0d7fa87bc066da,84eb7f700493ea666410e696a3c87073ea78198e,https://github.com/rust-lang/simpleinfra/blob/385329684454974c1801d3aecb0d7fa87bc066da/terragrunt/modules/ecs-cluster/cluster.tf#L18,https://github.com/rust-lang/simpleinfra/blob/84eb7f700493ea666410e696a3c87073ea78198e/terragrunt/modules/ecs-cluster/cluster.tf,2022-12-16 16:13:09+01:00,2022-12-16 16:13:09+01:00,2,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,481,fast/stages/02-security/core-dev.tf,fast/stages/2-security/core-dev.tf,1,# todo,# TODO(ludo): add support for conditions to Fabric modules,# TODO(ludo): add support for conditions to Fabric modules,"resource ""google_project_iam_member"" ""dev_key_admin_delegated"" {
  for_each = toset(try(var.kms_restricted_admins.dev, []))
  project  = module.dev-sec-project.project_id
  role     = ""roles/cloudkms.admin""
  member   = each.key
  condition {
    title       = ""kms_sa_delegated_grants""
    description = ""Automation service account delegated grants""
    expression = format(
      ""api.getAttribute('iam.googleapis.com/modifiedGrantsByRole', []).hasOnly([%s])"",
      join("","", formatlist(""'%s'"", [
        ""roles/cloudkms.cryptoKeyEncrypterDecrypter"",
        ""roles/cloudkms.cryptoKeyEncrypterDecrypterViaDelegation""
      ]))
    )
  }
  depends_on = [module.dev-sec-project]
}
",resource,the block associated got renamed or deleted,,45,,34e845fcdd4862f37409a843b3ffba52210e9a12,121598dbea2e72f9b4df57e5f0ca60c8bd7d0990,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/34e845fcdd4862f37409a843b3ffba52210e9a12/fast/stages/02-security/core-dev.tf#L45,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/121598dbea2e72f9b4df57e5f0ca60c8bd7d0990/fast/stages/2-security/core-dev.tf,2022-01-17 10:30:26+01:00,2023-09-17 00:21:36+02:00,17,1,0,1,0,1,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-rds,1,modules/db_parameter_group/main.tf,modules/db_parameter_group/main.tf,0,implement,// @todo: implement this,"// @todo: implement this 
 //  parameter = [""${var.parameters}""]  
 //  parameter = [ 
 //    { 
 //      name  = ""character_set_server"" 
 //      value = ""utf8"" 
 //    }, 
 //    { 
 //      name  = ""character_set_client"" 
 //      value = ""utf18"" 
 //    }, 
 //  ]","resource ""aws_db_parameter_group"" ""this"" {
  count = ""${var.count}""

  name_prefix = ""${var.name_prefix}""
  description = ""Database parameter group for ${var.identifier}""
  family      = ""${var.family}""

  // @todo: implement this
  //  parameter = [""${var.parameters}""]

  //  parameter = [
  //    {
  //      name  = ""character_set_server""
  //      value = ""utf8""
  //    },
  //    {
  //      name  = ""character_set_client""
  //      value = ""utf18""
  //    },
  //  ]
  parameter {
    name  = ""character_set_server""
    value = ""utf8""
  }
  parameter {
    name  = ""character_set_client""
    value = ""utf8""
  }
  tags = ""${merge(var.tags, map(""Name"", format(""%s"", var.identifier)))}""
}
",resource,"resource ""aws_db_parameter_group"" ""this"" {
  count = ""${var.count}""

  name_prefix = ""${var.name_prefix}""
  description = ""Database parameter group for ${var.identifier}""
  family      = ""${var.family}""

  parameter = [""${var.parameters}""]

  tags = ""${merge(var.tags, map(""Name"", format(""%s"", var.identifier)))}""
}
",resource,11,,3e6e7dd55518388b476de3f50202d9368e09b622,5cf5f93d63fe80488a68060ca207fbc6aefe77ee,https://github.com/terraform-aws-modules/terraform-aws-rds/blob/3e6e7dd55518388b476de3f50202d9368e09b622/modules/db_parameter_group/main.tf#L11,https://github.com/terraform-aws-modules/terraform-aws-rds/blob/5cf5f93d63fe80488a68060ca207fbc6aefe77ee/modules/db_parameter_group/main.tf,2017-09-13 22:43:28+02:00,2017-09-20 16:35:35-04:00,2,1,1,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-project-factory,110,modules/fabric-project/vars.tf,modules/fabric-project/variables.tf,1,fix,# TODO: revert to a single map once the following issue is fixed,"# TODO: revert to a single map once the following issue is fixed 
 # https://github.com/hashicorp/terraform/issues/12570 ","variable ""extra_bindings_roles"" {
  description = ""List of roles for additional IAM bindings, pair with members list below.""
  default     = []
}
",variable,"variable ""extra_bindings_roles"" {
  description = ""List of roles for additional IAM bindings, pair with members list below.""
  type        = list(string)
  default     = []
}
",variable,79,86.0,6a786a6d668df3a254ed64b83f3c5779cb6540f3,97eee8801f9c63456f814f90331d635dd8cea99a,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/6a786a6d668df3a254ed64b83f3c5779cb6540f3/modules/fabric-project/vars.tf#L79,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/97eee8801f9c63456f814f90331d635dd8cea99a/modules/fabric-project/variables.tf#L86,2019-05-15 01:35:34-05:00,2019-11-01 10:55:38+01:00,5,0,0,1,1,0,0,0,0,0
https://github.com/nebari-dev/nebari,11,qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf,qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf,0,implement,#       A decision has been made to not implement an actual schema at,"# NOTE: While we define a schema, it is a dummy schema that doesn't 
 #       validate anything. We just have it to comply with the schema of 
 #       a CustomResourceDefinition that requires it. 
 # 
 #       A decision has been made to not implement an actual schema at 
 #       this point in time due to the additional maintenance work it 
 #       would require. 
 # 
 #       Reference: https://github.com/dask/dask-gateway/issues/434 
 #","resource ""kubernetes_manifest"" ""main"" {
  manifest = {
    apiVersion = ""apiextensions.k8s.io/v1""
    kind       = ""CustomResourceDefinition""
    metadata = {
      name = ""daskclusters.gateway.dask.org""
    }
    spec = {
      group   = ""gateway.dask.org""
      names = {
        kind     = ""DaskCluster""
        listKind = ""DaskClusterList""
        plural   = ""daskclusters""
        singular = ""daskcluster""
      }
      scope = ""Namespaced""
      versions = [{
        name = ""v1alpha1""
        served = true
        storage = true
        subresources = {
          status = {}
        }

        # NOTE: While we define a schema, it is a dummy schema that doesn't
        #       validate anything. We just have it to comply with the schema of
        #       a CustomResourceDefinition that requires it.
        #
        #       A decision has been made to not implement an actual schema at
        #       this point in time due to the additional maintenance work it
        #       would require.
        #
        #       Reference: https://github.com/dask/dask-gateway/issues/434
        #
        schema = {
          openAPIV3Schema = {
            type = ""object""
            # FIXME: Make this an actual schema instead of this dummy schema that
            #        is a workaround to meet the requirement of having a schema.
            x-kubernetes-preserve-unknown-fields = true
          }
        }
      }]
    }
  }
}
",resource,"resource ""kubernetes_manifest"" ""main"" {
  manifest = {
    apiVersion = ""apiextensions.k8s.io/v1""
    kind       = ""CustomResourceDefinition""
    metadata = {
      name = ""daskclusters.gateway.dask.org""
    }
    spec = {
      group = ""gateway.dask.org""
      names = {
        kind     = ""DaskCluster""
        listKind = ""DaskClusterList""
        plural   = ""daskclusters""
        singular = ""daskcluster""
      }
      scope = ""Namespaced""
      versions = [{
        name    = ""v1alpha1""
        served  = true
        storage = true
        subresources = {
          status = {}
        }

        # NOTE: While we define a schema, it is a dummy schema that doesn't
        #       validate anything. We just have it to comply with the schema of
        #       a CustomResourceDefinition that requires it.
        #
        #       A decision has been made to not implement an actual schema at
        #       this point in time due to the additional maintenance work it
        #       would require.
        #
        #       Reference: https://github.com/dask/dask-gateway/issues/434
        #
        schema = {
          openAPIV3Schema = {
            type = ""object""
            # FIXME: Make this an actual schema instead of this dummy schema that
            #        is a workaround to meet the requirement of having a schema.
            x-kubernetes-preserve-unknown-fields = true
          }
        }
      }]
    }
  }
}
",resource,29,29.0,e65621ed9fc3e374626cc3929742df6ba94fc8d7,d0cc26638fbc9e69aa736105ffd61cfb50d561d6,https://github.com/nebari-dev/nebari/blob/e65621ed9fc3e374626cc3929742df6ba94fc8d7/qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf#L29,https://github.com/nebari-dev/nebari/blob/d0cc26638fbc9e69aa736105ffd61cfb50d561d6/qhub/template/stages/07-kubernetes-services/modules/kubernetes/services/dask-gateway/crds.tf#L29,2022-02-03 11:12:40-05:00,2022-05-26 14:22:33-07:00,2,0,1,1,0,0,0,0,0,0
https://github.com/awslabs/data-on-eks,7,ai-ml/ray/terraform/examples/pytorch/main.tf,ai-ml/ray/terraform/examples/pytorch/main.tf,0,workaround,# workaround for protobuf protoc >= 3.19.0 issue,# workaround for protobuf protoc >= 3.19.0 issue,"module ""pytorch_cluster"" {
  source = ""../../modules/ray-cluster""

  namespace        = local.name
  ray_cluster_name = local.name
  eks_cluster_name = local.eks_cluster

  helm_values = [
    yamlencode({
      image = {
        repository = ""rayproject/ray-ml""
        # This is a different version than the xgboost version
        tag        = ""2.3.0""
        pullPolicy = ""IfNotPresent""
      }
      head = {
        enableInTreeAutoscaling = ""True""
        resources = {
          limits = {
            cpu    = ""4""
            memory = ""24G""
          }
          requests = {
            cpu    = ""4""
            memory = ""12G""
          }
        }
        tolerations = [
          {
            key      = local.name
            effect   = ""NoSchedule""
            operator = ""Exists""
          }
        ]
        containerEnv = [
          {
            name  = ""RAY_LOG_TO_STDERR""
            value = ""1""
          },
          {
            # workaround for protobuf protoc >= 3.19.0 issue
            name  = ""PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION""
            value = ""python""
          }
        ]
      }
      worker = {
        resources = {
          limits = {
            cpu    = ""8""
            memory = ""24G""
          }
          requests = {
            cpu    = ""4""
            memory = ""12G""
          }
        }
        tolerations = [
          {
            key      = local.name
            effect   = ""NoSchedule""
            operator = ""Exists""
          }
        ]
        replicas    = ""0""
        minReplicas = ""0""
        maxReplicas = ""30""
        containerEnv = [
          {
            name  = ""RAY_LOG_TO_STDERR""
            value = ""1""
          },
          {
            # workaround for protobuf protoc >= 3.19.0 issue
            name  = ""PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION""
            value = ""python""
          }
        ]
      }
    })
  ]
}
",module,"module ""pytorch_cluster"" {
  source = ""../../modules/ray-cluster""

  namespace        = local.name
  ray_cluster_name = local.name
  eks_cluster_name = local.eks_cluster

  helm_values = [
    yamlencode({
      image = {
        repository = ""rayproject/ray-ml""
        # This is a different version than the xgboost version
        tag        = ""2.3.0""
        pullPolicy = ""IfNotPresent""
      }
      head = {
        enableInTreeAutoscaling = ""True""
        resources = {
          limits = {
            cpu    = ""4""
            memory = ""24G""
          }
          requests = {
            cpu    = ""4""
            memory = ""12G""
          }
        }
        tolerations = [
          {
            key      = local.name
            effect   = ""NoSchedule""
            operator = ""Exists""
          }
        ]
        containerEnv = [
          {
            name  = ""RAY_LOG_TO_STDERR""
            value = ""1""
          },
          {
            # workaround for protobuf protoc >= 3.19.0 issue
            name  = ""PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION""
            value = ""python""
          }
        ]
      }
      worker = {
        resources = {
          limits = {
            cpu    = ""8""
            memory = ""24G""
          }
          requests = {
            cpu    = ""4""
            memory = ""12G""
          }
        }
        tolerations = [
          {
            key      = local.name
            effect   = ""NoSchedule""
            operator = ""Exists""
          }
        ]
        replicas    = ""0""
        minReplicas = ""0""
        maxReplicas = ""30""
        containerEnv = [
          {
            name  = ""RAY_LOG_TO_STDERR""
            value = ""1""
          },
          {
            # workaround for protobuf protoc >= 3.19.0 issue
            name  = ""PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION""
            value = ""python""
          }
        ]
      }
    })
  ]
}
",module,114,114.0,84d14e8fee669fb5db87dd5f4d0e40554ee3a0d3,e31667b33a449fcf25f3067a01445c2444b53349,https://github.com/awslabs/data-on-eks/blob/84d14e8fee669fb5db87dd5f4d0e40554ee3a0d3/ai-ml/ray/terraform/examples/pytorch/main.tf#L114,https://github.com/awslabs/data-on-eks/blob/e31667b33a449fcf25f3067a01445c2444b53349/ai-ml/ray/terraform/examples/pytorch/main.tf#L114,2023-03-21 08:33:37+00:00,2023-05-22 14:26:59-05:00,2,0,1,0,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,23,main.tf,main.tf,0,# todo,# TODO: Should be configured to create multiple tables on central dataset,# TODO: Should be configured to create multiple tables on central dataset,"resource ""google_bigquery_table"" ""main"" {
  dataset_id = ""${google_bigquery_dataset.main.dataset_id}""
  table_id   = ""${var.table_id}""
  project    = ""${var.project_id}""

  #TODO: terraform 0.12 will enable ""time_partitioning ? time_partitioning_is_required : null"" (https://github.com/hashicorp/terraform/issues/17968)
  time_partitioning {
    type = ""${var.time_partitioning}""
  }

  labels = ""${var.table_labels}""

  schema = ""${file(""${var.schema_file}"")}""
}
",resource,"resource ""google_bigquery_table"" ""main"" {
  dataset_id = ""${google_bigquery_dataset.main.dataset_id}""
  table_id   = ""${var.table_id}""
  project    = ""${var.project_id}""

  time_partitioning {
    type = ""${var.time_partitioning}""
  }

  labels = ""${var.table_labels}""

  schema = ""${file(""${var.schema_file}"")}""
}
",resource,42,,7f922f7e9df197df38c9b09dfa6e3614d71f19f5,a7966ae4afc7bb73842fb9fd6f0f708b359cf36e,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/7f922f7e9df197df38c9b09dfa6e3614d71f19f5/main.tf#L42,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/a7966ae4afc7bb73842fb9fd6f0f708b359cf36e/main.tf,2019-01-16 18:10:54-05:00,2019-01-28 12:41:47-05:00,2,1,1,0,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd,46,modules/secure-cd/main.tf,modules/secure-cd/main.tf,0,//todo,//TODO?,require_attestations_by = each.value.attestations //TODO?,"resource ""google_binary_authorization_policy"" ""deployment_policy"" {
  for_each = var.deploy_branch_clusters
  project  = each.value.project_id
  
  admission_whitelist_patterns {
    name_pattern = ""gcr.io/google_containers/*""
  }

  default_admission_rule {
    evaluation_mode  = ""ALWAYS_DENY""
    enforcement_mode = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
  }

  global_policy_evaluation_mode = ""ENABLE""

  cluster_admission_rules {
    cluster                 = ""${each.value.location}.${each.value.cluster}"" // TODO: customer config
    evaluation_mode         = ""REQUIRE_ATTESTATION""
    enforcement_mode        = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
    require_attestations_by = each.value.attestations //TODO?
  }
}
",resource,"resource ""google_binary_authorization_policy"" ""deployment_policy"" {
  for_each = local.binary_authorization_map
  project  = each.key

  default_admission_rule {
    evaluation_mode  = ""ALWAYS_DENY""
    enforcement_mode = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
  }

  global_policy_evaluation_mode = ""ENABLE""

  dynamic ""cluster_admission_rules"" {
    for_each = each.value
    content {
      cluster                 = ""${cluster_admission_rules.value.location}.${cluster_admission_rules.value.cluster}""
      evaluation_mode         = ""REQUIRE_ATTESTATION""
      enforcement_mode        = ""ENFORCED_BLOCK_AND_AUDIT_LOG""
      require_attestations_by = cluster_admission_rules.value.required_attestations
    }
  }
}
",resource,77,,6249c4ca90692e593bc0c7bc6d603580150ff255,c8839114f847e80edd056da098f0489492f58e76,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/6249c4ca90692e593bc0c7bc6d603580150ff255/modules/secure-cd/main.tf#L77,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/c8839114f847e80edd056da098f0489492f58e76/modules/secure-cd/main.tf,2021-10-26 17:18:47-05:00,2021-12-01 11:14:01-06:00,18,1,0,1,0,0,0,0,0,0
https://github.com/compiler-explorer/infra,117,terraform/ec2.tf,terraform/ec2.tf,0,todo,"// TODO reconsider, make an SG specifically for builder","// TODO reconsider, make an SG specifically for builder","resource ""aws_instance"" ""BuilderNode"" {
  ami                         = local.builder_image_id
  // TODO bring into the fold
  iam_instance_profile        = ""GccBuilder""
  ebs_optimized               = true
  // TODO make 4xlarge or similar
  instance_type               = ""c5d.large""
  monitoring                  = false
  key_name                    = ""mattgodbolt""
  subnet_id                   = aws_subnet.ce-1a.id
  // TODO reconsider, make an SG specifically for builder
  vpc_security_group_ids      = [aws_security_group.AdminNode.id]
  associate_public_ip_address = true
  source_dest_check           = false

  root_block_device {
    volume_type           = ""gp2""
    volume_size           = 24
    delete_on_termination = true
  }

  tags = {
    Name = ""Builder-New""
  }
}
",resource,"resource ""aws_instance"" ""BuilderNode"" {
  ami                         = local.builder_image_id
  iam_instance_profile        = aws_iam_instance_profile.Builder.name
  ebs_optimized               = true
  instance_type               = ""c5d.4xlarge""
  monitoring                  = false
  key_name                    = ""mattgodbolt""
  subnet_id                   = aws_subnet.ce-1a.id
  vpc_security_group_ids      = [aws_security_group.Builder.id]
  associate_public_ip_address = true
  source_dest_check           = false

  root_block_device {
    volume_type           = ""gp2""
    volume_size           = 24
    delete_on_termination = true
  }

  lifecycle {
    ignore_changes = [
      // Seemingly needed to not replace stopped instances
      associate_public_ip_address
    ]
  }

  tags = {
    Name = ""Builder""
  }
}
",resource,79,,b419a94ffc423c637e8722d79fb4f42770acf05e,b95d2fa4883a28322df51001553dcf38d0638d09,https://github.com/compiler-explorer/infra/blob/b419a94ffc423c637e8722d79fb4f42770acf05e/terraform/ec2.tf#L79,https://github.com/compiler-explorer/infra/blob/b95d2fa4883a28322df51001553dcf38d0638d09/terraform/ec2.tf,2021-08-30 22:47:49-05:00,2021-09-02 18:02:04-05:00,5,1,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,615,examples/data-solutions/data-platform-foundations/variables.tf,examples/data-solutions/data-platform-foundations/variables.tf,0,#todo,"#TODO Add Env variables, Airflow version","#TODO Add Env variables, Airflow version","variable ""composer_config"" {
  type = object({
    node_count = number
    #TODO Move to network
    ip_range_cloudsql   = string
    ip_range_gke_master = string
    ip_range_web_server = string
    #TODO hardcoded
    project_policy_boolean = map(bool)
    region                 = string
    ip_allocation_policy = object({
      use_ip_aliases                = string
      cluster_secondary_range_name  = string
      services_secondary_range_name = string
    })
    #TODO Add Env variables, Airflow version
  })
  default = {
    node_count             = 3
    ip_range_cloudsql      = ""10.20.10.0/24""
    ip_range_gke_master    = ""10.20.11.0/28""
    ip_range_web_server    = ""10.20.11.16/28""
    project_policy_boolean = null
    region                 = ""europe-west1""
    ip_allocation_policy = {
      use_ip_aliases                = ""true""
      cluster_secondary_range_name  = ""pods""
      services_secondary_range_name = ""services""
    }
  }
}
",variable,"variable ""composer_config"" {
  type = object({
    node_count      = number
    airflow_version = string
    env_variables   = map(string)
  })
  default = {
    node_count      = 3
    airflow_version = ""composer-1.17.5-airflow-2.1.4""
    env_variables   = {}
  }
}
",variable,45,,2e560407c118e7b7abc32f8ac1788a3f48563f21,d8bad5779036aa31639e4611e4935287fc79a4bc,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2e560407c118e7b7abc32f8ac1788a3f48563f21/examples/data-solutions/data-platform-foundations/variables.tf#L45,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d8bad5779036aa31639e4611e4935287fc79a4bc/examples/data-solutions/data-platform-foundations/variables.tf,2022-02-07 17:51:06+01:00,2022-02-07 21:28:54+01:00,2,1,0,1,0,0,0,0,0,0
https://github.com/nasa/cumulus,23,tf-modules/cumulus/ecs_cluster.tf,tf-modules/cumulus/ecs_cluster.tf,0,todo,# TODO Get this value dynamically instead of from a variable,# TODO Get this value dynamically instead of from a variable,"data ""aws_iam_policy_document"" ""ecs_cluster_instance_policy"" {
  statement {
    actions   = [""dynamodb:UpdateItem""]
    resources = [data.aws_dynamodb_table.async_operations.arn]
  }

  statement {
    actions = [
      ""autoscaling:CompleteLifecycleAction"",
      ""autoscaling:DescribeAutoScalingInstances"",
      ""autoscaling:DescribeLifecycleHooks"",
      ""autoscaling:RecordLifecycleActionHeartbeat"",
      ""cloudwatch:GetMetricStatistics"",
      ""ec2:DescribeInstances"",
      ""ecr:BatchCheckLayerAvailability"",
      ""ecr:BatchGetImage"",
      ""ecr:GetAuthorizationToken"",
      ""ecr:GetDownloadUrlForLayer"",
      ""ecs:DeregisterContainerInstance"",
      ""ecs:DescribeClusters"",
      ""ecs:DescribeContainerInstances"",
      ""ecs:DescribeServices"",
      ""ecs:DiscoverPollEndpoint"",
      ""ecs:ListContainerInstances"",
      ""ecs:ListServices"",
      ""ecs:ListTaskDefinitions"",
      ""ecs:ListTasks"",
      ""ecs:Poll"",
      ""ecs:RegisterContainerInstance"",
      ""ecs:RunTask"",
      ""ecs:StartTelemetrySession"",
      ""ecs:Submit*"",
      ""ecs:UpdateContainerInstancesState"",
      ""lambda:GetFunction"",
      ""lambda:invokeFunction"",
      ""logs:CreateLogGroup"",
      ""logs:CreateLogStream"",
      ""logs:DescribeLogStreams"",
      ""logs:PutLogEvents"",
      ""ssm:GetParameter""
    ]
    resources = [""*""]
  }

  statement {
    actions = [
      ""states:DescribeActivity"",
      ""states:GetActivityTask"",
      ""states:GetExecutionHistory"",
      ""states:SendTaskFailure"",
      ""states:SendTaskSuccess""
    ]
    resources = [""arn:aws:states:*:*:*""]
  }

  statement {
    actions = [
      ""s3:GetAccelerateConfiguration"",
      ""s3:GetBucket*"",
      ""s3:GetLifecycleConfiguration"",
      ""s3:GetReplicationConfiguration"",
      ""s3:ListBucket*"",
      ""s3:PutAccelerateConfiguration"",
      ""s3:PutBucket*"",
      ""s3:PutLifecycleConfiguration"",
      ""s3:PutReplicationConfiguration""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}""]
  }

  statement {
    actions = [
      ""s3:AbortMultipartUpload"",
      ""s3:DeleteObject"",
      ""s3:DeleteObjectVersion"",
      ""s3:GetObject*"",
      ""s3:ListMultipartUploadParts"",
      ""s3:PutObject*""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}/*""]
  }

  statement {
    actions = [""dynamodb:Scan""]
    # TODO I don't like the fact that we're making an assumption here about the names of our tables
    resources = [""arn:aws:dynamodb:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:table/${var.prefix}-*""]
  }

  statement {
    actions = [
      ""es:ESHttpDelete"",
      ""es:ESHttpGet"",
      ""es:ESHttpHead"",
      ""es:ESHttpPost"",
      ""es:ESHttpPut""
    ]
    # TODO Get this value dynamically instead of from a variable
    resources = [var.elasticsearch_arn]
  }
}
",data,"data ""aws_iam_policy_document"" ""ecs_cluster_instance_policy"" {
  statement {
    actions   = [""dynamodb:UpdateItem""]
    resources = [data.aws_dynamodb_table.async_operations.arn]
  }

  statement {
    actions = [
      ""autoscaling:CompleteLifecycleAction"",
      ""autoscaling:DescribeAutoScalingInstances"",
      ""autoscaling:DescribeLifecycleHooks"",
      ""autoscaling:RecordLifecycleActionHeartbeat"",
      ""cloudwatch:GetMetricStatistics"",
      ""ec2:DescribeInstances"",
      ""ecr:BatchCheckLayerAvailability"",
      ""ecr:BatchGetImage"",
      ""ecr:GetAuthorizationToken"",
      ""ecr:GetDownloadUrlForLayer"",
      ""ecs:DeregisterContainerInstance"",
      ""ecs:DescribeClusters"",
      ""ecs:DescribeContainerInstances"",
      ""ecs:DescribeServices"",
      ""ecs:DiscoverPollEndpoint"",
      ""ecs:ListContainerInstances"",
      ""ecs:ListServices"",
      ""ecs:ListTaskDefinitions"",
      ""ecs:ListTasks"",
      ""ecs:Poll"",
      ""ecs:RegisterContainerInstance"",
      ""ecs:RunTask"",
      ""ecs:StartTelemetrySession"",
      ""ecs:Submit*"",
      ""ecs:UpdateContainerInstancesState"",
      ""lambda:GetFunction"",
      ""lambda:invokeFunction"",
      ""logs:CreateLogGroup"",
      ""logs:CreateLogStream"",
      ""logs:DescribeLogStreams"",
      ""logs:PutLogEvents"",
      ""ssm:GetParameter""
    ]
    resources = [""*""]
  }

  statement {
    actions = [
      ""states:DescribeActivity"",
      ""states:GetActivityTask"",
      ""states:GetExecutionHistory"",
      ""states:SendTaskFailure"",
      ""states:SendTaskSuccess""
    ]
    resources = [""arn:aws:states:*:*:*""]
  }

  statement {
    actions = [
      ""s3:GetAccelerateConfiguration"",
      ""s3:GetBucket*"",
      ""s3:GetLifecycleConfiguration"",
      ""s3:GetReplicationConfiguration"",
      ""s3:ListBucket*"",
      ""s3:PutAccelerateConfiguration"",
      ""s3:PutBucket*"",
      ""s3:PutLifecycleConfiguration"",
      ""s3:PutReplicationConfiguration""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}""]
  }

  statement {
    actions = [
      ""s3:AbortMultipartUpload"",
      ""s3:DeleteObject"",
      ""s3:DeleteObjectVersion"",
      ""s3:GetObject*"",
      ""s3:ListMultipartUploadParts"",
      ""s3:PutObject*""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}/*""]
  }

  statement {
    actions = [""dynamodb:Scan""]
    # TODO I don't like the fact that we're making an assumption here about the names of our tables
    resources = [""arn:aws:dynamodb:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:table/${var.prefix}-*""]
  }

  statement {
    actions = [
      ""es:ESHttpDelete"",
      ""es:ESHttpGet"",
      ""es:ESHttpHead"",
      ""es:ESHttpPost"",
      ""es:ESHttpPut""
    ]
    resources = [var.elasticsearch_domain_arn]
  }
}
",data,102,,1da53282470313085da6e713a94458500df71f6c,ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7,https://github.com/nasa/cumulus/blob/1da53282470313085da6e713a94458500df71f6c/tf-modules/cumulus/ecs_cluster.tf#L102,https://github.com/nasa/cumulus/blob/ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7/tf-modules/cumulus/ecs_cluster.tf,2019-08-02 16:32:51-04:00,2019-08-14 14:23:38-04:00,3,1,0,1,0,1,0,0,0,0
https://github.com/deckhouse/deckhouse,1,candi/cloud-providers/yandex/terraform-modules/master-node/main.tf,candi/cloud-providers/yandex/terraform-modules/master-node/main.tf,0,todo,// TODO apply external_subnet_id_from_ids to external_subnet_id directly after remove externalSubnetID,// TODO apply external_subnet_id_from_ids to external_subnet_id directly after remove externalSubnetID,"locals {
  zone_to_subnet = {
    ""ru-central1-a"" = data.yandex_vpc_subnet.kube_a
    ""ru-central1-b"" = data.yandex_vpc_subnet.kube_b
    ""ru-central1-c"" = data.yandex_vpc_subnet.kube_c
  }

  actual_zones = lookup(var.providerClusterConfiguration, ""zones"", null) != null ? tolist(setintersection(keys(local.zone_to_subnet), var.providerClusterConfiguration.zones)) : keys(local.zone_to_subnet)
  zones = lookup(var.providerClusterConfiguration.masterNodeGroup, ""zones"", null) != null ? tolist(setintersection(local.actual_zones, var.providerClusterConfiguration.masterNodeGroup[""zones""])) : local.actual_zones
  subnets = length(local.zones) > 0 ? [for z in local.zones : local.zone_to_subnet[z]] : values(local.zone_to_subnet)
  internal_subnet = element(local.subnets, var.nodeIndex)

  // TODO apply external_subnet_id_from_ids to external_subnet_id directly after remove externalSubnetID
  external_subnet_id_from_ids = length(local.external_subnet_ids) > 0 ? local.external_subnet_ids[var.nodeIndex] : null

  external_subnet_id = local.external_subnet_id_from_ids == null ? local.external_subnet_id_deprecated : local.external_subnet_id_from_ids
  external_ip_address = length(local.external_ip_addresses) > 0 ? local.external_ip_addresses[var.nodeIndex] : null
  assign_external_ip_address = (local.external_subnet_id == null) && (local.external_ip_address != null) ? true : false

}
",locals,"locals {
  mapping = lookup(var.providerClusterConfiguration, ""existingZoneToSubnetIDMap"", {})

  zone_to_subnet = length(local.mapping) == 0 ? {
    ""ru-central1-a"" = length(data.yandex_vpc_subnet.kube_a) > 0 ? data.yandex_vpc_subnet.kube_a[0] : object({})
    ""ru-central1-b"" = length(data.yandex_vpc_subnet.kube_b) > 0 ? data.yandex_vpc_subnet.kube_b[0] : object({})
    ""ru-central1-c"" = length(data.yandex_vpc_subnet.kube_c) > 0 ? data.yandex_vpc_subnet.kube_c[0] : object({})
    ""ru-central1-d"" = length(data.yandex_vpc_subnet.kube_d) > 0 ? data.yandex_vpc_subnet.kube_d[0] : object({})
  } : data.yandex_vpc_subnet.existing

  actual_zones    = lookup(var.providerClusterConfiguration, ""zones"", null) != null ? tolist(setintersection(keys(local.zone_to_subnet), var.providerClusterConfiguration.zones)) : keys(local.zone_to_subnet)
  zones           = lookup(var.providerClusterConfiguration.masterNodeGroup, ""zones"", null) != null ? tolist(setintersection(local.actual_zones, var.providerClusterConfiguration.masterNodeGroup[""zones""])) : local.actual_zones
  subnets         = length(local.zones) > 0 ? [for z in local.zones : local.zone_to_subnet[z]] : values(local.zone_to_subnet)
  internal_subnet = element(local.subnets, var.nodeIndex)

  // TODO apply external_subnet_id_from_ids to external_subnet_id directly after remove externalSubnetID
  external_subnet_id_from_ids = length(local.external_subnet_ids) > 0 ? element(local.external_subnet_ids, var.nodeIndex) : null

  external_subnet_id         = local.external_subnet_id_from_ids == null ? local.external_subnet_id_deprecated : local.external_subnet_id_from_ids
  assign_external_ip_address = (local.external_subnet_id == null) && (local.external_ip_address != null) ? true : false

}
",locals,39,30.0,12c4247d312bc32bebf7b1393b3783e64f0ef3d8,31d3b87485eb8740318b216508ee703d1bff2ea8,https://github.com/deckhouse/deckhouse/blob/12c4247d312bc32bebf7b1393b3783e64f0ef3d8/candi/cloud-providers/yandex/terraform-modules/master-node/main.tf#L39,https://github.com/deckhouse/deckhouse/blob/31d3b87485eb8740318b216508ee703d1bff2ea8/candi/cloud-providers/yandex/terraform-modules/master-node/main.tf#L30,2021-06-28 18:47:36+00:00,2023-12-29 10:55:04+01:00,8,0,0,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,19,community/modules/scheduler/gke-cluster/main.tf,community/modules/scheduler/gke-cluster/main.tf,0,take care,# is not allowed. Dataplane V2 will take care of network policy enforcement,"# Enabling NetworkPolicy for clusters with DatapathProvider=ADVANCED_DATAPATH 
 # is not allowed. Dataplane V2 will take care of network policy enforcement 
 # instead.","resource ""google_container_cluster"" ""gke_cluster"" {
  provider = google-beta

  project         = var.project_id
  name            = local.name
  location        = var.region
  resource_labels = var.labels

  # decouple node pool lifecyle from cluster life cycle
  remove_default_node_pool = true
  initial_node_count       = 1 # must be set when remove_default_node_pool is set

  network    = var.network_id
  subnetwork = var.subnetwork_self_link

  # Note: the existence of the ""master_authorized_networks_config"" block enables
  # the master authorized networks even if it's empty.
  master_authorized_networks_config {
  }

  private_ipv6_google_access = var.enable_private_ipv6_google_access ? ""PRIVATE_IPV6_GOOGLE_ACCESS_TO_GOOGLE"" : null

  master_auth {
    client_certificate_config {
      issue_client_certificate = false
    }
  }

  pod_security_policy_config {
    enabled = true
  }

  enable_shielded_nodes = true

  cluster_autoscaling { # Auto provisioning of node-pools
    enabled = false
    # Recomended profile if we ever turn on
    # autoscaling_profile = ""OPTIMIZE_UTILIZATION""
  }

  datapath_provider = var.enable_dataplane_v2 ? ""ADVANCED_DATAPATH"" : ""LEGACY_DATAPATH""

  network_policy {
    # Enabling NetworkPolicy for clusters with DatapathProvider=ADVANCED_DATAPATH
    # is not allowed. Dataplane V2 will take care of network policy enforcement
    # instead.
    enabled = false
    # GKE Dataplane V2 support. This must be set to PROVIDER_UNSPECIFIED in
    # order to let the datapath_provider take effect.
    # https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/issues/656#issuecomment-720398658
    provider = ""PROVIDER_UNSPECIFIED""
  }

  private_cluster_config {
    enable_private_nodes    = var.enable_private_nodes
    enable_private_endpoint = var.enable_private_endpoint
    master_ipv4_cidr_block  = var.master_ipv4_cidr_block
    master_global_access_config {
      enabled = var.enable_master_global_access
    }
  }

  ip_allocation_policy {
    cluster_secondary_range_name  = var.pods_ip_range_name
    services_secondary_range_name = var.services_ip_range_name
  }

  workload_identity_config {
    workload_pool = ""${var.project_id}.svc.id.goog""
  }

  dynamic ""authenticator_groups_config"" {
    for_each = local.cluster_authenticator_security_group
    content {
      security_group = authenticator_groups_config.value.security_group
    }
  }

  release_channel {
    channel = var.release_channel
  }

  maintenance_policy {
    daily_maintenance_window {
      start_time = var.maintenance_start_time
    }

    dynamic ""maintenance_exclusion"" {
      for_each = var.maintenance_exclusions
      content {
        exclusion_name = maintenance_exclusion.value.name
        start_time     = maintenance_exclusion.value.start_time
        end_time       = maintenance_exclusion.value.end_time
        exclusion_options {
          scope = maintenance_exclusion.value.exclusion_scope
        }
      }
    }
  }

  addons_config {
    # Istio is required if there is any pod-to-pod communication.
    istio_config {
      disabled = !var.enable_istio
      auth     = var.istio_auth
    }

    gce_persistent_disk_csi_driver_config {
      enabled = true
    }
  }

  lifecycle {
    # Ignore all changes to the default node pool. It's being removed after creation.
    ignore_changes = [
      node_config
    ]
  }

  logging_service    = ""logging.googleapis.com/kubernetes""
  monitoring_service = ""monitoring.googleapis.com/kubernetes""
}
",resource,"resource ""google_container_cluster"" ""gke_cluster"" {
  provider = google-beta

  project         = var.project_id
  name            = local.name
  location        = var.region
  resource_labels = local.labels

  # decouple node pool lifecycle from cluster life cycle
  remove_default_node_pool = true
  initial_node_count       = 1 # must be set when remove_default_node_pool is set

  network    = var.network_id
  subnetwork = var.subnetwork_self_link

  # Note: the existence of the ""master_authorized_networks_config"" block enables
  # the master authorized networks even if it's empty.
  master_authorized_networks_config {
    dynamic ""cidr_blocks"" {
      for_each = var.master_authorized_networks
      content {
        cidr_block   = cidr_blocks.value.cidr_block
        display_name = cidr_blocks.value.display_name
      }
    }
  }

  private_ipv6_google_access = var.enable_private_ipv6_google_access ? ""PRIVATE_IPV6_GOOGLE_ACCESS_TO_GOOGLE"" : null

  master_auth {
    client_certificate_config {
      issue_client_certificate = false
    }
  }

  enable_shielded_nodes = true

  cluster_autoscaling {
    # Controls auto provisioning of node-pools
    enabled = false

    # Controls autoscaling algorithm of node-pools
    autoscaling_profile = var.autoscaling_profile
  }

  datapath_provider = var.enable_dataplane_v2 ? ""ADVANCED_DATAPATH"" : ""LEGACY_DATAPATH""

  network_policy {
    # Enabling NetworkPolicy for clusters with DatapathProvider=ADVANCED_DATAPATH
    # is not allowed. Dataplane V2 will take care of network policy enforcement
    # instead.
    enabled = false
    # GKE Dataplane V2 support. This must be set to PROVIDER_UNSPECIFIED in
    # order to let the datapath_provider take effect.
    # https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/issues/656#issuecomment-720398658
    provider = ""PROVIDER_UNSPECIFIED""
  }

  private_cluster_config {
    enable_private_nodes    = var.enable_private_nodes
    enable_private_endpoint = var.enable_private_endpoint
    master_ipv4_cidr_block  = var.master_ipv4_cidr_block
    master_global_access_config {
      enabled = var.enable_master_global_access
    }
  }

  ip_allocation_policy {
    cluster_secondary_range_name  = var.pods_ip_range_name
    services_secondary_range_name = var.services_ip_range_name
  }

  workload_identity_config {
    workload_pool = ""${var.project_id}.svc.id.goog""
  }

  dynamic ""authenticator_groups_config"" {
    for_each = local.cluster_authenticator_security_group
    content {
      security_group = authenticator_groups_config.value.security_group
    }
  }

  release_channel {
    channel = var.release_channel
  }
  min_master_version = var.min_master_version

  maintenance_policy {
    daily_maintenance_window {
      start_time = var.maintenance_start_time
    }

    dynamic ""maintenance_exclusion"" {
      for_each = var.maintenance_exclusions
      content {
        exclusion_name = maintenance_exclusion.value.name
        start_time     = maintenance_exclusion.value.start_time
        end_time       = maintenance_exclusion.value.end_time
        exclusion_options {
          scope = maintenance_exclusion.value.exclusion_scope
        }
      }
    }
  }

  addons_config {
    gcp_filestore_csi_driver_config {
      enabled = var.enable_filestore_csi
    }
    gcs_fuse_csi_driver_config {
      enabled = var.enable_gcsfuse_csi
    }
    gce_persistent_disk_csi_driver_config {
      enabled = var.enable_persistent_disk_csi
    }
  }

  timeouts {
    create = var.timeout_create
    update = var.timeout_update
  }

  lifecycle {
    # Ignore all changes to the default node pool. It's being removed after creation.
    ignore_changes = [
      node_config
    ]
  }

  logging_service    = ""logging.googleapis.com/kubernetes""
  monitoring_service = ""monitoring.googleapis.com/kubernetes""
}
",resource,78,88.0,79f3f33f5570371dc5483ddec8c55a5400816aeb,a7adc269a3069a1ab27baed7c8f5e136a3f46f3e,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/79f3f33f5570371dc5483ddec8c55a5400816aeb/community/modules/scheduler/gke-cluster/main.tf#L78,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/a7adc269a3069a1ab27baed7c8f5e136a3f46f3e/community/modules/scheduler/gke-cluster/main.tf#L88,2023-04-04 15:04:33-07:00,2024-02-13 17:40:08-08:00,19,0,1,0,0,1,1,0,0,0
https://github.com/terraform-google-modules/terraform-google-iam,1,test/fixtures/full/iam.tf,test/fixtures/helper/iam.tf,1,# todo,## TODO(jmccune): Disabled as per discussion with Aaron.  Re-enable post 0.12,"## TODO(jmccune): Disabled as per discussion with Aaron.  Re-enable post 0.12 
 # considering public pull requests. 
 # module ""iam_binding_organization"" { 
 #   source        = ""../../.."" 
 #   mode          = var.mode 
 #   organizations = [var.org_id] 
 #   bindings = local.org_bindings 
 # } ","module ""iam_binding_folder"" {
  source  = ""../../..""
  mode    = var.mode
  folders = module.base.folders

  bindings = local.basic_bindings
}
",module,"module ""iam_binding_folder"" {
  source   = ""../../../modules/folders_iam""
  mode     = var.mode
  folders  = module.base.folders
  bindings = local.folder_bindings
}
",module,32,33.0,2897ee8fd869ee2494933856414a8e584ae55e7b,91ff044511481248165cdfcb9cf5e1d5f9b48d77,https://github.com/terraform-google-modules/terraform-google-iam/blob/2897ee8fd869ee2494933856414a8e584ae55e7b/test/fixtures/full/iam.tf#L32,https://github.com/terraform-google-modules/terraform-google-iam/blob/91ff044511481248165cdfcb9cf5e1d5f9b48d77/test/fixtures/helper/iam.tf#L33,2019-07-12 15:16:02-07:00,2023-07-25 14:33:53-05:00,12,0,0,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-log-export,1,test/setup/iam.tf,test/setup/iam.tf,0,workaround,# Adding a pause as a workaround for of the provider issue,"# Adding a pause as a workaround for of the provider issue 
 # https://github.com/terraform-providers/terraform-provider-google/issues/1131","resource ""null_resource"" ""wait_permissions"" {
  # Adding a pause as a workaround for of the provider issue
  # https://github.com/terraform-providers/terraform-provider-google/issues/1131
  provisioner ""local-exec"" {
    command = ""echo sleep 120s for permissions to get granted; sleep 120""
  }
  depends_on = [
    google_billing_account_iam_member.int_test,
    google_folder_iam_member.int_test,
    google_organization_iam_member.int_test,
    google_project_iam_member.int_test
  ]
}
",resource,"resource ""null_resource"" ""wait_permissions"" {
  # Adding a pause as a workaround for of the provider issue
  # https://github.com/terraform-providers/terraform-provider-google/issues/1131
  provisioner ""local-exec"" {
    command = ""echo sleep 120s for permissions to get granted; sleep 120""
  }
  depends_on = [
    google_billing_account_iam_member.int_test,
    google_folder_iam_member.int_test,
    google_organization_iam_member.int_test,
    google_project_iam_member.int_test,
    google_project_iam_member.int_test_logbkt
  ]
}
",resource,114,147.0,3b701bf57a515dc98c2e31c90e8c9a2b2815fa6b,44758c29c820d4c299e4c53e2ff08081d19e7f75,https://github.com/terraform-google-modules/terraform-google-log-export/blob/3b701bf57a515dc98c2e31c90e8c9a2b2815fa6b/test/setup/iam.tf#L114,https://github.com/terraform-google-modules/terraform-google-log-export/blob/44758c29c820d4c299e4c53e2ff08081d19e7f75/test/setup/iam.tf#L147,2019-09-30 19:00:53+03:00,2022-08-09 12:13:44-05:00,4,0,0,0,1,0,0,1,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,363,main.tf,main.tf,0,todo,# TODO - does cluster_encryption_config need to be a list?!,# TODO - does cluster_encryption_config need to be a list?!,"resource ""aws_iam_policy"" ""cluster_encryption"" {
  count = var.create && var.attach_cluster_encryption_policy && length(var.cluster_encryption_config) > 0 ? 1 : 0

  name_prefix = ""${local.iam_role_name}-ClusterEncryption-""
  description = ""Cluster encryption policy to allow cluster role to utilize CMK provided""

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""kms:Encrypt"",
          ""kms:Decrypt"",
          ""kms:ListGrants"",
          ""kms:DescribeKey"",
        ]
        Effect = ""Allow""
        # TODO - does cluster_encryption_config need to be a list?!
        Resource = [for config in var.cluster_encryption_config : config.provider_key_arn]
      },
    ]
  })

  tags = var.tags
}
",resource,"resource ""aws_iam_policy"" ""cluster_encryption"" {
  count = local.create_iam_role && var.attach_cluster_encryption_policy && length(var.cluster_encryption_config) > 0 ? 1 : 0

  name        = var.cluster_encryption_policy_use_name_prefix ? null : local.cluster_encryption_policy_name
  name_prefix = var.cluster_encryption_policy_use_name_prefix ? local.cluster_encryption_policy_name : null
  description = var.cluster_encryption_policy_description
  path        = var.cluster_encryption_policy_path

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""kms:Encrypt"",
          ""kms:Decrypt"",
          ""kms:ListGrants"",
          ""kms:DescribeKey"",
        ]
        Effect   = ""Allow""
        Resource = [for config in var.cluster_encryption_config : config.provider_key_arn]
      },
    ]
  })

  tags = merge(var.tags, var.cluster_encryption_policy_tags)
}
",resource,246,,7644952131a466ca22ba5b3e62cd988e01eff716,2df1572b8a031fbd31a845cc5c61f015ec387f56,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/7644952131a466ca22ba5b3e62cd988e01eff716/main.tf#L246,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/2df1572b8a031fbd31a845cc5c61f015ec387f56/main.tf,2022-03-02 18:29:35+01:00,2022-03-09 15:13:18+01:00,3,1,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1070,examples/gke-serverless/multitenant-fleet/variables.tf,blueprints/gke/multitenant-fleet/variables.tf,1,// todo,// TODO(jccb) is there any situation where the control plane VPC would export any routes?,// TODO(jccb) is there any situation where the control plane VPC would export any routes?,"variable ""peering_config"" {
  description = ""Configure peering with the control plane VPC. Requires compute.networks.updatePeering. Set to null if you don't want to update the default peering configuration.""
  type = object({
    export_routes = bool
    import_routes = bool
  })
  default = {
    export_routes = true
    // TODO(jccb) is there any situation where the control plane VPC would export any routes?
    import_routes = false
  }
}
",variable,the block associated got renamed or deleted,,231,,b1d9b27ac3087d9f2bcc592b2818da84602088f5,e8056577ce1767a975bece532dcddc481ff6bb37,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b1d9b27ac3087d9f2bcc592b2818da84602088f5/examples/gke-serverless/multitenant-fleet/variables.tf#L231,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/e8056577ce1767a975bece532dcddc481ff6bb37/blueprints/gke/multitenant-fleet/variables.tf,2022-08-30 20:39:47+02:00,2022-10-12 12:59:36+02:00,5,1,0,1,0,0,1,0,0,0
https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules,5,modules/vm-bootstrap/main.tf,modules/vm-bootstrap/main.tf,0,workaround,# Workaround: use random_id above to cause the full destroy/create of a file.,"# Live above is equivalent to:   `source = each.key`  but it re-creates the file every time the content changes. 
 # The replace() is not actually doing anything, except tricking Terraform to destroy a resource. 
 # There is a field content_md5 designed specifically for that. But I see a bug in the provider: 
 # When content_md5 is changed, the re-upload seemingly succeeds, result being however a totally empty file (size zero). 
 # Workaround: use random_id above to cause the full destroy/create of a file.","resource ""azurerm_storage_share_file"" ""this"" {
  for_each = var.files

  name             = regex(""[^/]*$"", each.value)
  path             = replace(each.value, ""/[/]*[^/]*$/"", """")
  storage_share_id = azurerm_storage_share.inbound-bootstrap-storage-share.id
  source           = replace(each.key, ""/CalculateMe[X]${random_id.file[each.key].id}/"", ""CalculateMeX${random_id.file[each.key].id}"")
  # Live above is equivalent to:   `source = each.key`  but it re-creates the file every time the content changes.
  # The replace() is not actually doing anything, except tricking Terraform to destroy a resource.
  # There is a field content_md5 designed specifically for that. But I see a bug in the provider: 
  # When content_md5 is changed, the re-upload seemingly succeeds, result being however a totally empty file (size zero).
  # Workaround: use random_id above to cause the full destroy/create of a file.
  depends_on = [
    azurerm_storage_share_directory.inbound-bootstrap-config-directory,
    azurerm_storage_share_directory.bootstrap-storage-directories,
  ]
}
",resource,,,49,0.0,2555bdd9f015bca2d6cda83130c3a365e694d677,451996d8a5926cb97e7d5c4db3534eade518bc84,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/2555bdd9f015bca2d6cda83130c3a365e694d677/modules/vm-bootstrap/main.tf#L49,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/451996d8a5926cb97e7d5c4db3534eade518bc84/modules/vm-bootstrap/main.tf#L0,2021-01-29 10:21:09+01:00,2021-03-31 16:20:24+02:00,8,2,1,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1968,fast/stages/0-bootstrap/automation.tf,fast/stages/0-bootstrap/automation.tf,0,implemented,# account token generation events. This is implemented within the,"# Enable IAM data access logs to capture impersonation and service 
 # account token generation events. This is implemented within the 
 # automation project to limit log volume. For heightened security, 
 # consider enabling it at the organization level. A log sink within 
 # the organization will collect and store these logs in a logging 
 # bucket. See 
 # https://cloud.google.com/iam/docs/audit-logging#audited_operations","module ""automation-project"" {
  source          = ""../../../modules/project""
  billing_account = var.billing_account.id
  name            = ""iac-core-0""
  parent = coalesce(
    var.project_parent_ids.automation, ""organizations/${var.organization.id}""
  )
  prefix = local.prefix
  contacts = (
    var.bootstrap_user != null || var.essential_contacts == null
    ? {}
    : { (var.essential_contacts) = [""ALL""] }
  )
  # human (groups) IAM bindings
  iam_by_principals = {
    (local.principals.gcp-devops) = [
      ""roles/iam.serviceAccountAdmin"",
      ""roles/iam.serviceAccountTokenCreator"",
    ]
    (local.principals.gcp-organization-admins) = [
      ""roles/iam.serviceAccountTokenCreator"",
      ""roles/iam.workloadIdentityPoolAdmin""
    ]
  }
  # machine (service accounts) IAM bindings
  iam = {
    ""roles/browser"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/owner"" = [
      module.automation-tf-bootstrap-sa.iam_email
    ]
    ""roles/cloudbuild.builds.editor"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/cloudbuild.builds.viewer"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/iam.serviceAccountAdmin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/iam.serviceAccountViewer"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/iam.workloadIdentityPoolAdmin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/iam.workloadIdentityPoolViewer"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/source.admin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/source.reader"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/storage.admin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    (module.organization.custom_role_id[""storage_viewer""]) = [
      module.automation-tf-bootstrap-r-sa.iam_email,
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/viewer"" = [
      module.automation-tf-bootstrap-r-sa.iam_email,
      module.automation-tf-resman-r-sa.iam_email
    ]
  }
  iam_bindings = {
    delegated_grants_resman = {
      members = [module.automation-tf-resman-sa.iam_email]
      role    = ""roles/resourcemanager.projectIamAdmin""
      condition = {
        title       = ""resman_delegated_grant""
        description = ""Resource manager service account delegated grant.""
        expression = format(
          ""api.getAttribute('iam.googleapis.com/modifiedGrantsByRole', []).hasOnly(['%s'])"",
          ""roles/serviceusage.serviceUsageConsumer""
        )
      }
    }
  }
  iam_bindings_additive = {
    serviceusage_resman = {
      member = module.automation-tf-resman-sa.iam_email
      role   = ""roles/serviceusage.serviceUsageConsumer""
    }
    serviceusage_resman_r = {
      member = module.automation-tf-resman-r-sa.iam_email
      role   = ""roles/serviceusage.serviceUsageViewer""
    }
  }
  org_policies = var.bootstrap_user != null ? {} : {
    ""compute.skipDefaultNetworkCreation"" = {
      rules = [{ enforce = true }]
    }
    ""iam.automaticIamGrantsForDefaultServiceAccounts"" = {
      rules = [{ enforce = true }]
    }
    ""iam.disableServiceAccountKeyCreation"" = {
      rules = [{ enforce = true }]
    }
  }
  services = concat(
    [
      ""accesscontextmanager.googleapis.com"",
      ""bigquery.googleapis.com"",
      ""bigqueryreservation.googleapis.com"",
      ""bigquerystorage.googleapis.com"",
      ""billingbudgets.googleapis.com"",
      ""cloudasset.googleapis.com"",
      ""cloudbilling.googleapis.com"",
      ""cloudkms.googleapis.com"",
      ""cloudquotas.googleapis.com"",
      ""cloudresourcemanager.googleapis.com"",
      ""essentialcontacts.googleapis.com"",
      ""iam.googleapis.com"",
      ""iamcredentials.googleapis.com"",
      ""orgpolicy.googleapis.com"",
      ""pubsub.googleapis.com"",
      ""servicenetworking.googleapis.com"",
      ""serviceusage.googleapis.com"",
      ""sourcerepo.googleapis.com"",
      ""stackdriver.googleapis.com"",
      ""storage-component.googleapis.com"",
      ""storage.googleapis.com"",
      ""sts.googleapis.com""
    ],
    # enable specific service only after org policies have been applied
    var.bootstrap_user != null ? [] : [
      ""cloudbuild.googleapis.com"",
      ""compute.googleapis.com"",
      ""container.googleapis.com"",
    ]
  )

  # Enable IAM data access logs to capture impersonation and service
  # account token generation events. This is implemented within the
  # automation project to limit log volume. For heightened security,
  # consider enabling it at the organization level. A log sink within
  # the organization will collect and store these logs in a logging
  # bucket. See
  # https://cloud.google.com/iam/docs/audit-logging#audited_operations
  logging_data_access = {
    ""iam.googleapis.com"" = {
      # ADMIN_READ captures impersonation and token generation/exchanges
      ADMIN_READ = []
      # enable DATA_WRITE if you want to capture configuration changes
      # to IAM-related resources (roles, deny policies, service
      # accounts, identity pools, etc)
      # DATA_WRITE = []
    }
  }
}
",module,"module ""automation-project"" {
  source          = ""../../../modules/project""
  billing_account = var.billing_account.id
  name            = ""iac-core-0""
  parent = coalesce(
    var.project_parent_ids.automation, ""organizations/${var.organization.id}""
  )
  prefix = local.prefix
  contacts = (
    var.bootstrap_user != null || var.essential_contacts == null
    ? {}
    : { (var.essential_contacts) = [""ALL""] }
  )
  # human (groups) IAM bindings
  iam_by_principals = {
    (local.principals.gcp-devops) = [
      ""roles/iam.serviceAccountAdmin"",
      ""roles/iam.serviceAccountTokenCreator"",
    ]
    (local.principals.gcp-organization-admins) = [
      ""roles/iam.serviceAccountTokenCreator"",
      ""roles/iam.workloadIdentityPoolAdmin""
    ]
  }
  # machine (service accounts) IAM bindings
  iam = {
    ""roles/browser"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/owner"" = [
      module.automation-tf-bootstrap-sa.iam_email
    ]
    ""roles/cloudbuild.builds.editor"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/cloudbuild.builds.viewer"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/iam.serviceAccountAdmin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/iam.serviceAccountViewer"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/iam.workloadIdentityPoolAdmin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/iam.workloadIdentityPoolViewer"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/source.admin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    ""roles/source.reader"" = [
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/storage.admin"" = [
      module.automation-tf-resman-sa.iam_email
    ]
    (module.organization.custom_role_id[""storage_viewer""]) = [
      module.automation-tf-bootstrap-r-sa.iam_email,
      module.automation-tf-resman-r-sa.iam_email
    ]
    ""roles/viewer"" = [
      module.automation-tf-bootstrap-r-sa.iam_email,
      module.automation-tf-resman-r-sa.iam_email
    ]
  }
  iam_bindings = {
    delegated_grants_resman = {
      members = [module.automation-tf-resman-sa.iam_email]
      role    = ""roles/resourcemanager.projectIamAdmin""
      condition = {
        title       = ""resman_delegated_grant""
        description = ""Resource manager service account delegated grant.""
        expression = format(
          ""api.getAttribute('iam.googleapis.com/modifiedGrantsByRole', []).hasOnly(['%s'])"",
          ""roles/serviceusage.serviceUsageConsumer""
        )
      }
    }
  }
  iam_bindings_additive = {
    serviceusage_resman = {
      member = module.automation-tf-resman-sa.iam_email
      role   = ""roles/serviceusage.serviceUsageConsumer""
    }
    serviceusage_resman_r = {
      member = module.automation-tf-resman-r-sa.iam_email
      role   = ""roles/serviceusage.serviceUsageViewer""
    }
  }
  org_policies = var.bootstrap_user != null ? {} : {
    ""compute.skipDefaultNetworkCreation"" = {
      rules = [{ enforce = true }]
    }
    ""iam.automaticIamGrantsForDefaultServiceAccounts"" = {
      rules = [{ enforce = true }]
    }
    ""iam.disableServiceAccountKeyCreation"" = {
      rules = [{ enforce = true }]
    }
  }
  services = concat(
    [
      ""accesscontextmanager.googleapis.com"",
      ""bigquery.googleapis.com"",
      ""bigqueryreservation.googleapis.com"",
      ""bigquerystorage.googleapis.com"",
      ""billingbudgets.googleapis.com"",
      ""cloudasset.googleapis.com"",
      ""cloudbilling.googleapis.com"",
      ""cloudkms.googleapis.com"",
      ""cloudquotas.googleapis.com"",
      ""cloudresourcemanager.googleapis.com"",
      ""essentialcontacts.googleapis.com"",
      ""iam.googleapis.com"",
      ""iamcredentials.googleapis.com"",
      ""orgpolicy.googleapis.com"",
      ""pubsub.googleapis.com"",
      ""servicenetworking.googleapis.com"",
      ""serviceusage.googleapis.com"",
      ""sourcerepo.googleapis.com"",
      ""stackdriver.googleapis.com"",
      ""storage-component.googleapis.com"",
      ""storage.googleapis.com"",
      ""sts.googleapis.com""
    ],
    # enable specific service only after org policies have been applied
    var.bootstrap_user != null ? [] : [
      ""cloudbuild.googleapis.com"",
      ""compute.googleapis.com"",
      ""container.googleapis.com"",
    ]
  )

  # Enable IAM data access logs to capture impersonation and service
  # account token generation events. This is implemented within the
  # automation project to limit log volume. For heightened security,
  # consider enabling it at the organization level. A log sink within
  # the organization will collect and store these logs in a logging
  # bucket. See
  # https://cloud.google.com/iam/docs/audit-logging#audited_operations
  logging_data_access = {
    ""iam.googleapis.com"" = {
      # ADMIN_READ captures impersonation and token generation/exchanges
      ADMIN_READ = []
      # enable DATA_WRITE if you want to capture configuration changes
      # to IAM-related resources (roles, deny policies, service
      # accounts, identity pools, etc)
      # DATA_WRITE = []
    }
  }
}
",module,161,163.0,99129d54a37da4c2d977b7db705de5024d530944,be9214f99a0718eb8698e9d539e2ad93cb442ac7,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/99129d54a37da4c2d977b7db705de5024d530944/fast/stages/0-bootstrap/automation.tf#L161,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/be9214f99a0718eb8698e9d539e2ad93cb442ac7/fast/stages/0-bootstrap/automation.tf#L163,2024-04-25 08:31:51+02:00,2024-05-21 10:39:47+02:00,2,0,0,0,0,1,0,0,1,0
https://github.com/Azure/sap-automation,19,deploy/terraform/terraform-units/modules/sap_library/storage_accounts.tf,deploy/terraform/terraform-units/modules/sap_library/storage_accounts.tf,0,// todo,// TODO: soft delete for file share,// TODO: soft delete for file share,"resource ""azurerm_storage_account"" ""storage_sapbits"" {
  provider                  = azurerm.main
  count                     = local.sa_sapbits_exists ? 0 : 1
  name                      = local.sa_sapbits_name
  resource_group_name       = local.rg_name
  location                  = local.rg_library_location
  account_replication_type  = local.sa_sapbits_account_replication_type
  account_tier              = local.sa_sapbits_account_tier
  account_kind              = local.sa_sapbits_account_kind
  enable_https_traffic_only = local.sa_sapbits_enable_secure_transfer
  // To support all access levels 'Blob' 'Private' and 'Container'
  allow_blob_public_access = true
  // TODO: soft delete for file share

  network_rules {
    default_action = ""Allow""
    ip_rules = var.use_private_endpoint ? (
      [length(local.deployer_public_ip_address) > 0 ? local.deployer_public_ip_address : null]) : (
      []
    )

    virtual_network_subnet_ids = var.use_private_endpoint ? [local.subnet_mgmt_id] : []
  }
}
",resource,"resource ""azurerm_storage_account"" ""storage_sapbits"" {
  provider                  = azurerm.main
  count                     = local.sa_sapbits_exists ? 0 : 1
  name                      = local.sa_sapbits_name
  resource_group_name       = local.rg_name
  location                  = local.rg_library_location
  account_replication_type  = local.sa_sapbits_account_replication_type
  account_tier              = local.sa_sapbits_account_tier
  account_kind              = local.sa_sapbits_account_kind
  enable_https_traffic_only = local.sa_sapbits_enable_secure_transfer

  network_rules {
    default_action = ""Allow""
    ip_rules = var.use_private_endpoint ? (
      [length(local.deployer_public_ip_address) > 0 ? local.deployer_public_ip_address : null]) : (
      []
    )

    virtual_network_subnet_ids = var.use_private_endpoint ? [local.subnet_management_id] : []
  }
}
",resource,115,,6ff0b891114c36d3aeccb850d830b698cd1fe52a,6343a9003b2bdbfa90c242903fabd92b8a3d0322,https://github.com/Azure/sap-automation/blob/6ff0b891114c36d3aeccb850d830b698cd1fe52a/deploy/terraform/terraform-units/modules/sap_library/storage_accounts.tf#L115,https://github.com/Azure/sap-automation/blob/6343a9003b2bdbfa90c242903fabd92b8a3d0322/deploy/terraform/terraform-units/modules/sap_library/storage_accounts.tf,2021-11-17 19:29:07+02:00,2022-03-25 09:58:42+02:00,6,1,1,1,0,1,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,331,modules/kubernetes-addons/main.tf,modules/kubernetes-addons/main.tf,0,todo,# TODO - remove local source and revert to remote once,"# source  = ""tetratelabs/tetrate-istio-addon/eksblueprints"" 
 # version = ""0.0.7""  
 # TODO - remove local source and revert to remote once 
 # https://github.com/tetratelabs/terraform-eksblueprints-tetrate-istio-addon/pull/12  is merged","module ""tetrate_istio"" {
  # source  = ""tetratelabs/tetrate-istio-addon/eksblueprints""
  # version = ""0.0.7""

  # TODO - remove local source and revert to remote once
  # https://github.com/tetratelabs/terraform-eksblueprints-tetrate-istio-addon/pull/12  is merged
  source = ""./tetrate-istio""

  count = var.enable_tetrate_istio ? 1 : 0

  distribution         = var.tetrate_istio_distribution
  distribution_version = var.tetrate_istio_version
  install_base         = var.tetrate_istio_install_base
  install_cni          = var.tetrate_istio_install_cni
  install_istiod       = var.tetrate_istio_install_istiod
  install_gateway      = var.tetrate_istio_install_gateway
  base_helm_config     = var.tetrate_istio_base_helm_config
  cni_helm_config      = var.tetrate_istio_cni_helm_config
  istiod_helm_config   = var.tetrate_istio_istiod_helm_config
  gateway_helm_config  = var.tetrate_istio_gateway_helm_config
  manage_via_gitops    = var.argocd_manage_add_ons
  addon_context        = local.addon_context
}
",module,,,347,0.0,fd55f69d2f73bd1e975d90826be76e9344bb769b,19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/fd55f69d2f73bd1e975d90826be76e9344bb769b/modules/kubernetes-addons/main.tf#L347,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1/modules/kubernetes-addons/main.tf#L0,2022-10-10 12:47:57-04:00,2023-06-05 10:07:47-04:00,28,2,1,1,1,0,0,0,0,0
https://github.com/awslabs/data-on-eks,18,ai-ml/trainium-inferentia/addons.tf,ai-ml/trainium-inferentia/addons.tf,0,implement,# Commented for visiblity to implement this feature in the future,"  # Commented for visiblity to implement this feature in the future
  #  placement {
  #   tenancy = ""default""
  #   availability_zone = ""${local.region}d""
  #   group_name        = local.karpenter_trn1_32xl_lt_name
  # }","resource ""aws_launch_template"" ""trn1_lt"" {
  name        = local.karpenter_trn1_32xl_lt_name
  description = ""Karpenter Trn1.32xlarge Launch Template""

  user_data = data.cloudinit_config.trn1_lt.rendered

  ebs_optimized = true

  image_id = data.aws_ami.eks_gpu.id

  iam_instance_profile {
    name = module.eks_blueprints_addons.karpenter.node_instance_profile_name
  }

  # Commented for visiblity to implement this feature in the future
  #  placement {
  #   tenancy = ""default""
  #   availability_zone = ""${local.region}d""
  #   group_name        = local.karpenter_trn1_32xl_lt_name
  # }

  metadata_options {
    http_endpoint               = ""enabled""
    http_tokens                 = ""required""
    http_put_response_hop_limit = 2
  }

  block_device_mappings {
    device_name = ""/dev/xvda""
    ebs {
      volume_size           = 100
      delete_on_termination = true
      volume_type           = ""gp3""
    }
  }

  monitoring {
    enabled = true
  }

  tag_specifications {
    resource_type = ""instance""

    tags = merge(local.tags, {
      ""karpenter.sh/discovery"" = local.name
    })
  }

  # First network interface with device_index=0 and network_card_index=0
  network_interfaces {
    device_index                = 0
    network_card_index          = 0
    associate_public_ip_address = false
    interface_type              = ""efa""
    delete_on_termination       = true
    security_groups             = [module.eks.node_security_group_id]
    description                 = ""Karpenter EFA config for Trainium""
  }

  # Additional network interfaces with device_index=1 and network_card_index ranging from 1 to 7
  dynamic ""network_interfaces"" {
    for_each = range(1, 8) # Create 7 additional network interfaces
    content {
      device_index                = 1
      network_card_index          = network_interfaces.value
      associate_public_ip_address = false
      interface_type              = ""efa""
      delete_on_termination       = true
      security_groups             = [module.eks.node_security_group_id]
      description                 = ""Karpenter EFA config for Trainium""
    }
  }
}
",resource,"resource ""aws_launch_template"" ""trn1_lt"" {
  name        = local.karpenter_trn1_32xl_lt_name
  description = ""Karpenter Trn1.32xlarge Launch Template""

  user_data = data.cloudinit_config.trn1_lt.rendered

  ebs_optimized = true

  image_id = data.aws_ami.eks_gpu.id

  iam_instance_profile {
    name = module.eks_blueprints_addons.karpenter.node_instance_profile_name
  }

  # Commented for visibility to implement this feature in the future
  #  placement {
  #   tenancy = ""default""
  #   availability_zone = ""${local.region}d""
  #   group_name        = local.karpenter_trn1_32xl_lt_name
  # }

  metadata_options {
    http_endpoint               = ""enabled""
    http_tokens                 = ""required""
    http_put_response_hop_limit = 2
  }

  block_device_mappings {
    device_name = ""/dev/xvda""
    ebs {
      volume_size           = 100
      delete_on_termination = true
      volume_type           = ""gp3""
    }
  }

  monitoring {
    enabled = true
  }

  tag_specifications {
    resource_type = ""instance""

    tags = merge(local.tags, {
      ""karpenter.sh/discovery"" = local.name
    })
  }

  # First network interface with device_index=0 and network_card_index=0
  network_interfaces {
    device_index                = 0
    network_card_index          = 0
    associate_public_ip_address = false
    interface_type              = ""efa""
    delete_on_termination       = true
    security_groups             = [module.eks.node_security_group_id]
    description                 = ""Karpenter EFA config for Trainium""
  }

  # Additional network interfaces with device_index=1 and network_card_index ranging from 1 to 7
  dynamic ""network_interfaces"" {
    for_each = range(1, 8) # Create 7 additional network interfaces
    content {
      device_index                = 1
      network_card_index          = network_interfaces.value
      associate_public_ip_address = false
      interface_type              = ""efa""
      delete_on_termination       = true
      security_groups             = [module.eks.node_security_group_id]
      description                 = ""Karpenter EFA config for Trainium""
    }
  }
}
",resource,424,,39e790ce0d4e45979d1374a86b2030e55a838441,352456833a6bee906dd49d28ad050c8f048c767b,https://github.com/awslabs/data-on-eks/blob/39e790ce0d4e45979d1374a86b2030e55a838441/ai-ml/trainium-inferentia/addons.tf#L424,https://github.com/awslabs/data-on-eks/blob/352456833a6bee906dd49d28ad050c8f048c767b/ai-ml/trainium-inferentia/addons.tf,2023-08-17 19:17:31+01:00,2023-12-08 15:47:41-05:00,9,1,1,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-sql-db,3,modules/backup/variables.tf,modules/backup/variables.tf,0,#todo,#TODO: test,"error_message = ""Must be a full GCS URI starting with gs://."" #TODO: test","variable ""export_uri"" {
  description = ""The bucket and path uri for exporting to GCS""
  type        = string
  validation {
    condition     = can(regex(""^gs:\\/\\/"", var.export_uri))
    error_message = ""Must be a full GCS URI starting with gs://."" #TODO: test
  }
}
",variable,"variable ""export_uri"" {
  description = ""The bucket and path uri for exporting to GCS""
  type        = string
  validation {
    condition     = can(regex(""^gs:\\/\\/"", var.export_uri))
    error_message = ""Must be a full GCS URI starting with gs://.""
  }
}
",variable,87,,c51bf296e392fca246aae1c9ba4299a5a97ef274,b1ef34d0ee1a84ef2c0be4141cb83448052264da,https://github.com/terraform-google-modules/terraform-google-sql-db/blob/c51bf296e392fca246aae1c9ba4299a5a97ef274/modules/backup/variables.tf#L87,https://github.com/terraform-google-modules/terraform-google-sql-db/blob/b1ef34d0ee1a84ef2c0be4141cb83448052264da/modules/backup/variables.tf,2022-05-13 10:59:53-05:00,2022-08-09 09:38:28-05:00,2,1,0,0,0,0,0,0,0,1
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1327,blueprints/data-solutions/shielded-folder/log-export.tf,blueprints/data-solutions/shielded-folder/log-export.tf,0,#todo,#TODO check if logging bucket support encryption.,#TODO check if logging bucket support encryption.,"module ""log-export-logbucket"" {
  source      = ""../../../modules/logging-bucket""
  for_each    = toset([for k, v in var.log_sinks : k if v.type == ""logging""])
  parent_type = ""project""
  parent      = module.log-export-project.project_id
  id          = ""audit-logs-${each.key}""
  location    = var.log_locations.logging
  #TODO check if logging bucket support encryption.
}
",module,"module ""log-export-logbucket"" {
  source      = ""../../../modules/logging-bucket""
  for_each    = var.enable_features.log_sink ? toset([for k, v in var.log_sinks : k if v.type == ""logging""]) : []
  parent_type = ""project""
  parent      = module.log-export-project[0].project_id
  id          = ""audit-logs-${each.key}""
  location    = var.log_locations.logging
  #TODO check if logging bucket support encryption.
}
",module,85,124.0,4007d42705a930e9e526a8da3616712ad0e646f6,3af7e257d21f889ffaf7b32a3bab974fdbfda6e4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/4007d42705a930e9e526a8da3616712ad0e646f6/blueprints/data-solutions/shielded-folder/log-export.tf#L85,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3af7e257d21f889ffaf7b32a3bab974fdbfda6e4/blueprints/data-solutions/shielded-folder/log-export.tf#L124,2023-01-21 01:08:51+01:00,2024-04-17 10:23:48+02:00,11,0,1,0,0,1,0,0,1,1
https://github.com/terraform-aws-modules/terraform-aws-eks,648,node_groups.tf,node_groups.tf,0,todo,# TODO - update this when `var.platform` is removed in v21.0,# TODO - update this when `var.platform` is removed in v21.0,"module ""self_managed_node_group"" {
  source = ""./modules/self-managed-node-group""

  for_each = { for k, v in var.self_managed_node_groups : k => v if var.create }

  create = try(each.value.create, true)

  cluster_name = time_sleep.this[0].triggers[""cluster_name""]

  # Autoscaling Group
  create_autoscaling_group = try(each.value.create_autoscaling_group, var.self_managed_node_group_defaults.create_autoscaling_group, true)

  name            = try(each.value.name, each.key)
  use_name_prefix = try(each.value.use_name_prefix, var.self_managed_node_group_defaults.use_name_prefix, true)

  availability_zones = try(each.value.availability_zones, var.self_managed_node_group_defaults.availability_zones, null)
  subnet_ids         = try(each.value.subnet_ids, var.self_managed_node_group_defaults.subnet_ids, var.subnet_ids)

  min_size                  = try(each.value.min_size, var.self_managed_node_group_defaults.min_size, 0)
  max_size                  = try(each.value.max_size, var.self_managed_node_group_defaults.max_size, 3)
  desired_size              = try(each.value.desired_size, var.self_managed_node_group_defaults.desired_size, 1)
  capacity_rebalance        = try(each.value.capacity_rebalance, var.self_managed_node_group_defaults.capacity_rebalance, null)
  min_elb_capacity          = try(each.value.min_elb_capacity, var.self_managed_node_group_defaults.min_elb_capacity, null)
  wait_for_elb_capacity     = try(each.value.wait_for_elb_capacity, var.self_managed_node_group_defaults.wait_for_elb_capacity, null)
  wait_for_capacity_timeout = try(each.value.wait_for_capacity_timeout, var.self_managed_node_group_defaults.wait_for_capacity_timeout, null)
  default_cooldown          = try(each.value.default_cooldown, var.self_managed_node_group_defaults.default_cooldown, null)
  default_instance_warmup   = try(each.value.default_instance_warmup, var.self_managed_node_group_defaults.default_instance_warmup, null)
  protect_from_scale_in     = try(each.value.protect_from_scale_in, var.self_managed_node_group_defaults.protect_from_scale_in, null)
  context                   = try(each.value.context, var.self_managed_node_group_defaults.context, null)

  target_group_arns         = try(each.value.target_group_arns, var.self_managed_node_group_defaults.target_group_arns, [])
  placement_group           = try(each.value.placement_group, var.self_managed_node_group_defaults.placement_group, null)
  health_check_type         = try(each.value.health_check_type, var.self_managed_node_group_defaults.health_check_type, null)
  health_check_grace_period = try(each.value.health_check_grace_period, var.self_managed_node_group_defaults.health_check_grace_period, null)

  force_delete           = try(each.value.force_delete, var.self_managed_node_group_defaults.force_delete, null)
  force_delete_warm_pool = try(each.value.force_delete_warm_pool, var.self_managed_node_group_defaults.force_delete_warm_pool, null)
  termination_policies   = try(each.value.termination_policies, var.self_managed_node_group_defaults.termination_policies, [])
  suspended_processes    = try(each.value.suspended_processes, var.self_managed_node_group_defaults.suspended_processes, [])
  max_instance_lifetime  = try(each.value.max_instance_lifetime, var.self_managed_node_group_defaults.max_instance_lifetime, null)

  enabled_metrics         = try(each.value.enabled_metrics, var.self_managed_node_group_defaults.enabled_metrics, [])
  metrics_granularity     = try(each.value.metrics_granularity, var.self_managed_node_group_defaults.metrics_granularity, null)
  service_linked_role_arn = try(each.value.service_linked_role_arn, var.self_managed_node_group_defaults.service_linked_role_arn, null)

  initial_lifecycle_hooks     = try(each.value.initial_lifecycle_hooks, var.self_managed_node_group_defaults.initial_lifecycle_hooks, [])
  instance_maintenance_policy = try(each.value.instance_maintenance_policy, var.self_managed_node_group_defaults.instance_maintenance_policy, {})
  instance_refresh            = try(each.value.instance_refresh, var.self_managed_node_group_defaults.instance_refresh, local.default_instance_refresh)
  use_mixed_instances_policy  = try(each.value.use_mixed_instances_policy, var.self_managed_node_group_defaults.use_mixed_instances_policy, false)
  mixed_instances_policy      = try(each.value.mixed_instances_policy, var.self_managed_node_group_defaults.mixed_instances_policy, null)
  warm_pool                   = try(each.value.warm_pool, var.self_managed_node_group_defaults.warm_pool, {})

  delete_timeout         = try(each.value.delete_timeout, var.self_managed_node_group_defaults.delete_timeout, null)
  autoscaling_group_tags = try(each.value.autoscaling_group_tags, var.self_managed_node_group_defaults.autoscaling_group_tags, {})

  # User data
  platform = try(each.value.platform, var.self_managed_node_group_defaults.platform, ""linux"")
  # TODO - update this when `var.platform` is removed in v21.0
  ami_type                 = try(each.value.ami_type, var.self_managed_node_group_defaults.ami_type, ""AL2_x86_64"")
  cluster_endpoint         = try(time_sleep.this[0].triggers[""cluster_endpoint""], """")
  cluster_auth_base64      = try(time_sleep.this[0].triggers[""cluster_certificate_authority_data""], """")
  cluster_service_cidr     = try(time_sleep.this[0].triggers[""cluster_service_cidr""], """")
  cluster_ip_family        = var.cluster_ip_family
  pre_bootstrap_user_data  = try(each.value.pre_bootstrap_user_data, var.self_managed_node_group_defaults.pre_bootstrap_user_data, """")
  post_bootstrap_user_data = try(each.value.post_bootstrap_user_data, var.self_managed_node_group_defaults.post_bootstrap_user_data, """")
  bootstrap_extra_args     = try(each.value.bootstrap_extra_args, var.self_managed_node_group_defaults.bootstrap_extra_args, """")
  user_data_template_path  = try(each.value.user_data_template_path, var.self_managed_node_group_defaults.user_data_template_path, """")
  cloudinit_pre_nodeadm    = try(each.value.cloudinit_pre_nodeadm, var.self_managed_node_group_defaults.cloudinit_pre_nodeadm, [])
  cloudinit_post_nodeadm   = try(each.value.cloudinit_post_nodeadm, var.self_managed_node_group_defaults.cloudinit_post_nodeadm, [])

  # Launch Template
  create_launch_template                 = try(each.value.create_launch_template, var.self_managed_node_group_defaults.create_launch_template, true)
  launch_template_id                     = try(each.value.launch_template_id, var.self_managed_node_group_defaults.launch_template_id, """")
  launch_template_name                   = try(each.value.launch_template_name, var.self_managed_node_group_defaults.launch_template_name, each.key)
  launch_template_use_name_prefix        = try(each.value.launch_template_use_name_prefix, var.self_managed_node_group_defaults.launch_template_use_name_prefix, true)
  launch_template_version                = try(each.value.launch_template_version, var.self_managed_node_group_defaults.launch_template_version, null)
  launch_template_default_version        = try(each.value.launch_template_default_version, var.self_managed_node_group_defaults.launch_template_default_version, null)
  update_launch_template_default_version = try(each.value.update_launch_template_default_version, var.self_managed_node_group_defaults.update_launch_template_default_version, true)
  launch_template_description            = try(each.value.launch_template_description, var.self_managed_node_group_defaults.launch_template_description, ""Custom launch template for ${try(each.value.name, each.key)} self managed node group"")
  launch_template_tags                   = try(each.value.launch_template_tags, var.self_managed_node_group_defaults.launch_template_tags, {})
  tag_specifications                     = try(each.value.tag_specifications, var.self_managed_node_group_defaults.tag_specifications, [""instance"", ""volume"", ""network-interface""])

  ebs_optimized   = try(each.value.ebs_optimized, var.self_managed_node_group_defaults.ebs_optimized, null)
  ami_id          = try(each.value.ami_id, var.self_managed_node_group_defaults.ami_id, """")
  cluster_version = try(each.value.cluster_version, var.self_managed_node_group_defaults.cluster_version, time_sleep.this[0].triggers[""cluster_version""])
  instance_type   = try(each.value.instance_type, var.self_managed_node_group_defaults.instance_type, ""m6i.large"")
  key_name        = try(each.value.key_name, var.self_managed_node_group_defaults.key_name, null)

  disable_api_termination              = try(each.value.disable_api_termination, var.self_managed_node_group_defaults.disable_api_termination, null)
  instance_initiated_shutdown_behavior = try(each.value.instance_initiated_shutdown_behavior, var.self_managed_node_group_defaults.instance_initiated_shutdown_behavior, null)
  kernel_id                            = try(each.value.kernel_id, var.self_managed_node_group_defaults.kernel_id, null)
  ram_disk_id                          = try(each.value.ram_disk_id, var.self_managed_node_group_defaults.ram_disk_id, null)

  block_device_mappings              = try(each.value.block_device_mappings, var.self_managed_node_group_defaults.block_device_mappings, {})
  capacity_reservation_specification = try(each.value.capacity_reservation_specification, var.self_managed_node_group_defaults.capacity_reservation_specification, {})
  cpu_options                        = try(each.value.cpu_options, var.self_managed_node_group_defaults.cpu_options, {})
  credit_specification               = try(each.value.credit_specification, var.self_managed_node_group_defaults.credit_specification, {})
  elastic_gpu_specifications         = try(each.value.elastic_gpu_specifications, var.self_managed_node_group_defaults.elastic_gpu_specifications, {})
  elastic_inference_accelerator      = try(each.value.elastic_inference_accelerator, var.self_managed_node_group_defaults.elastic_inference_accelerator, {})
  enclave_options                    = try(each.value.enclave_options, var.self_managed_node_group_defaults.enclave_options, {})
  hibernation_options                = try(each.value.hibernation_options, var.self_managed_node_group_defaults.hibernation_options, {})
  instance_requirements              = try(each.value.instance_requirements, var.self_managed_node_group_defaults.instance_requirements, {})
  instance_market_options            = try(each.value.instance_market_options, var.self_managed_node_group_defaults.instance_market_options, {})
  license_specifications             = try(each.value.license_specifications, var.self_managed_node_group_defaults.license_specifications, {})
  metadata_options                   = try(each.value.metadata_options, var.self_managed_node_group_defaults.metadata_options, local.metadata_options)
  enable_monitoring                  = try(each.value.enable_monitoring, var.self_managed_node_group_defaults.enable_monitoring, true)
  enable_efa_support                 = try(each.value.enable_efa_support, var.self_managed_node_group_defaults.enable_efa_support, false)
  network_interfaces                 = try(each.value.network_interfaces, var.self_managed_node_group_defaults.network_interfaces, [])
  placement                          = try(each.value.placement, var.self_managed_node_group_defaults.placement, {})
  maintenance_options                = try(each.value.maintenance_options, var.self_managed_node_group_defaults.maintenance_options, {})
  private_dns_name_options           = try(each.value.private_dns_name_options, var.self_managed_node_group_defaults.private_dns_name_options, {})

  # IAM role
  create_iam_instance_profile   = try(each.value.create_iam_instance_profile, var.self_managed_node_group_defaults.create_iam_instance_profile, true)
  iam_instance_profile_arn      = try(each.value.iam_instance_profile_arn, var.self_managed_node_group_defaults.iam_instance_profile_arn, null)
  iam_role_name                 = try(each.value.iam_role_name, var.self_managed_node_group_defaults.iam_role_name, null)
  iam_role_use_name_prefix      = try(each.value.iam_role_use_name_prefix, var.self_managed_node_group_defaults.iam_role_use_name_prefix, true)
  iam_role_path                 = try(each.value.iam_role_path, var.self_managed_node_group_defaults.iam_role_path, null)
  iam_role_description          = try(each.value.iam_role_description, var.self_managed_node_group_defaults.iam_role_description, ""Self managed node group IAM role"")
  iam_role_permissions_boundary = try(each.value.iam_role_permissions_boundary, var.self_managed_node_group_defaults.iam_role_permissions_boundary, null)
  iam_role_tags                 = try(each.value.iam_role_tags, var.self_managed_node_group_defaults.iam_role_tags, {})
  iam_role_attach_cni_policy    = try(each.value.iam_role_attach_cni_policy, var.self_managed_node_group_defaults.iam_role_attach_cni_policy, true)
  # To better understand why this `lookup()` logic is required, see:
  # https://github.com/hashicorp/terraform/issues/31646#issuecomment-1217279031
  iam_role_additional_policies = lookup(each.value, ""iam_role_additional_policies"", lookup(var.self_managed_node_group_defaults, ""iam_role_additional_policies"", {}))

  # Access entry
  create_access_entry = try(each.value.create_access_entry, var.self_managed_node_group_defaults.create_access_entry, true)
  iam_role_arn        = try(each.value.iam_role_arn, var.self_managed_node_group_defaults.iam_role_arn, null)

  # Autoscaling group schedule
  create_schedule = try(each.value.create_schedule, var.self_managed_node_group_defaults.create_schedule, true)
  schedules       = try(each.value.schedules, var.self_managed_node_group_defaults.schedules, {})

  # Security group
  vpc_security_group_ids            = compact(concat([local.node_security_group_id], try(each.value.vpc_security_group_ids, var.self_managed_node_group_defaults.vpc_security_group_ids, [])))
  cluster_primary_security_group_id = try(each.value.attach_cluster_primary_security_group, var.self_managed_node_group_defaults.attach_cluster_primary_security_group, false) ? aws_eks_cluster.this[0].vpc_config[0].cluster_security_group_id : null

  tags = merge(var.tags, try(each.value.tags, var.self_managed_node_group_defaults.tags, {}))
}
",module,"module ""self_managed_node_group"" {
  source = ""./modules/self-managed-node-group""

  for_each = { for k, v in var.self_managed_node_groups : k => v if var.create }

  create = try(each.value.create, true)

  cluster_name = time_sleep.this[0].triggers[""cluster_name""]

  # Autoscaling Group
  create_autoscaling_group = try(each.value.create_autoscaling_group, var.self_managed_node_group_defaults.create_autoscaling_group, true)

  name            = try(each.value.name, each.key)
  use_name_prefix = try(each.value.use_name_prefix, var.self_managed_node_group_defaults.use_name_prefix, true)

  availability_zones = try(each.value.availability_zones, var.self_managed_node_group_defaults.availability_zones, null)
  subnet_ids         = try(each.value.subnet_ids, var.self_managed_node_group_defaults.subnet_ids, var.subnet_ids)

  min_size                  = try(each.value.min_size, var.self_managed_node_group_defaults.min_size, 0)
  max_size                  = try(each.value.max_size, var.self_managed_node_group_defaults.max_size, 3)
  desired_size              = try(each.value.desired_size, var.self_managed_node_group_defaults.desired_size, 1)
  capacity_rebalance        = try(each.value.capacity_rebalance, var.self_managed_node_group_defaults.capacity_rebalance, null)
  min_elb_capacity          = try(each.value.min_elb_capacity, var.self_managed_node_group_defaults.min_elb_capacity, null)
  wait_for_elb_capacity     = try(each.value.wait_for_elb_capacity, var.self_managed_node_group_defaults.wait_for_elb_capacity, null)
  wait_for_capacity_timeout = try(each.value.wait_for_capacity_timeout, var.self_managed_node_group_defaults.wait_for_capacity_timeout, null)
  default_cooldown          = try(each.value.default_cooldown, var.self_managed_node_group_defaults.default_cooldown, null)
  default_instance_warmup   = try(each.value.default_instance_warmup, var.self_managed_node_group_defaults.default_instance_warmup, null)
  protect_from_scale_in     = try(each.value.protect_from_scale_in, var.self_managed_node_group_defaults.protect_from_scale_in, null)
  context                   = try(each.value.context, var.self_managed_node_group_defaults.context, null)

  target_group_arns         = try(each.value.target_group_arns, var.self_managed_node_group_defaults.target_group_arns, [])
  placement_group           = try(each.value.placement_group, var.self_managed_node_group_defaults.placement_group, null)
  health_check_type         = try(each.value.health_check_type, var.self_managed_node_group_defaults.health_check_type, null)
  health_check_grace_period = try(each.value.health_check_grace_period, var.self_managed_node_group_defaults.health_check_grace_period, null)

  force_delete           = try(each.value.force_delete, var.self_managed_node_group_defaults.force_delete, null)
  force_delete_warm_pool = try(each.value.force_delete_warm_pool, var.self_managed_node_group_defaults.force_delete_warm_pool, null)
  termination_policies   = try(each.value.termination_policies, var.self_managed_node_group_defaults.termination_policies, [])
  suspended_processes    = try(each.value.suspended_processes, var.self_managed_node_group_defaults.suspended_processes, [])
  max_instance_lifetime  = try(each.value.max_instance_lifetime, var.self_managed_node_group_defaults.max_instance_lifetime, null)

  enabled_metrics         = try(each.value.enabled_metrics, var.self_managed_node_group_defaults.enabled_metrics, [])
  metrics_granularity     = try(each.value.metrics_granularity, var.self_managed_node_group_defaults.metrics_granularity, null)
  service_linked_role_arn = try(each.value.service_linked_role_arn, var.self_managed_node_group_defaults.service_linked_role_arn, null)

  initial_lifecycle_hooks     = try(each.value.initial_lifecycle_hooks, var.self_managed_node_group_defaults.initial_lifecycle_hooks, [])
  instance_maintenance_policy = try(each.value.instance_maintenance_policy, var.self_managed_node_group_defaults.instance_maintenance_policy, {})
  instance_refresh            = try(each.value.instance_refresh, var.self_managed_node_group_defaults.instance_refresh, local.default_instance_refresh)
  use_mixed_instances_policy  = try(each.value.use_mixed_instances_policy, var.self_managed_node_group_defaults.use_mixed_instances_policy, false)
  mixed_instances_policy      = try(each.value.mixed_instances_policy, var.self_managed_node_group_defaults.mixed_instances_policy, null)
  warm_pool                   = try(each.value.warm_pool, var.self_managed_node_group_defaults.warm_pool, {})

  delete_timeout         = try(each.value.delete_timeout, var.self_managed_node_group_defaults.delete_timeout, null)
  autoscaling_group_tags = try(each.value.autoscaling_group_tags, var.self_managed_node_group_defaults.autoscaling_group_tags, {})

  # User data
  platform = try(each.value.platform, var.self_managed_node_group_defaults.platform, ""linux"")
  # TODO - update this when `var.platform` is removed in v21.0
  ami_type                 = try(each.value.ami_type, var.self_managed_node_group_defaults.ami_type, ""AL2_x86_64"")
  cluster_endpoint         = try(time_sleep.this[0].triggers[""cluster_endpoint""], """")
  cluster_auth_base64      = try(time_sleep.this[0].triggers[""cluster_certificate_authority_data""], """")
  cluster_service_cidr     = try(time_sleep.this[0].triggers[""cluster_service_cidr""], """")
  cluster_ip_family        = var.cluster_ip_family
  pre_bootstrap_user_data  = try(each.value.pre_bootstrap_user_data, var.self_managed_node_group_defaults.pre_bootstrap_user_data, """")
  post_bootstrap_user_data = try(each.value.post_bootstrap_user_data, var.self_managed_node_group_defaults.post_bootstrap_user_data, """")
  bootstrap_extra_args     = try(each.value.bootstrap_extra_args, var.self_managed_node_group_defaults.bootstrap_extra_args, """")
  user_data_template_path  = try(each.value.user_data_template_path, var.self_managed_node_group_defaults.user_data_template_path, """")
  cloudinit_pre_nodeadm    = try(each.value.cloudinit_pre_nodeadm, var.self_managed_node_group_defaults.cloudinit_pre_nodeadm, [])
  cloudinit_post_nodeadm   = try(each.value.cloudinit_post_nodeadm, var.self_managed_node_group_defaults.cloudinit_post_nodeadm, [])

  # Launch Template
  create_launch_template                 = try(each.value.create_launch_template, var.self_managed_node_group_defaults.create_launch_template, true)
  launch_template_id                     = try(each.value.launch_template_id, var.self_managed_node_group_defaults.launch_template_id, """")
  launch_template_name                   = try(each.value.launch_template_name, var.self_managed_node_group_defaults.launch_template_name, each.key)
  launch_template_use_name_prefix        = try(each.value.launch_template_use_name_prefix, var.self_managed_node_group_defaults.launch_template_use_name_prefix, true)
  launch_template_version                = try(each.value.launch_template_version, var.self_managed_node_group_defaults.launch_template_version, null)
  launch_template_default_version        = try(each.value.launch_template_default_version, var.self_managed_node_group_defaults.launch_template_default_version, null)
  update_launch_template_default_version = try(each.value.update_launch_template_default_version, var.self_managed_node_group_defaults.update_launch_template_default_version, true)
  launch_template_description            = try(each.value.launch_template_description, var.self_managed_node_group_defaults.launch_template_description, ""Custom launch template for ${try(each.value.name, each.key)} self managed node group"")
  launch_template_tags                   = try(each.value.launch_template_tags, var.self_managed_node_group_defaults.launch_template_tags, {})
  tag_specifications                     = try(each.value.tag_specifications, var.self_managed_node_group_defaults.tag_specifications, [""instance"", ""volume"", ""network-interface""])

  ebs_optimized   = try(each.value.ebs_optimized, var.self_managed_node_group_defaults.ebs_optimized, null)
  ami_id          = try(each.value.ami_id, var.self_managed_node_group_defaults.ami_id, """")
  cluster_version = try(each.value.cluster_version, var.self_managed_node_group_defaults.cluster_version, time_sleep.this[0].triggers[""cluster_version""])
  instance_type   = try(each.value.instance_type, var.self_managed_node_group_defaults.instance_type, ""m6i.large"")
  key_name        = try(each.value.key_name, var.self_managed_node_group_defaults.key_name, null)

  disable_api_termination              = try(each.value.disable_api_termination, var.self_managed_node_group_defaults.disable_api_termination, null)
  instance_initiated_shutdown_behavior = try(each.value.instance_initiated_shutdown_behavior, var.self_managed_node_group_defaults.instance_initiated_shutdown_behavior, null)
  kernel_id                            = try(each.value.kernel_id, var.self_managed_node_group_defaults.kernel_id, null)
  ram_disk_id                          = try(each.value.ram_disk_id, var.self_managed_node_group_defaults.ram_disk_id, null)

  block_device_mappings              = try(each.value.block_device_mappings, var.self_managed_node_group_defaults.block_device_mappings, {})
  capacity_reservation_specification = try(each.value.capacity_reservation_specification, var.self_managed_node_group_defaults.capacity_reservation_specification, {})
  cpu_options                        = try(each.value.cpu_options, var.self_managed_node_group_defaults.cpu_options, {})
  credit_specification               = try(each.value.credit_specification, var.self_managed_node_group_defaults.credit_specification, {})
  elastic_gpu_specifications         = try(each.value.elastic_gpu_specifications, var.self_managed_node_group_defaults.elastic_gpu_specifications, {})
  elastic_inference_accelerator      = try(each.value.elastic_inference_accelerator, var.self_managed_node_group_defaults.elastic_inference_accelerator, {})
  enclave_options                    = try(each.value.enclave_options, var.self_managed_node_group_defaults.enclave_options, {})
  hibernation_options                = try(each.value.hibernation_options, var.self_managed_node_group_defaults.hibernation_options, {})
  instance_requirements              = try(each.value.instance_requirements, var.self_managed_node_group_defaults.instance_requirements, {})
  instance_market_options            = try(each.value.instance_market_options, var.self_managed_node_group_defaults.instance_market_options, {})
  license_specifications             = try(each.value.license_specifications, var.self_managed_node_group_defaults.license_specifications, {})
  metadata_options                   = try(each.value.metadata_options, var.self_managed_node_group_defaults.metadata_options, local.metadata_options)
  enable_monitoring                  = try(each.value.enable_monitoring, var.self_managed_node_group_defaults.enable_monitoring, true)
  enable_efa_support                 = try(each.value.enable_efa_support, var.self_managed_node_group_defaults.enable_efa_support, false)
  network_interfaces                 = try(each.value.network_interfaces, var.self_managed_node_group_defaults.network_interfaces, [])
  placement                          = try(each.value.placement, var.self_managed_node_group_defaults.placement, {})
  maintenance_options                = try(each.value.maintenance_options, var.self_managed_node_group_defaults.maintenance_options, {})
  private_dns_name_options           = try(each.value.private_dns_name_options, var.self_managed_node_group_defaults.private_dns_name_options, {})

  # IAM role
  create_iam_instance_profile   = try(each.value.create_iam_instance_profile, var.self_managed_node_group_defaults.create_iam_instance_profile, true)
  iam_instance_profile_arn      = try(each.value.iam_instance_profile_arn, var.self_managed_node_group_defaults.iam_instance_profile_arn, null)
  iam_role_name                 = try(each.value.iam_role_name, var.self_managed_node_group_defaults.iam_role_name, null)
  iam_role_use_name_prefix      = try(each.value.iam_role_use_name_prefix, var.self_managed_node_group_defaults.iam_role_use_name_prefix, true)
  iam_role_path                 = try(each.value.iam_role_path, var.self_managed_node_group_defaults.iam_role_path, null)
  iam_role_description          = try(each.value.iam_role_description, var.self_managed_node_group_defaults.iam_role_description, ""Self managed node group IAM role"")
  iam_role_permissions_boundary = try(each.value.iam_role_permissions_boundary, var.self_managed_node_group_defaults.iam_role_permissions_boundary, null)
  iam_role_tags                 = try(each.value.iam_role_tags, var.self_managed_node_group_defaults.iam_role_tags, {})
  iam_role_attach_cni_policy    = try(each.value.iam_role_attach_cni_policy, var.self_managed_node_group_defaults.iam_role_attach_cni_policy, true)
  # To better understand why this `lookup()` logic is required, see:
  # https://github.com/hashicorp/terraform/issues/31646#issuecomment-1217279031
  iam_role_additional_policies = lookup(each.value, ""iam_role_additional_policies"", lookup(var.self_managed_node_group_defaults, ""iam_role_additional_policies"", {}))

  # Access entry
  create_access_entry = try(each.value.create_access_entry, var.self_managed_node_group_defaults.create_access_entry, true)
  iam_role_arn        = try(each.value.iam_role_arn, var.self_managed_node_group_defaults.iam_role_arn, null)

  # Autoscaling group schedule
  create_schedule = try(each.value.create_schedule, var.self_managed_node_group_defaults.create_schedule, true)
  schedules       = try(each.value.schedules, var.self_managed_node_group_defaults.schedules, {})

  # Security group
  vpc_security_group_ids            = compact(concat([local.node_security_group_id], try(each.value.vpc_security_group_ids, var.self_managed_node_group_defaults.vpc_security_group_ids, [])))
  cluster_primary_security_group_id = try(each.value.attach_cluster_primary_security_group, var.self_managed_node_group_defaults.attach_cluster_primary_security_group, false) ? aws_eks_cluster.this[0].vpc_config[0].cluster_security_group_id : null

  tags = merge(var.tags, try(each.value.tags, var.self_managed_node_group_defaults.tags, {}))
}
",module,469,469.0,74d39187d855932dd976da6180eda42dcfe09873,74d39187d855932dd976da6180eda42dcfe09873,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/74d39187d855932dd976da6180eda42dcfe09873/node_groups.tf#L469,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/74d39187d855932dd976da6180eda42dcfe09873/node_groups.tf#L469,2024-05-08 08:04:19-04:00,2024-05-08 08:04:19-04:00,1,0,1,1,0,0,0,0,0,0
https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules,35,modules/vmseries/variables.tf,modules/vmseries/variables.tf,0,fix,# FIXME maybe `subnet_id` instead of `subnet`,"variable ""interfaces"" { # FIXME maybe `subnet_id` instead of `subnet`","variable ""interfaces"" { # FIXME maybe `subnet_id` instead of `subnet`
  description = <<-EOF
  List of the network interface specifications.
  The first should be the Management network interface, which does not participate in data filtering.
  The remaining ones are the dataplane interfaces.

  - `subnet`: Subnet object to use.
  - `lb_backend_pool_id`: Identifier of the backend pool of the load balancer to associate.
  - `enable_backend_pool`: If false, ignore `lb_backend_pool_id`. Default it false.
  - `public_ip_address_id`: Identifier of the existing public IP to associate.

  Example:

  ```
  [
    {
      subnet              = { id = var.vmseries_subnet_id_public }
      lb_backend_pool_id  = module.inbound_lb.backend-pool-id
      enable_backend_pool = true
    },
    {
      subnet              = { id = var.vmseries_subnet_id_private }
      lb_backend_pool_id  = module.outbound_lb.backend-pool-id
      enable_backend_pool = true
    },
  ]
  ```

  EOF
}
",variable,"variable ""interfaces"" {
  description = <<-EOF
  List of the network interface specifications.
  The first should be the Management network interface, which does not participate in data filtering.
  The remaining ones are the dataplane interfaces.

  - `subnet_id`: Identifier of the existing subnet to use.
  - `lb_backend_pool_id`: Identifier of the existing backend pool of the load balancer to associate.
  - `enable_backend_pool`: If false, ignore `lb_backend_pool_id`. Default it false.
  - `public_ip_address_id`: Identifier of the existing public IP to associate.

  Example:

  ```
  [
    {
      subnet_id            = azurerm_subnet.my_mgmt_subnet.id
      public_ip_address_id = azurerm_public_ip.my_mgmt_ip.id
    },
    {
      subnet_id           = azurerm_subnet.my_pub_subnet.id
      lb_backend_pool_id  = module.inbound_lb.backend_pool_id
      enable_backend_pool = true
    },
  ]
  ```

  EOF
}
",variable,29,,75b886d45bb898f4c908b10951906d875e6cd4f3,96b36698922904be306a659b6cbe9d05f6afbd51,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/75b886d45bb898f4c908b10951906d875e6cd4f3/modules/vmseries/variables.tf#L29,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/96b36698922904be306a659b6cbe9d05f6afbd51/modules/vmseries/variables.tf,2021-03-22 12:11:14+01:00,2021-04-01 09:40:32+02:00,4,1,1,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,18,community/modules/remote-desktop-linux/main.tf,community/modules/remote-desktop-linux/main.tf,0,todo,# todo change this to chrome install script & merge with xfce install script,# todo change this to chrome install script & merge with xfce install script,"locals {
  resource_prefix = var.name_prefix != null ? var.name_prefix : ""${var.deployment_name}-chrome-remote-desktop""

  /*
  #
  # if a machine type is a2-*-?g it will automatically fill in the guest_accelerator structure.
  #
  is_a2_vm = length(regexall(""a2-[a-z]+-\\d+g"", var.machine_type)) > 0
  accelerator_types = {
    ""highgpu""  = ""nvidia-tesla-a100""
    ""megagpu""  = ""nvidia-tesla-a100""
    ""ultragpu"" = ""nvidia-a100-80gb""
  }
  guest_accelerator = var.guest_accelerator == null && local.is_a2_vm ? [{
    type  = lookup(local.accelerator_types, regex(""a2-([A-Za-z]+)-"", var.machine_type)[0], """"),
    count = one(regex(""a2-[A-Za-z]+-(\\d+)"", var.machine_type)),
  }] : var.guest_accelerator

  gpu_count = length(local.guest_accelerator) > 0 ? 0 : local.guest_accelerator[0].count

*/
  user_startup_script_runners = var.startup_script == null ? [] : [
    {
      type        = ""shell""
      content     = var.startup_script
      destination = ""user_startup_script.sh""
    }
  ]

  ssh_args = join("""", [
    ""-e host_name_prefix=${local.resource_prefix}""
  ])

  configure_ssh_runners = [
    {
      type        = ""data""
      source      = ""${path.module}/scripts/setup-ssh-keys.sh""
      destination = ""/usr/local/ghpc/setup-ssh-keys.sh""
    },
    {
      type        = ""data""
      source      = ""${path.module}/scripts/setup-ssh-keys.yml""
      destination = ""/usr/local/ghpc/setup-ssh-keys.yml""
    },
    {
      type        = ""ansible-local""
      content     = file(""${path.module}/scripts/configure-ssh.yml"")
      destination = ""configure-ssh.yml""
      args        = local.ssh_args
    }
  ]
  # todo change this to driver install script
  configure_nvidia_driver_runners = var.install_nvidia_driver == false ? [] : [
    {
      type        = ""shell""
      content     = file(""${path.module}/scripts/configure-grid-drivers.sh"")
      destination = ""/usr/local/ghpc/configure-grid-drivers.yml""
    }
  ]
  # todo change this to chrome install script & merge with xfce install script
  configure_chrome_remote_desktop_runners = var.configure_chrome_remote_desktop == false ? [] : [
    {
      type        = ""shell""
      content     = file(""${path.module}/scripts/configure-chrome-desktop.sh"")
      destination = ""/usr/local/ghpc/configure-chrome-desktop.yml""
    }
  ]

  driver     = { install-nvidia-driver = var.install_nvidia_driver }
  logging    = var.enable_google_logging ? { google-logging-enable = 1 } : { google-logging-enable = 0 }
  monitoring = var.enable_google_monitoring ? { google-monitoring-enable = 1 } : { google-monitoring-enable = 0 }
  shutdown   = { shutdown-script = ""/opt/deeplearning/bin/shutdown_script.sh"" }
  metadata   = merge(local.driver, local.logging, local.monitoring, local.shutdown, var.metadata)
}
",locals,,,77,0.0,cb06601fe71434d7539ea073a9e1c3c03981e641,1fe9b4674767a89081bf6aa27b34e9e4338e8111,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/cb06601fe71434d7539ea073a9e1c3c03981e641/community/modules/remote-desktop-linux/main.tf#L77,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/1fe9b4674767a89081bf6aa27b34e9e4338e8111/community/modules/remote-desktop-linux/main.tf#L0,2023-01-11 06:57:26+00:00,2023-01-18 12:58:23+11:00,2,2,0,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-slo,8,modules/slo/main.tf,modules/slo/main.tf,0,workaround,# Workaround for https://github.com/terraform-providers/terraform-provider-random/issues/95,"# Generate a random uuid that will regenerate when the SLO config or the Error 
 # Budget Policy is updated. 
 # Workaround for https://github.com/terraform-providers/terraform-provider-random/issues/95","resource ""random_uuid"" ""config_hash"" {
  keepers = {
    for content in [local_file.slo.content, local_file.error_budget_policy.content] :
    content => md5(content)
  }
}
",resource,the block associated got renamed or deleted,,77,,825194fc70002c5b310442faceccf313321768d0,7b2bb290a79dab58f9a7232441909c6949bfcab1,https://github.com/terraform-google-modules/terraform-google-slo/blob/825194fc70002c5b310442faceccf313321768d0/modules/slo/main.tf#L77,https://github.com/terraform-google-modules/terraform-google-slo/blob/7b2bb290a79dab58f9a7232441909c6949bfcab1/modules/slo/main.tf,2019-12-07 23:32:23+01:00,2019-12-13 16:05:15-08:00,2,1,0,0,1,0,0,0,0,0
https://github.com/Worklytics/psoxy,4393,infra/modules/aws-host/variables.tf,infra/modules/aws-host/variables.tf,0,# todo,"# TODO: generalize to 'secrets', regardless of store (AWS SSM, AWS Secrets Manager, etc)","# TODO: generalize to 'secrets', regardless of store (AWS SSM, AWS Secrets Manager, etc)","variable ""aws_ssm_key_id"" {
  type        = string
  description = ""KMS key id to use for encrypting SSM SecureString parameters created by this module, in any. (by default, will encrypt with AWS-managed keys)""
  default     = null
}
",variable,"variable ""aws_ssm_key_id"" {
  type        = string
  description = ""KMS key id to use for encrypting SSM SecureString parameters created by this module, in any. (by default, will encrypt with AWS-managed keys)""
  default     = null
}
",variable,21,33.0,005e1fed5f46b4310d81d41d29862bb1c4f360b0,078f8d7ab53aae24a1510fabb9f2d2f1482e3461,https://github.com/Worklytics/psoxy/blob/005e1fed5f46b4310d81d41d29862bb1c4f360b0/infra/modules/aws-host/variables.tf#L21,https://github.com/Worklytics/psoxy/blob/078f8d7ab53aae24a1510fabb9f2d2f1482e3461/infra/modules/aws-host/variables.tf#L33,2024-02-06 19:07:07+00:00,2024-04-01 09:04:07-07:00,8,0,0,1,0,1,0,0,0,0
https://github.com/ministryofjustice/modernisation-platform,200,terraform/environments/data-platform-apps-and-tools/opensearch.tf,terraform/environments/data-platform-apps-and-tools/opensearch.tf,0,// todo,// TODO: Find source for this policy @jacobwoffenden,"// TODO: Find source for this policy @jacobwoffenden 
 #checkov:skip=CKV_AWS_283:","data ""aws_iam_policy_document"" ""opensearch_domain"" {
  // TODO: Find source for this policy @jacobwoffenden
  #checkov:skip=CKV_AWS_283:
  statement {
    effect  = ""Allow""
    actions = [""es:ESHttp*""]
    principals {
      type        = ""AWS""
      identifiers = [""*""]
    }
    resources = [""${aws_opensearch_domain.openmetadata.arn}/*""]
  }
}
",data,,,74,0.0,90002b7667baf7c1eb23d0e49dcfa9d9eaee7567,951ffdb805c4255e3bb6d0a5febe5f1bb21407c9,https://github.com/ministryofjustice/modernisation-platform/blob/90002b7667baf7c1eb23d0e49dcfa9d9eaee7567/terraform/environments/data-platform-apps-and-tools/opensearch.tf#L74,https://github.com/ministryofjustice/modernisation-platform/blob/951ffdb805c4255e3bb6d0a5febe5f1bb21407c9/terraform/environments/data-platform-apps-and-tools/opensearch.tf#L0,2023-10-18 15:43:32+01:00,2023-12-19 15:49:47+00:00,3,2,0,1,0,1,0,0,0,0
https://github.com/chanzuckerberg/cztack,102,aws-lambda-function/main.tf,aws-lambda-function/main.tf,0,todo,# TODO scope this policy down,# TODO scope this policy down,"resource aws_iam_policy lambda_logging {
  name_prefix = ""${local.name}-lambda-logging""
  path        = ""/""
  description = ""IAM policy for logging from the ${local.name} lambda.""

  # TODO scope this policy down
  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": [
        ""logs:CreateLogStream"",
        ""logs:PutLogEvents""
      ],
      ""Resource"": ""${aws_cloudwatch_log_group.log.arn}"",
      ""Effect"": ""Allow""
    }
  ]
}
EOF

}
",resource,"data ""aws_iam_policy_document"" ""lambda_logging_policy"" {
  statement {
    effect = ""Allow""
    actions = compact([
      ""logs:CreateLogStream"",
      ""logs:PutLogEvents"",
      var.at_edge ? ""logs:CreateLogGroup"" : """"
    ])

    resources = [
      var.at_edge ?
      ""*"" :
      ""${aws_cloudwatch_log_group.log.arn}:*""
    ]
  }
}
",data,73,84.0,0ce8face5837bbe379a9dbf3ce1b7820e4b7d100,1882eb86cffd9382dc75c0c5d5b89116324fb87f,https://github.com/chanzuckerberg/cztack/blob/0ce8face5837bbe379a9dbf3ce1b7820e4b7d100/aws-lambda-function/main.tf#L73,https://github.com/chanzuckerberg/cztack/blob/1882eb86cffd9382dc75c0c5d5b89116324fb87f/aws-lambda-function/main.tf#L84,2020-07-22 11:53:07-07:00,2023-03-31 14:28:49+00:00,13,0,0,1,0,1,0,0,1,0
https://github.com/alphagov/govuk-aws,126,terraform/projects/infra-security-groups/backend.tf,terraform/projects/infra-security-groups/backend.tf,0,# todo,# TODO: replace this with ingress from the backend LBs when we build them.,# TODO: replace this with ingress from the backend LBs when we build them.,"resource ""aws_security_group_rule"" ""allow_management_to_backend_elb"" {
  type      = ""ingress""
  from_port = 443
  to_port   = 443
  protocol  = ""tcp""

  security_group_id        = ""${aws_security_group.backend_elb.id}""
  source_security_group_id = ""${aws_security_group.management.id}""
}
",resource,the block associated got renamed or deleted,,47,,2bcd507fd7191532adf6109c13fd3e6d1a0b9899,923e481addb08a4badef9a4ac1d531d4bbfea4b5,https://github.com/alphagov/govuk-aws/blob/2bcd507fd7191532adf6109c13fd3e6d1a0b9899/terraform/projects/infra-security-groups/backend.tf#L47,https://github.com/alphagov/govuk-aws/blob/923e481addb08a4badef9a4ac1d531d4bbfea4b5/terraform/projects/infra-security-groups/backend.tf,2017-07-20 12:27:27+01:00,2017-09-15 17:02:09+01:00,4,1,0,1,0,1,1,0,0,0
https://github.com/Worklytics/psoxy,4405,infra/modules/aws-secretsmanager-secrets/main.tf,infra/modules/aws-secretsmanager-secrets/main.tf,0,implementation,# NOTE: value of this module is a consistent interface across potential Secret store implementations,"# stores secrets as AWS Secret Manager Secrets 
 # NOTE: value of this module is a consistent interface across potential Secret store implementations 
 #   eg, GCP Secret Manager, AWS SSM Parameter Store, Hashicorp Vault, AWS Secrets Manager, etc. 
 #  but is this good Terraform style? clearly in AWS case, this module doesn't do much ... ","locals {
  # AWS SSM param name must be fully qualified if contains `/`;
  # so test for that case, and prefix with `/` if needed
  non_empty_path           = length(var.path) > 0
  non_fully_qualified_path = length(regexall(""/"", var.path)) > 0 && !startswith(var.path, ""/"")
  path_prefix              = local.non_empty_path && local.non_fully_qualified_path ? ""/${var.path}"" : var.path
  PLACEHOLDER_VALUE        = ""fill me""

  # externally_managed_secrets = { for k, spec in var.secrets : k => spec if !(spec.value_managed_by_tf) }
  terraform_managed_secrets = { for k, spec in var.secrets : k => spec if spec.value_managed_by_tf }

  tf_management_description_appendix = ""Value managed by a Terraform configuration; changes outside Terraform may be overwritten by subsequent 'terraform apply' runs""
}
",locals,"locals {
  # AWS SSM param name must be fully qualified if contains `/`;
  # so test for that case, and prefix with `/` if needed
  non_empty_path           = length(var.path) > 0
  non_fully_qualified_path = length(regexall(""/"", var.path)) > 0 && !startswith(var.path, ""/"")
  path_prefix              = local.non_empty_path && local.non_fully_qualified_path ? ""/${var.path}"" : var.path
  PLACEHOLDER_VALUE        = ""fill me""

  # externally_managed_secrets = { for k, spec in var.secrets : k => spec if !(spec.value_managed_by_tf) }
  terraform_managed_secrets = { for k, spec in var.secrets : k => spec if spec.value_managed_by_tf }

  tf_management_description_appendix = ""Value managed by a Terraform configuration; changes outside Terraform may be overwritten by subsequent 'terraform apply' runs""
}
",locals,2,2.0,005e1fed5f46b4310d81d41d29862bb1c4f360b0,005e1fed5f46b4310d81d41d29862bb1c4f360b0,https://github.com/Worklytics/psoxy/blob/005e1fed5f46b4310d81d41d29862bb1c4f360b0/infra/modules/aws-secretsmanager-secrets/main.tf#L2,https://github.com/Worklytics/psoxy/blob/005e1fed5f46b4310d81d41d29862bb1c4f360b0/infra/modules/aws-secretsmanager-secrets/main.tf#L2,2024-02-06 19:07:07+00:00,2024-02-06 19:07:07+00:00,1,0,0,1,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,13,infra/gcp/clusters/modules/k8s-infra-gke-cluster/main.tf,infra/gcp/clusters/modules/gke-cluster/main.tf,1,workaround,// Workaround from https://github.com/hashicorp/terraform/issues/22544#issuecomment-582974372,"// BigQuery dataset for usage data 
 // Workaround from https://github.com/hashicorp/terraform/issues/22544#issuecomment-582974372 
 // to set delete_contents_on_destroy to false if is_prod_cluster 
 // keep prod_ and test_ identical except for ""unique to "" comments","resource ""google_bigquery_dataset"" ""prod_usage_metering"" {
  count       = var.is_prod_cluster == ""true"" ? 1 : 0
  dataset_id  = replace(""usage_metering_${var.cluster_name}"", ""-"", ""_"")
  project     = var.project_name
  description = ""GKE Usage Metering for cluster '${var.cluster_name}'""
  location    = var.bigquery_location

  access {
    role          = ""OWNER""
    special_group = ""projectOwners""
  }
  access {
    role          = ""WRITER""
    user_by_email = google_service_account.cluster_node_sa.email
  }

  // This restricts deletion of this dataset if there is data in it
  // unique to prod_usage_metering
  delete_contents_on_destroy = false
}
",resource,"resource ""google_bigquery_dataset"" ""prod_usage_metering"" {
  count       = var.is_prod_cluster == ""true"" ? 1 : 0
  dataset_id  = replace(""usage_metering_${var.cluster_name}"", ""-"", ""_"")
  project     = var.project_name
  description = ""GKE Usage Metering for cluster '${var.cluster_name}'""
  location    = var.bigquery_location

  access {
    role          = ""OWNER""
    special_group = ""projectOwners""
  }
  access {
    role          = ""WRITER""
    user_by_email = google_service_account.cluster_node_sa.email
  }

  // NOTE: unique to prod_usage_metering
  // This restricts deletion of this dataset if there is data in it
  delete_contents_on_destroy = false
}
",resource,42,,59c074f856b19f13b9e75479f2aaf09d9c95c7d6,0d83cf8820bd2d550c7032d8557aacb836bca743,https://github.com/kubernetes/k8s.io/blob/59c074f856b19f13b9e75479f2aaf09d9c95c7d6/infra/gcp/clusters/modules/k8s-infra-gke-cluster/main.tf#L42,https://github.com/kubernetes/k8s.io/blob/0d83cf8820bd2d550c7032d8557aacb836bca743/infra/gcp/clusters/modules/gke-cluster/main.tf,2020-05-06 14:16:38-07:00,2020-05-15 18:34:32-07:00,2,1,1,1,1,0,0,1,0,0
https://github.com/terraform-google-modules/terraform-google-project-factory,175,modules/core_project_factory/main.tf,modules/core_project_factory/main.tf,0,todo,# TODO update source once released,"# TODO update source once released 
 #source  = ""terraform-google-modules/gcloud/google"" 
 #version = ""~> 0.1""","module ""gcloud_depriviledge"" {
  # TODO update source once released
  #source  = ""terraform-google-modules/gcloud/google""
  #version = ""~> 0.1""
  source = ""github.com/taylorludwig/terraform-google-gcloud?ref=feature%2Frun-script""

  enabled = var.default_service_account == ""depriviledge""

  create_script           = ""${path.module}/scripts/modify-service-account.sh""
  create_script_arguments = <<-EOT
    --project_id='${google_project.main.project_id}' \
    --sa_id='${data.null_data_source.default_service_account.outputs[""email""]}' \
    --credentials_path='${var.credentials_path}' \
    --impersonate-service-account='${var.impersonate_service_account}' \
    --action='depriviledge'
  EOT

  create_script_triggers = {
    default_service_account = data.null_data_source.default_service_account.outputs[""email""]
    activated_apis          = join("","", local.activate_apis)
    project_services        = module.project_services.project_id
  }
}
",module,the block associated got renamed or deleted,,212,,c0411a25011092d73d3b06dfd6a3e2c7b69acded,af94e3c4d48c039ff77a375fc122a37c0dcf83dc,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/c0411a25011092d73d3b06dfd6a3e2c7b69acded/modules/core_project_factory/main.tf#L212,https://github.com/terraform-google-modules/terraform-google-project-factory/blob/af94e3c4d48c039ff77a375fc122a37c0dcf83dc/modules/core_project_factory/main.tf,2019-12-19 14:13:08-08:00,2019-12-21 22:57:14-08:00,3,1,0,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,91,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,0,# todo,# TODO: is it used? should remove it?,client_install_runner = map(string) # TODO: is it used? should remove it?,"variable ""network_storage"" {
  description = ""An array of network attached storage mounts to be configured on all instances.""
  type = list(object({
    server_ip             = string,
    remote_mount          = string,
    local_mount           = string,
    fs_type               = string,
    mount_options         = string,
    client_install_runner = map(string) # TODO: is it used? should remove it?
    mount_runner          = map(string)
  }))
  default = []
}
",variable,"variable ""network_storage"" {
  description = ""An array of network attached storage mounts to be configured on all instances.""
  type = list(object({
    server_ip             = string,
    remote_mount          = string,
    local_mount           = string,
    fs_type               = string,
    mount_options         = string,
    client_install_runner = map(string) # TODO: is it used? should remove it?
    mount_runner          = map(string)
  }))
  default = []
}
",variable,351,405.0,33bf402eaa82607a027754c6048fb0dce6d7668c,367dbbc03d00a523c457d066229e6c9376bcd5c9,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/33bf402eaa82607a027754c6048fb0dce6d7668c/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf#L351,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/367dbbc03d00a523c457d066229e6c9376bcd5c9/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf#L405,2023-10-26 09:59:26-07:00,2024-05-01 19:59:12+00:00,24,0,1,1,0,0,1,0,0,0
https://github.com/terraform-google-modules/terraform-google-slo,1,examples/simple_example/main.tf,examples/simple_example/main.tf,0,# todo,# TODO: Add project to Stackdriver host workspace here,# TODO: Add project to Stackdriver host workspace here,"module ""slo-pipeline"" {
  source                      = ""../../modules/slo-pipeline""
  function_name               = ""slo-export""
  region                      = var.region
  project_id                  = module.slo-project.project_id
  bigquery_project_id         = module.slo-project.project_id
  bigquery_dataset_name       = ""slo_reports""
  bucket_name                 = var.bucket_name
  stackdriver_host_project_id = var.stackdriver_host_project_id
}
",module,the block associated got renamed or deleted,,44,,4236fcdd363233b69a439c1755154c8082ba32dd,1da067573e522895223077bd116231200b15e9ce,https://github.com/terraform-google-modules/terraform-google-slo/blob/4236fcdd363233b69a439c1755154c8082ba32dd/examples/simple_example/main.tf#L44,https://github.com/terraform-google-modules/terraform-google-slo/blob/1da067573e522895223077bd116231200b15e9ce/examples/simple_example/main.tf,2019-09-04 09:54:21+02:00,2019-09-09 17:40:22+02:00,4,1,0,1,0,0,0,0,1,0
https://github.com/wireapp/wire-server-deploy,55,terraform/modules/sft/outputs.tf,terraform/modules/sft/outputs.tf,0,# todo,# TODO: It is absurd that srv-announcer requires this. All route53 resources are,"# TODO: It is absurd that srv-announcer requires this. All route53 resources are 
 # scoped globally, figure out if we really need to do this.","data ""aws_region"" ""current"" {}
",data,"data ""aws_region"" ""current"" {}
",data,1,1.0,ddf967a4a5d4f3e8f6ffbb9a93b47ee2089f60c0,d8e12109f7193074bb4c065a6e3c0580a24cbceb,https://github.com/wireapp/wire-server-deploy/blob/ddf967a4a5d4f3e8f6ffbb9a93b47ee2089f60c0/terraform/modules/sft/outputs.tf#L1,https://github.com/wireapp/wire-server-deploy/blob/d8e12109f7193074bb4c065a6e3c0580a24cbceb/terraform/modules/sft/outputs.tf#L1,2020-09-18 11:48:52+02:00,2020-10-15 12:30:35+02:00,4,0,0,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,142,infra/modules/aws/main.tf,infra/modules/aws/main.tf,0,#todo,#TODO: add condition referencing GCP service account that will auth with Worklytics AWS account to call,"#TODO: add condition referencing GCP service account that will auth with Worklytics AWS account to call 
 #""Condition"" : { 
 #  ""StringEquals"" : { 
 #    ""sts:ExternalId"" : var.caller_aws_user_id 
 #  } 
 #}","resource ""aws_iam_role"" ""api-caller"" {
  name = ""PsoxyApiCaller""

  # who can assume this role
  assume_role_policy = jsonencode({
    ""Version"" : ""2012-10-17"",
    ""Statement"" : [
      {
        ""Action"" = ""sts:AssumeRole""
        ""Effect"" : ""Allow""
        ""Principal"" : {
          ""AWS"" : ""arn:aws:iam::${var.caller_aws_account_id}""
        }
        #TODO: add condition referencing GCP service account that will auth with Worklytics AWS account to call
        #""Condition"" : {
        #  ""StringEquals"" : {
        #    ""sts:ExternalId"" : var.caller_aws_user_id
        #  }
        #}
      }
    ]
  })

  # what this role can do (invoke anything in the API gateway )
  inline_policy {
    name = ""invoke""
    policy = jsonencode({
      ""Version"" : ""2012-10-17"",
      ""Statement"" : [
        {
          ""Effect"" : ""Allow""
          ""Action"" : ""execute-api:Invoke""
          ""Resource"" : [
            ""${aws_apigatewayv2_api.psoxy-api.arn}/*/*/*"",
          ]
        }
      ]
    })
  }
}
",resource,"resource ""aws_iam_role"" ""api-caller"" {
  name = ""PsoxyApiCaller""

  # who can assume this role
  assume_role_policy = jsonencode({
    ""Version"" : ""2012-10-17"",
    ""Statement"" : [
      {
        ""Action"" = ""sts:AssumeRole""
        ""Effect"" : ""Allow""
        ""Principal"" : {
          ""AWS"" : ""arn:aws:iam::${var.caller_aws_account_id}""
        }
      },
      # allows service account to assume role
      {
        ""Effect"": ""Allow"",
        ""Principal"": {
          ""Federated"": ""accounts.google.com""
        },
        ""Action"": ""sts:AssumeRoleWithWebIdentity"",
        ""Condition"": {
          ""StringEquals"": {
            ""accounts.google.com:aud"": ""${var.caller_external_user_id}""
          }
        }
      }
    ]
  })

  # what this role can do (invoke anything in the API gateway )
  inline_policy {
    name = ""lambda-invoker""
    policy = jsonencode({
      ""Version"" : ""2012-10-17"",
      ""Statement"" : [
        {
          ""Effect"": ""Allow"",
          ""Action"": ""execute-api:Invoke"",
          ""Resource"": ""arn:aws:execute-api:*:${var.aws_account_id}:*/*/GET/*"",
        }
      ]
    })
  }
}
",resource,38,,0695d5d70696539227a4054208cc7393683913a2,c408940e26fbaebc0c78e068c25a8e0fca3aa503,https://github.com/Worklytics/psoxy/blob/0695d5d70696539227a4054208cc7393683913a2/infra/modules/aws/main.tf#L38,https://github.com/Worklytics/psoxy/blob/c408940e26fbaebc0c78e068c25a8e0fca3aa503/infra/modules/aws/main.tf,2022-01-12 22:04:03-08:00,2022-01-21 14:29:14-08:00,8,1,0,1,0,1,0,0,0,0
https://github.com/ministryofjustice/modernisation-platform,199,terraform/environments/data-platform-apps-and-tools/kubernetes-secrets.tf,terraform/environments/data-platform-apps-and-tools/kubernetes-secrets.tf,0,// todo,// TODO: Rename this?,// TODO: Rename this?,"resource ""kubernetes_secret"" ""openmetadata_airflow"" {
  metadata {
    name      = ""openmetadata-airflow""
    namespace = kubernetes_namespace.openmetadata.metadata[0].name
  }
  data = {
    openmetadata-airflow-password = random_password.openmetadata_airflow.result
  }
  type = ""Opaque""
}
",resource,,,1,0.0,90002b7667baf7c1eb23d0e49dcfa9d9eaee7567,951ffdb805c4255e3bb6d0a5febe5f1bb21407c9,https://github.com/ministryofjustice/modernisation-platform/blob/90002b7667baf7c1eb23d0e49dcfa9d9eaee7567/terraform/environments/data-platform-apps-and-tools/kubernetes-secrets.tf#L1,https://github.com/ministryofjustice/modernisation-platform/blob/951ffdb805c4255e3bb6d0a5febe5f1bb21407c9/terraform/environments/data-platform-apps-and-tools/kubernetes-secrets.tf#L0,2023-10-18 15:43:32+01:00,2023-12-19 15:49:47+00:00,2,2,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1305,blueprints/data-solutions/shielded-folder/maint.tf,blueprints/data-solutions/shielded-folder/main.tf,1,#todo,#TODO logsink,#TODO logsink,"module ""folder"" {
  source                 = ""../../../modules/folder""
  folder_create          = var.folder_create != null
  parent                 = try(var.folder_create.parent, null)
  name                   = try(var.folder_create.display_name, null)
  id                     = var.folder_id
  group_iam              = local.group_iam
  org_policies_data_path = ""${var.data_dir}/org-policies""
  firewall_policy_factory = {
    cidr_file   = ""${var.data_dir}/firewall-policies/cidrs.yaml""
    policy_name = ""hierarchical-policy""
    rules_file  = ""${var.data_dir}/firewall-policies/hierarchical-policy-rules.yaml""
  }
  #TODO logsink
}
",module,"module ""folder"" {
  source        = ""../../../modules/folder""
  folder_create = var.folder_create != null
  parent        = try(var.folder_create.parent, null)
  name          = try(var.folder_create.display_name, null)
  id            = var.folder_id
  iam = {
    ""roles/owner""                          = [""serviceAccount:${var.bootstrap_service_account}""]
    ""roles/resourcemanager.projectCreator"" = [""serviceAccount:${var.bootstrap_service_account}""]
  }
  group_iam              = local.group_iam
  org_policies_data_path = ""${var.data_dir}/org-policies""
  firewall_policy_factory = {
    cidr_file   = ""${var.data_dir}/firewall-policies/cidrs.yaml""
    policy_name = ""hierarchical-policy""
    rules_file  = ""${var.data_dir}/firewall-policies/hierarchical-policy-rules.yaml""
  }
  logging_sinks = {
    for name, attrs in var.log_sinks : name => {
      bq_partitioned_table = attrs.type == ""bigquery""
      destination          = local.log_sink_destinations[name].id
      filter               = attrs.filter
      type                 = attrs.type
    }
  }
}
",module,49,,84be665172b21220938ee702c4654e1a0cd0a584,4007d42705a930e9e526a8da3616712ad0e646f6,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/84be665172b21220938ee702c4654e1a0cd0a584/blueprints/data-solutions/shielded-folder/maint.tf#L49,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/4007d42705a930e9e526a8da3616712ad0e646f6/blueprints/data-solutions/shielded-folder/main.tf,2023-01-17 08:49:04+01:00,2023-01-21 01:08:51+01:00,2,1,0,1,0,0,0,0,1,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1888,modules/dataproc/outputs.tf,modules/dataproc/outputs.tf,0,fix,# FIXME: 2024-03-08: broken in provider,"# FIXME: 2024-03-08: broken in provider 
 #output ""instance_names"" { 
 #  description = ""List of instance names which have been assigned to the cluster."" 
 #  value = { 
 #    master             = google_dataproc_cluster.cluster.cluster_config.0.master_config.0.instance_names 
 #    worker             = google_dataproc_cluster.cluster.cluster_config.0.worker_config.0.instance_names 
 #    preemptible_worker = google_dataproc_cluster.cluster.cluster_config.0.preemptible_worker_config.0.instance_names 
 #  } 
 #} ","output ""name"" {
  description = ""The name of the cluster.""
  value       = google_dataproc_cluster.cluster.name
}
",output,"output ""name"" {
  description = ""The name of the cluster.""
  value       = google_dataproc_cluster.cluster.name
}
",output,35,35.0,1a235cbcecef59886a3e5caa498cf1f7cdada943,3af7e257d21f889ffaf7b32a3bab974fdbfda6e4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/1a235cbcecef59886a3e5caa498cf1f7cdada943/modules/dataproc/outputs.tf#L35,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3af7e257d21f889ffaf7b32a3bab974fdbfda6e4/modules/dataproc/outputs.tf#L35,2024-03-11 11:05:33+01:00,2024-04-17 10:23:48+02:00,2,0,0,0,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1891,modules/project-factory/variables.tf,modules/project-factory/variables.tf,0,# todo,# TODO: allow defining notification channels via YAML files,# TODO: allow defining notification channels via YAML files,"variable ""factories_config"" {
  description = ""Path to folder with YAML resource description data files.""
  type = object({
    hierarchy = optional(object({
      folders_data_path = string
      parent_ids        = optional(map(string), {})
    }))
    projects_data_path = optional(string)
    budgets = optional(object({
      billing_account   = string
      budgets_data_path = string
      # TODO: allow defining notification channels via YAML files
      notification_channels = optional(map(any), {})
    }))
  })
  nullable = false
}
",variable,"variable ""factories_config"" {
  description = ""Path to folder with YAML resource description data files.""
  type = object({
    hierarchy = optional(object({
      folders_data_path = string
      parent_ids        = optional(map(string), {})
    }))
    projects_data_path = optional(string)
    budgets = optional(object({
      billing_account   = string
      budgets_data_path = string
      # TODO: allow defining notification channels via YAML files
      notification_channels = optional(map(any), {})
    }))
  })
  nullable = false
}
",variable,102,107.0,28f02688eeb48f29e5b8640168a5ee569820708d,309792c559bde5c1dbae1c0550f62a28df85cd79,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/28f02688eeb48f29e5b8640168a5ee569820708d/modules/project-factory/variables.tf#L102,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/309792c559bde5c1dbae1c0550f62a28df85cd79/modules/project-factory/variables.tf#L107,2024-03-14 15:03:42+03:00,2024-04-22 09:28:01+02:00,2,0,0,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-kubernetes-engine,6,modules/safer-cluster/main.tf,modules/safer-cluster/main.tf,0,// todo,// TODO(mmontan): check whether we need to restrict these,"// TODO(mmontan): check whether we need to restrict these 
 // settings.","module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // Disable the dashboard. It creates risk by running as a very sensitive user.
  kubernetes_dashboard = false

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy          = true
  network_policy_provider = ""CALICO""

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  disable_legacy_metadata_endpoints = true

  node_pools        = var.node_pools
  node_pools_labels = var.node_pools_labels

  // TODO(mmontan): check whether we need to restrict these
  // settings.
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  // TODO(mmontan): we generally considered applying
  // just the cloud-platofrm scope and use Cloud IAM
  // If we have Workload Identity, are there advantages
  // in restricting scopes even more?
  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  // We should use IP Alias.
  configure_ip_masq = false

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = length(var.compute_engine_service_account) > 0 ? false : true
  service_account        = var.compute_engine_service_account

  // TODO(mmontan): define a registry_project parameter in the private_beta_cluster,
  // so that we can give GCS permissions to the service account on a project
  // that hosts only container-images and not data.
  grant_registry_access = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // TODO(mmontan): link to a couple of policies.
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id
  node_metadata                    = ""SECURE""

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // TODO(mmontan): investigate whether this should be a recommended setting
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group
}
",module,"module ""gke"" {
  source             = ""../beta-private-cluster/""
  project_id         = var.project_id
  name               = var.name
  regional           = var.regional
  region             = var.region
  network            = var.network
  network_project_id = var.network_project_id

  // We need to enforce a minimum Kubernetes Version to ensure
  // that the necessary security features are enabled.
  kubernetes_version = ""latest""

  // Nodes are created with a default version. The nodepool enables
  // auto_upgrade so that the node versions can be kept up to date with
  // the master upgrades.
  //
  // https://cloud.google.com/kubernetes-engine/versioning-and-upgrades
  node_version = """"

  master_authorized_networks_config = var.master_authorized_networks_config

  subnetwork        = var.subnetwork
  ip_range_pods     = var.ip_range_pods
  ip_range_services = var.ip_range_services

  horizontal_pod_autoscaling = var.horizontal_pod_autoscaling
  http_load_balancing        = var.http_load_balancing

  // We suggest the use coarse network policies to enforce restrictions in the
  // communication between pods.
  //
  // NOTE: Enabling network policy is not sufficient to enforce restrictions.
  // NetworkPolicies need to be configured in every namespace. The network
  // policies should be under the control of a cental cluster management team,
  // rather than individual teams.
  network_policy = true

  maintenance_start_time = var.maintenance_start_time

  initial_node_count = var.initial_node_count

  // We suggest removing the default node pull, as it cannot be modified without
  // destroying the cluster.
  remove_default_node_pool = true

  node_pools          = var.node_pools
  node_pools_labels   = var.node_pools_labels
  node_pools_metadata = var.node_pools_metadata
  node_pools_taints   = var.node_pools_taints
  node_pools_tags     = var.node_pools_tags

  node_pools_oauth_scopes = var.node_pools_oauth_scopes

  stub_domains         = var.stub_domains
  upstream_nameservers = var.upstream_nameservers

  logging_service    = var.logging_service
  monitoring_service = var.monitoring_service

  // We never use the default service account for the cluster. The default
  // project/editor permissions can create problems if nodes were to be ever
  // compromised.

  // We either:
  // - Create a dedicated service account with minimal permissions to run nodes.
  //   All applications shuold run with an identity defined via Workload Identity anyway.
  // - Use a service account passed as a parameter to the module, in case the user
  //   wants to maintain control of their service accounts.
  create_service_account = var.compute_engine_service_account == """" ? true : false
  service_account        = var.compute_engine_service_account
  registry_project_id    = var.registry_project_id
  grant_registry_access  = true

  // Basic Auth disabled
  basic_auth_username = """"
  basic_auth_password = """"

  issue_client_certificate = false

  cluster_ipv4_cidr = var.cluster_ipv4_cidr

  cluster_resource_labels = var.cluster_resource_labels

  // We enable private endpoints to limit exposure.
  enable_private_endpoint       = true
  deploy_using_private_endpoint = true

  // Private nodes better control public exposure, and reduce
  // the ability of nodes to reach to the Internet without
  // additional configurations.
  enable_private_nodes = true

  master_ipv4_cidr_block = var.master_ipv4_cidr_block

  // Istio is recommended for pod-to-pod communications.
  istio    = var.istio
  cloudrun = var.cloudrun

  default_max_pods_per_node = var.default_max_pods_per_node

  database_encryption = var.database_encryption

  // We suggest to define policies about  which images can run on a cluster.
  enable_binary_authorization = true

  // Define PodSecurityPolicies for differnet applications.
  // Example: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#example
  pod_security_policy_config = [{
    ""enabled"" = true
  }]

  resource_usage_export_dataset_id = var.resource_usage_export_dataset_id

  // Sandbox is needed if the cluster is going to run any untrusted workload (e.g., user submitted code).
  // Sandbox can also provide increased protection in other cases, at some performance cost.
  sandbox_enabled = var.sandbox_enabled

  // Intranode Visibility enables you to capture flow logs for traffic between pods and create FW rules that apply to traffic between pods.
  enable_intranode_visibility = var.enable_intranode_visibility

  enable_vertical_pod_autoscaling = var.enable_vertical_pod_autoscaling

  // We enable identity namespace by default.
  identity_namespace = ""${var.project_id}.svc.id.goog""

  authenticator_security_group = var.authenticator_security_group

  enable_shielded_nodes = var.enable_shielded_nodes
}
",module,74,,e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5,1a5eb6354dc0281e7cf4c1a0f4e2159afed08e48,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/e8688fce06b3a2aedb7f9df34b96d9f2eae4e7c5/modules/safer-cluster/main.tf#L74,https://github.com/terraform-google-modules/terraform-google-kubernetes-engine/blob/1a5eb6354dc0281e7cf4c1a0f4e2159afed08e48/modules/safer-cluster/main.tf,2019-10-04 15:21:33-07:00,2019-11-18 08:19:13-06:00,7,1,1,1,0,0,0,0,0,1
https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules,45,examples/transit_vnet_common/main.tf,examples/transit_vnet_common/main.tf,0,fix,# FIXME automatize,backend_name        = var.inbound_lb_name # FIXME automatize,"module ""inbound_lb"" {
  source = ""../../modules/loadbalancer""

  name_lb             = var.inbound_lb_name
  location            = var.location
  resource_group_name = azurerm_resource_group.this.name
  backend_name        = var.inbound_lb_name # FIXME automatize
  frontend_ips        = var.frontend_ips
}
",module,"module ""inbound_lb"" {
  source = ""../../modules/loadbalancer""

  name                = var.inbound_lb_name
  location            = var.location
  resource_group_name = azurerm_resource_group.this.name
  frontend_ips        = var.frontend_ips
}
",module,46,,e69a3cf69a8139f84d2c072ec554184f739c8e8f,481fefe1044fde2f794407a5a1902791b4445ab1,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/e69a3cf69a8139f84d2c072ec554184f739c8e8f/examples/transit_vnet_common/main.tf#L46,https://github.com/PaloAltoNetworks/terraform-azurerm-vmseries-modules/blob/481fefe1044fde2f794407a5a1902791b4445ab1/examples/transit_vnet_common/main.tf,2021-04-22 10:50:23+02:00,2021-04-22 10:50:23+02:00,2,1,0,1,0,0,1,0,0,0
https://github.com/rancherfederal/rke2-aws-tf,2,main.tf,main.tf,0,# todo,"# TODO: Ideally set this to `length(var.servers)`, but currently blocked by: https://github.com/rancher/rke2/issues/349","# TODO: Ideally set this to `length(var.servers)`, but currently blocked by: https://github.com/rancher/rke2/issues/349","module ""servers"" {
  source = ""./modules/nodepool""
  name   = ""${local.uname}-server""

  vpc_id                 = var.vpc_id
  subnets                = var.subnets
  ami                    = var.ami
  block_device_mappings  = var.block_device_mappings
  vpc_security_group_ids = [aws_security_group.server.id, aws_security_group.cluster.id]
  spot                   = var.spot
  target_group_arns = [
    module.cp_lb.server_tg_arn,
    module.cp_lb.server_supervisor_tg_arn,
  ]

  # Overrideable variables
  userdata             = data.template_cloudinit_config.this.rendered
  iam_instance_profile = var.iam_instance_profile == """" ? module.iam[0].iam_instance_profile : var.iam_instance_profile

  # Don't allow something not recommended within etcd scaling, set max deliberately and only control desired
  asg = { min : 1, max : 7, desired : var.servers }

  # TODO: Ideally set this to `length(var.servers)`, but currently blocked by: https://github.com/rancher/rke2/issues/349
  min_elb_capacity = 1

  tags = merge({
    ""Role"" = ""server"",
  }, local.ccm_tags, var.tags)
}
",module,"module ""servers"" {
  source = ""./modules/nodepool""
  name   = ""${local.uname}-server""

  vpc_id                      = var.vpc_id
  subnets                     = var.subnets
  ami                         = var.ami
  instance_type               = var.instance_type
  block_device_mappings       = var.block_device_mappings
  extra_block_device_mappings = var.extra_block_device_mappings
  vpc_security_group_ids = concat(
    [aws_security_group.cluster.id, aws_security_group.server.id],
  var.extra_security_group_ids)
  spot                        = var.spot
  target_group_arns           = local.target_group_arns
  wait_for_capacity_timeout   = var.wait_for_capacity_timeout
  metadata_options            = var.metadata_options
  associate_public_ip_address = var.associate_public_ip_address

  # Overrideable variables
  userdata             = data.cloudinit_config.this.rendered
  iam_instance_profile = var.iam_instance_profile == """" ? module.iam[0].iam_instance_profile : var.iam_instance_profile

  # Don't allow something not recommended within etcd scaling, set max deliberately and only control desired
  asg = {
    min                  = 1
    max                  = 7
    desired              = var.servers
    suspended_processes  = var.suspended_processes
    termination_policies = var.termination_policies
  }

  # TODO: Ideally set this to `length(var.servers)`, but currently blocked by: https://github.com/rancher/rke2/issues/349
  min_elb_capacity = 1

  tags = merge({
    ""Role"" = ""server"",
  }, local.ccm_tags, var.tags)
}
",module,193,224.0,e864f5bf26b72c24f35ffce486d801247e723582,26629708e56f3d961cfefaa19052e93d958f4469,https://github.com/rancherfederal/rke2-aws-tf/blob/e864f5bf26b72c24f35ffce486d801247e723582/main.tf#L193,https://github.com/rancherfederal/rke2-aws-tf/blob/26629708e56f3d961cfefaa19052e93d958f4469/main.tf#L224,2020-10-22 20:55:46-06:00,2023-12-18 14:11:04-06:00,26,0,1,1,1,0,1,0,0,0
https://github.com/uyuni-project/sumaform,122,openstack_host/main.tf,modules/openstack/host/main.tf,1,hack,// HACK: this output artificially depends on the instance id,"// HACK: this output artificially depends on the instance id 
 // any resource using this output will have to wait until instance is fully up","output ""hostname"" {
  // HACK: this output artificially depends on the instance id
  // any resource using this output will have to wait until instance is fully up
  value = ""${coalesce(""${var.name}.${var.domain}"", openstack_compute_instance_v2.instance.id)}""
}
",output,the block associated got renamed or deleted,,66,,7b44a14d4d6a74a6e0387ceb7238c62df9765b92,a3b700ceca72ca9a01cf59626300698dd6776ada,https://github.com/uyuni-project/sumaform/blob/7b44a14d4d6a74a6e0387ceb7238c62df9765b92/openstack_host/main.tf#L66,https://github.com/uyuni-project/sumaform/blob/a3b700ceca72ca9a01cf59626300698dd6776ada/modules/openstack/host/main.tf,2016-09-05 14:18:52+02:00,2017-02-23 07:41:20+01:00,15,1,1,1,0,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,476,modules/workers/instance.tf,modules/workers/instance.tf,0,todo,# TODO Not updateable remove when supported,"agent_config, # TODO Not updateable; remove when supported","resource ""oci_core_instance"" ""workers"" {
  for_each             = local.enabled_instances
  availability_domain  = element(each.value.availability_domains, 1)
  fault_domain         = try(each.value.placement_fds[0], null)
  compartment_id       = each.value.compartment_id
  display_name         = each.key
  preserve_boot_volume = false
  shape                = each.value.shape

  defined_tags            = each.value.defined_tags
  freeform_tags           = each.value.freeform_tags
  extended_metadata       = each.value.extended_metadata
  capacity_reservation_id = each.value.capacity_reservation_id

  dynamic ""shape_config"" {
    for_each = length(regexall(""Flex"", each.value.shape)) > 0 ? [1] : []
    content {
      ocpus = each.value.ocpus
      memory_in_gbs = ( # If > 64GB memory/core, correct input to exactly 64GB memory/core
        (each.value.memory / each.value.ocpus) > 64 ? each.value.ocpus * 64 : each.value.memory
      )
    }
  }

  dynamic ""platform_config"" {
    for_each = each.value.platform_config != null ? [1] : []
    content {
      type = lookup(
        # Attempt lookup against data source for the associated 'type' of configured worker shape
        lookup(local.platform_config_by_shape, each.value.shape, {}), ""type"",
        # Fall back to 'type' on pool with custom platform_config, or INTEL_VM default
        lookup(each.value.platform_config, ""type"", ""INTEL_VM"")
      )
      # Remaining parameters as configured, validated by instance/instance config resource
      are_virtual_instructions_enabled               = lookup(each.value.platform_config, ""are_virtual_instructions_enabled"", null)
      is_access_control_service_enabled              = lookup(each.value.platform_config, ""is_access_control_service_enabled"", null)
      is_input_output_memory_management_unit_enabled = lookup(each.value.platform_config, ""is_input_output_memory_management_unit_enabled"", null)
      is_measured_boot_enabled                       = lookup(each.value.platform_config, ""is_measured_boot_enabled"", null)
      is_memory_encryption_enabled                   = lookup(each.value.platform_config, ""is_memory_encryption_enabled"", null)
      is_secure_boot_enabled                         = lookup(each.value.platform_config, ""is_secure_boot_enabled"", null)
      is_symmetric_multi_threading_enabled           = lookup(each.value.platform_config, ""is_symmetric_multi_threading_enabled"", null)
      is_trusted_platform_module_enabled             = lookup(each.value.platform_config, ""is_trusted_platform_module_enabled"", null)
      numa_nodes_per_socket                          = lookup(each.value.platform_config, ""numa_nodes_per_socket"", null)
      percentage_of_cores_enabled                    = lookup(each.value.platform_config, ""percentage_of_cores_enabled"", null)
    }
  }

  agent_config {
    are_all_plugins_disabled = each.value.agent_config.are_all_plugins_disabled
    is_management_disabled   = each.value.agent_config.is_management_disabled
    is_monitoring_disabled   = each.value.agent_config.is_monitoring_disabled
    dynamic ""plugins_config"" {
      for_each = each.value.agent_config.plugins_config
      content {
        name          = each.key
        desired_state = each.value
      }
    }
  }

  create_vnic_details {
    assign_private_dns_record = var.assign_dns
    assign_public_ip          = each.value.assign_public_ip
    nsg_ids                   = each.value.nsg_ids
    subnet_id                 = each.value.subnet_id
    defined_tags              = each.value.defined_tags
    freeform_tags             = each.value.freeform_tags
  }

  instance_options {
    are_legacy_imds_endpoints_disabled = false
  }

  metadata = merge(
    {
      apiserver_host           = var.apiserver_private_host
      cluster_ca_cert          = var.cluster_ca_cert
      oke-k8version            = var.kubernetes_version
      oke-kubeproxy-proxy-mode = var.kubeproxy_mode
      oke-tenancy-id           = var.tenancy_id
      oke-initial-node-labels  = join("","", [for k, v in each.value.node_labels : format(""%v=%v"", k, v)])
      secondary_vnics          = jsonencode(lookup(each.value, ""secondary_vnics"", {}))
      ssh_authorized_keys      = var.ssh_public_key
      user_data                = lookup(lookup(data.cloudinit_config.workers, lookup(each.value, ""key"", """"), {}), ""rendered"", """")
    },

    # Only provide cluster DNS service address if set explicitly; determined automatically in practice.
    coalesce(var.cluster_dns, ""none"") == ""none"" ? {} : { kubedns_svc_ip = var.cluster_dns },

    # Extra user-defined fields merged last
    var.node_metadata,                       # global
    lookup(each.value, ""node_metadata"", {}), # pool-specific
  )

  source_details {
    boot_volume_size_in_gbs = each.value.boot_volume_size
    source_id               = each.value.image_id
    source_type             = ""image""
  }

  lifecycle {
    precondition {
      condition     = coalesce(each.value.image_id, ""none"") != ""none""
      error_message = <<-EOT
      Missing image_id; check provided value if image_type is 'custom', or image_os/image_os_version if image_type is 'oke' or 'platform'.
        pool: ${each.key}
        image_type: ${coalesce(each.value.image_type, ""none"")}
        image_id: ${coalesce(each.value.image_id, ""none"")}
      EOT
    }

    ignore_changes = [
      agent_config, # TODO Not updateable; remove when supported
      defined_tags, freeform_tags, display_name,
      metadata[""cluster_ca_cert""], metadata[""user_data""],
      create_vnic_details[0].defined_tags,
      create_vnic_details[0].freeform_tags,
    ]
  }
}
",resource,"resource ""oci_core_instance"" ""workers"" {
  for_each             = local.enabled_instances
  availability_domain  = element(each.value.availability_domains, 1)
  fault_domain         = try(each.value.placement_fds[0], null)
  compartment_id       = each.value.compartment_id
  display_name         = each.key
  preserve_boot_volume = false
  shape                = each.value.shape

  defined_tags            = each.value.defined_tags
  freeform_tags           = each.value.freeform_tags
  extended_metadata       = each.value.extended_metadata
  capacity_reservation_id = each.value.capacity_reservation_id

  dynamic ""shape_config"" {
    for_each = length(regexall(""Flex"", each.value.shape)) > 0 ? [1] : []
    content {
      ocpus = each.value.ocpus
      memory_in_gbs = ( # If > 64GB memory/core, correct input to exactly 64GB memory/core
        (each.value.memory / each.value.ocpus) > 64 ? each.value.ocpus * 64 : each.value.memory
      )
    }
  }

  dynamic ""platform_config"" {
    for_each = each.value.platform_config != null ? [1] : []
    content {
      type = lookup(
        # Attempt lookup against data source for the associated 'type' of configured worker shape
        lookup(local.platform_config_by_shape, each.value.shape, {}), ""type"",
        # Fall back to 'type' on pool with custom platform_config, or INTEL_VM default
        lookup(each.value.platform_config, ""type"", ""INTEL_VM"")
      )
      # Remaining parameters as configured, validated by instance/instance config resource
      are_virtual_instructions_enabled               = lookup(each.value.platform_config, ""are_virtual_instructions_enabled"", null)
      is_access_control_service_enabled              = lookup(each.value.platform_config, ""is_access_control_service_enabled"", null)
      is_input_output_memory_management_unit_enabled = lookup(each.value.platform_config, ""is_input_output_memory_management_unit_enabled"", null)
      is_measured_boot_enabled                       = lookup(each.value.platform_config, ""is_measured_boot_enabled"", null)
      is_memory_encryption_enabled                   = lookup(each.value.platform_config, ""is_memory_encryption_enabled"", null)
      is_secure_boot_enabled                         = lookup(each.value.platform_config, ""is_secure_boot_enabled"", null)
      is_symmetric_multi_threading_enabled           = lookup(each.value.platform_config, ""is_symmetric_multi_threading_enabled"", null)
      is_trusted_platform_module_enabled             = lookup(each.value.platform_config, ""is_trusted_platform_module_enabled"", null)
      numa_nodes_per_socket                          = lookup(each.value.platform_config, ""numa_nodes_per_socket"", null)
      percentage_of_cores_enabled                    = lookup(each.value.platform_config, ""percentage_of_cores_enabled"", null)
    }
  }

  agent_config {
    are_all_plugins_disabled = each.value.agent_config.are_all_plugins_disabled
    is_management_disabled   = each.value.agent_config.is_management_disabled
    is_monitoring_disabled   = each.value.agent_config.is_monitoring_disabled
    dynamic ""plugins_config"" {
      for_each = each.value.agent_config.plugins_config
      content {
        name          = plugins_config.key
        desired_state = plugins_config.value
      }
    }
  }

  create_vnic_details {
    assign_private_dns_record = var.assign_dns
    assign_public_ip          = each.value.assign_public_ip
    nsg_ids                   = each.value.nsg_ids
    subnet_id                 = each.value.subnet_id
    defined_tags              = each.value.defined_tags
    freeform_tags             = each.value.freeform_tags
  }

  instance_options {
    are_legacy_imds_endpoints_disabled = false
  }

  metadata = merge(
    {
      apiserver_host           = var.apiserver_private_host
      cluster_ca_cert          = var.cluster_ca_cert
      oke-k8version            = var.kubernetes_version
      oke-kubeproxy-proxy-mode = var.kubeproxy_mode
      oke-tenancy-id           = var.tenancy_id
      oke-initial-node-labels  = join("","", [for k, v in each.value.node_labels : format(""%v=%v"", k, v)])
      secondary_vnics          = jsonencode(lookup(each.value, ""secondary_vnics"", {}))
      ssh_authorized_keys      = var.ssh_public_key
      user_data                = lookup(lookup(data.cloudinit_config.workers, lookup(each.value, ""key"", """"), {}), ""rendered"", """")
    },

    # Only provide cluster DNS service address if set explicitly; determined automatically in practice.
    coalesce(var.cluster_dns, ""none"") == ""none"" ? {} : { kubedns_svc_ip = var.cluster_dns },

    # Extra user-defined fields merged last
    var.node_metadata,                       # global
    lookup(each.value, ""node_metadata"", {}), # pool-specific
  )

  source_details {
    boot_volume_size_in_gbs = each.value.boot_volume_size
    boot_volume_vpus_per_gb = each.value.boot_volume_vpus_per_gb
    source_id               = each.value.image_id
    source_type             = ""image""
  }

  lifecycle {
    precondition {
      condition     = coalesce(each.value.image_id, ""none"") != ""none""
      error_message = <<-EOT
      Missing image_id; check provided value if image_type is 'custom', or image_os/image_os_version if image_type is 'oke' or 'platform'.
        pool: ${each.key}
        image_type: ${coalesce(each.value.image_type, ""none"")}
        image_id: ${coalesce(each.value.image_id, ""none"")}
      EOT
    }

    ignore_changes = [
      agent_config, # TODO Not updateable; remove when supported
      defined_tags, freeform_tags, display_name,
      metadata[""cluster_ca_cert""], metadata[""user_data""],
      create_vnic_details[0].defined_tags,
      create_vnic_details[0].freeform_tags,
    ]
  }
}
",resource,113,114.0,9cc7db89975e38e16fc4ca25ad55bedc3117c2a9,a1fdfcb7de5e777f0191a43940ef276175a20ba9,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/9cc7db89975e38e16fc4ca25ad55bedc3117c2a9/modules/workers/instance.tf#L113,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/a1fdfcb7de5e777f0191a43940ef276175a20ba9/modules/workers/instance.tf#L114,2023-11-03 18:43:09-06:00,2024-02-12 09:53:40+11:00,3,0,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,393,modules/organization/firewall-policy.tf,modules/organization/firewall-policy.tf,0,fix,# TODO: remove once provider issues is fixed,"# TODO: remove once provider issues is fixed 
 # https://github.com/hashicorp/terraform-provider-google/issues/7790","resource ""google_compute_organization_security_policy_rule"" ""rule"" {
  provider                = google-beta
  for_each                = local.firewall_rules
  policy_id               = google_compute_organization_security_policy.policy[each.value.policy].id
  action                  = each.value.action
  direction               = each.value.direction
  priority                = try(each.value.priority, null)
  target_resources        = try(each.value.target_resources, null)
  target_service_accounts = try(each.value.target_service_accounts, null)
  enable_logging          = try(each.value.logging, null)
  # preview                 = each.value.preview
  match {
    description = each.value.description
    config {
      src_ip_ranges  = each.value.direction == ""INGRESS"" ? each.value.ranges : null
      dest_ip_ranges = each.value.direction == ""EGRESS"" ? each.value.ranges : null
      dynamic ""layer4_config"" {
        for_each = each.value.ports
        iterator = port
        content {
          ip_protocol = port.key
          ports       = port.value
        }
      }
    }
  }
  # TODO: remove once provider issues is fixed
  # https://github.com/hashicorp/terraform-provider-google/issues/7790
  lifecycle {
    ignore_changes = [description]
  }
}
",resource,the block associated got renamed or deleted,,95,,e2f5b96f4aecf420c3949f4685f0e5cab5d66799,f78902aee85d1ab10f1acf3b9727d188c349cdb9,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/e2f5b96f4aecf420c3949f4685f0e5cab5d66799/modules/organization/firewall-policy.tf#L95,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/f78902aee85d1ab10f1acf3b9727d188c349cdb9/modules/organization/firewall-policy.tf,2021-12-22 10:46:27+01:00,2021-12-31 13:06:35+01:00,3,1,0,1,1,1,0,0,0,0
https://github.com/alphagov/govuk-aws,1222,terraform/projects/app-locations-api/main.tf,terraform/projects/app-locations-api/main.tf,0,# todo,"# target_group_health_check_path   = ""/api/locations-api"" # TODO","# target_group_health_check_path   = ""/api/locations-api"" # TODO ","module ""internal_lb"" {
  source                           = ""../../modules/aws/lb""
  name                             = ""${var.stackname}-locations-api-internal""
  internal                         = true
  vpc_id                           = ""${data.terraform_remote_state.infra_vpc.vpc_id}""
  access_logs_bucket_name          = ""${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}""
  access_logs_bucket_prefix        = ""elb/locations-api-internal-lb""
  listener_certificate_domain_name = ""${var.elb_internal_certname}""
  # target_group_health_check_path   = ""/api/locations-api"" # TODO

  listener_action = {
    ""HTTPS:443"" = ""HTTP:80""
  }

  subnets         = [""${data.terraform_remote_state.infra_networking.private_subnet_ids}""]
  security_groups = [""${data.terraform_remote_state.infra_security_groups.sg_locations-api_internal_lb_id}""]
  alarm_actions   = [""${data.terraform_remote_state.infra_monitoring.sns_topic_cloudwatch_alarms_arn}""]

  default_tags = {
    Project         = ""${var.stackname}""
    aws_migration   = ""locations_api""
    aws_stackname   = ""${var.stackname}""
    aws_environment = ""${var.aws_environment}""
  }
}
",module,"module ""internal_lb"" {
  source                           = ""../../modules/aws/lb""
  name                             = ""${var.stackname}-locations-api-internal""
  internal                         = true
  vpc_id                           = ""${data.terraform_remote_state.infra_vpc.vpc_id}""
  access_logs_bucket_name          = ""${data.terraform_remote_state.infra_monitoring.aws_logging_bucket_id}""
  access_logs_bucket_prefix        = ""elb/locations-api-internal-lb""
  listener_certificate_domain_name = ""${var.elb_internal_certname}""

  listener_action = {
    ""HTTPS:443"" = ""HTTP:80""
  }

  subnets         = [""${data.terraform_remote_state.infra_networking.private_subnet_ids}""]
  security_groups = [""${data.terraform_remote_state.infra_security_groups.sg_locations-api_internal_lb_id}""]
  alarm_actions   = [""${data.terraform_remote_state.infra_monitoring.sns_topic_cloudwatch_alarms_arn}""]

  default_tags = {
    Project         = ""${var.stackname}""
    aws_migration   = ""locations_api""
    aws_stackname   = ""${var.stackname}""
    aws_environment = ""${var.aws_environment}""
  }
}
",module,106,,fb9927c2442fd1464aed4736d4625b1b5ff90faf,e94ad50dd6fefb46c096d89f74c946c74ff50322,https://github.com/alphagov/govuk-aws/blob/fb9927c2442fd1464aed4736d4625b1b5ff90faf/terraform/projects/app-locations-api/main.tf#L106,https://github.com/alphagov/govuk-aws/blob/e94ad50dd6fefb46c096d89f74c946c74ff50322/terraform/projects/app-locations-api/main.tf,2022-03-10 07:52:45+00:00,2022-03-10 07:53:37+00:00,2,1,0,1,0,0,1,0,0,0
https://github.com/alphagov/govuk-aws,826,terraform/modules/aws/lb_listener_rules/main.tf,terraform/modules/aws/lb_listener_rules/main.tf,0,implement,"/** * ## Modules: aws/lb_listener_rules * * This module creates Load Balancer listener rules and target groups for * an existing listener resource. * * Limitations: *  - The target group deregistration_delay, health_check_interval and health_check_timeout * values can be configured with variables, but will be the same for all the target groups *  - With Terraform we can't provide a 'count' or list for listener_rule condition blocks, * so at the moment only one condition can be specified per rule *  - At the moment this module only implements Host Header based rules*/","/** 
 * ## Modules: aws/lb_listener_rules 
 * 
 * This module creates Load Balancer listener rules and target groups for 
 * an existing listener resource. 
 * 
 * Limitations: 
 *  - The target group deregistration_delay, health_check_interval and health_check_timeout 
 * values can be configured with variables, but will be the same for all the target groups 
 *  - With Terraform we can't provide a 'count' or list for listener_rule condition blocks, 
 * so at the moment only one condition can be specified per rule 
 *  - At the moment this module only implements Host Header based rules 
 */ ","variable ""default_tags"" {
  type        = ""map""
  description = ""Additional resource tags""
  default     = {}
}
",variable,"variable ""default_tags"" {
  type        = ""map""
  description = ""Additional resource tags""
  default     = {}
}
",variable,1,,5a798362e1d5c877a960847379a7aa278ec185d0,a272a6f68c811aa9bb956809fd9e2caec8fa8089,https://github.com/alphagov/govuk-aws/blob/5a798362e1d5c877a960847379a7aa278ec185d0/terraform/modules/aws/lb_listener_rules/main.tf#L1,https://github.com/alphagov/govuk-aws/blob/a272a6f68c811aa9bb956809fd9e2caec8fa8089/terraform/modules/aws/lb_listener_rules/main.tf,2019-05-24 14:55:00+01:00,2019-06-12 17:18:39+01:00,4,1,0,1,1,0,1,0,0,0
https://github.com/kubernetes/k8s.io,485,infra/aws/terraform/kops-infra-ci/eks.tf,infra/aws/terraform/kops-infra-ci/eks.tf,0,//todo,//TODO(ameukam): Use access entries,//TODO(ameukam): Use access entries,"module ""eks-auth"" {
  source  = ""terraform-aws-modules/eks/aws//modules/aws-auth""
  version = ""~> 20.0""

  manage_aws_auth_configmap = true

  aws_auth_roles = [
    {
      rolearn  = ""arn:aws:iam::468814281478:role/Prow-EKS-Admin""
      username = ""arn:aws:iam::468814281478:role/Prow-EKS-Admin""
      groups   = [""system:masters""]
    },
  ]

  aws_auth_users = [
    {
      userarn  = ""arn:aws:iam::${data.aws_organizations_organization.current.id}:user/ameukam""
      username = ""ameukam""
      groups   = [""system:masters""]
    },
  ]
}
",module,"module ""eks-auth"" {
  source  = ""terraform-aws-modules/eks/aws//modules/aws-auth""
  version = ""~> 20.0""

  manage_aws_auth_configmap = true

  aws_auth_roles = [
    {
      rolearn  = ""arn:aws:iam::468814281478:role/Prow-EKS-Admin""
      username = ""arn:aws:iam::468814281478:role/Prow-EKS-Admin""
      groups   = [""system:masters""]
    },
  ]

  aws_auth_users = [
    {
      userarn  = ""arn:aws:iam::${data.aws_organizations_organization.current.id}:user/ameukam""
      username = ""ameukam""
      groups   = [""system:masters""]
    },
  ]
}
",module,146,146.0,d67626296482f3df01968377c828ffac093efee8,d67626296482f3df01968377c828ffac093efee8,https://github.com/kubernetes/k8s.io/blob/d67626296482f3df01968377c828ffac093efee8/infra/aws/terraform/kops-infra-ci/eks.tf#L146,https://github.com/kubernetes/k8s.io/blob/d67626296482f3df01968377c828ffac093efee8/infra/aws/terraform/kops-infra-ci/eks.tf#L146,2024-03-12 17:14:02+01:00,2024-03-12 17:14:02+01:00,1,0,1,0,0,1,0,0,0,0
https://github.com/rust-lang/simpleinfra,13,terragrunt/modules/docs-rs/cloudfront.tf,terragrunt/modules/docs-rs/cloudfront.tf,0,// todo,// TODO: replace with real origin,// TODO: replace with real origin,"resource ""aws_cloudfront_distribution"" ""webapp"" {
  comment = var.domain

  enabled             = true
  wait_for_deployment = false
  is_ipv6_enabled     = true
  price_class         = ""PriceClass_All""
  http_version        = ""http2and3""

  aliases = [var.domain]
  viewer_certificate {
    acm_certificate_arn      = module.certificate.arn
    ssl_support_method       = ""sni-only""
    minimum_protocol_version = ""TLSv1.1_2016""
  }

  default_cache_behavior {
    target_origin_id       = ""ec2""
    allowed_methods        = [""GET"", ""HEAD"", ""OPTIONS""]
    cached_methods         = [""GET"", ""HEAD"", ""OPTIONS""]
    compress               = true
    viewer_protocol_policy = ""redirect-to-https""

    default_ttl = 900 // 15 minutes
    min_ttl     = 0
    max_ttl     = 31536000 // 1 year

    forwarded_values {
      headers = [
        // Allow detecting HTTPS from the webapp
        ""CloudFront-Forwarded-Proto"",
        // Allow detecting the domain name from the webapp
        ""Host"",
      ]
      query_string = true
      cookies {
        forward = ""none""
      }
    }
  }

  origin {
    origin_id = ""ec2""
    // TODO: replace with real origin
    domain_name = ""http://example.com""

    custom_origin_config {
      http_port              = 80
      https_port             = 443
      origin_protocol_policy = ""http-only""
      origin_ssl_protocols   = [""TLSv1.2""]
    }
  }

  restrictions {
    geo_restriction {
      restriction_type = ""none""
    }
  }

  // Stop CloudFront from caching error responses.
  //
  // Before we did this users were seeing error pages even after we resolved
  // outages, forcing us to invalidate the caches every time. The team agreed
  // the best solution was instead to stop CloudFront from caching error
  // responses altogether.
  dynamic ""custom_error_response"" {
    for_each = toset([400, 403, 404, 405, 414, 500, 501, 502, 503, 504])
    content {
      error_code            = custom_error_response.value
      error_caching_min_ttl = 0
    }
  }
}
",resource,"resource ""aws_cloudfront_distribution"" ""webapp"" {
  comment = var.domain

  enabled             = true
  wait_for_deployment = false
  is_ipv6_enabled     = true
  price_class         = ""PriceClass_All""
  http_version        = ""http2and3""

  aliases = [var.domain]
  viewer_certificate {
    acm_certificate_arn      = module.certificate.arn
    ssl_support_method       = ""sni-only""
    minimum_protocol_version = ""TLSv1.1_2016""
  }

  default_cache_behavior {
    target_origin_id       = ""ec2""
    allowed_methods        = [""GET"", ""HEAD"", ""OPTIONS""]
    cached_methods         = [""GET"", ""HEAD"", ""OPTIONS""]
    compress               = true
    viewer_protocol_policy = ""redirect-to-https""

    default_ttl = 900 // 15 minutes
    min_ttl     = 0
    max_ttl     = 31536000 // 1 year

    forwarded_values {
      headers = [
        // Allow detecting HTTPS from the webapp
        ""CloudFront-Forwarded-Proto"",
        // Allow detecting the domain name from the webapp
        ""Host"",
      ]
      query_string = true
      cookies {
        forward = ""none""
      }
    }
  }

  origin {
    origin_id   = ""ec2""
    domain_name = local.web_domain

    custom_origin_config {
      http_port              = 80
      https_port             = 443
      origin_protocol_policy = ""http-only""
      origin_ssl_protocols   = [""TLSv1.2""]
    }
  }

  restrictions {
    geo_restriction {
      restriction_type = ""none""
    }
  }

  // Stop CloudFront from caching error responses.
  //
  // Before we did this users were seeing error pages even after we resolved
  // outages, forcing us to invalidate the caches every time. The team agreed
  // the best solution was instead to stop CloudFront from caching error
  // responses altogether.
  dynamic ""custom_error_response"" {
    for_each = toset([400, 403, 404, 405, 414, 500, 501, 502, 503, 504])
    content {
      error_code            = custom_error_response.value
      error_caching_min_ttl = 0
    }
  }
}
",resource,54,,0b061f4b31ad08a3761960de431377549e76d0c4,8d588e18da3b236eaa53a9920a5af7219450cf75,https://github.com/rust-lang/simpleinfra/blob/0b061f4b31ad08a3761960de431377549e76d0c4/terragrunt/modules/docs-rs/cloudfront.tf#L54,https://github.com/rust-lang/simpleinfra/blob/8d588e18da3b236eaa53a9920a5af7219450cf75/terragrunt/modules/docs-rs/cloudfront.tf,2023-01-04 17:28:18+01:00,2023-01-10 20:16:01+01:00,3,1,1,1,0,0,1,0,0,0
https://github.com/pivotal-cf/terraforming-azure,12,weblb.tf,weblb.tf,0,workaround,# Workaround until the backend_address_pool and probe resources output their own ids,# Workaround until the backend_address_pool and probe resources output their own ids,"resource ""azurerm_lb_rule"" ""web-http-rule"" {
  name                = ""web-http-rule""
  location            = ""${var.location}""
  resource_group_name = ""${azurerm_resource_group.pcf_resource_group.name}""
  loadbalancer_id     = ""${azurerm_lb.web.id}""

  frontend_ip_configuration_name = ""frontendip""
  protocol                       = ""Tcp""
  frontend_port                  = 80
  backend_port                   = 80

  # Workaround until the backend_address_pool and probe resources output their own ids
  backend_address_pool_id = ""${azurerm_lb.web.id}/backendAddressPools/${azurerm_lb_backend_address_pool.web-backend-pool.name}""
  probe_id                = ""${azurerm_lb.web.id}/probes/${azurerm_lb_probe.web-http-probe.name}""
}
",resource,"resource ""azurerm_lb_rule"" ""web-http-rule"" {
  name                = ""web-http-rule""
  location            = ""${var.location}""
  resource_group_name = ""${azurerm_resource_group.pcf_resource_group.name}""
  loadbalancer_id     = ""${azurerm_lb.web.id}""

  frontend_ip_configuration_name = ""frontendip""
  protocol                       = ""TCP""
  frontend_port                  = 80
  backend_port                   = 80

  backend_address_pool_id = ""${azurerm_lb_backend_address_pool.web-backend-pool.id}""
  probe_id                = ""${azurerm_lb_probe.web-http-probe.id}""
}
",resource,71,,033638f5608bf6bbec84abe9557692c1b431e719,50e14eaf3aeee1711cd2335ffa73774fec13c312,https://github.com/pivotal-cf/terraforming-azure/blob/033638f5608bf6bbec84abe9557692c1b431e719/weblb.tf#L71,https://github.com/pivotal-cf/terraforming-azure/blob/50e14eaf3aeee1711cd2335ffa73774fec13c312/weblb.tf,2016-10-17 13:49:20-07:00,2016-11-10 17:43:22-08:00,7,1,0,1,0,0,1,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,394,azure/network.tf,azure/network.tf,0,todo,// TODO make the monitoring rule more stricter for some instance later on,"// Todo this security group need to be redifined by cluster roles instead of 1 generic 
 // TODO make the monitoring rule more stricter for some instance later on","resource ""azurerm_network_security_group"" ""mysecgroup"" {
  name                = ""mysecgroup""
  location            = var.az_region
  resource_group_name = azurerm_resource_group.myrg.name
// Todo this security group need to be redifined by cluster roles instead of 1 generic
// TODO make the monitoring rule more stricter for some instance later on
  security_rule {
    name                       = ""OUTALL""
    priority                   = 100
    direction                  = ""Outbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""*""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""LOCAL""
    priority                   = 101
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""*""
    source_address_prefix      = ""10.74.0.0/16""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""SSH""
    priority                   = 1001
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""22""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HTTP""
    priority                   = 1002
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""80""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HTTPS""
    priority                   = 1003
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""443""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HAWK""
    priority                   = 1004
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""7630""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  // monitoring rules
    security_rule {
    name                       = ""nodeExporter""
    priority                   = 1005
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9100""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  security_rule {
    name                       = ""hanadbExporter""
    priority                   = 1006
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""8001""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  security_rule {
    name                       = ""hawkExporter""
    priority                   = 1007
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9001""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""prometheus""
    priority                   = 1008
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9090""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }


  tags = {
    workspace = terraform.workspace
  }
}
",resource,"resource ""azurerm_network_security_group"" ""mysecgroup"" {
  name                = ""mysecgroup""
  location            = var.az_region
  resource_group_name = azurerm_resource_group.myrg.name
  security_rule {
    name                       = ""OUTALL""
    priority                   = 100
    direction                  = ""Outbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""*""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""LOCAL""
    priority                   = 101
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""*""
    source_address_prefix      = ""10.74.0.0/16""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""SSH""
    priority                   = 1001
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""22""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HTTP""
    priority                   = 1002
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""80""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HTTPS""
    priority                   = 1003
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""443""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""HAWK""
    priority                   = 1004
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""Tcp""
    source_port_range          = ""*""
    destination_port_range     = ""7630""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  // monitoring rules
  security_rule {
    name                       = ""nodeExporter""
    priority                   = 1005
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9100""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  security_rule {
    name                       = ""hanadbExporter""
    priority                   = 1006
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""8001""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }
  security_rule {
    name                       = ""hawkExporter""
    priority                   = 1007
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9001""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }

  security_rule {
    name                       = ""prometheus""
    priority                   = 1008
    direction                  = ""Inbound""
    access                     = ""Allow""
    protocol                   = ""*""
    source_port_range          = ""*""
    destination_port_range     = ""9090""
    source_address_prefix      = ""*""
    destination_address_prefix = ""*""
  }


  tags = {
    workspace = terraform.workspace
  }
}
",resource,305,,517f39e8d665afe6d504a325cda67274995c8e59,9adcf152486838f9f5b600ead5e4ef373918a786,https://github.com/SUSE/ha-sap-terraform-deployments/blob/517f39e8d665afe6d504a325cda67274995c8e59/azure/network.tf#L305,https://github.com/SUSE/ha-sap-terraform-deployments/blob/9adcf152486838f9f5b600ead5e4ef373918a786/azure/network.tf,2019-09-05 00:02:24+02:00,2019-09-05 18:08:13+02:00,3,1,0,0,0,1,1,0,1,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1308,blueprints/data-solutions/shielded-folder/variables.tf,blueprints/data-solutions/shielded-folder/variables.tf,0,#todo,"#TODO data-security  = ""gcp-data-security""","#TODO data-security  = ""gcp-data-security""","variable ""groups"" {
  description = ""User groups.""
  type        = map(string)
  default = {
    #TODO data-analysts  = ""gcp-data-analysts""
    data-engineers = ""gcp-data-engineers""
    #TODO data-security  = ""gcp-data-security""
  }
}
",variable,"variable ""groups"" {
  description = ""User groups.""
  type        = map(string)
  default = {
    #TODO data-analysts  = ""gcp-data-analysts""
    data-engineers = ""gcp-data-engineers""
    data-security  = ""gcp-data-security""
  }
}
",variable,59,,84be665172b21220938ee702c4654e1a0cd0a584,4007d42705a930e9e526a8da3616712ad0e646f6,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/84be665172b21220938ee702c4654e1a0cd0a584/blueprints/data-solutions/shielded-folder/variables.tf#L59,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/4007d42705a930e9e526a8da3616712ad0e646f6/blueprints/data-solutions/shielded-folder/variables.tf,2023-01-17 08:49:04+01:00,2023-01-21 01:08:51+01:00,3,1,0,1,0,1,0,0,0,0
https://github.com/kbst/terraform-kubestack,2,azurerm/_modules/aks/cluster_services.tf,azurerm/_modules/aks/cluster_services.tf,0,hack,"# hack, because modules can't have depends_on","# hack, because modules can't have depends_on 
 # prevent a race between kubernetes provider and cluster services/kustomize 
 # creating the namespace and the provider erroring out during apply","module ""cluster_services"" {
  source = ""../../../common/cluster_services""

  cluster_type = ""aks""

  metadata_labels = ""${var.metadata_labels}""
  label_namespace = ""${var.metadata_label_namespace}""

  template_string = ""${file(""${path.module}/templates/kubeconfig.tpl"")}""

  template_vars = {
    cluster_name     = ""${azurerm_kubernetes_cluster.current.name}""
    cluster_endpoint = ""${azurerm_kubernetes_cluster.current.kube_config.0.host}""
    cluster_ca       = ""${azurerm_kubernetes_cluster.current.kube_config.0.cluster_ca_certificate}""
    client_cert      = ""${azurerm_kubernetes_cluster.current.kube_config.0.client_certificate}""
    client_key       = ""${azurerm_kubernetes_cluster.current.kube_config.0.client_key}""
    path_cwd         = ""${path.cwd}""

    # hack, because modules can't have depends_on
    # prevent a race between kubernetes provider and cluster services/kustomize
    # creating the namespace and the provider erroring out during apply
    not_used = ""${kubernetes_namespace.current.metadata.0.name}""
  }
}
",module,"module ""cluster_services"" {
  source = ""../../../common/cluster_services""

  cluster_type = ""aks""

  metadata_labels = var.metadata_labels
  label_namespace = var.metadata_label_namespace

  template_string = file(""${path.module}/templates/kubeconfig.tpl"")

  template_vars = {
    cluster_name     = azurerm_kubernetes_cluster.current.name
    cluster_endpoint = azurerm_kubernetes_cluster.current.kube_config[0].host
    cluster_ca       = azurerm_kubernetes_cluster.current.kube_config[0].cluster_ca_certificate
    client_cert      = azurerm_kubernetes_cluster.current.kube_config[0].client_certificate
    client_key       = azurerm_kubernetes_cluster.current.kube_config[0].client_key
    path_cwd         = path.cwd
  }
}
",module,19,,9dca6071a0ffed2973ecc836044700432e3a5af3,e5caa6d20926d546a045144ebe79c7cc8c0b4c8a,https://github.com/kbst/terraform-kubestack/blob/9dca6071a0ffed2973ecc836044700432e3a5af3/azurerm/_modules/aks/cluster_services.tf#L19,https://github.com/kbst/terraform-kubestack/blob/e5caa6d20926d546a045144ebe79c7cc8c0b4c8a/azurerm/_modules/aks/cluster_services.tf,2019-03-31 11:29:36+02:00,2020-01-26 13:45:22+01:00,3,1,1,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1594,blueprints/factories/project-factory/main.tf,modules/project-factory/main.tf,1,# todo,# TODO: concat lists for each key,# TODO: concat lists for each key,"module ""projects"" {
  source              = ""../../../modules/project""
  for_each            = local.projects
  billing_account     = each.value.billing_account
  name                = each.key
  parent              = try(each.value.parent, null)
  prefix              = each.value.prefix
  auto_create_network = try(each.value.auto_create_network, false)
  compute_metadata    = try(each.value.compute_metadata, {})
  # TODO: concat lists for each key
  contacts = merge(
    each.value.contacts, var.data_merges.contacts
  )
  default_service_account = try(each.value.default_service_account, ""keep"")
  descriptive_name        = try(each.value.descriptive_name, null)
  group_iam               = try(each.value.group_iam, {})
  iam                     = try(each.value.iam, {})
  iam_bindings            = try(each.value.iam_bindings, {})
  iam_bindings_additive   = try(each.value.iam_bindings_additive, {})
  labels                  = each.value.labels
  lien_reason             = try(each.value.lien_reason, null)
  logging_data_access     = try(each.value.logging_data_access, {})
  logging_exclusions      = try(each.value.logging_exclusions, {})
  logging_sinks           = try(each.value.logging_sinks, {})
  metric_scopes = distinct(concat(
    each.value.metric_scopes, var.data_merges.metric_scopes
  ))
  service_encryption_key_ids = merge(
    each.value.service_encryption_key_ids,
    var.data_merges.service_encryption_key_ids
  )
  service_perimeter_bridges = distinct(concat(
    each.value.service_perimeter_bridges,
    var.data_merges.service_perimeter_bridges
  ))
  service_perimeter_standard = each.value.service_perimeter_standard
  services = distinct(concat(
    each.value.services,
    var.data_merges.services
  ))
  shared_vpc_service_config = each.value.shared_vpc_service_config
  tag_bindings = merge(
    each.value.tag_bindings,
    var.data_merges.tag_bindings
  )
}
",module,"module ""projects"" {
  source          = ""../project""
  for_each        = local.projects
  billing_account = each.value.billing_account
  name            = each.key
  parent = try(
    lookup(local.hierarchy, each.value.parent, each.value.parent), null
  )
  prefix              = each.value.prefix
  auto_create_network = try(each.value.auto_create_network, false)
  compute_metadata    = try(each.value.compute_metadata, {})
  # TODO: concat lists for each key
  contacts = merge(
    each.value.contacts, var.data_merges.contacts
  )
  default_service_account = try(each.value.default_service_account, ""keep"")
  descriptive_name        = try(each.value.descriptive_name, null)
  # IAM interpolates automation service accounts
  iam = {
    for k, v in lookup(each.value, ""iam"", {}) : k => [
      for vv in v : try(
        module.automation-service-accounts[""${each.key}/${vv}""].iam_email,
        vv
      )
    ]
  }
  iam_bindings = {
    for k, v in lookup(each.value, ""iam_bindings"", {}) : k => merge(v, {
      members = [
        for vv in v.members : try(
          module.automation-service-accounts[""${each.key}/${vv}""].iam_email,
          vv
        )
      ]
    })
  }
  iam_bindings_additive = {
    for k, v in lookup(each.value, ""iam_bindings_additive"", {}) : k => merge(v, {
      member = try(
        module.automation-service-accounts[""${each.key}/${v.member}""].iam_email,
        v.member
      )
    })
  }
  # IAM principals would trigger dynamic key errors so we don't interpolate
  iam_by_principals = try(each.value.iam_by_principals, {})
  labels = merge(
    each.value.labels, var.data_merges.labels
  )
  lien_reason         = try(each.value.lien_reason, null)
  logging_data_access = try(each.value.logging_data_access, {})
  logging_exclusions  = try(each.value.logging_exclusions, {})
  logging_sinks       = try(each.value.logging_sinks, {})
  metric_scopes = distinct(concat(
    each.value.metric_scopes, var.data_merges.metric_scopes
  ))
  org_policies = each.value.org_policies
  service_encryption_key_ids = merge(
    each.value.service_encryption_key_ids,
    var.data_merges.service_encryption_key_ids
  )
  services = distinct(concat(
    each.value.services,
    var.data_merges.services
  ))
  shared_vpc_host_config    = each.value.shared_vpc_host_config
  shared_vpc_service_config = each.value.shared_vpc_service_config
  tag_bindings = merge(
    each.value.tag_bindings,
    var.data_merges.tag_bindings
  )
  vpc_sc = each.value.vpc_sc
}
",module,26,30.0,819894d2bab4b440f1b52b1ac8035912fb107004,ef5178c92901980294a90dbce384baa6b6479547,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/819894d2bab4b440f1b52b1ac8035912fb107004/blueprints/factories/project-factory/main.tf#L26,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/ef5178c92901980294a90dbce384baa6b6479547/modules/project-factory/main.tf#L30,2023-08-20 09:44:20+02:00,2024-05-22 07:56:34+00:00,13,0,0,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-lb-http,4,main.tf,main.tf,0,# todo,# TODO: Infer type of health check to use from backends.protocol,# TODO: Infer type of health check to use from backends.protocol,"resource ""google_compute_health_check"" ""default"" {
  for_each = var.backends
  project  = var.project
  name     = ""${var.name}-backend-${each.key}""

  check_interval_sec  = lookup(each.value[""health_check""], ""check_interval_sec"", 5)
  timeout_sec         = lookup(each.value[""health_check""], ""timeout_sec"", 5)
  healthy_threshold   = lookup(each.value[""health_check""], ""healthy_threshold"", 2)
  unhealthy_threshold = lookup(each.value[""health_check""], ""unhealthy_threshold"", 2)

  # TODO: Infer type of health check to use from backends.protocol
  dynamic ""http_health_check"" {
    for_each = lookup(each.value[""health_check""], ""http_health_check"", {}) == {} ? [] : [each.value[""health_check""][""http_health_check""]]
    content {
      host         = lookup(http_health_check.value, ""host"", null)
      request_path = lookup(http_health_check.value, ""request_path"", null)
      response     = lookup(http_health_check.value, ""response"", null)

      port               = lookup(http_health_check.value, ""port"", null)
      port_name          = lookup(http_health_check.value, ""port_name"", null)
      port_specification = lookup(http_health_check.value, ""port_specification"", null)
    }
  }

  dynamic ""https_health_check"" {
    for_each = lookup(each.value[""health_check""], ""https_health_check"", {}) == {} ? [] : [each.value[""health_check""][""https_health_check""]]

    content {
      host         = lookup(https_health_check.value, ""host"", null)
      request_path = lookup(https_health_check.value, ""request_path"", null)
      response     = lookup(https_health_check.value, ""response"", null)

      port               = lookup(https_health_check.value, ""port"", null)
      port_name          = lookup(https_health_check.value, ""port_name"", null)
      port_specification = lookup(https_health_check.value, ""port_specification"", null)
    }
  }

  dynamic ""http2_health_check"" {
    for_each = lookup(each.value[""health_check""], ""http2_health_check"", {}) == {} ? [] : [each.value[""health_check""][""http2_health_check""]]

    content {
      host         = lookup(http2_health_check.value, ""host"", null)
      request_path = lookup(http2_health_check.value, ""request_path"", null)
      response     = lookup(http2_health_check.value, ""response"", null)

      port               = lookup(http2_health_check.value, ""port"", null)
      port_name          = lookup(http2_health_check.value, ""port_name"", null)
      port_specification = lookup(http2_health_check.value, ""port_specification"", null)
    }
  }

}
",resource,"resource ""google_compute_health_check"" ""default"" {
  for_each = var.backends
  project  = var.project
  name     = ""${var.name}-backend-${each.key}""

  check_interval_sec  = lookup(each.value[""health_check""], ""check_interval_sec"", 5)
  timeout_sec         = lookup(each.value[""health_check""], ""timeout_sec"", 5)
  healthy_threshold   = lookup(each.value[""health_check""], ""healthy_threshold"", 2)
  unhealthy_threshold = lookup(each.value[""health_check""], ""unhealthy_threshold"", 2)

  dynamic ""http_health_check"" {
    for_each = each.value[""protocol""] == ""HTTP"" ? [
      {
        host               = lookup(each.value[""health_check""], ""host"", null)
        request_path       = lookup(each.value[""health_check""], ""request_path"", null)
        response           = lookup(each.value[""health_check""], ""response"", null)
        port               = lookup(each.value[""health_check""], ""port"", null)
        port_name          = lookup(each.value[""health_check""], ""port_name"", null)
        port_specification = lookup(each.value[""health_check""], ""port_specification"", null)
      }
    ] : []

    content {
      host         = lookup(http_health_check.value, ""host"", null)
      request_path = lookup(http_health_check.value, ""request_path"", null)
      response     = lookup(http_health_check.value, ""response"", null)

      port               = lookup(http_health_check.value, ""port"", null)
      port_name          = lookup(http_health_check.value, ""port_name"", null)
      port_specification = lookup(http_health_check.value, ""port_specification"", null)
    }
  }

  dynamic ""https_health_check"" {
    for_each = each.value[""protocol""] == ""HTTPS"" ? [
      {
        host               = lookup(each.value[""health_check""], ""host"", null)
        request_path       = lookup(each.value[""health_check""], ""request_path"", null)
        response           = lookup(each.value[""health_check""], ""response"", null)
        port               = lookup(each.value[""health_check""], ""port"", null)
        port_name          = lookup(each.value[""health_check""], ""port_name"", null)
        port_specification = lookup(each.value[""health_check""], ""port_specification"", null)
      }
    ] : []

    content {
      host         = lookup(https_health_check.value, ""host"", null)
      request_path = lookup(https_health_check.value, ""request_path"", null)
      response     = lookup(https_health_check.value, ""response"", null)

      port               = lookup(https_health_check.value, ""port"", null)
      port_name          = lookup(https_health_check.value, ""port_name"", null)
      port_specification = lookup(https_health_check.value, ""port_specification"", null)
    }
  }

  dynamic ""http2_health_check"" {
    for_each = each.value[""protocol""] == ""HTTP2"" ? [
      {
        host               = lookup(each.value[""health_check""], ""host"", null)
        request_path       = lookup(each.value[""health_check""], ""request_path"", null)
        response           = lookup(each.value[""health_check""], ""response"", null)
        port               = lookup(each.value[""health_check""], ""port"", null)
        port_name          = lookup(each.value[""health_check""], ""port_name"", null)
        port_specification = lookup(each.value[""health_check""], ""port_specification"", null)
      }
    ] : []

    content {
      host         = lookup(http2_health_check.value, ""host"", null)
      request_path = lookup(http2_health_check.value, ""request_path"", null)
      response     = lookup(http2_health_check.value, ""response"", null)

      port               = lookup(http2_health_check.value, ""port"", null)
      port_name          = lookup(http2_health_check.value, ""port_name"", null)
      port_specification = lookup(http2_health_check.value, ""port_specification"", null)
    }
  }

}
",resource,149,,9e9fc7746c7b1dfeaa1506f09e0410e0e70d18d7,67ae7540846611edb3311d7e668158e98a7c8135,https://github.com/terraform-google-modules/terraform-google-lb-http/blob/9e9fc7746c7b1dfeaa1506f09e0410e0e70d18d7/main.tf#L149,https://github.com/terraform-google-modules/terraform-google-lb-http/blob/67ae7540846611edb3311d7e668158e98a7c8135/main.tf,2019-10-23 18:15:14+01:00,2019-10-23 18:15:14+01:00,2,1,0,0,0,0,1,0,1,0
https://github.com/compiler-explorer/infra,66,terraform/audit.tf,terraform/audit.tf,0,todo,// TODO one day,"  // TODO one day
  //  versioning {
  //    mfa_delete = true
  //  }","resource ""aws_s3_bucket"" ""cloudtrail"" {
  bucket        = ""cloudtrail.godbolt.org""
  force_destroy = true

  // TODO one day
  //  versioning {
  //    mfa_delete = true
  //  }

  lifecycle_rule {
    enabled = true
    expiration {
      days = 200
    }
    noncurrent_version_expiration {
      days = 1
    }
  }
}
",resource,"resource ""aws_s3_bucket"" ""cloudtrail"" {
  bucket        = ""cloudtrail.godbolt.org""
  force_destroy = true

  tags = {
    S3-Bucket-Name = ""cloudtrail.godbolt.org""
  }
}
",resource,47,,b15a4a55a344a8ab58d9b1ca511e08f252034bd2,fc51de832669630cc2f3cad2d8e3f5f838f16c94,https://github.com/compiler-explorer/infra/blob/b15a4a55a344a8ab58d9b1ca511e08f252034bd2/terraform/audit.tf#L47,https://github.com/compiler-explorer/infra/blob/fc51de832669630cc2f3cad2d8e3f5f838f16c94/terraform/audit.tf,2020-06-15 22:57:58-05:00,2024-03-10 19:26:48-05:00,8,1,1,1,0,0,0,0,0,0
https://github.com/Azure/terraform-azurerm-caf-enterprise-scale,2,archetypes/locals.policy_definitions.tf,modules/archetypes/locals.policy_definitions.tf,1,implemented,# Logic implemented to determine whether Policy Definitions,"# Generate the Policy Definition configurations for the specified archetype. 
 # Logic implemented to determine whether Policy Definitions 
 # need to be loaded to save on compute and memory resources 
 # when none defined in archetype definition.","locals {
  archetype_policy_definitions_list      = local.archetype_definition.policy_definitions
  archetype_policy_definitions_specified = try(length(local.archetype_policy_definitions_list) > 0, false)
}
",locals,"locals {
  archetype_policy_definitions_list      = local.archetype_definition.policy_definitions
  archetype_policy_definitions_specified = try(length(local.archetype_policy_definitions_list) > 0, false)
}
",locals,2,2.0,a05be92e463f90e577936a84395ffb88cfd045e7,1b41f7062f286e790be9b2e16a74562936f691df,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/a05be92e463f90e577936a84395ffb88cfd045e7/archetypes/locals.policy_definitions.tf#L2,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/1b41f7062f286e790be9b2e16a74562936f691df/modules/archetypes/locals.policy_definitions.tf#L2,2020-09-25 20:39:19+01:00,2022-01-27 09:57:26+00:00,6,0,0,0,0,1,0,0,0,0
https://github.com/alphagov/govuk-aws,699,terraform/projects/app-draft-cache/main.tf,terraform/projects/app-draft-cache/main.tf,0,todo,# TODO publicapi is a special set of nginx config that routes /api requests to,"# TODO publicapi is a special set of nginx config that routes /api requests to 
 # their relevant apps upstream.","resource ""aws_route53_record"" ""draft-cache_publicapi_service_record"" {
  zone_id = ""${data.terraform_remote_state.infra_stack_dns_zones.internal_zone_id}""
  name    = ""draft-publicapi.${data.terraform_remote_state.infra_stack_dns_zones.internal_domain_name}""
  type    = ""CNAME""
  records = [""draft-cache.${data.terraform_remote_state.infra_stack_dns_zones.internal_domain_name}""]
  ttl     = 300
}
",resource,,,134,0.0,9f011edb46f4934d57ca530017aba313d337ba20,241558af7d6786415e64eea48e193f23518625c2,https://github.com/alphagov/govuk-aws/blob/9f011edb46f4934d57ca530017aba313d337ba20/terraform/projects/app-draft-cache/main.tf#L134,https://github.com/alphagov/govuk-aws/blob/241558af7d6786415e64eea48e193f23518625c2/terraform/projects/app-draft-cache/main.tf#L0,2018-10-26 13:18:26+01:00,2023-05-10 15:30:22+01:00,23,2,0,0,0,0,1,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,304,libvirt/main.tf,libvirt/main.tf,0,// todo,// TODO: this can moved to a tfvars,// TODO: this can moved to a tfvars,"resource ""libvirt_volume"" ""base_image"" {
  // the base image will be ""cloned"" and used by other domains, 
  // it is the central  image.
  name   = ""${terraform.workspace}-baseimage""
  source = var.base_image
  // TODO: this can moved to a tfvars
  pool   = var.storage_pool
}
",resource,"resource ""libvirt_volume"" ""base_image"" {
  // baseimage is ""cloned"" and used centrally by other domains
  name   = ""${terraform.workspace}-baseimage""
  source = var.base_image
  pool   = var.storage_pool
}
",resource,17,,b11930acacfa1ed7479d9490adc7b066a3973790,03ab687eae834b6cef248743e4d67ea04430bb3c,https://github.com/SUSE/ha-sap-terraform-deployments/blob/b11930acacfa1ed7479d9490adc7b066a3973790/libvirt/main.tf#L17,https://github.com/SUSE/ha-sap-terraform-deployments/blob/03ab687eae834b6cef248743e4d67ea04430bb3c/libvirt/main.tf,2019-08-28 13:42:24+02:00,2019-08-28 13:49:04+02:00,2,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,981,fast/stages/03-gke-multitenant/module/gke-hub.tf,fast/stages/03-gke-multitenant/module/gke-hub.tf,0,# todo,# TODO: service account,"/** 
 * Copyright 2022 Google LLC 
 * 
 * Licensed under the Apache License, Version 2.0 (the ""License""); 
 * you may not use this file except in compliance with the License. 
 * You may obtain a copy of the License at 
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, software 
 * distributed under the License is distributed on an ""AS IS"" BASIS, 
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 * See the License for the specific language governing permissions and 
 * limitations under the License. 
 */  
 # TODO: service account 
 # https://cloud.google.com/kubernetes-engine/docs/how-to/msc-setup-with-shared-vpc-networks#shared-service-project-iam 
 # TODO: add roles/multiclusterservicediscovery.serviceAgent and 
 #       roles/compute.networkViewer to IAM condition for GKE stage SA ","locals {
  fleet_enabled = (
    var.fleet_features != null || var.fleet_workload_identity
  )
  # TODO: add condition
  fleet_mcs_enabled = false
}
",locals,"locals {
  fleet_enabled = (
    var.fleet_features != null || var.fleet_workload_identity
  )
  fleet_mcs_enabled = local.fleet_enabled && lookup(
    coalesce(var.fleet_features, {}), ""multiclusterservicediscovery"", false
  ) == true
}
",locals,17,,133fd078232ef202140450d921bb8018b60e700f,c24e66138339bb5c599e52cd88236267acac4611,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/133fd078232ef202140450d921bb8018b60e700f/fast/stages/03-gke-multitenant/module/gke-hub.tf#L17,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c24e66138339bb5c599e52cd88236267acac4611/fast/stages/03-gke-multitenant/module/gke-hub.tf,2022-07-29 11:31:34+02:00,2022-07-29 14:01:35+02:00,2,1,0,1,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,445,infra/aws/terraform/prow-build-cluster/modules/cluster-autoscaler/cluster-autoscaler.tf,infra/aws/terraform/prow-build-cluster/modules/cluster-autoscaler/cluster-autoscaler.tf,0,# todo,# TODO(xmudrii-ubuntu): Temporary mount certificates from different known paths,# TODO(xmudrii-ubuntu): Temporary mount certificates from different known paths,"resource ""kubernetes_manifest"" ""deployment_kube_system_cluster_autoscaler"" {
  manifest = {
    ""apiVersion"" = ""apps/v1""
    ""kind""       = ""Deployment""
    ""metadata"" = {
      ""labels"" = {
        ""app"" = ""cluster-autoscaler""
      }
      ""name""      = ""cluster-autoscaler""
      ""namespace"" = ""kube-system""
    }
    ""spec"" = {
      ""replicas"" = 1
      ""selector"" = {
        ""matchLabels"" = {
          ""app"" = ""cluster-autoscaler""
        }
      }
      ""template"" = {
        ""metadata"" = {
          ""annotations"" = {
            ""cluster-autoscaler.kubernetes.io/safe-to-evict"" = ""false""
            ""prometheus.io/port""                             = ""8085""
            ""prometheus.io/scrape""                           = ""true""
          }
          ""labels"" = {
            ""app"" = ""cluster-autoscaler""
          }
        }
        ""spec"" = {
          ""containers"" = [
            {
              ""command"" = [
                ""./cluster-autoscaler"",
                ""--v=4"",
                ""--stderrthreshold=info"",
                ""--cloud-provider=aws"",
                ""--skip-nodes-with-local-storage=false"",
                ""--expander=least-waste"",
                ""--node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/${var.cluster_name}"",
                ""--balance-similar-node-groups"",
                ""--skip-nodes-with-system-pods=false"",
              ]
              ""image""           = ""registry.k8s.io/autoscaling/cluster-autoscaler:${var.cluster_autoscaler_version}""
              ""imagePullPolicy"" = ""Always""
              ""name""            = ""cluster-autoscaler""
              ""resources"" = {
                ""limits"" = {
                  ""cpu""    = ""100m""
                  ""memory"" = ""600Mi""
                }
                ""requests"" = {
                  ""cpu""    = ""100m""
                  ""memory"" = ""600Mi""
                }
              }
              ""volumeMounts"" = [
                # TODO(xmudrii-ubuntu): Temporary mount certificates from different known paths
                {
                  ""mountPath"" = ""/etc/ssl/certs""
                  ""name""      = ""ssl-certs""
                  ""readOnly""  = true
                },
                {
                  ""mountPath"" = ""/etc/pki""
                  ""name""      = ""pki-certs""
                  ""readOnly""  = true
                }
              ]
            },
          ]
          ""priorityClassName"" = ""system-cluster-critical""
          ""securityContext"" = {
            ""fsGroup""      = 65534
            ""runAsNonRoot"" = true
            ""runAsUser""    = 65534
          }
          ""serviceAccountName"" = ""cluster-autoscaler""
          ""volumes"" = [
            # TODO(xmudrii-ubuntu): Temporary mount certificates from different known paths
            {
              ""hostPath"" = {
                ""path"" = ""/etc/ssl/certs""
                ""type"" = ""Directory""
              }
              ""name"" = ""ssl-certs""
            },
            {
              ""hostPath"" = {
                ""path"" = ""/etc/pki""
                ""type"" = ""Directory""
              }
              ""name"" = ""pki-certs""
            }
          ]
        }
      }
    }
  }
}
",resource,,,415,0.0,ce20b9f5b43878cf51fd23a581829f7634efc97d,1f1eac3ac2ee01cae8e5fdbbe50e5270a11126d4,https://github.com/kubernetes/k8s.io/blob/ce20b9f5b43878cf51fd23a581829f7634efc97d/infra/aws/terraform/prow-build-cluster/modules/cluster-autoscaler/cluster-autoscaler.tf#L415,https://github.com/kubernetes/k8s.io/blob/1f1eac3ac2ee01cae8e5fdbbe50e5270a11126d4/infra/aws/terraform/prow-build-cluster/modules/cluster-autoscaler/cluster-autoscaler.tf#L0,2023-10-16 10:30:37+02:00,2024-02-19 18:06:16+03:00,3,2,1,1,0,1,0,0,0,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,225,init.tf,init.tf,0,todo,"# ToDo: See ToDo in locals.tf. Also: Refactor - e.g. do not install helm by pipe to bash, pin cilium chart version etc.","# Apply cilium and wait for it to initialize all nodes 
 # We can't use the HelmChart custom resource to install cilium due to the fact that it does not tolerate the cilium taint. 
 # ToDo: See ToDo in locals.tf. Also: Refactor - e.g. do not install helm by pipe to bash, pin cilium chart version etc. 
 # Use something like this (the example below is non working) after apply instead of sleep to check until cilium has removed the taint from all nodes 
 # Might not even be required due to pods from the remaining kustomization pending until taint is removed? Needs testing. 
 # timeout 300 bash <<EOF 
 #   until [[ -n ""\$(kubectl get nodes -o jsonpath={.items[].spec.taints[?(@.effect==NoExecute)].effect}{""\t""}{.items[].metadata.name})"" ]]; do 
 #     echo ""Waiting for the cluster to become ready..."" 
 #     sleep 2 
 #   done 
 # EOF","resource ""null_resource"" ""kustomization"" {
  connection {
    user           = ""root""
    private_key    = var.ssh_private_key
    agent_identity = local.ssh_agent_identity
    host           = module.control_planes[keys(module.control_planes)[0]].ipv4_address
    port           = var.ssh_port
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content = yamlencode({
      apiVersion = ""kustomize.config.k8s.io/v1beta1""
      kind       = ""Kustomization""

      resources = concat(
        [
          ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",
          ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",
          ""https://raw.githubusercontent.com/rancher/system-upgrade-controller/master/manifests/system-upgrade-controller.yaml"",
        ],
        var.disable_hetzner_csi ? [] : [
          ""https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml""
        ],
        var.traefik_enabled ? [""traefik_config.yaml""] : [],
        lookup(local.cni_install_resources, var.cni_plugin, []),
        var.enable_longhorn ? [""longhorn.yaml""] : [],
        var.enable_cert_manager || var.enable_rancher ? [""cert_manager.yaml""] : [],
        var.enable_rancher ? [""rancher.yaml""] : [],
        var.rancher_registration_manifest_url != """" ? [var.rancher_registration_manifest_url] : []
      ),
      patchesStrategicMerge = concat(
        [
          file(""${path.module}/kustomize/kured.yaml""),
          file(""${path.module}/kustomize/system-upgrade-controller.yaml""),
          ""ccm.yaml"",
        ],
        lookup(local.cni_install_resource_patches, var.cni_plugin, [])
      )
    })
    destination = ""/var/post_install/kustomization.yaml""
  }

  # Upload traefik config
  provisioner ""file"" {
    content = local.using_klipper_lb || var.traefik_enabled == false ? """" : templatefile(
      ""${path.module}/templates/traefik_config.yaml.tpl"",
      {
        name                       = ""${var.cluster_name}-traefik""
        load_balancer_disable_ipv6 = var.load_balancer_disable_ipv6
        load_balancer_type         = var.load_balancer_type
        location                   = var.load_balancer_location
        traefik_acme_tls           = var.traefik_acme_tls
        traefik_acme_email         = var.traefik_acme_email
        traefik_additional_options = var.traefik_additional_options
        using_hetzner_lb           = !local.using_klipper_lb
    })
    destination = ""/var/post_install/traefik_config.yaml""
  }

  # Upload the CCM patch config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/ccm.yaml.tpl"",
      {
        cluster_cidr_ipv4                 = local.cluster_cidr_ipv4
        allow_scheduling_on_control_plane = local.allow_scheduling_on_control_plane
        default_lb_location               = var.load_balancer_location
        using_hetzner_lb                  = !local.using_klipper_lb
    })
    destination = ""/var/post_install/ccm.yaml""
  }

  # Upload the calico patch config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/calico.yaml.tpl"",
      {
        cluster_cidr_ipv4 = local.cluster_cidr_ipv4
    })
    destination = ""/var/post_install/calico.yaml""
  }

  # Upload the cilium install file
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cilium.yaml.tpl"",
      {
        values = trimspace(fileexists(""cilium_values.yaml"") ? file(""cilium_values.yaml"") : local.default_cilium_values)
    })
    destination = ""/tmp/cilium.yaml""
  }

  # Upload the system upgrade controller plans config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/plans.yaml.tpl"",
      {
        channel = var.initial_k3s_channel
    })
    destination = ""/var/post_install/plans.yaml""
  }

  # Upload the Longhorn config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/longhorn.yaml.tpl"",
      {
        disable_hetzner_csi    = var.disable_hetzner_csi,
        longhorn_fstype        = var.longhorn_fstype,
        longhorn_replica_count = var.longhorn_replica_count
    })
    destination = ""/var/post_install/longhorn.yaml""
  }

  # Upload the cert-manager config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cert_manager.yaml.tpl"",
    {})
    destination = ""/var/post_install/cert_manager.yaml""
  }

  # Upload the Rancher config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/rancher.yaml.tpl"",
      {
        rancher_install_channel    = var.rancher_install_channel
        rancher_hostname           = var.rancher_hostname
        rancher_bootstrap_password = length(var.rancher_bootstrap_password) == 0 ? resource.random_password.rancher_bootstrap[0].result : var.rancher_bootstrap_password
        number_control_plane_nodes = length(local.control_plane_nodes)
    })
    destination = ""/var/post_install/rancher.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""set -ex"",
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name} --dry-run=client -o yaml | kubectl apply -f -"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token} --dry-run=client -o yaml | kubectl apply -f -"",
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = concat([
      ""set -ex"",

      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /var/post_install/kustomization.yaml"",

      # Wait for k3s to become ready (we check one more time) because in some edge cases,
      # the cluster had become unvailable for a few seconds, at this very instant.
      <<-EOT
      timeout 180 bash <<EOF
        until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do
          echo ""Waiting for the cluster to become ready...""
          sleep 2
        done
      EOF
      EOT
      ]
      ,

      # Apply cilium and wait for it to initialize all nodes
      # We can't use the HelmChart custom resource to install cilium due to the fact that it does not tolerate the cilium taint.
      # ToDo: See ToDo in locals.tf. Also: Refactor - e.g. do not install helm by pipe to bash, pin cilium chart version etc.
      # Use something like this (the example below is non working) after apply instead of sleep to check until cilium has removed the taint from all nodes
      # Might not even be required due to pods from the remaining kustomization pending until taint is removed? Needs testing.
      # timeout 300 bash <<EOF
      #   until [[ -n ""\$(kubectl get nodes -o jsonpath={.items[].spec.taints[?(@.effect==NoExecute)].effect}{""\t""}{.items[].metadata.name})"" ]]; do
      #     echo ""Waiting for the cluster to become ready...""
      #     sleep 2
      #   done
      # EOF
      var.cni_plugin == ""cilium"" ? [
        <<-EOT
        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
        helm repo add cilium https://helm.cilium.io/
        helm template cilium cilium/cilium --namespace kube-system --values /tmp/cilium.yaml --include-crds | kubectl apply -f -
        sleep 30
        rm -rf /tmp/cilium.yaml
        EOT
      ] : []
      ,

      [
        # Ready, set, go for the kustomization
        ""kubectl apply -k /var/post_install"",
        ""echo 'Waiting for the system-upgrade-controller deployment to become available...'"",
        ""kubectl -n system-upgrade wait --for=condition=available --timeout=180s deployment/system-upgrade-controller"",
        ""sleep 5"", # important as the system upgrade controller CRDs sometimes don't get ready right away, especially with Cilium.
        ""kubectl -n system-upgrade apply -f /var/post_install/plans.yaml""
      ],
      local.using_klipper_lb || var.traefik_enabled == false ? [] : [
        <<-EOT
      timeout 120 bash <<EOF
      until [ -n ""\$(kubectl get -n kube-system service/traefik --output=jsonpath='{.status.loadBalancer.ingress[0].ip}' 2> /dev/null)"" ]; do
          echo ""Waiting for load-balancer to get an IP...""
          sleep 2
      done
      EOF
      EOT
    ])
  }

  depends_on = [
    null_resource.first_control_plane,
    local_sensitive_file.kubeconfig,
    random_password.rancher_bootstrap
  ]
}
",resource,"resource ""null_resource"" ""kustomization"" {
  connection {
    user           = ""root""
    private_key    = var.ssh_private_key
    agent_identity = local.ssh_agent_identity
    host           = module.control_planes[keys(module.control_planes)[0]].ipv4_address
    port           = var.ssh_port
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content = yamlencode({
      apiVersion = ""kustomize.config.k8s.io/v1beta1""
      kind       = ""Kustomization""

      resources = concat(
        [
          ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",
          ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",
          ""https://raw.githubusercontent.com/rancher/system-upgrade-controller/master/manifests/system-upgrade-controller.yaml"",
        ],
        var.disable_hetzner_csi ? [] : [
          ""https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml""
        ],
        var.traefik_enabled ? [""traefik_config.yaml""] : [],
        lookup(local.cni_install_resources, var.cni_plugin, []),
        var.enable_longhorn ? [""longhorn.yaml""] : [],
        var.enable_cert_manager || var.enable_rancher ? [""cert_manager.yaml""] : [],
        var.enable_rancher ? [""rancher.yaml""] : [],
        var.rancher_registration_manifest_url != """" ? [var.rancher_registration_manifest_url] : []
      ),
      patchesStrategicMerge = concat(
        [
          file(""${path.module}/kustomize/kured.yaml""),
          file(""${path.module}/kustomize/system-upgrade-controller.yaml""),
          ""ccm.yaml"",
        ],
        lookup(local.cni_install_resource_patches, var.cni_plugin, [])
      )
    })
    destination = ""/var/post_install/kustomization.yaml""
  }

  # Upload traefik config
  provisioner ""file"" {
    content = local.using_klipper_lb || var.traefik_enabled == false ? """" : templatefile(
      ""${path.module}/templates/traefik_config.yaml.tpl"",
      {
        name                       = ""${var.cluster_name}-traefik""
        load_balancer_disable_ipv6 = var.load_balancer_disable_ipv6
        load_balancer_type         = var.load_balancer_type
        location                   = var.load_balancer_location
        traefik_acme_tls           = var.traefik_acme_tls
        traefik_acme_email         = var.traefik_acme_email
        traefik_additional_options = var.traefik_additional_options
        using_hetzner_lb           = !local.using_klipper_lb
    })
    destination = ""/var/post_install/traefik_config.yaml""
  }

  # Upload the CCM patch config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/ccm.yaml.tpl"",
      {
        cluster_cidr_ipv4                 = local.cluster_cidr_ipv4
        allow_scheduling_on_control_plane = local.allow_scheduling_on_control_plane
        default_lb_location               = var.load_balancer_location
        using_hetzner_lb                  = !local.using_klipper_lb
    })
    destination = ""/var/post_install/ccm.yaml""
  }

  # Upload the calico patch config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/calico.yaml.tpl"",
      {
        cluster_cidr_ipv4 = local.cluster_cidr_ipv4
    })
    destination = ""/var/post_install/calico.yaml""
  }

  # Upload the cilium install file
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cilium.yaml.tpl"",
      {
        values = indent(4, trimspace(fileexists(""cilium_values.yaml"") ? file(""cilium_values.yaml"") : local.default_cilium_values))
    })
    destination = ""/var/post_install/cilium.yaml""
  }

  # Upload the system upgrade controller plans config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/plans.yaml.tpl"",
      {
        channel = var.initial_k3s_channel
    })
    destination = ""/var/post_install/plans.yaml""
  }

  # Upload the Longhorn config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/longhorn.yaml.tpl"",
      {
        disable_hetzner_csi    = var.disable_hetzner_csi,
        longhorn_fstype        = var.longhorn_fstype,
        longhorn_replica_count = var.longhorn_replica_count
    })
    destination = ""/var/post_install/longhorn.yaml""
  }

  # Upload the cert-manager config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cert_manager.yaml.tpl"",
    {})
    destination = ""/var/post_install/cert_manager.yaml""
  }

  # Upload the Rancher config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/rancher.yaml.tpl"",
      {
        rancher_install_channel    = var.rancher_install_channel
        rancher_hostname           = var.rancher_hostname
        rancher_bootstrap_password = length(var.rancher_bootstrap_password) == 0 ? resource.random_password.rancher_bootstrap[0].result : var.rancher_bootstrap_password
        number_control_plane_nodes = length(local.control_plane_nodes)
    })
    destination = ""/var/post_install/rancher.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""set -ex"",
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name} --dry-run=client -o yaml | kubectl apply -f -"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token} --dry-run=client -o yaml | kubectl apply -f -"",
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = concat([
      ""set -ex"",

      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /var/post_install/kustomization.yaml"",

      # Wait for k3s to become ready (we check one more time) because in some edge cases,
      # the cluster had become unvailable for a few seconds, at this very instant.
      <<-EOT
      timeout 180 bash <<EOF
        until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do
          echo ""Waiting for the cluster to become ready...""
          sleep 2
        done
      EOF
      EOT
      ]
      ,

      [
        # Ready, set, go for the kustomization
        ""kubectl apply -k /var/post_install"",
        ""echo 'Waiting for the system-upgrade-controller deployment to become available...'"",
        ""kubectl -n system-upgrade wait --for=condition=available --timeout=180s deployment/system-upgrade-controller"",
        ""sleep 5"", # important as the system upgrade controller CRDs sometimes don't get ready right away, especially with Cilium.
        ""kubectl -n system-upgrade apply -f /var/post_install/plans.yaml""
      ],
      local.using_klipper_lb || var.traefik_enabled == false ? [] : [
        <<-EOT
      timeout 120 bash <<EOF
      until [ -n ""\$(kubectl get -n kube-system service/traefik --output=jsonpath='{.status.loadBalancer.ingress[0].ip}' 2> /dev/null)"" ]; do
          echo ""Waiting for load-balancer to get an IP...""
          sleep 2
      done
      EOF
      EOT
    ])
  }

  depends_on = [
    null_resource.first_control_plane,
    local_sensitive_file.kubeconfig,
    random_password.rancher_bootstrap
  ]
}
",resource,254,,85a20e91ca41fa82b34456dd3d82b84cd9495554,4d2ba124929af924e01d86c446fce37eb003d601,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/85a20e91ca41fa82b34456dd3d82b84cd9495554/init.tf#L254,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/4d2ba124929af924e01d86c446fce37eb003d601/init.tf,2022-08-28 13:43:57+02:00,2022-08-29 15:59:35+02:00,2,1,1,1,0,0,0,1,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,102,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,0,# todo,# TODO: rename to subnetwork_self_link,# TODO: rename to subnetwork_self_link,"variable ""nodeset"" {
  description = ""Define nodesets, as a list.""
  type = list(object({
    node_count_static      = optional(number, 0)
    node_count_dynamic_max = optional(number, 1)
    node_conf              = optional(map(string), {})
    nodeset_name           = string
    additional_disks = optional(list(object({
      disk_name    = optional(string)
      device_name  = optional(string)
      disk_size_gb = optional(number)
      disk_type    = optional(string)
      disk_labels  = optional(map(string), {})
      auto_delete  = optional(bool, true)
      boot         = optional(bool, false)
    })), [])
    bandwidth_tier         = optional(string, ""platform_default"")
    can_ip_forward         = optional(bool, false)
    disable_smt            = optional(bool, false)
    disk_auto_delete       = optional(bool, true)
    disk_labels            = optional(map(string), {})
    disk_size_gb           = optional(number)
    disk_type              = optional(string)
    enable_confidential_vm = optional(bool, false)
    enable_placement       = optional(bool, false)
    enable_public_ip       = optional(bool, false)
    enable_oslogin         = optional(bool, true)
    enable_shielded_vm     = optional(bool, false)
    gpu = optional(object({
      count = number
      type  = string
    }))
    instance_template   = optional(string)
    labels              = optional(map(string), {})
    machine_type        = optional(string)
    metadata            = optional(map(string), {})
    min_cpu_platform    = optional(string)
    network_tier        = optional(string, ""STANDARD"")
    on_host_maintenance = optional(string)
    preemptible         = optional(bool, false)
    region              = optional(string)
    service_account = optional(object({
      email  = optional(string)
      scopes = optional(list(string), [""https://www.googleapis.com/auth/cloud-platform""])
    }))
    shielded_instance_config = optional(object({
      enable_integrity_monitoring = optional(bool, true)
      enable_secure_boot          = optional(bool, true)
      enable_vtpm                 = optional(bool, true)
    }))
    source_image_family  = optional(string)
    source_image_project = optional(string)
    source_image         = optional(string)
    subnetwork_project   = optional(string)
    # TODO: rename to subnetwork_self_link 
    subnetwork         = optional(string)
    spot               = optional(bool, false)
    tags               = optional(list(string), [])
    termination_action = optional(string)
    zones              = optional(list(string), [])
    zone_target_shape  = optional(string, ""ANY_SINGLE_ZONE"")
  }))
  default = []

  validation {
    condition     = length(distinct([for x in var.nodeset : x.nodeset_name])) == length(var.nodeset)
    error_message = ""All nodesets must have a unique name.""
  }
}
",variable,"variable ""nodeset"" {
  description = ""Define nodesets, as a list.""
  type = list(object({
    node_count_static      = optional(number, 0)
    node_count_dynamic_max = optional(number, 1)
    node_conf              = optional(map(string), {})
    nodeset_name           = string
    additional_disks = optional(list(object({
      disk_name    = optional(string)
      device_name  = optional(string)
      disk_size_gb = optional(number)
      disk_type    = optional(string)
      disk_labels  = optional(map(string), {})
      auto_delete  = optional(bool, true)
      boot         = optional(bool, false)
    })), [])
    bandwidth_tier         = optional(string, ""platform_default"")
    can_ip_forward         = optional(bool, false)
    disable_smt            = optional(bool, false)
    disk_auto_delete       = optional(bool, true)
    disk_labels            = optional(map(string), {})
    disk_size_gb           = optional(number)
    disk_type              = optional(string)
    enable_confidential_vm = optional(bool, false)
    enable_placement       = optional(bool, false)
    enable_public_ip       = optional(bool, false)
    enable_oslogin         = optional(bool, true)
    enable_shielded_vm     = optional(bool, false)
    gpu = optional(object({
      count = number
      type  = string
    }))
    instance_template   = optional(string)
    labels              = optional(map(string), {})
    machine_type        = optional(string)
    metadata            = optional(map(string), {})
    min_cpu_platform    = optional(string)
    network_tier        = optional(string, ""STANDARD"")
    on_host_maintenance = optional(string)
    preemptible         = optional(bool, false)
    region              = optional(string)
    service_account = optional(object({
      email  = optional(string)
      scopes = optional(list(string), [""https://www.googleapis.com/auth/cloud-platform""])
    }))
    shielded_instance_config = optional(object({
      enable_integrity_monitoring = optional(bool, true)
      enable_secure_boot          = optional(bool, true)
      enable_vtpm                 = optional(bool, true)
    }))
    source_image_family  = optional(string)
    source_image_project = optional(string)
    source_image         = optional(string)
    subnetwork_self_link = string
    spot                 = optional(bool, false)
    tags                 = optional(list(string), [])
    termination_action   = optional(string)
    zones                = optional(list(string), [])
    zone_target_shape    = optional(string, ""ANY_SINGLE_ZONE"")
  }))
  default = []

  validation {
    condition     = length(distinct([for x in var.nodeset : x.nodeset_name])) == length(var.nodeset)
    error_message = ""All nodesets must have a unique name.""
  }
}
",variable,221,,82199854e24ac10e7293605c5c4eae45748e8c99,3d1072da48450aa22b844bb5c288415b270616cc,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/82199854e24ac10e7293605c5c4eae45748e8c99/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf#L221,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/3d1072da48450aa22b844bb5c288415b270616cc/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/variables.tf,2023-11-09 19:50:03+00:00,2024-01-02 14:51:06-08:00,7,1,0,1,0,0,1,0,0,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,369,autoscaler-agents.tf,autoscaler-agents.tf,0,implement,"#! for now we use x86 snapshot, this needs to implement logic to autodetect between x86 and aarch64 based on server type","#! for now we use x86 snapshot, this needs to implement logic to autodetect between x86 and aarch64 based on server type","locals {
  cluster_prefix = var.use_cluster_name_in_node_name ? ""${var.cluster_name}-"" : """"
  autoscaler_yaml = length(var.autoscaler_nodepools) == 0 ? """" : templatefile(
    ""${path.module}/templates/autoscaler.yaml.tpl"",
    {
      cloudinit_config = base64encode(data.cloudinit_config.autoscaler-config[0].rendered)
      ca_image         = var.cluster_autoscaler_image
      ca_version       = var.cluster_autoscaler_version
      ssh_key          = local.hcloud_ssh_key_id
      # for now we use the k3s network, as we cannot reference subnet-ids in autoscaler
      ipv4_subnet_id = hcloud_network.k3s.id
      #! for now we use x86 snapshot, this needs to implement logic to autodetect between x86 and aarch64 based on server type
      snapshot_id    = data.hcloud_image.microos_x86_snapshot.id
      firewall_id    = hcloud_firewall.k3s.id
      cluster_name   = local.cluster_prefix
      node_pools     = var.autoscaler_nodepools
  })
  # A concatenated list of all autoscaled nodes
  autoscaled_nodes = length(var.autoscaler_nodepools) == 0 ? {} : {
    for v in concat([
      for k, v in data.
      hcloud_servers.autoscaled_nodes : [for v in v.servers : v]
    ]...) : v.name => v
  }
}
",locals,"locals {
  cluster_prefix = var.use_cluster_name_in_node_name ? ""${var.cluster_name}-"" : """"
  autoscaler_yaml = length(var.autoscaler_nodepools) == 0 ? """" : templatefile(
    ""${path.module}/templates/autoscaler.yaml.tpl"",
    {
      cloudinit_config = base64encode(data.cloudinit_config.autoscaler-config[0].rendered)
      ca_image         = var.cluster_autoscaler_image
      ca_version       = var.cluster_autoscaler_version
      ssh_key          = local.hcloud_ssh_key_id
      # for now we use the k3s network, as we cannot reference subnet-ids in autoscaler
      ipv4_subnet_id = hcloud_network.k3s.id
      # for now we use x86 snapshot, this needs to implement logic to autodetect between x86 and aarch64 based on server type
      snapshot_id    = data.hcloud_image.microos_x86_snapshot.id
      firewall_id    = hcloud_firewall.k3s.id
      cluster_name   = local.cluster_prefix
      node_pools     = var.autoscaler_nodepools
  })
  # A concatenated list of all autoscaled nodes
  autoscaled_nodes = length(var.autoscaler_nodepools) == 0 ? {} : {
    for v in concat([
      for k, v in data.
      hcloud_servers.autoscaled_nodes : [for v in v.servers : v]
    ]...) : v.name => v
  }
}
",locals,12,,338b42de4849df86985cd5a08fb8029f8fa345c9,297f4a16d496bfaa684bbbcbeb9139501638d3f2,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/338b42de4849df86985cd5a08fb8029f8fa345c9/autoscaler-agents.tf#L12,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/297f4a16d496bfaa684bbbcbeb9139501638d3f2/autoscaler-agents.tf,2023-04-14 16:00:33+02:00,2023-04-14 16:01:31+02:00,2,1,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,607,examples/factories/project-factory/main.tf,examples/factories/project-factory/main.tf,0,# todo,# TODO(jccb): we should probably change this to non-authoritative bindings,# TODO(jccb): we should probably change this to non-authoritative bindings,"resource ""google_compute_subnetwork_iam_binding"" ""binding"" {
  for_each   = local.vpc_setup ? coalesce(var.vpc.subnets_iam, {}) : {}
  project    = local.vpc_host_project
  subnetwork = ""projects/${local.vpc_host_project}/regions/${split(""/"", each.key)[0]}/subnetworks/${split(""/"", each.key)[1]}""
  region     = split(""/"", each.key)[0]
  role       = ""roles/compute.networkUser""
  members    = concat(each.value, local.network_user_service_accounts)
}
",resource,the block associated got renamed or deleted,,138,,82b181f34e832a67dd99f8eb5355cca0bf8585e9,40cb46e1cc59c36cac9dd3198c841f32cee11733,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/82b181f34e832a67dd99f8eb5355cca0bf8585e9/examples/factories/project-factory/main.tf#L138,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/40cb46e1cc59c36cac9dd3198c841f32cee11733/examples/factories/project-factory/main.tf,2022-02-07 11:54:19+01:00,2022-02-09 11:06:51+01:00,2,1,0,1,0,1,1,0,0,0
https://github.com/cattle-ops/terraform-aws-gitlab-runner,3,variables.tf,variables.tf,0,todo,# TODO remove as soon as subnet_id_runners and subnet_ids_gitlab_runner are gone. Variable is mandatory now.,"default     = """" # TODO remove as soon as subnet_id_runners and subnet_ids_gitlab_runner are gone. Variable is mandatory now.","variable ""subnet_id"" {
  description = ""Subnet id used for the runner and executors. Must belong to the VPC specified above.""
  type        = string
  default     = """" # TODO remove as soon as subnet_id_runners and subnet_ids_gitlab_runner are gone. Variable is mandatory now.
}
",variable,"variable ""subnet_id"" {
  description = <<-EOT
    Subnet id used for the Runner and Runner Workers. Must belong to the `vpc_id`. In case the fleet mode is used, multiple subnets for
    the Runner Workers can be provided with runner_worker_docker_machine_instance.subnet_ids.
  EOT
  type        = string
}
",variable,25,,36633fa6286343cce72b0d9c0c3451305a36be11,c8a3b89c46f749214461bade8e1e6d161d0ef860,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/36633fa6286343cce72b0d9c0c3451305a36be11/variables.tf#L25,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/c8a3b89c46f749214461bade8e1e6d161d0ef860/variables.tf,2022-01-13 23:50:43+01:00,2023-09-07 17:14:21+02:00,54,1,0,1,0,0,1,0,0,0
https://github.com/chanzuckerberg/cztack,17,aws-ecs-job-fargate/iam.tf,aws-ecs-job-fargate/iam.tf,0,# todo,# TODO(mbarrien): We can probably narrow this down to allowing access to only,"# TODO(mbarrien): We can probably narrow this down to allowing access to only 
 # the specific ECR arn if applicable, and the specific cloudwatch log group. 
 # Either pass both identifiers in, or pass the entire role ARN as an argument","resource ""aws_iam_role_policy_attachment"" ""task_execution_role"" {
  count      = var.registry_secretsmanager_arn != null ? 1 : 0
  role       = aws_iam_role.task_execution_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy""
}
",resource,"resource ""aws_iam_role_policy_attachment"" ""task_execution_role"" {
  role       = aws_iam_role.task_execution_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy""
}
",resource,18,19.0,6918848f1dab99c67e49d21bdc839d907ff8b647,5696d1db18ded14983fcd5c480468753c20eef53,https://github.com/chanzuckerberg/cztack/blob/6918848f1dab99c67e49d21bdc839d907ff8b647/aws-ecs-job-fargate/iam.tf#L18,https://github.com/chanzuckerberg/cztack/blob/5696d1db18ded14983fcd5c480468753c20eef53/aws-ecs-job-fargate/iam.tf#L19,2019-09-25 09:47:44-07:00,2022-07-07 15:58:52-07:00,5,0,0,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,1825,infra/modules/psoxy-constants/main.tf,infra/modules/psoxy-constants/main.tf,0,# todo,"# TODO: create IAM policy document, which installer could use to create their own policy as","# TODO: create IAM policy document, which installer could use to create their own policy as 
 # alternative to using AWS Managed policies ","locals {

  # AWS Managed polices
  # see: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#aws-managed-policies
  required_aws_roles_to_provision_host = {
    ""arn:aws:iam::aws:policy/IAMFullAccess""        = ""IAMFullAccess""
    ""arn:aws:iam::aws:policy/AmazonS3FullAccess""   = ""AmazonS3FullAccess""
    ""arn:aws:iam::aws:policy/CloudWatchFullAccess"" = ""CloudWatchFullAccess""
    ""arn:aws:iam::aws:policy/AmazonSSMFullAccess""  = ""AmazonSSMFullAccess""
    ""arn:aws:iam::aws:policy/AWSLambda_FullAccess"" = ""AWSLambda_FullAccess""
  }
  # TODO: create IAM policy document, which installer could use to create their own policy as
  # alternative to using AWS Managed policies

  required_gcp_roles_to_provision_host = {
    ""roles/storage.admin"" = {
      display_name    = ""Storage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#storage.admin""
    },
    ""roles/iam.roleAdmin"" = {
      display_name    = ""IAM Role Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.roleAdmin""
    },
    ""roles/secretmanager.admin"" = {
      display_name    = ""Secret Manager Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#secretmanager.admin""
    },
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    },
    ""roles/cloudfunctions.admin"" = {
      display_name    = ""Cloud Functions Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#cloudfunctions.admin""
    },
  }  # TODO: add list of permissions, which customer could use to create custom role as alternative

  required_gcp_roles_to_provision_google_workspace_source = {
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    }
  }
  # TODO: add list of permissions, which customer could use to create custom role as alternative

  required_azuread_roles_to_provision_msft_365_source = {
    ""7ab1d382-f21e-4acd-a863-ba3e13f7da61"" = ""Cloud Application Administrator"",
  }
}
",locals,"locals {

  # AWS Managed polices
  # see: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#aws-managed-policies
  required_aws_roles_to_provision_host = {
    ""arn:aws:iam::aws:policy/IAMFullAccess""        = ""IAMFullAccess""
    ""arn:aws:iam::aws:policy/AmazonS3FullAccess""   = ""AmazonS3FullAccess"" # only if using bulk sources, although 95% do
    ""arn:aws:iam::aws:policy/CloudWatchFullAccess"" = ""CloudWatchFullAccess""
    ""arn:aws:iam::aws:policy/AmazonSSMFullAccess""  = ""AmazonSSMFullAccess""
    ""arn:aws:iam::aws:policy/AWSLambda_FullAccess"" = ""AWSLambda_FullAccess""
  }
  # AWS managed policy required to consume Microsoft 365 data
  # (in addition to above)
  required_aws_managed_policies_to_consume_msft_365_source = {
    ""arn:aws:iam::aws:policy/AmazonCognitoPowerUser"" = ""AmazonCognitoPowerUser""
  }

  # subset of https://docs.aws.amazon.com/aws-managed-policy/latest/reference/SecretsManagerReadWrite.html
  # as that seems like overkill
  #  - if you're going to use KMS to encrypt the secrets, then you'll need to add the KMS permissions
  #    on the key you intend to use.
  #  - you can/should modify the Resource part of this to limit to a subset of secrets, if this
  #    is being deployed to an AWS account that's used for purposes beyond this proxy deployment
  required_aws_policy_to_use_secrets_manager = {
    ""Version"" : ""2012-10-17"",
    ""Statement"" : [
      {
        ""Effect"" : ""Allow"",
        ""Action"" : [
          ""secretsmanager:*"",
          ""tag:GetResources""
        ],
        ""Resource"" : ""*""
      }
    ]
  }


  # TODO: create IAM policy document, which installer could use to create their own policy as
  # alternative to using AWS Managed policies

  # initial GCP APIs that must be enabled in projects that will host the proxy.
  # (Terraform apply will enabled additional ones)
  required_gcp_apis_to_host = {
    # https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com
    ""iamcredentials.googleapis.com"" = ""IAM Service Account Credentials API"",
    # https://console.cloud.google.com/apis/library/serviceusage.googleapis.com
    ""serviceusage.googleapis.com"" = ""Service Usage API"",
  }

  required_gcp_roles_to_provision_host = {
    ""roles/storage.admin"" = {
      display_name    = ""Storage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#storage.admin""
    },
    ""roles/iam.roleAdmin"" = {
      display_name    = ""IAM Role Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.roleAdmin""
    },
    ""roles/secretmanager.admin"" = {
      display_name    = ""Secret Manager Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#secretmanager.admin""
    },
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    },
    ""roles/cloudfunctions.admin"" = {
      display_name    = ""Cloud Functions Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#cloudfunctions.admin""
    },
  }
  # TODO: add list of permissions, which customer could use to create custom role as alternative


  # TODO: confirm that this is indeed the same list (believe it is)
  required_gcp_apis_to_provision_google_workspace_source = local.required_gcp_apis_to_host

  required_gcp_roles_to_provision_google_workspace_source = {
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    }
  }
  # TODO: add list of permissions, which customer could use to create custom role as alternative

  required_azuread_roles_to_provision_msft_365_source = {
    ""7ab1d382-f21e-4acd-a863-ba3e13f7da61"" = ""Cloud Application Administrator"",
  }
}
",locals,12,39.0,afe4f8e792fc412e4d03b347a7e566d47a7fa0d2,005e1fed5f46b4310d81d41d29862bb1c4f360b0,https://github.com/Worklytics/psoxy/blob/afe4f8e792fc412e4d03b347a7e566d47a7fa0d2/infra/modules/psoxy-constants/main.tf#L12,https://github.com/Worklytics/psoxy/blob/005e1fed5f46b4310d81d41d29862bb1c4f360b0/infra/modules/psoxy-constants/main.tf#L39,2023-06-23 19:10:00+00:00,2024-02-06 19:07:07+00:00,5,0,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,292,modules/network/multivpc/main.tf,modules/network/multivpc/main.tf,0,implementation,# consider changing to explicit var.subnetworks implementation,"# the value 0 creates a single subnetwork that spans the entire range above 
 # consider changing to explicit var.subnetworks implementation","module ""vpcs"" {
  source = ""github.com/GoogleCloudPlatform/hpc-toolkit//modules/network/vpc?ref=v1.31.1&depth=1""

  count = var.network_count

  project_id            = var.project_id
  deployment_name       = var.deployment_name
  region                = var.region
  network_address_range = cidrsubnet(local.global_ip_cidr_valid, local.subnetwork_new_bits, count.index)
  # the value 0 creates a single subnetwork that spans the entire range above
  # consider changing to explicit var.subnetworks implementation
  default_primary_subnetwork_size = 0

  network_name                           = ""${local.network_name}-${count.index}""
  subnetwork_name                        = ""${local.network_name}-${count.index}-subnet""
  allowed_ssh_ip_ranges                  = var.allowed_ssh_ip_ranges
  delete_default_internet_gateway_routes = var.delete_default_internet_gateway_routes
  enable_iap_rdp_ingress                 = var.enable_iap_rdp_ingress
  enable_iap_ssh_ingress                 = var.enable_iap_ssh_ingress
  enable_iap_winrm_ingress               = var.enable_iap_winrm_ingress
  enable_internal_traffic                = var.enable_internal_traffic
  extra_iap_ports                        = var.extra_iap_ports
  firewall_rules                         = var.firewall_rules
  ips_per_nat                            = var.ips_per_nat
  mtu                                    = var.mtu
  network_description                    = var.network_description
  network_routing_mode                   = var.network_routing_mode
}
",module,"module ""vpcs"" {
  source = ""github.com/GoogleCloudPlatform/hpc-toolkit//modules/network/vpc?ref=v1.32.1&depth=1""

  count = var.network_count

  project_id            = var.project_id
  deployment_name       = var.deployment_name
  region                = var.region
  network_address_range = cidrsubnet(local.global_ip_cidr_valid, local.subnetwork_new_bits, count.index)
  # the value 0 creates a single subnetwork that spans the entire range above
  # consider changing to explicit var.subnetworks implementation
  default_primary_subnetwork_size = 0

  network_name                           = ""${local.network_name}-${count.index}""
  subnetwork_name                        = ""${local.network_name}-${count.index}-subnet""
  allowed_ssh_ip_ranges                  = var.allowed_ssh_ip_ranges
  delete_default_internet_gateway_routes = var.delete_default_internet_gateway_routes
  enable_iap_rdp_ingress                 = var.enable_iap_rdp_ingress
  enable_iap_ssh_ingress                 = var.enable_iap_ssh_ingress
  enable_iap_winrm_ingress               = var.enable_iap_winrm_ingress
  enable_internal_traffic                = var.enable_internal_traffic
  extra_iap_ports                        = var.extra_iap_ports
  firewall_rules                         = var.firewall_rules
  ips_per_nat                            = var.ips_per_nat
  mtu                                    = var.mtu
  network_description                    = var.network_description
  network_routing_mode                   = var.network_routing_mode
}
",module,56,56.0,da5237a68099dd5ae7c69020609272a6b64e531a,0aec7fb77c813bd747de536bc927542877d69de6,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/da5237a68099dd5ae7c69020609272a6b64e531a/modules/network/multivpc/main.tf#L56,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/0aec7fb77c813bd747de536bc927542877d69de6/modules/network/multivpc/main.tf#L56,2024-04-12 17:18:10+00:00,2024-04-19 19:04:24+00:00,2,0,0,1,0,0,1,0,0,0
https://github.com/nasa/cumulus,30,tf-modules/cumulus/archive.tf,tf-modules/cumulus/archive.tf,0,todo,# TODO This should end up coming from the ingest module at some point,# TODO This should end up coming from the ingest module at some point,"data ""aws_lambda_function"" ""message_consumer"" {
  function_name = ""${var.prefix}-messageConsumer""
}
",data,the block associated got renamed or deleted,,6,,ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7,275b9a88869f413f231765ff1784ed9b3c6e042b,https://github.com/nasa/cumulus/blob/ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7/tf-modules/cumulus/archive.tf#L6,https://github.com/nasa/cumulus/blob/275b9a88869f413f231765ff1784ed9b3c6e042b/tf-modules/cumulus/archive.tf,2019-08-14 14:23:38-04:00,2019-08-26 10:31:45-04:00,3,1,1,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,4294,infra/modules/aws/main.tf,infra/modules/aws/main.tf,0,# todo,"# TODO: it would maximize granularity of policy to push this into `aws-psoxy-rest` module, and","# TODO: it would maximize granularity of policy to push this into `aws-psoxy-rest` module, and 
 # do the statements based on configured list of http methods; but cost of that is policy + attachment 
 # for each instance, instead of one per deployment","resource ""aws_iam_policy"" ""invoke_api"" {
  count = var.use_api_gateway_v2 ? 1 : 0

  name_prefix = ""${var.deployment_id}InvokeAPI""

  policy = jsonencode({
    ""Version"" : ""2012-10-17"",
    ""Statement"" : [
      {
        ""Effect"" : ""Allow"",
        ""Action"" : ""execute-api:Invoke"",
        ""Resource"" : ""arn:aws:execute-api:*:${var.aws_account_id}:${aws_apigatewayv2_api.proxy_api[0].id}/*/GET/*"",
      },
      {
        ""Effect"" : ""Allow"",
        ""Action"" : ""execute-api:Invoke"",
        ""Resource"" : ""arn:aws:execute-api:*:${var.aws_account_id}:${aws_apigatewayv2_api.proxy_api[0].id}/*/HEAD/*"",
      },
      {
        ""Effect"" : ""Allow"",
        ""Action"" : ""execute-api:Invoke"",
        ""Resource"" : ""arn:aws:execute-api:*:${var.aws_account_id}:${aws_apigatewayv2_api.proxy_api[0].id}/*/POST/*"",
      },
    ]
  })
}
",resource,"resource ""aws_iam_policy"" ""invoke_api"" {
  count = var.use_api_gateway_v2 ? 1 : 0

  name_prefix = ""${var.deployment_id}InvokeAPI""

  policy = jsonencode({
    ""Version"" : ""2012-10-17"",
    ""Statement"" : [
      {
        ""Effect"" : ""Allow"",
        ""Action"" : ""execute-api:Invoke"",
        ""Resource"" : ""arn:aws:execute-api:*:${var.aws_account_id}:${aws_apigatewayv2_api.proxy_api[0].id}/*/GET/*"",
      },
      {
        ""Effect"" : ""Allow"",
        ""Action"" : ""execute-api:Invoke"",
        ""Resource"" : ""arn:aws:execute-api:*:${var.aws_account_id}:${aws_apigatewayv2_api.proxy_api[0].id}/*/HEAD/*"",
      },
      {
        ""Effect"" : ""Allow"",
        ""Action"" : ""execute-api:Invoke"",
        ""Resource"" : ""arn:aws:execute-api:*:${var.aws_account_id}:${aws_apigatewayv2_api.proxy_api[0].id}/*/POST/*"",
      },
    ]
  })
}
",resource,172,172.0,5ed7bc9ef2e9108a02f4d42e7d0d156eb5c7184d,e07a69ceca80240af2462aa09465535cc795d0b6,https://github.com/Worklytics/psoxy/blob/5ed7bc9ef2e9108a02f4d42e7d0d156eb5c7184d/infra/modules/aws/main.tf#L172,https://github.com/Worklytics/psoxy/blob/e07a69ceca80240af2462aa09465535cc795d0b6/infra/modules/aws/main.tf#L172,2024-01-31 10:34:59-08:00,2024-03-05 12:38:07-08:00,2,0,0,1,0,1,0,0,0,0
https://github.com/kubernetes-sigs/kubespray,1,contrib/terraform/aws/modules/vpc/main.tf,contrib/terraform/aws/modules/vpc/main.tf,0,#todo,#TODO: Do we need two routing tables for each subnet for redundancy or is one enough?,"#Routing in VPC  
 #TODO: Do we need two routing tables for each subnet for redundancy or is one enough? ","resource ""aws_route_table"" ""kubernetes-public"" {
    vpc_id = ""${aws_vpc.cluster-vpc.id}""
    route {
        cidr_block = ""0.0.0.0/0""
        gateway_id = ""${aws_internet_gateway.cluster-vpc-internetgw.id}""
    }
    tags {
        Name = ""kubernetes-${var.aws_cluster_name}-routetable-public""
    }
}
",resource,"resource ""aws_route_table"" ""kubernetes-public"" {
  vpc_id = aws_vpc.cluster-vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.cluster-vpc-internetgw.id
  }

  tags = merge(var.default_tags, tomap({
    Name = ""kubernetes-${var.aws_cluster_name}-routetable-public""
  }))
}
",resource,61,60.0,3c6b1480b806bcf8ac77e87a9cba0a637c04fe6d,3d4baea01c2af2f8174fc64a24a2768c4b2dbb96,https://github.com/kubernetes-sigs/kubespray/blob/3c6b1480b806bcf8ac77e87a9cba0a637c04fe6d/contrib/terraform/aws/modules/vpc/main.tf#L61,https://github.com/kubernetes-sigs/kubespray/blob/3d4baea01c2af2f8174fc64a24a2768c4b2dbb96/contrib/terraform/aws/modules/vpc/main.tf#L60,2017-03-06 12:52:02+01:00,2022-04-12 10:05:23-07:00,10,0,0,1,0,0,1,0,0,0
https://github.com/Worklytics/psoxy,391,infra/modules/hashicorp-vault-secrets/main.tf,infra/modules/hashicorp-vault-secrets/main.tf,0,implementation,# q: is to ALSO pass in some notion of access? except very different per implementation,"# for use in explicit IAM policy grants? 
 # q: good idea? breaks notion of AWS SSM parameters secrets being an implementation of a generic 
 # secrets-store interface 
 # q: is to ALSO pass in some notion of access? except very different per implementation","output ""secret_ids"" {
  value = { for k, v in var.secrets : k => aws_ssm_parameter.secret[k].path }
}
",output,,,18,0.0,e8f084f7a3cb3950efeaf655a06bcbc8dcbaa137,31311227708efa985f4963b3b9c47bf2547ef506,https://github.com/Worklytics/psoxy/blob/e8f084f7a3cb3950efeaf655a06bcbc8dcbaa137/infra/modules/hashicorp-vault-secrets/main.tf#L18,https://github.com/Worklytics/psoxy/blob/31311227708efa985f4963b3b9c47bf2547ef506/infra/modules/hashicorp-vault-secrets/main.tf#L0,2022-10-04 10:51:26-07:00,2022-10-17 12:58:48-07:00,2,2,0,1,0,1,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,203,modules/iam/group-workers.tf,modules/iam/group-workers.tf,0,todo,# TODO support keys defined at worker group level,# TODO support keys defined at worker group level,"locals {
  worker_group_name          = ""oke-workers-${var.state_id}""
  worker_compartments        = coalescelist(var.worker_compartments, [var.compartment_id])
  worker_compartment_matches = formatlist(""instance.compartment.id = '%s'"", local.worker_compartments)
  worker_compartment_rule    = format(""ANY {%s}"", join("", "", local.worker_compartment_matches))

  worker_group_rules = var.use_defined_tags ? format(""ALL {%s}"", join("", "", [
    ""tag.${var.tag_namespace}.role.value='worker'"",
    ""tag.${var.tag_namespace}.state_id.value='${var.state_id}'"",
  ])) : local.worker_compartment_rule

  cluster_join_where_clause = format(""ALL {%s}"", join("", "", compact([
    ""target.resource.kind = 'cluster'"",
    var.create_worker_policy ? ""target.cluster.id = '${var.cluster_id}'"" : null,
  ])))

  cluster_join_statements = formatlist(
    ""Allow dynamic-group %s to {CLUSTER_JOIN} in compartment id %s where %s"",
    local.worker_group_name, local.worker_compartments, local.cluster_join_where_clause
  )

  # TODO support keys defined at worker group level
  worker_kms_volume_templates = tolist([
    ""Allow service oke to USE key-delegates in compartment id %s where target.key.id = '%s'"",
    ""Allow service blockstorage to USE keys in compartment id %s where target.key.id = '%s'"",
    ""Allow dynamic-group ${local.worker_group_name} to USE key-delegates in compartment id %s where target.key.id = '%s'""
  ])

  # Block volume encryption using OCI Key Management System (KMS)
  worker_kms_volume_statements = coalesce(var.worker_volume_kms_key_id, ""none"") != ""none"" ? tolist([
    for statement in local.worker_kms_volume_templates :
    formatlist(statement, local.worker_compartments, var.worker_volume_kms_key_id)
  ]) : []

  worker_policy_statements = var.create_worker_policy ? concat(
    local.cluster_join_statements,
    local.worker_kms_volume_statements,
  ) : []
}
",locals,"locals {
  worker_group_name          = format(""oke-workers-%v"", var.state_id)
  worker_compartments        = coalescelist(var.worker_compartments, [var.compartment_id])
  worker_compartment_matches = formatlist(""instance.compartment.id = '%v'"", local.worker_compartments)
  worker_compartment_rule    = format(""ANY {%v}"", join("", "", local.worker_compartment_matches))

  worker_group_rules = var.use_defined_tags ? format(""ALL {%v}"", join("", "", [
    format(""tag.%v.role.value='worker'"", var.tag_namespace),
    format(""tag.%v.state_id.value='%v'"", var.tag_namespace, var.state_id),
  ])) : local.worker_compartment_rule

  cluster_join_where_clause = format(""ALL {%v}"", join("", "", compact([
    var.create_iam_worker_policy && var.cluster_id != null
    ? format(""target.cluster.id = %v"", var.cluster_id) : null
  ])))

  cluster_join_statements = formatlist(
    ""Allow dynamic-group %v to {CLUSTER_JOIN} in compartment id %v where %v"",
    local.worker_group_name, local.worker_compartments, local.cluster_join_where_clause
  )

  # TODO support keys defined at worker group level
  worker_kms_volume_templates = tolist([
    ""Allow service oke to USE key-delegates in compartment id %v where target.key.id = '%v'"",
    ""Allow service blockstorage to USE keys in compartment id %v where target.key.id = '%v'"",
    ""Allow dynamic-group ${local.worker_group_name} to USE key-delegates in compartment id %v where target.key.id = '%v'""
  ])

  # Block volume encryption using OCI Key Management System (KMS)
  worker_kms_volume_statements = coalesce(var.worker_volume_kms_key_id, ""none"") != ""none"" ? flatten(tolist([
    for statement in local.worker_kms_volume_templates :
    formatlist(statement, local.worker_compartments, var.worker_volume_kms_key_id)
  ])) : []

  worker_policy_statements = var.create_iam_worker_policy ? tolist(concat(
    local.cluster_join_statements,
    local.worker_kms_volume_statements,
  )) : []
}
",locals,25,25.0,6c867cd8e9cbf559742f56658989bcded0d1fd89,3ffaf123f92e82daa6d36bb6d9ef890d597ba234,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/modules/iam/group-workers.tf#L25,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/3ffaf123f92e82daa6d36bb6d9ef890d597ba234/modules/iam/group-workers.tf#L25,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,5,0,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,662,examples/data-solutions/data-platform-foundations/03-composer.tf,examples/data-solutions/data-platform-foundations/03-composer.tf,0,# todo,# TODO: descriptive name,# TODO: descriptive name,"module ""orch-sa-cmp-0"" {
  source     = ""../../../modules/iam-service-account""
  project_id = module.orch-project.project_id
  prefix     = var.prefix
  name       = ""orc-cmp-0""
  # TODO: descriptive name
  display_name = ""TODO""
  iam = {
    ""roles/iam.serviceAccountTokenCreator"" = [local.groups_iam.data-engineers]
    ""roles/iam.serviceAccountUser""         = [module.orch-sa-cmp-0.iam_email]
  }
}
",module,"module ""orch-sa-cmp-0"" {
  source       = ""../../../modules/iam-service-account""
  project_id   = module.orch-project.project_id
  prefix       = var.prefix
  name         = ""orc-cmp-0""
  display_name = ""Data platform Composer service account""
  iam = {
    ""roles/iam.serviceAccountTokenCreator"" = [local.groups_iam.data-engineers]
    ""roles/iam.serviceAccountUser""         = [module.orch-sa-cmp-0.iam_email]
  }
}
",module,22,,db1dc76e74850afc59c67fba0f21c8730687a56a,cdc6c7fc94985fea9e36ee79472275076a4cf95e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/db1dc76e74850afc59c67fba0f21c8730687a56a/examples/data-solutions/data-platform-foundations/03-composer.tf#L22,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/cdc6c7fc94985fea9e36ee79472275076a4cf95e/examples/data-solutions/data-platform-foundations/03-composer.tf,2022-02-10 07:54:52+01:00,2022-02-12 10:20:14+01:00,4,1,0,1,0,0,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,149,modules/node_groups/variables.tf,modules/node_groups/variables.tf,0,hack,# Hack for a homemade `depends_on` https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2,"# Hack for a homemade `depends_on` https://discuss.hashicorp.com/t/tips-howto-implement-module-depends-on-emulation/2305/2 
 # Will be removed in Terraform 0.13 with the support of module's `depends_on` https://github.com/hashicorp/terraform/issues/10462","variable ""ng_depends_on"" {
  description = ""List of references to other resources this submodule depends on""
  type        = any
  default     = null
}
",variable,the block associated got renamed or deleted,,38,,616d30ec674ff1d125710755f5073b1665bbd1af,56e93d77de58f311f1d1d7051f40bf77e7b03524,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/616d30ec674ff1d125710755f5073b1665bbd1af/modules/node_groups/variables.tf#L38,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/56e93d77de58f311f1d1d7051f40bf77e7b03524/modules/node_groups/variables.tf,2020-06-28 02:31:23+02:00,2021-11-06 20:19:03+01:00,7,1,0,1,1,0,0,0,0,0
https://github.com/Azure/terraform-azurerm-caf-enterprise-scale,3,archetypes/locals.policy_set_definitions.tf,modules/archetypes/locals.policy_set_definitions.tf,1,implemented,# Logic implemented to determine whether Policy Set Definitions,"# Generate the Policy Set Definition configurations for the specified archetype. 
 # Logic implemented to determine whether Policy Set Definitions 
 # need to be loaded to save on compute and memory resources 
 # when none defined in archetype definition.","locals {
  archetype_policy_set_definitions_list      = local.archetype_definition.policy_set_definitions
  archetype_policy_set_definitions_specified = try(length(local.archetype_policy_set_definitions_list) > 0, false)
}
",locals,"locals {
  archetype_policy_set_definitions_list      = local.archetype_definition.policy_set_definitions
  archetype_policy_set_definitions_specified = try(length(local.archetype_policy_set_definitions_list) > 0, false)
}
",locals,2,2.0,a05be92e463f90e577936a84395ffb88cfd045e7,1b41f7062f286e790be9b2e16a74562936f691df,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/a05be92e463f90e577936a84395ffb88cfd045e7/archetypes/locals.policy_set_definitions.tf#L2,https://github.com/Azure/terraform-azurerm-caf-enterprise-scale/blob/1b41f7062f286e790be9b2e16a74562936f691df/modules/archetypes/locals.policy_set_definitions.tf#L2,2020-09-25 20:39:19+01:00,2022-01-27 09:57:26+00:00,7,0,0,0,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,1625,infra/modules/gcp-psoxy-bulk/main.tf,infra/modules/gcp-psoxy-bulk/main.tf,0,# todo,# TODO: moved in 0.4.25 remove in 0.5,# TODO: moved in 0.4.25 remove in 0.5,"moved {
  from = google_service_account.service-account
  to   = google_service_account.service_account
}
",moved,the block associated got renamed or deleted,,115,,da62a471ef535349e3645f4a3644ada7cb23ed8e,bb41a6c349584a82517cb604a30196de7d807a75,https://github.com/Worklytics/psoxy/blob/da62a471ef535349e3645f4a3644ada7cb23ed8e/infra/modules/gcp-psoxy-bulk/main.tf#L115,https://github.com/Worklytics/psoxy/blob/bb41a6c349584a82517cb604a30196de7d807a75/infra/modules/gcp-psoxy-bulk/main.tf,2023-06-20 23:36:14+00:00,2023-10-23 15:56:19-07:00,9,1,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,617,examples/data-solutions/data-platform-foundations/variables.tf,examples/data-solutions/data-platform-foundations/variables.tf,0,#todo,#TODO hardcoded VPC ranges,#TODO hardcoded VPC ranges,"variable ""network_config"" {
  description = ""Network configurations to use. Specify a shared VPC to use, if null networks will be created in projects.""
  type = object({
    #TODO hardcoded Cloud NAT
    network_self_link = string
    #TODO hardcoded VPC ranges
    subnet_self_links = object({
      load           = string
      transformation = string
      orchestration  = string
    })
    composer_ip_ranges = object({
      cloudsql   = string
      gke_master = string
      web_server = string
    })
    composer_secondary_ranges = object({
      pods     = string
      services = string
    })
  })
  default = {
    enable_cloud_nat = false
    host_project     = null
    network          = null

    vpc_subnet = {
      load = {
        range           = ""10.10.0.0/24""
        secondary_range = null
      }
      transformation = {
        range           = ""10.10.0.0/24""
        secondary_range = null
      }
      orchestration = {
        range = ""10.10.0.0/24""
        secondary_range = {
          pods     = ""10.10.8.0/22""
          services = ""10.10.12.0/24""
        }
      }
    }
    vpc_subnet_self_link = null
  }
}
",variable,"variable ""network_config"" {
  description = ""Shared VPC network configurations to use. If null networks will be created in projects with preconfigured values.""
  type = object({
    network_self_link = string
    subnet_self_links = object({
      load           = string
      transformation = string
      orchestration  = string
    })
    composer_ip_ranges = object({
      cloudsql   = string
      gke_master = string
      web_server = string
    })
    composer_secondary_ranges = object({
      pods     = string
      services = string
    })
  })

  default = {
    network_self_link         = null
    subnet_self_links         = null
    composer_ip_ranges        = null
    composer_secondary_ranges = null
  }
}
",variable,83,,2e560407c118e7b7abc32f8ac1788a3f48563f21,d8bad5779036aa31639e4611e4935287fc79a4bc,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2e560407c118e7b7abc32f8ac1788a3f48563f21/examples/data-solutions/data-platform-foundations/variables.tf#L83,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d8bad5779036aa31639e4611e4935287fc79a4bc/examples/data-solutions/data-platform-foundations/variables.tf,2022-02-07 17:51:06+01:00,2022-02-07 21:28:54+01:00,2,1,0,1,0,0,1,0,0,0
https://github.com/apache/beam,7,playground/terraform/provider.tf,playground/terraform/provider.tf,0,todo,# TODO Please remove it when all resources are available in the stable version,# TODO Please remove it when all resources are available in the stable version,"provider ""google-beta"" {
  region = ""us-central""
}
",provider,"provider ""google-beta"" {
  region = var.region
  project = var.project_id
  // TODO may need to run module.setup first independent of this solution and add the terraform service account as a variable
  // This allows us to use a service account to provision resources without downloading or storing service account keys
  #  impersonate_service_account = module.setup.terraform_service_account_email
}",provider,23,,675c0bc10f813ea593702f5e6a0fd2ce38caf720,ad21d8353c856152346408f1d5029c9af05957c8,https://github.com/apache/beam/blob/675c0bc10f813ea593702f5e6a0fd2ce38caf720/playground/terraform/provider.tf#L23,https://github.com/apache/beam/blob/ad21d8353c856152346408f1d5029c9af05957c8/playground/terraform/provider.tf,2022-02-22 10:04:20-08:00,2022-03-16 14:13:22-07:00,2,1,0,1,0,0,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-lb-http,1,main.tf,main.tf,0,broken,# as the cross-project reference broken in https://github.com/terraform-google-modules/terraform-google-vm/issues/29,"# as the cross-project reference broken in https://github.com/terraform-google-modules/terraform-google-vm/issues/29 
 # and as there are some level of uncertenity that support for more than one is cross-referenced project ever worked 
 # we're limiting cross-referenced project to only one replacing list with string","resource ""google_compute_firewall"" ""default"" {
  for_each = toset(var.firewall_networks)

  name          = ""${var.name}-${each.value}-hc""
  # as the cross-project reference broken in https://github.com/terraform-google-modules/terraform-google-vm/issues/29
  # and as there are some level of uncertenity that support for more than one is cross-referenced project ever worked
  # we're limiting cross-referenced project to only one replacing list with string
  project       = var.firewall_projects != null ? var.firewall_projects : var.project
  network       = each.value
  source_ranges = [
    ""130.211.0.0/22"",
    ""35.191.0.0/16"",
    ""209.85.152.0/22"",
    ""209.85.204.0/22""]
  target_tags   = var.target_tags

  dynamic ""allow"" {
    for_each = distinct(var.backend_params)
    content {
      protocol = ""tcp""
      ports    = [
        split("","", allow.value)[2]]
    }
  }
}
",resource,the block associated got renamed or deleted,,155,,26efad92aea00aad9f7899670d843500b87dd988,fa368c1ab56dd6737a2771f89f33cb97f6c8c024,https://github.com/terraform-google-modules/terraform-google-lb-http/blob/26efad92aea00aad9f7899670d843500b87dd988/main.tf#L155,https://github.com/terraform-google-modules/terraform-google-lb-http/blob/fa368c1ab56dd6737a2771f89f33cb97f6c8c024/main.tf,2019-09-06 19:30:08+03:00,2019-09-10 13:06:05+03:00,2,1,0,1,1,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,596,examples/data-solutions/data-platform-foundations/03-orchestration.tf,examples/data-solutions/data-platform-foundations/03-orchestration.tf,0,#todo,#TODO Check with Simo/Ludo,#TODO Check with Simo/Ludo,"resource ""google_project_iam_binding"" ""composer_shared_vpc_agent"" {
  count   = var.network_config.network != null ? 1 : 0
  project = var.network_config.host_project
  role    = ""roles/composer.sharedVpcAgent""
  members = [
    ""serviceAccount:${module.orc-prj.service_accounts.robots.composer}""
  ]
}
",resource,"resource ""google_project_iam_binding"" ""composer_shared_vpc_agent"" {
  count   = var.network_config.network_self_link != null ? 1 : 0
  project = local._shared_vpc_project
  role    = ""roles/composer.sharedVpcAgent""
  members = [
    ""serviceAccount:${module.orc-prj.service_accounts.robots.composer}""
  ]
}
",resource,128,,74b850b4b8edb1acfb80958f665c2aad10945fd0,d8bad5779036aa31639e4611e4935287fc79a4bc,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/74b850b4b8edb1acfb80958f665c2aad10945fd0/examples/data-solutions/data-platform-foundations/03-orchestration.tf#L128,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/d8bad5779036aa31639e4611e4935287fc79a4bc/examples/data-solutions/data-platform-foundations/03-orchestration.tf,2022-02-05 09:04:18+01:00,2022-02-07 21:28:54+01:00,4,1,0,0,0,0,0,0,0,1
https://github.com/ministryofjustice/modernisation-platform,173,terraform/environments/bootstrap/member-bootstrap/iam.tf,terraform/environments/bootstrap/member-bootstrap/iam.tf,0,//todo,"#route53:* and ec2:Describe* are already allowed, but keeping the below for now, in case this is going to be moved elsewhere //TODO cleanup the permissions here","#route53:* and ec2:Describe* are already allowed, but keeping the below for now, in case this is going to be moved elsewhere //TODO cleanup the permissions here 
 //      ""route53:CreateHostedZone"", 
 //      ""route53:GetHostedZone"", 
 //      ""route53:DeleteHostedZone"", 
 //      ""route53:ListHostedZonesByName"", 
 //      ""route53:CreateHealthCheck"", 
 //      ""route53:GetHealthCheck"", 
 //      ""route53:DeleteHealthCheck"", 
 //      ""route53:UpdateHealthCheck"", 
 //      ""route53:ChangeResourceRecordSets"", 
 //      ""ec2:DescribeInstances"", 
 //      ""ec2:DescribeVpcs"", 
 //      ""ec2:DescribeRegions"" 
 # This is the end of permissions needed for the ServiceDiscovery and the AWS Cloud Map, see the doc: https://docs.aws.amazon.com/cloud-map/latest/dg/cloud-map-api-permissions-ref.html","data ""aws_iam_policy_document"" ""member-access"" {
  statement {
    #checkov:skip=CKV_AWS_108
    #checkov:skip=CKV_AWS_111
    #checkov:skip=CKV_AWS_107
    #checkov:skip=CKV_AWS_109
    #checkov:skip=CKV_AWS_110
    #checkov:skip=CKV2_AWS_40
    effect = ""Allow""
    actions = [
      ""acm-pca:*"",
      ""acm:*"",
      ""application-autoscaling:*"",
      ""applicationinsights:*"",
      ""athena:*"",
      ""autoscaling:*"",
      ""backup:*"",
      ""cloudfront:*"",
      ""cloudwatch:*"",
      ""codebuild:*"",
      ""codedeploy:*"",
      ""codepipeline:*"",
      ""dbqms:*"",
      ""dlm:*"",
      ""dms:*"",
      ""ds:CheckAlias"",
      ""ds:Describe*"",
      ""ds:List*"",
      ""ds:*Tags*"",
      ""ds:CancelSchemaExtension"",
      ""ds:CreateComputer"",
      ""ds:CreateAlias"",
      ""ds:CreateDirectory"",
      ""ds:CreateLogSubscription"",
      ""ds:CreateMicrosoftAD"",
      ""ds:CreateSnapshot"",
      ""ds:DeleteDirectory"",
      ""ds:DeleteLogSubscription"",
      ""ds:DeleteSnapshot"",
      ""ds:DeregisterCertificate"",
      ""ds:DeregisterEventTopic"",
      ""ds:DisableClientAuthentication"",
      ""ds:DisableLDAPS"",
      ""ds:DisableRadius"",
      ""ds:EnableClientAuthentication"",
      ""ds:EnableLDAPS"",
      ""ds:EnableRadius"",
      ""ds:RegisterCertificate"",
      ""ds:RegisterEventTopic"",
      ""ds:ResetUserPassword"",
      ""ds:RestoreFromSnapshot"",
      ""ds:StartSchemaExtension"",
      ""ds:UpdateDirectorySetup"",
      ""ds:UpdateNumberOfDomainControllers"",
      ""ds:UpdateRadius"",
      ""ds:UpdateSettings"",
      ""dynamodb:*"",
      ""ebs:*"",
      ""ec2:Describe*"",
      ""ec2:*SecurityGroup*"",
      ""ec2:*KeyPair*"",
      ""ec2:*Tags*"",
      ""ec2:*Volume*"",
      ""ec2:*Snapshot*"",
      ""ec2:*Ebs*"",
      ""ec2:*NetworkInterface*"",
      ""ec2:*Address*"",
      ""ec2:*Image*"",
      ""ec2:*Event*"",
      ""ec2:*Instance*"",
      ""ec2:*CapacityReservation*"",
      ""ec2:*Fleet*"",
      ""ec2:Get*"",
      ""ec2:SendDiagnosticInterrupt"",
      ""ec2:*LaunchTemplate*"",
      ""ec2:*PlacementGroup*"",
      ""ec2:*IdFormat*"",
      ""ec2:*Spot*"",
      ""ecr-public:*"",
      ""ecr:*"",
      ""ecs:*"",
      ""elasticfilesystem:*"",
      ""elasticloadbalancing:*"",
      ""events:*"",
      ""firehose:*"",
      ""glacier:*"",
      ""glue:*"",
      ""guardduty:get*"",
      ""iam:*"",
      ""kinesis:*"",
      ""kms:*"",
      ""lambda:*"",
      ""logs:*"",
      ""organizations:Describe*"",
      ""organizations:List*"",
      ""quicksight:*"",
      ""rds-db:*"",
      ""rds:*"",
      ""rds-data:*"",
      ""route53:*"",
      ""s3:*"",
      ""secretsmanager:*"",
      ""ses:*"",
      ""sns:*"",
      ""sqs:*"",
      ""ssm:*"",
      ""waf:*"",
      ""wafv2:*"",
      ""resource-groups:*"",
      ""redshift:*"",
      ""redshift-data:*"",
      ""redshift-serverless:*"",
      # The following permissions are needed for the ServiceDiscovery and the AWS Cloud Map, see the doc: https://docs.aws.amazon.com/cloud-map/latest/dg/cloud-map-api-permissions-ref.html
      ""servicediscovery:CreateHttpNamespace"",
      ""servicediscovery:UpdateHttpNamespace"",
      ""servicediscovery:DeleteNamespace"",
      ""servicediscovery:GetNamespace"",
      ""servicediscovery:ListNamespaces"",
      ""servicediscovery:CreatePrivateDnsNamespace"",
      ""servicediscovery:CreatePublicDnsNamespace"",
      ""servicediscovery:UpdatePrivateDnsNamespace"",
      ""servicediscovery:UpdatePublicDnsNamespace"",
      ""servicediscovery:CreateService"",
      ""servicediscovery:DeleteService"",
      ""servicediscovery:UpdateService"",
      ""servicediscovery:GetService"",
      ""servicediscovery:ListServices"",
      ""servicediscovery:DeregisterInstance"",
      ""servicediscovery:DiscoverInstances"",
      ""servicediscovery:RegisterInstance"",
      ""servicediscovery:GetInstance"",
      ""servicediscovery:ListInstances"",
      ""servicediscovery:GetInstancesHealthStatus"",
      ""servicediscovery:UpdateInstanceCustomHealthStatus"",
      ""servicediscovery:GetOperation"",
      ""servicediscovery:ListOperations""
      #route53:* and ec2:Describe* are already allowed, but keeping the below for now, in case this is going to be moved elsewhere //TODO cleanup the permissions here
//      ""route53:CreateHostedZone"",
//      ""route53:GetHostedZone"",
//      ""route53:DeleteHostedZone"",
//      ""route53:ListHostedZonesByName"",
//      ""route53:CreateHealthCheck"",
//      ""route53:GetHealthCheck"",
//      ""route53:DeleteHealthCheck"",
//      ""route53:UpdateHealthCheck"",
//      ""route53:ChangeResourceRecordSets"",
//      ""ec2:DescribeInstances"",
//      ""ec2:DescribeVpcs"",
//      ""ec2:DescribeRegions""
      # This is the end of permissions needed for the ServiceDiscovery and the AWS Cloud Map, see the doc: https://docs.aws.amazon.com/cloud-map/latest/dg/cloud-map-api-permissions-ref.html
    ]
    resources = [""*""] #tfsec:ignore:AWS099 tfsec:ignore:AWS097
  }

  statement {
    effect = ""Deny""
    actions = [
      ""ec2:CreateVpc"",
      ""ec2:CreateSubnet"",
      ""ec2:CreateVpcPeeringConnection"",
      ""iam:AddClientIDToOpenIDConnectProvider"",
      ""iam:AddUserToGroup"",
      ""iam:AttachGroupPolicy"",
      ""iam:AttachUserPolicy"",
      ""iam:CreateAccountAlias"",
      ""iam:CreateGroup"",
      ""iam:CreateLoginProfile"",
      ""iam:CreateOpenIDConnectProvider"",
      ""iam:CreateSAMLProvider"",
      ""iam:CreateUser"",
      ""iam:CreateVirtualMFADevice"",
      ""iam:DeactivateMFADevice"",
      ""iam:DeleteAccountAlias"",
      ""iam:DeleteAccountPasswordPolicy"",
      ""iam:DeleteGroup"",
      ""iam:DeleteGroupPolicy"",
      ""iam:DeleteLoginProfile"",
      ""iam:DeleteOpenIDConnectProvider"",
      ""iam:DeleteSAMLProvider"",
      ""iam:DeleteUser"",
      ""iam:DeleteUserPermissionsBoundary"",
      ""iam:DeleteUserPolicy"",
      ""iam:DeleteVirtualMFADevice"",
      ""iam:DetachGroupPolicy"",
      ""iam:DetachUserPolicy"",
      ""iam:EnableMFADevice"",
      ""iam:RemoveClientIDFromOpenIDConnectProvider"",
      ""iam:RemoveUserFromGroup"",
      ""iam:ResyncMFADevice"",
      ""iam:UpdateAccountPasswordPolicy"",
      ""iam:UpdateGroup"",
      ""iam:UpdateLoginProfile"",
      ""iam:UpdateOpenIDConnectProviderThumbprint"",
      ""iam:UpdateSAMLProvider"",
      ""iam:UpdateUser""
    ]
    resources = [""*""]
  }

  statement {
    effect = ""Deny""
    actions = [
      ""iam:AttachRolePolicy"",
      ""iam:DeleteRole"",
      ""iam:DeleteRolePermissionsBoundary"",
      ""iam:DeleteRolePolicy"",
      ""iam:DetachRolePolicy"",
      ""iam:PutRolePermissionsBoundary"",
      ""iam:PutRolePolicy"",
      ""iam:UpdateAssumeRolePolicy"",
      ""iam:UpdateRole"",
      ""iam:UpdateRoleDescription""
    ]
    resources = [""arn:aws:iam::*:user/cicd-member-user""]
  }
}
",data,"data ""aws_iam_policy_document"" ""member-access"" {
  statement {
    #checkov:skip=CKV_AWS_108
    #checkov:skip=CKV_AWS_111
    #checkov:skip=CKV_AWS_107
    #checkov:skip=CKV_AWS_109
    #checkov:skip=CKV_AWS_110
    #checkov:skip=CKV2_AWS_40
    effect = ""Allow""
    actions = [
      ""acm-pca:*"",
      ""acm:*"",
      ""application-autoscaling:*"",
      ""applicationinsights:*"",
      ""athena:*"",
      ""autoscaling:*"",
      ""backup:*"",
      ""cloudfront:*"",
      ""cloudwatch:*"",
      ""codebuild:*"",
      ""codedeploy:*"",
      ""codepipeline:*"",
      ""dbqms:*"",
      ""dlm:*"",
      ""dms:*"",
      ""ds:CheckAlias"",
      ""ds:Describe*"",
      ""ds:List*"",
      ""ds:*Tags*"",
      ""ds:CancelSchemaExtension"",
      ""ds:CreateComputer"",
      ""ds:CreateAlias"",
      ""ds:CreateDirectory"",
      ""ds:CreateLogSubscription"",
      ""ds:CreateMicrosoftAD"",
      ""ds:CreateSnapshot"",
      ""ds:DeleteDirectory"",
      ""ds:DeleteLogSubscription"",
      ""ds:DeleteSnapshot"",
      ""ds:DeregisterCertificate"",
      ""ds:DeregisterEventTopic"",
      ""ds:DisableClientAuthentication"",
      ""ds:DisableLDAPS"",
      ""ds:DisableRadius"",
      ""ds:EnableClientAuthentication"",
      ""ds:EnableLDAPS"",
      ""ds:EnableRadius"",
      ""ds:RegisterCertificate"",
      ""ds:RegisterEventTopic"",
      ""ds:ResetUserPassword"",
      ""ds:RestoreFromSnapshot"",
      ""ds:StartSchemaExtension"",
      ""ds:UpdateDirectorySetup"",
      ""ds:UpdateNumberOfDomainControllers"",
      ""ds:UpdateRadius"",
      ""ds:UpdateSettings"",
      ""dynamodb:*"",
      ""ebs:*"",
      ""ec2:Describe*"",
      ""ec2:*SecurityGroup*"",
      ""ec2:*KeyPair*"",
      ""ec2:*Tags*"",
      ""ec2:*Volume*"",
      ""ec2:*Snapshot*"",
      ""ec2:*Ebs*"",
      ""ec2:*NetworkInterface*"",
      ""ec2:*Address*"",
      ""ec2:*Image*"",
      ""ec2:*Event*"",
      ""ec2:*Instance*"",
      ""ec2:*CapacityReservation*"",
      ""ec2:*Fleet*"",
      ""ec2:Get*"",
      ""ec2:SendDiagnosticInterrupt"",
      ""ec2:*LaunchTemplate*"",
      ""ec2:*PlacementGroup*"",
      ""ec2:*IdFormat*"",
      ""ec2:*Spot*"",
      ""ecr-public:*"",
      ""ecr:*"",
      ""ecs:*"",
      ""elasticfilesystem:*"",
      ""elasticloadbalancing:*"",
      ""events:*"",
      ""firehose:*"",
      ""glacier:*"",
      ""glue:*"",
      ""guardduty:get*"",
      ""iam:*"",
      ""kinesis:*"",
      ""kms:*"",
      ""lambda:*"",
      ""logs:*"",
      ""organizations:Describe*"",
      ""organizations:List*"",
      ""quicksight:*"",
      ""rds-db:*"",
      ""rds:*"",
      ""rds-data:*"",
      ""route53:*"",
      ""s3:*"",
      ""secretsmanager:*"",
      ""ses:*"",
      ""sns:*"",
      ""sqs:*"",
      ""ssm:*"",
      ""waf:*"",
      ""wafv2:*"",
      ""resource-groups:*"",
      ""redshift:*"",
      ""redshift-data:*"",
      ""redshift-serverless:*""
    ]
    resources = [""*""] #tfsec:ignore:AWS099 tfsec:ignore:AWS097
  }

  statement {
    effect = ""Deny""
    actions = [
      ""ec2:CreateVpc"",
      ""ec2:CreateSubnet"",
      ""ec2:CreateVpcPeeringConnection"",
      ""iam:AddClientIDToOpenIDConnectProvider"",
      ""iam:AddUserToGroup"",
      ""iam:AttachGroupPolicy"",
      ""iam:AttachUserPolicy"",
      ""iam:CreateAccountAlias"",
      ""iam:CreateGroup"",
      ""iam:CreateLoginProfile"",
      ""iam:CreateOpenIDConnectProvider"",
      ""iam:CreateSAMLProvider"",
      ""iam:CreateUser"",
      ""iam:CreateVirtualMFADevice"",
      ""iam:DeactivateMFADevice"",
      ""iam:DeleteAccountAlias"",
      ""iam:DeleteAccountPasswordPolicy"",
      ""iam:DeleteGroup"",
      ""iam:DeleteGroupPolicy"",
      ""iam:DeleteLoginProfile"",
      ""iam:DeleteOpenIDConnectProvider"",
      ""iam:DeleteSAMLProvider"",
      ""iam:DeleteUser"",
      ""iam:DeleteUserPermissionsBoundary"",
      ""iam:DeleteUserPolicy"",
      ""iam:DeleteVirtualMFADevice"",
      ""iam:DetachGroupPolicy"",
      ""iam:DetachUserPolicy"",
      ""iam:EnableMFADevice"",
      ""iam:RemoveClientIDFromOpenIDConnectProvider"",
      ""iam:RemoveUserFromGroup"",
      ""iam:ResyncMFADevice"",
      ""iam:UpdateAccountPasswordPolicy"",
      ""iam:UpdateGroup"",
      ""iam:UpdateLoginProfile"",
      ""iam:UpdateOpenIDConnectProviderThumbprint"",
      ""iam:UpdateSAMLProvider"",
      ""iam:UpdateUser""
    ]
    resources = [""*""]
  }

  statement {
    effect = ""Deny""
    actions = [
      ""iam:AttachRolePolicy"",
      ""iam:DeleteRole"",
      ""iam:DeleteRolePermissionsBoundary"",
      ""iam:DeleteRolePolicy"",
      ""iam:DetachRolePolicy"",
      ""iam:PutRolePermissionsBoundary"",
      ""iam:PutRolePolicy"",
      ""iam:UpdateAssumeRolePolicy"",
      ""iam:UpdateRole"",
      ""iam:UpdateRoleDescription""
    ]
    resources = [""arn:aws:iam::*:user/cicd-member-user""]
  }
}
",data,153,,1a3f6074f866a5302b6a4f917ccfb5dc6b66e751,82b96b6253a012664e19390dbc00cec75b278963,https://github.com/ministryofjustice/modernisation-platform/blob/1a3f6074f866a5302b6a4f917ccfb5dc6b66e751/terraform/environments/bootstrap/member-bootstrap/iam.tf#L153,https://github.com/ministryofjustice/modernisation-platform/blob/82b96b6253a012664e19390dbc00cec75b278963/terraform/environments/bootstrap/member-bootstrap/iam.tf,2023-03-21 15:25:00+00:00,2023-03-30 11:27:01+01:00,4,1,0,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,4293,infra/modules/aws-psoxy-rest/main.tf,infra/modules/aws-psoxy-rest/main.tf,0,# todo,# TODO: limit by http method here too?,"# The /*/*/ part allows invocation from any stage, method and resource path 
 # within API Gateway REST API. 
 # TODO: limit by http method here too?","resource ""aws_lambda_permission"" ""api_gateway"" {
  count = local.use_api_gateway ? 1 : 0

  statement_id  = ""Allow${module.psoxy_lambda.function_name}Invoke""
  action        = ""lambda:InvokeFunction""
  function_name = module.psoxy_lambda.function_name
  principal     = ""apigateway.amazonaws.com""


  # The /*/*/ part allows invocation from any stage, method and resource path
  # within API Gateway REST API.
  # TODO: limit by http method here too?
  source_arn = ""${var.api_gateway_v2.execution_arn}/*/*/${module.psoxy_lambda.function_name}/{proxy+}""
}
",resource,"resource ""aws_lambda_permission"" ""api_gateway"" {
  count = local.use_api_gateway ? 1 : 0

  statement_id  = ""Allow${module.psoxy_lambda.function_name}Invoke""
  action        = ""lambda:InvokeFunction""
  function_name = module.psoxy_lambda.function_name
  principal     = ""apigateway.amazonaws.com""


  # The /*/*/ part allows invocation from any stage, method and resource path
  # within API Gateway REST API.
  # TODO: limit by http method here too?
  source_arn = ""${var.api_gateway_v2.execution_arn}/*/*/${module.psoxy_lambda.function_name}/{proxy+}""
}
",resource,106,111.0,5ed7bc9ef2e9108a02f4d42e7d0d156eb5c7184d,95b5a184cd35eb5a8eb1a44f3178c5c095260ccc,https://github.com/Worklytics/psoxy/blob/5ed7bc9ef2e9108a02f4d42e7d0d156eb5c7184d/infra/modules/aws-psoxy-rest/main.tf#L106,https://github.com/Worklytics/psoxy/blob/95b5a184cd35eb5a8eb1a44f3178c5c095260ccc/infra/modules/aws-psoxy-rest/main.tf#L111,2024-01-31 10:34:59-08:00,2024-03-21 17:02:36-07:00,7,0,1,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,695,infra/modules/worklytics-psoxy-connection-generic/main.tf,infra/modules/worklytics-psoxy-connection-generic/main.tf,0,todo,"# TODO try to avoid repetition of ""per_setting"" instructions (manual vs. deep linking)","# TODO try to avoid repetition of ""per_setting"" instructions (manual vs. deep linking) 
 # use 4 whitespace indentation for sub-lists","locals {
  # for backwards compatibility < 0.4.6
  instance_id = coalesce(var.psoxy_instance_id, var.display_name)

  # build TODO

  worklytics_add_connection_url = ""https://intl.worklytics.co/analytics/connect/""

  # map of Worklytics setting key --> display name
  autofilled_settings = {
    PROXY_AWS_ROLE_ARN = ""AWS Psoxy Role ARN"",
    PROXY_AWS_REGION   = ""AWS Psoxy Region""
    PROXY_ENDPOINT     = ""Psoxy Base URL""
    PROXY_BUCKET_NAME  = ""Bucket Name""
  }

  query_params = [for k, v in local.autofilled_settings : ""${k}=${urlencode(var.settings_to_provide[(v)])}""
  if contains(keys(var.settings_to_provide), v)]
  query_param_string = join(""&"", local.query_params)

  # TODO try to avoid repetition of ""per_setting"" instructions (manual vs. deep linking)
  # use 4 whitespace indentation for sub-lists
  per_setting_instructions      = [for k, v in var.settings_to_provide : ""    - Copy and paste `${v}` as the value for \""${k}\""."" if !contains(values(local.autofilled_settings), k)]
  per_setting_instructions_text = length(var.settings_to_provide) > 0 ? ""\n${join(""\n"", tolist(local.per_setting_instructions))}"" : """"

  per_setting_instructions_manual      = [for k, v in var.settings_to_provide : ""    - Copy and paste `${v}` as the value for \""${k}\"".""]
  per_setting_instructions_manual_text = length(var.settings_to_provide) > 0 ? ""\n${join(""\n"", tolist(local.per_setting_instructions_manual))}"" : """"

  deep_link_base = ""${local.worklytics_add_connection_url}${var.connector_id}/settings?PROXY_DEPLOYMENT_KIND=${var.psoxy_host_platform_id}&${local.query_param_string}""

  manual_instructions = <<EOT
1. Visit https://intl.worklytics.co/analytics/integrations (or login into Worklytics, and navigate to
   Manage --> Data Connections)
2. Click on the 'Add new connection' in the upper right.
3. Find the connector named ""${var.display_name}"" and click 'Connect'.
    - If presented with a further screen with several options, choose the 'via Psoxy' one.
4. Review instructions and click 'Connect' again.
5. Select `${var.psoxy_host_platform_id}` for ""Proxy Instance Type"".${local.per_setting_instructions_manual_text}
6. Review any additional settings that connector supports, adjusting values as you see fit, then
   click ""Connect"".
EOT


  deep_link_instructions = <<EOT
1. Ensure you're authenticated with Worklytics. Either sign-in at `https://app.worklytics.co` with
   your organization's SSO provider *or* request OTP link from your Worklytics support team.
2. Visit `${local.deep_link_base}`.${local.per_setting_instructions_text}
3. Review any additional settings that connector supports, adjusting values as you see fit, then
   click ""Connect"".

Alternatively, you may follow the manual instructions below:
${local.manual_instructions}
EOT

}
",locals,"locals {
  # for backwards compatibility < 0.4.6
  instance_id = coalesce(var.psoxy_instance_id, var.display_name)

  # build TODO

  worklytics_add_connection_url = ""https://${var.worklytics_host}/analytics/connect/""

  # map of Worklytics setting key --> display name (matches `settings_to_provide` keys)
  autofilled_settings = {
    PROXY_AWS_ROLE_ARN  = ""AWS Psoxy Role ARN"",
    PROXY_AWS_REGION    = ""AWS Psoxy Region""
    PROXY_ENDPOINT      = ""Psoxy Base URL""
    PROXY_BUCKET_NAME   = ""Bucket Name""
    parserId            = ""Parser""
    CLOUD_ID            = ""Jira Cloud Id""
    GITHUB_ORGANIZATION = ""GitHub Organization""
  }

  query_params = [for param_name, ux_name in local.autofilled_settings : ""${param_name}=${urlencode(var.settings_to_provide[ux_name])}""
  if contains(keys(var.settings_to_provide), ux_name) && try(var.settings_to_provide[ux_name] != null, false)]
  query_param_string = join(""&"", local.query_params)

  # TODO try to avoid repetition of ""per_setting"" instructions (manual vs. deep linking)
  # use 4 whitespace indentation for sub-lists
  per_setting_instructions      = [for ux_name, value_to_provide in var.settings_to_provide : ""    - Copy and paste `${value_to_provide}` as the value for \""${ux_name}\""."" if !contains(values(local.autofilled_settings), ux_name)]
  per_setting_instructions_text = length(var.settings_to_provide) > 0 ? ""\n${join(""\n"", tolist(local.per_setting_instructions))}"" : """"

  per_setting_instructions_manual      = [for ux_name, value_to_provide in var.settings_to_provide : ""    - Copy and paste `${value_to_provide}` as the value for \""${ux_name}\""."" if value_to_provide != null]
  per_setting_instructions_manual_text = length(var.settings_to_provide) > 0 ? ""\n${join(""\n"", tolist(local.per_setting_instructions_manual))}"" : """"

  deep_link_base = ""${local.worklytics_add_connection_url}${var.connector_id}/settings?PROXY_DEPLOYMENT_KIND=${var.psoxy_host_platform_id}&${local.query_param_string}""

  manual_instructions = <<EOT
1. Visit https://intl.worklytics.co/analytics/integrations (or login into Worklytics, and navigate to
   Manage --> Data Connections)
2. Click on the 'Add new connection' in the upper right.
3. Find the connector named ""${var.display_name}"" and click 'Connect'.
    - If presented with a further screen with several options, choose the 'via Psoxy' one.
4. Review instructions and click 'Connect' again.
5. Select `${var.psoxy_host_platform_id}` for ""Proxy Instance Type"".${local.per_setting_instructions_manual_text}
6. Review any additional settings that connector supports, adjusting values as you see fit, then
   click ""Connect"".
EOT


  deep_link_instructions = <<EOT
1. Ensure you're authenticated with Worklytics. Either sign-in at `https://app.worklytics.co` with
   your organization's SSO provider *or* request OTP link from your Worklytics support team.
2. Visit `${local.deep_link_base}`.${local.per_setting_instructions_text}
3. Review any additional settings that connector supports, adjusting values as you see fit, then
   click ""Connect"".

Alternatively, you may follow the manual instructions below:
${local.manual_instructions}
EOT

  todo_content = <<EOT
Complete the following steps in Worklytics AFTER you have deployed the Psoxy instance for your connection:

${var.connector_id == """" ? local.manual_instructions : local.deep_link_instructions}

Worklytics will attempt some basic health checks to ensure your Psoxy instance is reachable and
configured correctly. If this fails, contact support@worklytics.co for guidance.

EOT

}
",locals,22,25.0,04b1de74258a32bef06b0e6329fb54b07154259a,5937e6a45055a94ff2b493f96f21863d91699825,https://github.com/Worklytics/psoxy/blob/04b1de74258a32bef06b0e6329fb54b07154259a/infra/modules/worklytics-psoxy-connection-generic/main.tf#L22,https://github.com/Worklytics/psoxy/blob/5937e6a45055a94ff2b493f96f21863d91699825/infra/modules/worklytics-psoxy-connection-generic/main.tf#L25,2023-02-15 12:28:37-08:00,2024-03-06 18:11:21+00:00,9,0,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd,122,modules/gke-cloudbuild-private-pool/main.tf,modules/gke-cloudbuild-private-pool/main.tf,0,# todo,## TODO: GKE network self_link,"network    = ""https://www.googleapis.com/compute/v1/projects/<PROJECT_ID>/global/networks/local-network"" ## TODO: GKE network self_link","module ""vpn_ha-2"" {
  source     = ""terraform-google-modules/vpn/google//modules/vpn_ha""
  version    = ""~> 1.3.0""
  project_id = var.project_id
  region     = var.location
  network    = ""https://www.googleapis.com/compute/v1/projects/<PROJECT_ID>/global/networks/local-network"" ## TODO: GKE network self_link
  name       = ""gke-to-cloudbuild""
  router_asn = 64513
  peer_gcp_gateway = module.vpn_ha-1.self_link
  tunnels = {
    remote-0 = {
      bgp_peer = {
        address = ""169.254.1.2""
        asn     = 64514
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.1.1/30""
      ike_version       = 2
      vpn_gateway_interface = 0
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1.random_secret
    }
    remote-1 = {
      bgp_peer = {
        address = ""169.254.2.2""
        asn     = 64514
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.2.1/30""
      ike_version       = 2
      vpn_gateway_interface = 1
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1.random_secret
    }
  }
}",module,"module ""vpn_ha-2"" {
  count = length(local.gke_networks)

  source     = ""terraform-google-modules/vpn/google//modules/vpn_ha""
  version    = ""~> 1.3.0""
  project_id = local.gke_networks[count.index].project_id
  region     = local.gke_networks[count.index].location
  network    = local.gke_networks[count.index].network 
  name       = ""${local.gke_networks[count.index].network}-to-cloudbuild""
  router_asn = 65002+(count.index*2)
  peer_gcp_gateway = module.vpn_ha-1[count.index].self_link
  tunnels = {
    remote-0 = {
      bgp_peer = {
        address = ""169.254.${1+(count.index*2)}.1""
        asn     = 65001+(count.index*2)
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.${1+(count.index*2)}.2/30""
      ike_version       = 2
      vpn_gateway_interface = 0
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1[count.index].random_secret
    }
    remote-1 = {
      bgp_peer = {
        address = ""169.254.${2+(count.index*2)}.1""
        asn     = 65001+(count.index*2)
      }
      bgp_peer_options  = null
      bgp_session_range = ""169.254.${2+(count.index*2)}.2/30""
      ike_version       = 2
      vpn_gateway_interface = 1
      peer_external_gateway_interface = null
      shared_secret     = module.vpn_ha-1[count.index].random_secret
    }
  }
}",module,106,,a24b05ad924b0f7de098ce76711bc8cf32d12dbd,e29ac91f2eedf8a48e82065434b81010d298a423,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/a24b05ad924b0f7de098ce76711bc8cf32d12dbd/modules/gke-cloudbuild-private-pool/main.tf#L106,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/e29ac91f2eedf8a48e82065434b81010d298a423/modules/gke-cloudbuild-private-pool/main.tf,2021-11-24 15:09:55-06:00,2021-12-06 17:46:36-06:00,3,1,0,1,0,0,1,0,0,0
https://github.com/kubernetes/k8s.io,439,infra/aws/terraform/modules/eks-prow-iam/policy_apply.tf,infra/aws/terraform/modules/eks-prow-iam/policy_apply.tf,0,# todo,# TODO(xmudrii-ubuntu): remove after removing ECR repo,# TODO(xmudrii-ubuntu): remove after removing ECR repo,"data ""aws_iam_policy_document"" ""eks_apply"" {
  statement {
    sid       = ""AllowEKSCreateOrUpadate""
    effect    = ""Allow""
    resources = [""*""]

    actions = [
      ""acm:AddTagsToCertificate"",
      ""acm:RequestCertificate"",
      ""autoscaling:CreateOrUpdateTags"",
      ""ec2:AllocateAddress"",
      ""ec2:AssociateRouteTable"",
      ""ec2:AssociateVpcCidrBlock"",
      ""ec2:AttachInternetGateway"",
      ""ec2:AuthorizeSecurityGroupEgress"",
      ""ec2:AuthorizeSecurityGroupIngress"",
      ""ec2:CreateEgressOnlyInternetGateway"",
      ""ec2:CreateInternetGateway"",
      ""ec2:CreateLaunchTemplate"",
      ""ec2:CreateLaunchTemplateVersion"",
      ""ec2:ModifyLaunchTemplate"",
      ""ec2:CreateNatGateway"",
      ""ec2:CreateRoute"",
      ""ec2:CreateRouteTable"",
      ""ec2:CreateSecurityGroup"",
      ""ec2:CreateSubnet"",
      ""ec2:CreateTags"",
      ""ec2:CreateVpc"",
      ""ec2:ModifySubnetAttribute"",
      ""ec2:ModifyVpcAttribute"",
      ""ec2:RevokeSecurityGroupEgress"",
      ""ec2:RunInstances"",
      ""ec2:ModifyInstanceAttribute"",
      ""ec2:TerminateInstances"",
      ""ec2:ImportKeyPair"",
      ""eks:CreateAddon"",
      ""eks:CreateCluster"",
      ""eks:CreateNodegroup"",
      ""eks:TagResource"",
      ""eks:UpdateAddon"",
      ""eks:UpdateClusterConfig"",
      ""eks:UpdateClusterVersion"",
      ""eks:UpdateNodegroupConfig"",
      ""eks:UpdateNodegroupVersion"",
      ""iam:CreateOpenIDConnectProvider"",
      ""iam:CreatePolicy"",
      ""iam:CreatePolicyVersion"",
      ""iam:PassRole"",
      ""iam:TagOpenIDConnectProvider"",
      ""iam:TagPolicy"",
      ""iam:TagRole"",
      ""iam:UpdateOpenIDConnectProviderThumbprint"",
      ""iam:UpdateAssumeRolePolicy"",
      ""kms:CreateAlias"",
      ""kms:CreateGrant"",
      ""kms:CreateKey"",
      ""kms:EnableKeyRotation"",
      ""kms:ListAliases"",
      ""kms:ListResourceTags"",
      ""kms:PutKeyPolicy"",
      ""kms:TagResource"",
      ""logs:CreateLogGroup"",
      ""logs:PutRetentionPolicy"",
      ""logs:TagLogGroup"",
      ""s3:PutObject"",
      # TODO(xmudrii-ubuntu): remove after removing ECR repo
      ""ecr-public:*""
    ]
  }

  // This statement effectively enforces EKSResourcesPermissionBoundary on IAM resources
  // created with this policy.
  statement {
    sid = ""AllowCreateOnlyWithBoundary""

    effect = ""Allow""

    actions = [
      ""iam:CreateRole"",
      ""iam:CreateUser"",
    ]

    resources = [""*""]

    condition {
      test     = ""StringEquals""
      variable = ""iam:PermissionsBoundary""
      values = [
        aws_iam_policy.eks_resources_permission_boundary.arn
      ]
    }
  }

  statement {
    sid       = ""AllowChangeOnlyWithEKSResourceBoundary""
    effect    = ""Allow""
    resources = [""*""]

    actions = [
      ""iam:AttachRolePolicy"",
      ""iam:DeleteRolePolicy"",
      ""iam:PutRolePolicy"",
      ""iam:PutRolePermissionsBoundary"",
      ""iam:AttachUserPolicy"",
      ""iam:DeleteUserPolicy"",
      ""iam:DetachUserPolicy"",
      ""iam:PutUserPolicy"",
      ""iam:PutUserPermissionsBoundary"",
    ]

    condition {
      test     = ""StringEquals""
      variable = ""iam:PermissionsBoundary""
      values = [
        aws_iam_policy.eks_resources_permission_boundary.arn
      ]
    }
  }

  statement {
    sid = ""DenyEditBoundaries""

    effect = ""Deny""

    actions = [
      ""iam:DeletePolicy"",
      ""iam:CreatePolicyVersion"",
      ""iam:DeletePolicyVersion"",
      ""iam:SetDefaultPolicyVersion""
    ]

    resources = [
      ""arn:aws:iam::${local.account_id}:policy/boundary/*""
    ]
  }

  statement {
    sid = ""DenyLeaveOrganisation""

    effect = ""Deny""

    actions = [
      ""organizations:LeaveOrganization""
    ]

    resources = [""*""]
  }
}
",data,"data ""aws_iam_policy_document"" ""eks_apply"" {
  statement {
    sid       = ""AllowEKSCreateOrUpadate""
    effect    = ""Allow""
    resources = [""*""]

    actions = [
      ""acm:AddTagsToCertificate"",
      ""acm:RequestCertificate"",
      ""autoscaling:CreateOrUpdateTags"",
      ""ec2:AllocateAddress"",
      ""ec2:AssociateRouteTable"",
      ""ec2:AssociateVpcCidrBlock"",
      ""ec2:AttachInternetGateway"",
      ""ec2:AuthorizeSecurityGroupEgress"",
      ""ec2:AuthorizeSecurityGroupIngress"",
      ""ec2:CreateEgressOnlyInternetGateway"",
      ""ec2:CreateInternetGateway"",
      ""ec2:CreateLaunchTemplate"",
      ""ec2:CreateLaunchTemplateVersion"",
      ""ec2:ModifyLaunchTemplate"",
      ""ec2:CreateNatGateway"",
      ""ec2:CreateRoute"",
      ""ec2:CreateRouteTable"",
      ""ec2:CreateSecurityGroup"",
      ""ec2:CreateSubnet"",
      ""ec2:CreateTags"",
      ""ec2:CreateVpc"",
      ""ec2:ModifySubnetAttribute"",
      ""ec2:ModifyVpcAttribute"",
      ""ec2:RevokeSecurityGroupEgress"",
      ""ec2:RunInstances"",
      ""ec2:ModifyInstanceAttribute"",
      ""ec2:TerminateInstances"",
      ""ec2:ImportKeyPair"",
      ""eks:CreateAddon"",
      ""eks:CreateCluster"",
      ""eks:CreateNodegroup"",
      ""eks:TagResource"",
      ""eks:UpdateAddon"",
      ""eks:UpdateClusterConfig"",
      ""eks:UpdateClusterVersion"",
      ""eks:UpdateNodegroupConfig"",
      ""eks:UpdateNodegroupVersion"",
      ""iam:CreateOpenIDConnectProvider"",
      ""iam:CreatePolicy"",
      ""iam:CreatePolicyVersion"",
      ""iam:PassRole"",
      ""iam:TagOpenIDConnectProvider"",
      ""iam:TagPolicy"",
      ""iam:TagRole"",
      ""iam:UpdateOpenIDConnectProviderThumbprint"",
      ""iam:UpdateAssumeRolePolicy"",
      ""kms:CreateAlias"",
      ""kms:CreateGrant"",
      ""kms:CreateKey"",
      ""kms:EnableKeyRotation"",
      ""kms:ListAliases"",
      ""kms:ListResourceTags"",
      ""kms:PutKeyPolicy"",
      ""kms:TagResource"",
      ""logs:CreateLogGroup"",
      ""logs:PutRetentionPolicy"",
      ""logs:TagLogGroup"",
      ""s3:PutObject"",
      # TODO(xmudrii-ubuntu): remove after removing ECR repo
      ""ecr-public:*""
    ]
  }

  // This statement effectively enforces EKSResourcesPermissionBoundary on IAM resources
  // created with this policy.
  statement {
    sid = ""AllowCreateOnlyWithBoundary""

    effect = ""Allow""

    actions = [
      ""iam:CreateRole"",
      ""iam:CreateUser"",
    ]

    resources = [""*""]

    condition {
      test     = ""StringEquals""
      variable = ""iam:PermissionsBoundary""
      values = [
        aws_iam_policy.eks_resources_permission_boundary.arn
      ]
    }
  }

  statement {
    sid       = ""AllowChangeOnlyWithEKSResourceBoundary""
    effect    = ""Allow""
    resources = [""*""]

    actions = [
      ""iam:AttachRolePolicy"",
      ""iam:DeleteRolePolicy"",
      ""iam:PutRolePolicy"",
      ""iam:PutRolePermissionsBoundary"",
      ""iam:AttachUserPolicy"",
      ""iam:DeleteUserPolicy"",
      ""iam:DetachUserPolicy"",
      ""iam:PutUserPolicy"",
      ""iam:PutUserPermissionsBoundary"",
    ]

    condition {
      test     = ""StringEquals""
      variable = ""iam:PermissionsBoundary""
      values = [
        aws_iam_policy.eks_resources_permission_boundary.arn
      ]
    }
  }

  statement {
    sid = ""DenyEditBoundaries""

    effect = ""Deny""

    actions = [
      ""iam:DeletePolicy"",
      ""iam:CreatePolicyVersion"",
      ""iam:DeletePolicyVersion"",
      ""iam:SetDefaultPolicyVersion""
    ]

    resources = [
      ""arn:aws:iam::${local.account_id}:policy/boundary/*""
    ]
  }

  statement {
    sid = ""DenyLeaveOrganisation""

    effect = ""Deny""

    actions = [
      ""organizations:LeaveOrganization""
    ]

    resources = [""*""]
  }
}
",data,93,93.0,aac03e99a6c18cf7399a5cfdcf8a7485186b70ae,aac03e99a6c18cf7399a5cfdcf8a7485186b70ae,https://github.com/kubernetes/k8s.io/blob/aac03e99a6c18cf7399a5cfdcf8a7485186b70ae/infra/aws/terraform/modules/eks-prow-iam/policy_apply.tf#L93,https://github.com/kubernetes/k8s.io/blob/aac03e99a6c18cf7399a5cfdcf8a7485186b70ae/infra/aws/terraform/modules/eks-prow-iam/policy_apply.tf#L93,2023-10-16 10:30:36+02:00,2023-10-16 10:30:36+02:00,1,0,0,1,0,1,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-vpc,1,main.tf,main.tf,0,workaround,"# Workaround for interpolation not being able to ""short-circuit"" the evaluation of the conditional branch that doesn't end up being used","############## 
 # NAT Gateway 
 ############## 
 # Workaround for interpolation not being able to ""short-circuit"" the evaluation of the conditional branch that doesn't end up being used 
 # Source: https://github.com/hashicorp/terraform/issues/11566#issuecomment-289417805 
 # 
 # The logical expression would be 
 # 
 #    nat_gateway_ips = var.reuse_nat_ips ? var.external_nat_ip_ids : aws_eip.nat.*.id 
 # 
 # but then when count of aws_eip.nat.*.id is zero, this would throw a resource not found error on aws_eip.nat.*.id.","locals {
  nat_gateway_ips = ""${split("","", (var.reuse_nat_ips ? join("","", var.external_nat_ip_ids) : join("","", aws_eip.nat.*.id)))}""
}
",locals,"locals {
  max_subnet_length = max(
    length(var.private_subnets),
    length(var.elasticache_subnets),
    length(var.database_subnets),
    length(var.redshift_subnets),
  )
  nat_gateway_count = var.single_nat_gateway ? 1 : var.one_nat_gateway_per_az ? length(var.azs) : local.max_subnet_length

  # Use `local.vpc_id` to give a hint to Terraform that subnets should be deleted before secondary CIDR blocks can be free!
  vpc_id = try(aws_vpc_ipv4_cidr_block_association.this[0].vpc_id, aws_vpc.this[0].id, """")
}
",locals,159,,5c111ad16c93b4982621b91dacdc120f7b4faa6c,d1adf743b27ef131b559ec15c7aadc37466a74b9,https://github.com/terraform-aws-modules/terraform-aws-vpc/blob/5c111ad16c93b4982621b91dacdc120f7b4faa6c/main.tf#L159,https://github.com/terraform-aws-modules/terraform-aws-vpc/blob/d1adf743b27ef131b559ec15c7aadc37466a74b9/main.tf,2017-12-07 16:32:22+01:00,2022-01-13 15:10:46+01:00,95,1,0,1,1,0,1,0,0,0
https://github.com/Azure/Avere,20,src/terraform/modules/hammerspace/anvil/main.tf,src/terraform/modules/hammerspace/anvil/main.tf,0,todo,// TODO - create the associations https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/network_interface_backend_address_pool_association,// TODO - create the associations https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/network_interface_backend_address_pool_association,"resource ""azurerm_lb_backend_address_pool"" ""anvilloadbalancerbepool"" {
  count               = local.is_high_availability ? 1 : 0
  resource_group_name = var.resource_group_name
  loadbalancer_id     = azurerm_lb.anvilloadbalancer[0].id
  name                = ""${var.unique_name}LoadBalancerBEPool""
}
",resource,"resource ""azurerm_lb_backend_address_pool"" ""anvilloadbalancerbepool"" {
  count           = local.is_high_availability ? 1 : 0
  loadbalancer_id = azurerm_lb.anvilloadbalancer[0].id
  name            = ""${var.unique_name}LoadBalancerBEPool""
}
",resource,48,,47847b35c4a7b8d24584e2c9690ea7a41d5bc3f1,a64c98d189a8f44b087f399cfa0864ecef7b3eeb,https://github.com/Azure/Avere/blob/47847b35c4a7b8d24584e2c9690ea7a41d5bc3f1/src/terraform/modules/hammerspace/anvil/main.tf#L48,https://github.com/Azure/Avere/blob/a64c98d189a8f44b087f399cfa0864ecef7b3eeb/src/terraform/modules/hammerspace/anvil/main.tf,2021-03-02 06:48:40-05:00,2021-05-02 10:11:01-04:00,7,1,0,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,147,modules/vpc-sc/outputs.tf,modules/vpc-sc/outputs.tf,0,fix,# FIXME(jccb): these deps don't exist (??),"# FIXME(jccb): these deps don't exist (??) 
 # depends_on = [ 
 #   google_organization_iam_audit_config, 
 #   google_organization_iam_binding.authoritative, 
 #   google_organization_iam_custom_role.roles, 
 #   google_organization_iam_member.additive, 
 #   google_organization_policy.boolean, 
 #   google_organization_policy.list 
 # ]","output ""org_id"" {
  description = ""Organization id dependent on module resources.""
  value       = var.org_id
  # FIXME(jccb): these deps don't exist (??)
  # depends_on = [
  #   google_organization_iam_audit_config,
  #   google_organization_iam_binding.authoritative,
  #   google_organization_iam_custom_role.roles,
  #   google_organization_iam_member.additive,
  #   google_organization_policy.boolean,
  #   google_organization_policy.list
  # ]
}
",output,the block associated got renamed or deleted,,20,,eecdee63e60d8d2d2768a0696caa3a2ae124a290,dda715670cfa1a40aa94929b8f8383ea03008269,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/eecdee63e60d8d2d2768a0696caa3a2ae124a290/modules/vpc-sc/outputs.tf#L20,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/dda715670cfa1a40aa94929b8f8383ea03008269/modules/vpc-sc/outputs.tf,2020-11-07 10:28:33+01:00,2020-11-16 18:04:12+01:00,2,1,0,1,0,0,0,0,0,1
https://github.com/nebari-dev/nebari,2,qhub/template/{{ cookiecutter.repo_directory }}/infrastructure/modules/kubernetes/ingress/main.tf,src/_nebari/stages/kubernetes_ingress/template/modules/kubernetes/ingress/main.tf,1,# todo,# TODO: eventually needs to be tied into traefik middle,"# allow access to the dashboard directly through the port 
 # TODO: eventually needs to be tied into traefik middle 
 # security possibly using jupyterhub auth this is not a 
 # security risk at the moment since this port is not 
 # externally accessible","resource ""kubernetes_deployment"" ""main"" {
  metadata {
    name      = ""${var.name}-traefik-ingress""
    namespace = var.namespace
  }

  spec {
    replicas = 1

    selector {
      match_labels = {
        ""app.kubernetes.io/component"" = ""traefik-ingress""
      }
    }

    template {
      metadata {
        labels = {
          ""app.kubernetes.io/component"" = ""traefik-ingress""
        }
      }

      spec {
        service_account_name             = kubernetes_service_account.main.metadata.0.name
        termination_grace_period_seconds = 60

        affinity {
          node_affinity {
            required_during_scheduling_ignored_during_execution {
              node_selector_term {
                match_expressions {
                  key      = var.node-group.key
                  operator = ""In""
                  values   = [var.node-group.value]
                }
              }
            }
          }
        }

        container {
          image = ""${var.traefik-image.image}:${var.traefik-image.tag}""
          name  = var.name

          security_context {
            capabilities {
              drop = [""ALL""]
              add  = [""NET_BIND_SERVICE""]
            }
          }

          args = concat([
            # Do not send usage stats
            ""--global.checknewversion=false"",
            ""--global.sendanonymoususage=false"",
            # allow access to the dashboard directly through the port
            # TODO: eventually needs to be tied into traefik middle
            # security possibly using jupyterhub auth this is not a
            # security risk at the moment since this port is not
            # externally accessible
            ""--api.insecure=true"",
            ""--api.dashboard=true"",
            ""--ping=true"",
            # Start the Traefik Kubernetes Ingress Controller
            ""--providers.kubernetesingress"",
            ""--providers.kubernetesingress.namespaces=${var.namespace}"",
            ""--providers.kubernetesingress.ingressclass=traefik"",
            # Start the Traefik Kubernetes CRD Controller Provider
            ""--providers.kubernetescrd"",
            ""--providers.kubernetescrd.namespaces=${var.namespace}"",
            ""--providers.kubernetescrd.throttleduration=2s"",
            ""--providers.kubernetescrd.allowcrossnamespace=false"",
            # Define two entrypoint ports, and setup a redirect from HTTP to HTTPS.
            ""--entryPoints.web.address=:80"",
            ""--entryPoints.websecure.address=:443"",
            ""--entrypoints.ssh.address=:8022"",
            ""--entrypoints.sftp.address=:8023"",
            ""--entryPoints.tcp.address=:8786"",
            ""--entryPoints.traefik.address=:9000"",
            ""--entrypoints.web.http.redirections.entryPoint.to=websecure"",
            ""--entrypoints.web.http.redirections.entryPoint.scheme=https"",
            # Enable debug logging. Useful to work out why something might not be
            # working. Fetch logs of the pod.
            ""--log.level=${var.loglevel}"",
            ], var.enable-certificates ? [
            ""--certificatesresolvers.default.acme.tlschallenge"",
            ""--certificatesresolvers.default.acme.email=${var.acme-email}"",
            ""--certificatesresolvers.default.acme.storage=acme.json"",
            ""--certificatesresolvers.default.acme.caserver=${var.acme-server}"",
          ] : [])

          port {
            name           = ""http""
            container_port = 80
          }

          port {
            name           = ""https""
            container_port = 443
          }

          port {
            name           = ""ssh""
            container_port = 8022
          }

          port {
            name           = ""sftp""
            container_port = 8023
          }

          port {
            name           = ""tcp""
            container_port = 8786
          }

          port {
            name           = ""traefik""
            container_port = 9000
          }

          liveness_probe {
            http_get {
              path = ""/ping""
              port = ""traefik""
            }

            initial_delay_seconds = 10
            timeout_seconds       = 2
            period_seconds        = 10
            failure_threshold     = 3
            success_threshold     = 1
          }

          readiness_probe {
            http_get {
              path = ""/ping""
              port = ""traefik""
            }

            initial_delay_seconds = 10
            timeout_seconds       = 2
            period_seconds        = 10
            failure_threshold     = 1
            success_threshold     = 1
          }
        }
      }
    }
  }
}
",resource,"resource ""kubernetes_deployment"" ""main"" {
  metadata {
    name      = ""${var.name}-traefik-ingress""
    namespace = var.namespace
  }

  spec {
    replicas = 1

    selector {
      match_labels = {
        ""app.kubernetes.io/component"" = ""traefik-ingress""
      }
    }

    template {
      metadata {
        labels = {
          ""app.kubernetes.io/component"" = ""traefik-ingress""
        }
      }

      spec {
        service_account_name             = kubernetes_service_account.main.metadata.0.name
        termination_grace_period_seconds = 60

        affinity {
          node_affinity {
            required_during_scheduling_ignored_during_execution {
              node_selector_term {
                match_expressions {
                  key      = var.node-group.key
                  operator = ""In""
                  values   = [var.node-group.value]
                }
              }
            }
          }
        }

        container {
          image = ""${var.traefik-image.image}:${var.traefik-image.tag}""
          name  = var.name

          volume_mount {
            mount_path = ""/mnt/acme-certificates""
            name       = ""acme-certificates""
          }
          security_context {
            capabilities {
              drop = [""ALL""]
              add  = [""NET_BIND_SERVICE""]
            }
          }

          args = concat([
            # Do not send usage stats
            ""--global.checknewversion=false"",
            ""--global.sendanonymoususage=false"",
            # allow access to the dashboard directly through the port
            # TODO: eventually needs to be tied into traefik middle
            # security possibly using jupyterhub auth this is not a
            # security risk at the moment since this port is not
            # externally accessible
            ""--api.insecure=true"",
            ""--api.dashboard=true"",
            ""--ping=true"",
            # Start the Traefik Kubernetes Ingress Controller
            ""--providers.kubernetesingress=true"",
            ""--providers.kubernetesingress.namespaces=${var.namespace}"",
            ""--providers.kubernetesingress.ingressclass=traefik"",
            # Start the Traefik Kubernetes CRD Controller Provider
            ""--providers.kubernetescrd"",
            ""--providers.kubernetescrd.namespaces=${var.namespace}"",
            ""--providers.kubernetescrd.throttleduration=2s"",
            ""--providers.kubernetescrd.allowcrossnamespace=false"",
            # Define two entrypoint ports, and setup a redirect from HTTP to HTTPS.
            ""--entryPoints.web.address=:80"",
            ""--entryPoints.websecure.address=:443"",
            ""--entrypoints.ssh.address=:8022"",
            ""--entrypoints.sftp.address=:8023"",
            ""--entryPoints.tcp.address=:8786"",
            ""--entryPoints.traefik.address=:9000"",
            # Define the entrypoint port for Minio
            ""--entryPoints.minio.address=:9080"",
            # Redirect http -> https
            ""--entrypoints.web.http.redirections.entryPoint.to=websecure"",
            ""--entrypoints.web.http.redirections.entryPoint.scheme=https"",
            # Enable Prometheus Monitoring of Traefik
            ""--metrics.prometheus=true"",
            # Enable debug logging. Useful to work out why something might not be
            # working. Fetch logs of the pod.
            ""--log.level=${var.loglevel}"",
            ],
            local.add-certificate,
            var.additional-arguments,
          )

          port {
            name           = ""http""
            container_port = 80
          }

          port {
            name           = ""https""
            container_port = 443
          }

          port {
            name           = ""ssh""
            container_port = 8022
          }

          port {
            name           = ""sftp""
            container_port = 8023
          }

          port {
            name           = ""tcp""
            container_port = 8786
          }

          port {
            name           = ""traefik""
            container_port = 9000
          }

          port {
            name           = ""minio""
            container_port = 9080
          }

          liveness_probe {
            http_get {
              path = ""/ping""
              port = ""traefik""
            }

            initial_delay_seconds = 10
            timeout_seconds       = 2
            period_seconds        = 10
            failure_threshold     = 3
            success_threshold     = 1
          }

          readiness_probe {
            http_get {
              path = ""/ping""
              port = ""traefik""
            }

            initial_delay_seconds = 10
            timeout_seconds       = 2
            period_seconds        = 10
            failure_threshold     = 1
            success_threshold     = 1
          }
        }
        volume {
          name = ""acme-certificates""
          persistent_volume_claim {
            claim_name = kubernetes_persistent_volume_claim.traefik_certs_pvc.metadata.0.name
          }
        }
      }
    }
  }
}
",resource,167,250.0,43d0a5db3aec048ac325a1db8b00991ad3ea2503,6bb76264e203f87a157aa08ac63094f20dc890b1,https://github.com/nebari-dev/nebari/blob/43d0a5db3aec048ac325a1db8b00991ad3ea2503/qhub/template/{{ cookiecutter.repo_directory }}/infrastructure/modules/kubernetes/ingress/main.tf#L167,https://github.com/nebari-dev/nebari/blob/6bb76264e203f87a157aa08ac63094f20dc890b1/src/_nebari/stages/kubernetes_ingress/template/modules/kubernetes/ingress/main.tf#L250,2021-05-13 15:33:02+01:00,2024-03-27 18:23:09-03:00,13,0,1,1,0,1,1,0,0,0
https://github.com/ministryofjustice/modernisation-platform,196,terraform/environments/data-platform-apps-and-tools/eks-cluster.tf,terraform/environments/data-platform-apps-and-tools/eks-cluster.tf,0,// todo,// TODO: Review these settings,// TODO: Review these settings,"module ""eks"" {
  #checkov:skip=CKV_TF_1:Module is from Terraform registry

  source  = ""terraform-aws-modules/eks/aws""
  version = ""19.16.0""

  cluster_name    = local.environment_configuration.eks_cluster_name
  cluster_version = local.environment_configuration.eks_versions.cluster

  cluster_endpoint_private_access = true
  cluster_endpoint_public_access  = true

  vpc_id                   = module.vpc.vpc_id
  control_plane_subnet_ids = module.vpc.private_subnets
  subnet_ids               = module.vpc.private_subnets

  cluster_enabled_log_types = [""api"", ""audit"", ""authenticator"", ""controllerManager"", ""scheduler""]

  cluster_addons = {
    coredns = {
      addon_version = local.environment_configuration.eks_versions.addon_coredns
    }
    kube-proxy = {
      addon_version = local.environment_configuration.eks_versions.addon_kube_proxy
    }
    vpc-cni = {
      addon_version = local.environment_configuration.eks_versions.addon_vpc_cni
    }
    aws-guardduty-agent = {
      addon_version = local.environment_configuration.eks_versions.addon_aws_guardduty_agent
    }
  }

  eks_managed_node_group_defaults = {
    ami_release_version = local.environment_configuration.eks_versions.ami_release
    ami_type            = ""BOTTLEROCKET_x86_64""
    platform            = ""bottlerocket""
    metadata_options = {
      http_endpoint               = ""enabled""
      http_put_response_hop_limit = 2
      http_tokens                 = ""required""
      instance_metadata_tags      = ""enabled""
    }

    block_device_mappings = {
      xvda = {
        device_name = ""/dev/xvda""
        ebs = {
          volume_size = 100
        }
      }
    }

    iam_role_additional_policies = {
      AmazonSSMManagedInstanceCore = ""arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore""
      CloudWatchAgentServerPolicy  = ""arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy""
    }
  }

  // TODO: Review these settings
  eks_managed_node_groups = {
    general = {
      min_size       = 3
      max_size       = 10
      desired_size   = 5
      instance_types = [""t3.xlarge""]
    }
  }

  manage_aws_auth_configmap = true

  aws_auth_roles = [
    {
      rolearn  = ""arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${one(data.aws_iam_roles.eks_sso_access_role.names)}""
      groups   = [""system:masters""]
      username = ""administrator""
    },
    {
      // rolearn cannot consume module.airflow_execution_role.arn because that role consumes module.eks.cluster_arn, but we can construct the ARN manually
      rolearn  = ""arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/${local.environment_configuration.airflow_execution_role_name}""
      groups   = [""airflow""]
      username = ""airflow""
    }
  ]

  tags = local.tags
}
",module,,,63,0.0,90002b7667baf7c1eb23d0e49dcfa9d9eaee7567,951ffdb805c4255e3bb6d0a5febe5f1bb21407c9,https://github.com/ministryofjustice/modernisation-platform/blob/90002b7667baf7c1eb23d0e49dcfa9d9eaee7567/terraform/environments/data-platform-apps-and-tools/eks-cluster.tf#L63,https://github.com/ministryofjustice/modernisation-platform/blob/951ffdb805c4255e3bb6d0a5febe5f1bb21407c9/terraform/environments/data-platform-apps-and-tools/eks-cluster.tf#L0,2023-10-18 15:43:32+01:00,2023-12-19 15:49:47+00:00,9,2,1,1,0,0,0,0,0,0
https://github.com/alphagov/govuk-aws,146,terraform/projects/infra-security-groups/whitehall-backend.tf,terraform/projects/infra-security-groups/whitehall-backend.tf,0,# todo,# TODO: replace this with ingress from the LBs when we build them.,# TODO: replace this with ingress from the LBs when we build them.,"resource ""aws_security_group_rule"" ""allow_management_to_whitehall-backend_elb"" {
  type      = ""ingress""
  from_port = 443
  to_port   = 443
  protocol  = ""tcp""

  security_group_id        = ""${aws_security_group.whitehall-backend_elb.id}""
  source_security_group_id = ""${aws_security_group.management.id}""
}
",resource,the block associated got renamed or deleted,,47,,b4e9a049cc7f645b739d9b021ed7d4fe9f929a8c,7fd13330f8d6238e108f76ce76a76a28a99caaaf,https://github.com/alphagov/govuk-aws/blob/b4e9a049cc7f645b739d9b021ed7d4fe9f929a8c/terraform/projects/infra-security-groups/whitehall-backend.tf#L47,https://github.com/alphagov/govuk-aws/blob/7fd13330f8d6238e108f76ce76a76a28a99caaaf/terraform/projects/infra-security-groups/whitehall-backend.tf,2017-07-25 12:48:54+01:00,2018-01-02 17:41:32+00:00,4,1,0,1,0,1,1,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,640,modules/_user_data/main.tf,modules/_user_data/main.tf,0,todo,# TODO - will be removed in v21.0,"# Try to use `ami_type` first, but fall back to current, default behavior 
 # TODO - will be removed in v21.0","locals {
  # Converts AMI type into user data type that represents the underlying format (bash, toml, PS1, nodeadm)
  # TODO - platform will be removed in v21.0 and only `ami_type` will be valid
  ami_type_to_user_data_type = {
    AL2_x86_64                 = ""linux""
    AL2_x86_64_GPU             = ""linux""
    AL2_ARM_64                 = ""linux""
    BOTTLEROCKET_ARM_64        = ""bottlerocket""
    BOTTLEROCKET_x86_64        = ""bottlerocket""
    BOTTLEROCKET_ARM_64_NVIDIA = ""bottlerocket""
    BOTTLEROCKET_x86_64_NVIDIA = ""bottlerocket""
    WINDOWS_CORE_2019_x86_64   = ""windows""
    WINDOWS_FULL_2019_x86_64   = ""windows""
    WINDOWS_CORE_2022_x86_64   = ""windows""
    WINDOWS_FULL_2022_x86_64   = ""windows""
    AL2023_x86_64_STANDARD     = ""al2023""
    AL2023_ARM_64_STANDARD     = ""al2023""
  }
  # Try to use `ami_type` first, but fall back to current, default behavior
  # TODO - will be removed in v21.0
  user_data_type = try(local.ami_type_to_user_data_type[var.ami_type], var.platform)

  template_path = {
    al2023       = ""${path.module}/../../templates/al2023_user_data.tpl""
    bottlerocket = ""${path.module}/../../templates/bottlerocket_user_data.tpl""
    linux        = ""${path.module}/../../templates/linux_user_data.tpl""
    windows      = ""${path.module}/../../templates/windows_user_data.tpl""
  }

  cluster_service_cidr = try(coalesce(var.cluster_service_ipv4_cidr, var.cluster_service_cidr), """")

  user_data = base64encode(templatefile(
    coalesce(var.user_data_template_path, local.template_path[local.user_data_type]),
    {
      # https://docs.aws.amazon.com/eks/latest/userguide/launch-templates.html#launch-template-custom-ami
      enable_bootstrap_user_data = var.enable_bootstrap_user_data

      # Required to bootstrap node
      cluster_name        = var.cluster_name
      cluster_endpoint    = var.cluster_endpoint
      cluster_auth_base64 = var.cluster_auth_base64

      cluster_service_cidr = local.cluster_service_cidr
      cluster_ip_family    = var.cluster_ip_family
      # Bottlerocket
      cluster_dns_ip = try(cidrhost(local.cluster_service_cidr, 10), """")

      # Optional
      bootstrap_extra_args     = var.bootstrap_extra_args
      pre_bootstrap_user_data  = var.pre_bootstrap_user_data
      post_bootstrap_user_data = var.post_bootstrap_user_data
    }
  ))

  user_data_type_to_rendered = {
    al2023 = {
      user_data = var.create ? try(data.cloudinit_config.al2023_eks_managed_node_group[0].rendered, local.user_data) : """"
    }
    bottlerocket = {
      user_data = var.create && local.user_data_type == ""bottlerocket"" && (var.enable_bootstrap_user_data || var.user_data_template_path != """" || var.bootstrap_extra_args != """") ? local.user_data : """"
    }
    linux = {
      user_data = var.create ? try(data.cloudinit_config.linux_eks_managed_node_group[0].rendered, local.user_data) : """"
    }
    windows = {
      user_data = var.create && local.user_data_type == ""windows"" && (var.enable_bootstrap_user_data || var.user_data_template_path != """" || var.pre_bootstrap_user_data != """") ? local.user_data : """"
    }
  }
}
",locals,"locals {
  # Converts AMI type into user data type that represents the underlying format (bash, toml, PS1, nodeadm)
  # TODO - platform will be removed in v21.0 and only `ami_type` will be valid
  ami_type_to_user_data_type = {
    AL2_x86_64                 = ""linux""
    AL2_x86_64_GPU             = ""linux""
    AL2_ARM_64                 = ""linux""
    BOTTLEROCKET_ARM_64        = ""bottlerocket""
    BOTTLEROCKET_x86_64        = ""bottlerocket""
    BOTTLEROCKET_ARM_64_NVIDIA = ""bottlerocket""
    BOTTLEROCKET_x86_64_NVIDIA = ""bottlerocket""
    WINDOWS_CORE_2019_x86_64   = ""windows""
    WINDOWS_FULL_2019_x86_64   = ""windows""
    WINDOWS_CORE_2022_x86_64   = ""windows""
    WINDOWS_FULL_2022_x86_64   = ""windows""
    AL2023_x86_64_STANDARD     = ""al2023""
    AL2023_ARM_64_STANDARD     = ""al2023""
  }
  # Try to use `ami_type` first, but fall back to current, default behavior
  # TODO - will be removed in v21.0
  user_data_type = try(local.ami_type_to_user_data_type[var.ami_type], var.platform)

  template_path = {
    al2023       = ""${path.module}/../../templates/al2023_user_data.tpl""
    bottlerocket = ""${path.module}/../../templates/bottlerocket_user_data.tpl""
    linux        = ""${path.module}/../../templates/linux_user_data.tpl""
    windows      = ""${path.module}/../../templates/windows_user_data.tpl""
  }

  cluster_service_cidr = try(coalesce(var.cluster_service_ipv4_cidr, var.cluster_service_cidr), """")

  user_data = base64encode(templatefile(
    coalesce(var.user_data_template_path, local.template_path[local.user_data_type]),
    {
      # https://docs.aws.amazon.com/eks/latest/userguide/launch-templates.html#launch-template-custom-ami
      enable_bootstrap_user_data = var.enable_bootstrap_user_data

      # Required to bootstrap node
      cluster_name        = var.cluster_name
      cluster_endpoint    = var.cluster_endpoint
      cluster_auth_base64 = var.cluster_auth_base64

      cluster_service_cidr = local.cluster_service_cidr
      cluster_ip_family    = var.cluster_ip_family
      # Bottlerocket
      cluster_dns_ip = try(cidrhost(local.cluster_service_cidr, 10), """")

      # Optional
      bootstrap_extra_args     = var.bootstrap_extra_args
      pre_bootstrap_user_data  = var.pre_bootstrap_user_data
      post_bootstrap_user_data = var.post_bootstrap_user_data
    }
  ))

  user_data_type_to_rendered = {
    al2023 = {
      user_data = var.create ? try(data.cloudinit_config.al2023_eks_managed_node_group[0].rendered, local.user_data) : """"
    }
    bottlerocket = {
      user_data = var.create && local.user_data_type == ""bottlerocket"" && (var.enable_bootstrap_user_data || var.user_data_template_path != """" || var.bootstrap_extra_args != """") ? local.user_data : """"
    }
    linux = {
      user_data = var.create ? try(data.cloudinit_config.linux_eks_managed_node_group[0].rendered, local.user_data) : """"
    }
    windows = {
      user_data = var.create && local.user_data_type == ""windows"" && (var.enable_bootstrap_user_data || var.user_data_template_path != """" || var.pre_bootstrap_user_data != """") ? local.user_data : """"
    }
  }
}
",locals,35,35.0,74d39187d855932dd976da6180eda42dcfe09873,74d39187d855932dd976da6180eda42dcfe09873,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/74d39187d855932dd976da6180eda42dcfe09873/modules/_user_data/main.tf#L35,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/74d39187d855932dd976da6180eda42dcfe09873/modules/_user_data/main.tf#L35,2024-05-08 08:04:19-04:00,2024-05-08 08:04:19-04:00,1,0,0,1,0,0,0,0,0,0
https://github.com/chanzuckerberg/cztack,101,aws-iam-secrets-reader-policy/main.tf,aws-iam-secrets-reader-policy/main.tf,0,todo,# TODO KMS permissions,# TODO KMS permissions,"data aws_iam_policy_document policy {
  statement {
    actions = [
      ""secretsmanager:GetSecretValue"",
    ]

    resources = var.secrets_arns
  }
}
",data,"data ""aws_iam_policy_document"" ""policy"" {
  statement {
    actions = [
      ""secretsmanager:GetSecretValue"",
    ]

    resources = var.secrets_arns
  }
}
",data,1,1.0,817778260c469703be66e3fb07ed6228f33c1960,9df439500dee7468643ca03a844cf7a5b1e1b313,https://github.com/chanzuckerberg/cztack/blob/817778260c469703be66e3fb07ed6228f33c1960/aws-iam-secrets-reader-policy/main.tf#L1,https://github.com/chanzuckerberg/cztack/blob/9df439500dee7468643ca03a844cf7a5b1e1b313/aws-iam-secrets-reader-policy/main.tf#L1,2020-07-22 10:22:01-07:00,2021-04-13 14:52:04-04:00,2,0,0,1,0,1,0,0,0,0
https://github.com/Azure/az-hop,1,tf/keyvault.tf,tf/keyvault.tf,0,todo,# TODO => Add the option to enable VMs to keep secrets in KV,# TODO => Add the option to enable VMs to keep secrets in KV,"resource ""azurerm_key_vault"" ""deployhpc"" {
  name                        = format(""%s%s"", ""kv"", random_string.random.result)
  location                    = azurerm_resource_group.rg.location
  resource_group_name         = azurerm_resource_group.rg.name
  enabled_for_disk_encryption = true
  tenant_id                   = data.azurerm_client_config.current.tenant_id
  soft_delete_enabled         = true
  soft_delete_retention_days  = 7
  purge_protection_enabled    = false
  # TODO => Add the option to enable VMs to keep secrets in KV
  sku_name = ""standard""

  access_policy {
    tenant_id = data.azurerm_client_config.current.tenant_id
    object_id = data.azurerm_client_config.current.object_id

    # QUESTION => Do we need this ?
    certificate_permissions = [
      ""get"",
      ""managecontacts"", 
    ]

    # QUESTION => Do we need this ?
    key_permissions = [
      ""get"",
    ]

    secret_permissions = [
      ""get"",
      ""set"",
      ""list"",
      ""delete"",
      ""purge"",
      ""recover"",
      ""restore""
    ]

    # QUESTION => Do we need this ?
    storage_permissions = [
      ""get"",
    ]
  }

  network_acls {
    default_action = ""Allow""
    bypass         = ""AzureServices""
  }
}
",resource,"resource ""azurerm_key_vault"" ""azhop"" {
  name                        = local.key_vault_name
  location                    = local.create_rg ? azurerm_resource_group.rg[0].location : data.azurerm_resource_group.rg[0].location
  resource_group_name         = local.create_rg ? azurerm_resource_group.rg[0].name : data.azurerm_resource_group.rg[0].name
  enabled_for_disk_encryption = true
  enabled_for_deployment      = true
  enabled_for_template_deployment = true
  tenant_id                   = local.tenant_id
  # soft delete is enabled by default now (2021-8-25), with 90 days retention
  # soft_delete_enabled         = true
  soft_delete_retention_days  = 7
  purge_protection_enabled    = true
  # TODO => Add the option to enable VMs to keep secrets in KV
  sku_name = ""standard""

  network_acls {
    default_action             = local.locked_down_network ? ""Deny"" : ""Allow""
    bypass                     = ""AzureServices""
    ip_rules                   = local.grant_access_from
    virtual_network_subnet_ids = [local.create_admin_subnet ? azurerm_subnet.admin[0].id : data.azurerm_subnet.admin[0].id]
  }
}
",resource,12,18.0,3401eed51efc83ffe43e672c0b863a41fed82427,c692256260053a8f543cc99411274f57d6d7a4b4,https://github.com/Azure/az-hop/blob/3401eed51efc83ffe43e672c0b863a41fed82427/tf/keyvault.tf#L12,https://github.com/Azure/az-hop/blob/c692256260053a8f543cc99411274f57d6d7a4b4/tf/keyvault.tf#L18,2020-12-04 10:09:00+00:00,2024-03-29 14:45:48+01:00,31,0,0,1,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,339,infra/gcp/terraform/modules/oci-proxy/oci-proxy.tf,infra/gcp/terraform/modules/oci-proxy/main.tf,1,fix,// - We need to be able to deploy registry fixes ASAP,"// NOTE: We deploy from staging because: 
 // - We pin by digest anyhow (so it's comparably secure) 
 // - We need to be able to deploy registry fixes ASAP 
 // - We will eventually auto-deploy staging by overriding the project and digest on the production config to avoid skew 
 // If you're interested in running this image yourself releases are available at registry.k8s.io/infra-tools/archeio","resource ""google_cloud_run_service"" ""oci-proxy"" {
  project  = google_project.project.project_id
  for_each = var.cloud_run_config
  name     = ""${var.project_id}-${each.key}""
  location = each.key

  template {
    metadata {
      annotations = {
        ""autoscaling.knative.dev/maxScale"" = ""10"" // TODO: adjust to control costs
        ""run.googleapis.com/launch-stage""  = ""BETA""
      }
    }
    spec {
      service_account_name = google_service_account.oci-proxy.email
      containers {
        // NOTE: We deploy from staging because:
        // - We pin by digest anyhow (so it's comparably secure)
        // - We need to be able to deploy registry fixes ASAP
        // - We will eventually auto-deploy staging by overriding the project and digest on the production config to avoid skew
        // If you're interested in running this image yourself releases are available at registry.k8s.io/infra-tools/archeio
        image = ""gcr.io/k8s-staging-infra-tools/archeio@${var.digest}""
        args  = [""-v=${var.verbosity}""]

        dynamic ""env"" {
          for_each = each.value.environment_variables
          content {
            name  = env.value[""name""]
            value = env.value[""value""]
          }
        }

        // ensure this match the value for template.spec.containers.resources.limits
        env {
          name  = ""GOMAXPROCS""
          value = ""1""
        }

        resources {
          limits = {
            ""cpu"" = ""1000m""
          }
        }
      }

      # we can probably hit 1k QPS/core (cloud run's maximum configurable)
      # but we are leaving in a little overhead, if we actually hit 1k qps in
      # a region we can scale to another 1 core instance
      container_concurrency = 800

      // we only serve cheap redirects, 60s is a rather long request
      timeout_seconds = 60
    }
  }

  traffic {
    percent         = 100
    latest_revision = true
  }

  depends_on = [
    google_project_service.project[""run.googleapis.com""]
  ]

  lifecycle {
    ignore_changes = [
      // This gets added by the Cloud Run API post deploy and causes diffs, can be ignored...
      template[0].metadata[0].annotations[""client.knative.dev/sandbox""],
      template[0].metadata[0].annotations[""run.googleapis.com/user-image""],
      template[0].metadata[0].annotations[""run.googleapis.com/client-name""],
      template[0].metadata[0].annotations[""run.googleapis.com/client-version""],
    ]
  }
}
",resource,"resource ""google_cloud_run_service"" ""oci-proxy"" {
  project  = var.project_id
  for_each = local.cloud_run_config
  name     = ""${var.project_id}-${each.key}""
  location = each.key

  template {
    metadata {
      annotations = {
        ""autoscaling.knative.dev/maxScale"" = ""10"" // TODO: adjust to control costs
        ""run.googleapis.com/launch-stage""  = ""BETA""
      }
      labels = {
        ""run.googleapis.com/startupProbeType"" = ""Default""
      }
    }
    spec {
      service_account_name = google_service_account.oci-proxy.email
      containers {
        // NOTE: We deploy from staging because:
        // - We pin by digest anyhow (so it's comparably secure)
        // - We need to be able to deploy registry fixes ASAP
        // - We will eventually auto-deploy staging by overriding the project and digest on the production config to avoid skew
        // If you're interested in running this image yourself releases are available at registry.k8s.io/infra-tools/archeio
        image = ""gcr.io/k8s-staging-infra-tools/archeio@${var.digest}""
        args  = [""-v=${var.verbosity}""]

        dynamic ""env"" {
          for_each = each.value.environment_variables
          content {
            name  = env.value[""name""]
            value = env.value[""value""]
          }
        }

        // ensure this match the value for template.spec.containers.resources.limits
        env {
          name  = ""GOMAXPROCS""
          value = ""1""
        }

        resources {
          limits = {
            ""cpu"" = ""1000m""
            // default, also the minimum permitted for second generation
            // https://cloud.google.com/run/docs/about-execution-environments
            ""memory"" = ""512Mi""
          }
        }

        startup_probe {
          failure_threshold     = 1
          initial_delay_seconds = 0
          period_seconds        = 240
          timeout_seconds       = 240
          tcp_socket {
            port = 8080
          }
        }
      }

      # we can probably hit 1k QPS/core (cloud run's maximum configurable)
      # but we are leaving in a little overhead, if we actually hit 1k qps in
      # a region we can scale to another 1 core instance
      container_concurrency = 800

      // we only serve cheap redirects, 60s is a rather long request
      timeout_seconds = 60
    }
  }

  traffic {
    percent         = 100
    latest_revision = true
  }

  depends_on = [
    google_project_service.project[""run.googleapis.com""]
  ]

  lifecycle {
    ignore_changes = [
      // This gets added by the Cloud Run API post deploy and causes diffs, can be ignored...
      template[0].metadata[0].annotations[""client.knative.dev/sandbox""],
      template[0].metadata[0].annotations[""run.googleapis.com/user-image""],
      template[0].metadata[0].annotations[""run.googleapis.com/client-name""],
      template[0].metadata[0].annotations[""run.googleapis.com/client-version""],
    ]
  }
}
",resource,91,483.0,8d0eae1fb71ae107c9add98ecde6740eecaccdab,b41eb06d1e7666284e2221d8105ac1f37d40f16f,https://github.com/kubernetes/k8s.io/blob/8d0eae1fb71ae107c9add98ecde6740eecaccdab/infra/gcp/terraform/modules/oci-proxy/oci-proxy.tf#L91,https://github.com/kubernetes/k8s.io/blob/b41eb06d1e7666284e2221d8105ac1f37d40f16f/infra/gcp/terraform/modules/oci-proxy/main.tf#L483,2023-04-02 17:09:27-07:00,2024-03-12 19:30:59+01:00,9,0,1,1,0,0,0,1,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1835,modules/vpc-sc/factory.tf,modules/vpc-sc/factory.tf,0,# todo,# TODO: add checks that emulate the variable validations,# TODO: add checks that emulate the variable validations,"locals {
  _data = {
    for k, v in local._data_paths : k => {
      for f in try(fileset(v, ""**/*.yaml""), []) :
      trimsuffix(f, "".yaml"") => yamldecode(file(""${v}/${f}""))
    }
  }
  _data_paths = {
    for k in [""access_levels"", ""egress_policies"", ""ingress_policies""] : k => (
      var.factories_config[k] == null
      ? null
      : pathexpand(var.factories_config[k])
    )
  }
  data = {
    access_levels = {
      for k, v in local._data.access_levels : k => {
        combining_function = try(v.combining_function, null)
        description        = try(v.description, null)
        conditions = [
          for c in try(v.conditions, []) : merge({
            device_policy          = null
            ip_subnetworks         = []
            members                = []
            negate                 = null
            regions                = []
            required_access_levels = []
          }, c)
        ]
      }
    }
    egress_policies = {
      for k, v in local._data.egress_policies : k => {
        from = merge({
          identity_type = null
          identities    = null
        }, try(v.from, {}))
        to = {
          operations = [
            for o in try(v.to.operations, []) : merge({
              method_selectors     = []
              permission_selectors = []
              service_name         = null
            }, o)
          ]
          resources              = try(v.to.resources, [])
          resource_type_external = try(v.to.resource_type_external, false)
        }
      }
    }
    ingress_policies = {
      for k, v in local._data.ingress_policies : k => {
        from = merge({
          access_levels = []
          identity_type = null
          identities    = null
          resources     = []
        }, try(v.from, {}))
        to = {
          operations = [
            for o in try(v.operations, []) : merge({
              method_selectors     = []
              permission_selectors = []
              service_name         = null
            }, o)
          ]
          resources = try(v.to.resources, [])
        }
      }
    }
  }
  # TODO: add checks that emulate the variable validations
}
",locals,"locals {
  _data = {
    for k, v in local._data_paths : k => {
      for f in try(fileset(v, ""**/*.yaml""), []) :
      trimsuffix(f, "".yaml"") => yamldecode(file(""${v}/${f}""))
    }
  }
  _data_paths = {
    for k in [""access_levels"", ""egress_policies"", ""ingress_policies""] : k => (
      var.factories_config[k] == null
      ? null
      : pathexpand(var.factories_config[k])
    )
  }
  data = {
    access_levels = {
      for k, v in local._data.access_levels : k => {
        combining_function = try(v.combining_function, null)
        description        = try(v.description, null)
        conditions = [
          for c in try(v.conditions, []) : merge({
            device_policy          = null
            ip_subnetworks         = []
            members                = []
            negate                 = null
            regions                = []
            required_access_levels = []
          }, c)
        ]
      }
    }
    egress_policies = {
      for k, v in local._data.egress_policies : k => {
        from = merge({
          identity_type = null
          identities    = null
        }, try(v.from, {}))
        to = {
          operations = [
            for o in try(v.to.operations, []) : merge({
              method_selectors     = []
              permission_selectors = []
              service_name         = null
            }, o)
          ]
          resources              = try(v.to.resources, [])
          resource_type_external = try(v.to.resource_type_external, false)
        }
      }
    }
    ingress_policies = {
      for k, v in local._data.ingress_policies : k => {
        from = merge({
          access_levels = []
          identity_type = null
          identities    = null
          resources     = []
        }, try(v.from, {}))
        to = {
          operations = [
            for o in try(v.to.operations, []) : merge({
              method_selectors     = []
              permission_selectors = []
              service_name         = null
            }, o)
          ]
          resources = try(v.to.resources, [])
        }
      }
    }
  }
  # TODO: add checks that emulate the variable validations
}
",locals,88,88.0,91615e014054ada63899da558f3b1c6cac5c8000,27a055a9cbffda216250af736ec2a68241bec12e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/91615e014054ada63899da558f3b1c6cac5c8000/modules/vpc-sc/factory.tf#L88,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/27a055a9cbffda216250af736ec2a68241bec12e/modules/vpc-sc/factory.tf#L88,2024-02-17 08:02:16+01:00,2024-05-01 18:50:30+02:00,2,0,0,1,0,0,0,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,395,azure/network.tf,azure/network.tf,0,todo,// TODO ALL THIS RULES NEED A PROPER NAME. Port number isn't human friendly,"# Load balancing rules for HANA 2.0 
 // TODO ALL THIS RULES NEED A PROPER NAME. Port number isn't human friendly 
 // use services name","resource ""azurerm_lb_rule"" ""lb_30313"" {
  resource_group_name            = azurerm_resource_group.myrg.name
  loadbalancer_id                = azurerm_lb.mylb.id
  name                           = ""hana-lb-30313""
  protocol                       = ""Tcp""
  frontend_ip_configuration_name = ""mylb-frontend""
  frontend_port                  = 30313
  backend_port                   = 30313
  backend_address_pool_id        = azurerm_lb_backend_address_pool.mylb.id
  probe_id                       = azurerm_lb_probe.mylb.id
  idle_timeout_in_minutes        = 30
  enable_floating_ip             = ""true""
}
",resource,"resource ""azurerm_lb_rule"" ""lb_30313"" {
  resource_group_name            = azurerm_resource_group.myrg.name
  loadbalancer_id                = azurerm_lb.mylb.id
  name                           = ""hana-lb-30313""
  protocol                       = ""Tcp""
  frontend_ip_configuration_name = ""mylb-frontend""
  frontend_port                  = 30313
  backend_port                   = 30313
  backend_address_pool_id        = azurerm_lb_backend_address_pool.mylb.id
  probe_id                       = azurerm_lb_probe.mylb.id
  idle_timeout_in_minutes        = 30
  enable_floating_ip             = ""true""
}
",resource,70,,5daf59b3bbcb57130d80f6d844ad35171f7c010a,9adcf152486838f9f5b600ead5e4ef373918a786,https://github.com/SUSE/ha-sap-terraform-deployments/blob/5daf59b3bbcb57130d80f6d844ad35171f7c010a/azure/network.tf#L70,https://github.com/SUSE/ha-sap-terraform-deployments/blob/9adcf152486838f9f5b600ead5e4ef373918a786/azure/network.tf,2019-09-05 00:05:54+02:00,2019-09-05 18:08:13+02:00,2,1,0,1,0,1,1,0,0,0
https://github.com/wireapp/wire-server-deploy,66,terraform/modules/hetzner-kubernetes/machines.variables.tf,terraform/modules/hetzner-kubernetes/machines.variables.tf,0,implement,# FUTUREWORK: replace 'any' by implementing https://www.terraform.io/docs/language/functions/defaults.html,"# FUTUREWORK: replace 'any' by implementing https://www.terraform.io/docs/language/functions/defaults.html 
 #","variable ""machines"" {
  description = ""list of machines""
  # type = list(object({
  #   group_name = string
  #   machine_id = string
  #   machine_type = string
  #   component_classes = list(string)
  #   volume = optional(object({
  #     size = number
  #     format = optional(string)
  #   }))
  # }))
  type = any
  default = []

  validation {
    condition = length(var.machines) > 0
    error_message = ""At least one machine must be defined.""
  }
}
",variable,"variable ""machines"" {
  description = ""list of machines""
  # type = list(object({
  #   group_name = string
  #   machine_id = string
  #   machine_type = string
  #   component_classes = list(string)
  #   volume = optional(object({
  #     size = number
  #     format = optional(string)
  #   }))
  # }))
  type = any
  default = []

  validation {
    condition = length(var.machines) > 0
    error_message = ""At least one machine must be defined.""
  }
}
",variable,14,14.0,3920e9276b86d927a007fd2d763caf4048b13f81,4982bdbd431d0a752e74556ec3d8b87d7257e576,https://github.com/wireapp/wire-server-deploy/blob/3920e9276b86d927a007fd2d763caf4048b13f81/terraform/modules/hetzner-kubernetes/machines.variables.tf#L14,https://github.com/wireapp/wire-server-deploy/blob/4982bdbd431d0a752e74556ec3d8b87d7257e576/terraform/modules/hetzner-kubernetes/machines.variables.tf#L14,2021-02-22 21:10:04+01:00,2023-07-27 17:19:32+02:00,2,0,1,1,0,0,0,0,0,0
https://github.com/uyuni-project/sumaform,1666,modules/proxy_containerized/main.tf,modules/proxy_containerized/main.tf,0,// todo,// TODO: Replace Uyuni images with OpenSUSE Leap Micro 15.5 images,"// TODO: Replace Uyuni images with OpenSUSE Leap Micro 15.5 images 
 // (not yet supported in sumaform)","variable ""images"" {
  default = {
    ""head""           = ""slemicro55o""
    // TODO: Replace Uyuni images with OpenSUSE Leap Micro 15.5 images
    // (not yet supported in sumaform)
    ""uyuni-master""   = ""opensuse155o""
    ""uyuni-released"" = ""opensuse155o""
    ""uyuni-pr""       = ""opensuse155o""
  }
}
",variable,"variable ""images"" {
  default = {
    ""head""           = ""slemicro55o""
    ""uyuni-master""   = ""leapmicro55o""
    ""uyuni-released"" = ""leapmicro55o""
    ""uyuni-pr""       = ""leapmicro55o""
  }
}
",variable,4,,2601871d9b76286f60fbfd5ddae395a64d944c0f,31f3f833cfe72cfc8efc420eb4b9507f00e16d4b,https://github.com/uyuni-project/sumaform/blob/2601871d9b76286f60fbfd5ddae395a64d944c0f/modules/proxy_containerized/main.tf#L4,https://github.com/uyuni-project/sumaform/blob/31f3f833cfe72cfc8efc420eb4b9507f00e16d4b/modules/proxy_containerized/main.tf,2024-02-01 15:23:28+01:00,2024-04-10 13:00:59+02:00,6,1,0,1,0,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,35,variables.tf,variables.tf,0,implemented,# placeholder variable for debugging scripts. To be implemented in future,# placeholder variable for debugging scripts. To be implemented in future,"variable ""debug_mode"" {
  default     = false
  description = ""Whether to turn on debug mode.""
  type        = bool
}
",variable,,,791,0.0,2b2991c788c70e2267603919f84a6d916b66baf9,6c867cd8e9cbf559742f56658989bcded0d1fd89,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/2b2991c788c70e2267603919f84a6d916b66baf9/variables.tf#L791,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/variables.tf#L0,2021-10-26 10:35:29+11:00,2023-10-25 16:40:02+11:00,45,2,0,1,0,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,34,modules/network/nsgs.tf,modules/network/nsgs.tf,0,# todo,"# TODO: condition for end-to-end SSL/SSL termination
",# TODO: condition for end-to-end SSL/SSL termination,"resource ""oci_core_network_security_group_security_rule"" ""int_lb_egress"" {
  network_security_group_id = oci_core_network_security_group.int_lb[0].id
  description               = local.int_lb_egress[count.index].description
  destination               = local.int_lb_egress[count.index].destination
  destination_type          = local.int_lb_egress[count.index].destination_type
  direction                 = ""EGRESS""
  protocol                  = local.int_lb_egress[count.index].protocol

  stateless = false
  # TODO: condition for end-to-end SSL/SSL termination
  dynamic ""tcp_options"" {
    for_each = local.int_lb_egress[count.index].protocol == local.tcp_protocol && local.int_lb_egress[count.index].port != -1 ? [1] : []
    content {
      destination_port_range {
        min = length(regexall(""-"", local.int_lb_egress[count.index].port)) > 0 ? tonumber(element(split(""-"", local.int_lb_egress[count.index].port), 0)) : local.int_lb_egress[count.index].port
        max = length(regexall(""-"", local.int_lb_egress[count.index].port)) > 0 ? tonumber(element(split(""-"", local.int_lb_egress[count.index].port), 1)) : local.int_lb_egress[count.index].port
      }
    }
  }

  dynamic ""icmp_options"" {
    for_each = local.int_lb_egress[count.index].protocol == local.icmp_protocol ? [1] : []
    content {
      type = 3
      code = 4
    }
  }

  lifecycle {
    ignore_changes = [destination, destination_type, direction, protocol, tcp_options]
  }

  count = var.load_balancers == ""internal"" || var.load_balancers == ""both"" ? length(local.int_lb_egress) : 0
}
",resource,,,291,0.0,2b2991c788c70e2267603919f84a6d916b66baf9,6c867cd8e9cbf559742f56658989bcded0d1fd89,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/2b2991c788c70e2267603919f84a6d916b66baf9/modules/network/nsgs.tf#L291,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/modules/network/nsgs.tf#L0,2021-10-26 10:35:29+11:00,2023-10-25 16:40:02+11:00,16,2,0,1,0,1,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,853,fast/stages/03-gke-multitenant/prod/variables.tf,fast/stages/03-gke-multitenant/dev/variables.tf,1,# todo,# TODO: review defaults,# TODO: review defaults,"variable ""cluster_defaults"" {
  description = ""Default values for optional cluster configurations.""
  type = object({
    cloudrun_config             = bool
    database_encryption_key     = string
    enable_binary_authorization = bool
    master_authorized_ranges    = map(string)
    max_pods_per_node           = number
    pod_security_policy         = bool
    release_channel             = string
    vertical_pod_autoscaling    = bool
  })
  default = {
    # TODO: review defaults
    cloudrun_config             = false
    database_encryption_key     = null
    enable_binary_authorization = false
    master_authorized_ranges = {
      rfc1918_1 = ""10.0.0.0/8""
      rfc1918_2 = ""172.16.0.0/12""
      rfc1918_3 = ""192.168.0.0/16""
    }
    max_pods_per_node        = 110
    pod_security_policy      = false
    release_channel          = ""STABLE""
    vertical_pod_autoscaling = false
  }
}
",variable,"variable ""cluster_defaults"" {
  description = ""Default values for optional cluster configurations.""
  type = object({
    cloudrun_config                 = bool
    database_encryption_key         = string
    master_authorized_ranges        = map(string)
    max_pods_per_node               = number
    pod_security_policy             = bool
    release_channel                 = string
    vertical_pod_autoscaling        = bool
    gcp_filestore_csi_driver_config = bool
  })
  default = {
    cloudrun_config         = false
    database_encryption_key = null
    # binary_authorization    = false
    master_authorized_ranges = {
      rfc1918_1 = ""10.0.0.0/8""
      rfc1918_2 = ""172.16.0.0/12""
      rfc1918_3 = ""192.168.0.0/16""
    }
    max_pods_per_node               = 110
    pod_security_policy             = false
    release_channel                 = ""STABLE""
    vertical_pod_autoscaling        = false
    gcp_filestore_csi_driver_config = false
  }
}
",variable,46,,f3f9a4a88cedd64f6fc91b64666046aa6726a2f3,a16cf9e2a8ec54f2ca83184acd851cfd60eb81ad,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/f3f9a4a88cedd64f6fc91b64666046aa6726a2f3/fast/stages/03-gke-multitenant/prod/variables.tf#L46,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/a16cf9e2a8ec54f2ca83184acd851cfd60eb81ad/fast/stages/03-gke-multitenant/dev/variables.tf,2022-06-08 11:41:50+02:00,2022-08-08 13:54:06+02:00,15,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,22,modules/gke-cluster/main.tf,modules/gke-cluster/main.tf,0,# todo,"# TODO(ludomagno): make optional, and support beta feature","# TODO(ludomagno): make optional, and support beta feature 
 # https://www.terraform.io/docs/providers/google/r/container_cluster.html#daily_maintenance_window","resource ""google_container_cluster"" ""cluster"" {
  provider                    = google-beta
  project                     = var.project_id
  name                        = var.name
  description                 = var.description
  location                    = var.location
  node_locations              = length(var.node_locations) == 0 ? null : var.node_locations
  min_master_version          = var.min_master_version
  network                     = var.network
  subnetwork                  = var.subnetwork
  logging_service             = var.logging_service
  monitoring_service          = var.monitoring_service
  resource_labels             = var.labels
  default_max_pods_per_node   = var.default_max_pods_per_node
  enable_binary_authorization = var.enable_binary_authorization
  enable_intranode_visibility = var.enable_intranode_visibility
  enable_shielded_nodes       = var.enable_shielded_nodes
  enable_tpu                  = var.enable_tpu
  initial_node_count          = 1
  remove_default_node_pool    = true

  # node_config

  addons_config {
    http_load_balancing {
      disabled = ! var.addons.http_load_balancing
    }
    horizontal_pod_autoscaling {
      disabled = ! var.addons.horizontal_pod_autoscaling
    }
    network_policy_config {
      disabled = ! var.addons.network_policy_config
    }
    # beta addons
    # cloudrun is dynamic as it tends to trigger cluster recreation on change
    dynamic cloudrun_config {
      for_each = var.addons.istio_config.enabled && var.addons.cloudrun_config ? [""""] : []
      content {
        disabled = false
      }
    }
    istio_config {
      disabled = ! var.addons.istio_config.enabled
      auth     = var.addons.istio_config.tls ? ""AUTH_MUTUAL_TLS"" : ""AUTH_NONE""
    }
  }

  # TODO(ludomagno): support setting address ranges instead of range names
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#cluster_ipv4_cidr_block
  ip_allocation_policy {
    cluster_secondary_range_name  = var.secondary_range_pods
    services_secondary_range_name = var.secondary_range_services
  }

  # TODO(ludomagno): make optional, and support beta feature
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#daily_maintenance_window
  maintenance_policy {
    daily_maintenance_window {
      start_time = var.maintenance_start_time
    }
  }

  master_auth {
    client_certificate_config {
      issue_client_certificate = false
    }
  }

  dynamic master_authorized_networks_config {
    for_each = length(var.master_authorized_ranges) == 0 ? [] : list(var.master_authorized_ranges)
    iterator = ranges
    content {
      dynamic cidr_blocks {
        for_each = ranges.value
        iterator = range
        content {
          cidr_block   = range.value
          display_name = range.key
        }
      }
    }
  }

  dynamic network_policy {
    for_each = var.addons.network_policy_config ? [""""] : []
    content {
      enabled  = true
      provider = ""CALICO""
    }
  }

  dynamic private_cluster_config {
    for_each = var.private_cluster_config != null ? [var.private_cluster_config] : []
    iterator = config
    content {
      enable_private_nodes    = config.value.enable_private_nodes
      enable_private_endpoint = config.value.enable_private_endpoint
      master_ipv4_cidr_block  = config.value.master_ipv4_cidr_block
    }
  }

  # beta features

  dynamic authenticator_groups_config {
    for_each = var.authenticator_security_group == null ? [] : [""""]
    content {
      security_group = var.authenticator_security_group
    }
  }

  dynamic cluster_autoscaling {
    for_each = var.cluster_autoscaling.enabled ? [var.cluster_autoscaling] : []
    iterator = config
    content {
      enabled = true
      resource_limits {
        resource_type = ""cpu""
        minimum       = config.cpu_min
        maximum       = config.cpu_max
      }
      resource_limits {
        resource_type = ""memory""
        minimum       = config.memory_min
        maximum       = config.memory_max
      }
    }
  }

  dynamic database_encryption {
    for_each = var.database_encryption.enabled ? [var.database_encryption] : []
    iterator = config
    content {
      state    = config.value.state
      key_name = config.value.key_name
    }
  }

  dynamic pod_security_policy_config {
    for_each = var.pod_security_policy != null ? [""""] : []
    content {
      enabled = var.pod_security_policy
    }
  }

  dynamic release_channel {
    for_each = var.release_channel != null ? [""""] : []
    content {
      channel = var.release_channel
    }
  }

  dynamic resource_usage_export_config {
    for_each = (
      var.resource_usage_export_config.enabled != null
      &&
      var.resource_usage_export_config.dataset != null
      ? [""""] : []
    )
    content {
      enable_network_egress_metering = var.resource_usage_export_config.enabled
      bigquery_destination {
        dataset_id = var.resource_usage_export_config.dataset
      }
    }
  }

  dynamic vertical_pod_autoscaling {
    for_each = var.vertical_pod_autoscaling == null ? [] : [""""]
    content {
      enabled = var.vertical_pod_autoscaling
    }
  }

  dynamic workload_identity_config {
    for_each = var.workload_identity ? [""""] : []
    content {
      identity_namespace = ""${var.project_id}.svc.id.goog""
    }
  }

}
",resource,"resource ""google_container_cluster"" ""cluster"" {
  provider                    = google-beta
  project                     = var.project_id
  name                        = var.name
  description                 = var.description
  location                    = var.location
  node_locations              = length(var.node_locations) == 0 ? null : var.node_locations
  min_master_version          = var.min_master_version
  network                     = var.network
  subnetwork                  = var.subnetwork
  logging_service             = var.logging_config == null ? var.logging_service : null
  monitoring_service          = var.monitoring_config == null ? var.monitoring_service : null
  resource_labels             = var.labels
  default_max_pods_per_node   = var.enable_autopilot ? null : var.default_max_pods_per_node
  enable_binary_authorization = var.enable_binary_authorization
  enable_intranode_visibility = var.enable_intranode_visibility
  enable_l4_ilb_subsetting    = var.enable_l4_ilb_subsetting
  enable_shielded_nodes       = var.enable_shielded_nodes
  enable_tpu                  = var.enable_tpu
  initial_node_count          = 1
  remove_default_node_pool    = var.enable_autopilot ? null : true
  datapath_provider           = var.enable_dataplane_v2 ? ""ADVANCED_DATAPATH"" : ""DATAPATH_PROVIDER_UNSPECIFIED""
  enable_autopilot            = var.enable_autopilot == true ? true : null

  # node_config {}
  # NOTE: Default node_pool is deleted, so node_config (here) is extranneous.
  # Specify that node_config as an parameter to gke-nodepool module instead.

  # TODO(ludomagno): compute addons map in locals and use a single dynamic block
  addons_config {
    dns_cache_config {
      enabled = var.addons.dns_cache_config
    }
    http_load_balancing {
      disabled = !var.addons.http_load_balancing
    }
    horizontal_pod_autoscaling {
      disabled = !var.addons.horizontal_pod_autoscaling
    }
    dynamic ""network_policy_config"" {
      for_each = !var.enable_autopilot ? [""""] : []
      content {
        disabled = !var.addons.network_policy_config
      }
    }
    cloudrun_config {
      disabled = !var.addons.cloudrun_config
    }
    istio_config {
      disabled = !var.addons.istio_config.enabled
      auth     = var.addons.istio_config.tls ? ""AUTH_MUTUAL_TLS"" : ""AUTH_NONE""
    }
    gce_persistent_disk_csi_driver_config {
      enabled = var.addons.gce_persistent_disk_csi_driver_config
    }
  }

  # TODO(ludomagno): support setting address ranges instead of range names
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#cluster_ipv4_cidr_block
  ip_allocation_policy {
    cluster_secondary_range_name  = var.secondary_range_pods
    services_secondary_range_name = var.secondary_range_services
  }

  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#daily_maintenance_window
  maintenance_policy {
    dynamic ""daily_maintenance_window"" {
      for_each = var.maintenance_config != null && lookup(var.maintenance_config, ""daily_maintenance_window"", null) != null ? [var.maintenance_config.daily_maintenance_window] : []
      iterator = config
      content {
        start_time = config.value.start_time
      }
    }

    dynamic ""recurring_window"" {
      for_each = var.maintenance_config != null && lookup(var.maintenance_config, ""recurring_window"", null) != null ? [var.maintenance_config.recurring_window] : []
      iterator = config
      content {
        start_time = config.value.start_time
        end_time   = config.value.end_time
        recurrence = config.value.recurrence
      }
    }

    dynamic ""maintenance_exclusion"" {
      for_each = var.maintenance_config != null && lookup(var.maintenance_config, ""maintenance_exclusion"", null) != null ? var.maintenance_config.maintenance_exclusion : []
      iterator = config
      content {
        exclusion_name = config.value.exclusion_name
        start_time     = config.value.start_time
        end_time       = config.value.end_time
      }
    }
  }

  master_auth {
    client_certificate_config {
      issue_client_certificate = false
    }
  }

  dynamic ""master_authorized_networks_config"" {
    for_each = (
      length(var.master_authorized_ranges) == 0
      ? []
      : [var.master_authorized_ranges]
    )
    iterator = ranges
    content {
      dynamic ""cidr_blocks"" {
        for_each = ranges.value
        iterator = range
        content {
          cidr_block   = range.value
          display_name = range.key
        }
      }
    }
  }

  #the network_policy block is enabled if network_policy_config and network_dataplane_v2 is set to false. Dataplane V2 has built-in network policies.
  dynamic ""network_policy"" {
    for_each = var.addons.network_policy_config ? [""""] : []
    content {
      enabled  = var.enable_dataplane_v2 ? false : true
      provider = var.enable_dataplane_v2 ? ""PROVIDER_UNSPECIFIED"" : ""CALICO""
    }
  }

  dynamic ""private_cluster_config"" {
    for_each = local.is_private ? [var.private_cluster_config] : []
    iterator = config
    content {
      enable_private_nodes    = config.value.enable_private_nodes
      enable_private_endpoint = config.value.enable_private_endpoint
      master_ipv4_cidr_block  = config.value.master_ipv4_cidr_block
      master_global_access_config {
        enabled = config.value.master_global_access
      }
    }
  }

  # beta features

  dynamic ""authenticator_groups_config"" {
    for_each = var.authenticator_security_group == null ? [] : [""""]
    content {
      security_group = var.authenticator_security_group
    }
  }

  dynamic ""cluster_autoscaling"" {
    for_each = var.cluster_autoscaling.enabled ? [var.cluster_autoscaling] : []
    iterator = config
    content {
      enabled = true
      resource_limits {
        resource_type = ""cpu""
        minimum       = config.value.cpu_min
        maximum       = config.value.cpu_max
      }
      resource_limits {
        resource_type = ""memory""
        minimum       = config.value.memory_min
        maximum       = config.value.memory_max
      }
      // TODO: support GPUs too
    }
  }

  dynamic ""database_encryption"" {
    for_each = var.database_encryption.enabled ? [var.database_encryption] : []
    iterator = config
    content {
      state    = config.value.state
      key_name = config.value.key_name
    }
  }

  dynamic ""pod_security_policy_config"" {
    for_each = var.pod_security_policy != null ? [""""] : []
    content {
      enabled = var.pod_security_policy
    }
  }

  dynamic ""release_channel"" {
    for_each = var.release_channel != null ? [""""] : []
    content {
      channel = var.release_channel
    }
  }

  dynamic ""resource_usage_export_config"" {
    for_each = (
      var.resource_usage_export_config.enabled != null
      &&
      var.resource_usage_export_config.dataset != null
      ? [""""] : []
    )
    content {
      enable_network_egress_metering = var.resource_usage_export_config.enabled
      bigquery_destination {
        dataset_id = var.resource_usage_export_config.dataset
      }
    }
  }

  dynamic ""vertical_pod_autoscaling"" {
    for_each = var.vertical_pod_autoscaling == null ? [] : [""""]
    content {
      enabled = var.vertical_pod_autoscaling
    }
  }

  dynamic ""workload_identity_config"" {
    for_each = var.workload_identity && !var.enable_autopilot ? [""""] : []
    content {
      identity_namespace = ""${var.project_id}.svc.id.goog""
    }
  }

  dynamic ""monitoring_config"" {
    for_each = var.monitoring_config != null ? [""""] : []
    content {
      enable_components = var.monitoring_config
    }
  }

  dynamic ""logging_config"" {
    for_each = var.logging_config != null ? [""""] : []
    content {
      enable_components = var.logging_config
    }
  }

  dynamic ""dns_config"" {
    for_each = var.dns_config != null ? [var.dns_config] : []
    iterator = config
    content {
      cluster_dns        = config.value.cluster_dns
      cluster_dns_scope  = config.value.cluster_dns_scope
      cluster_dns_domain = config.value.cluster_dns_domain
    }
  }
}
",resource,71,,c486bfc66f9814e33b410602cb557a5e4d532912,e1b79bc7f6e2e3361e0ec3ee4256dd44b98b26fe,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c486bfc66f9814e33b410602cb557a5e4d532912/modules/gke-cluster/main.tf#L71,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/e1b79bc7f6e2e3361e0ec3ee4256dd44b98b26fe/modules/gke-cluster/main.tf,2020-04-03 14:06:48+02:00,2021-10-20 18:21:05+02:00,17,1,1,1,0,0,0,0,0,0
https://github.com/jenkins-x/terraform-google-jx,1,main.tf,main.tf,0,// todo,// TODO: remove parent_domain & parent_domain_gcp_project when their deprecations are complete,"// ---------------------------------------------------------------------------- 
 // Setup ExternalDNS 
 // TODO: remove parent_domain & parent_domain_gcp_project when their deprecations are complete 
 // ----------------------------------------------------------------------------","module ""dns"" {
  source = ""./modules/dns""

  gcp_project                     = var.gcp_project
  cluster_name                    = local.cluster_name
  apex_domain                     = var.apex_domain != """" ? var.apex_domain : var.parent_domain
  jenkins_x_namespace             = module.cluster.jenkins_x_namespace
  jx2                             = var.jx2
  subdomain                       = var.subdomain
  apex_domain_gcp_project         = var.apex_domain_gcp_project != """" ? var.apex_domain_gcp_project : (var.parent_domain_gcp_project != """" ? var.parent_domain_gcp_project : var.gcp_project)
  apex_domain_integration_enabled = var.apex_domain_integration_enabled

  depends_on = [
    module.cluster
  ]
}
",module,"module ""dns"" {
  source = ""./modules/dns""

  gcp_project                     = var.gcp_project
  cluster_name                    = local.cluster_name
  apex_domain                     = var.apex_domain != """" ? var.apex_domain : var.parent_domain
  jenkins_x_namespace             = module.cluster.jenkins_x_namespace
  jx2                             = var.jx2
  subdomain                       = var.subdomain
  apex_domain_gcp_project         = var.apex_domain_gcp_project != """" ? var.apex_domain_gcp_project : (var.parent_domain_gcp_project != """" ? var.parent_domain_gcp_project : var.gcp_project)
  apex_domain_integration_enabled = var.apex_domain_integration_enabled

  depends_on = [
    module.cluster
  ]
}
",module,229,237.0,ffc0b68673f1e9341ef89e88f6af157631a9086d,6d61937b9d1a81a879246040a6f6e11ed5626a4e,https://github.com/jenkins-x/terraform-google-jx/blob/ffc0b68673f1e9341ef89e88f6af157631a9086d/main.tf#L229,https://github.com/jenkins-x/terraform-google-jx/blob/6d61937b9d1a81a879246040a6f6e11ed5626a4e/main.tf#L237,2020-12-09 11:35:14+10:00,2024-04-25 13:02:59+02:00,30,0,0,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,100,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf,community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf,0,# todo,# TODO: resolve cyclic dependency and use,"# TODO: resolve cyclic dependency and use 
 # try(module.slurm_controller_instance.cloudsql_secret, null)","module ""slurm_files"" {
  source = ""github.com/SchedMD/slurm-gcp.git//terraform/slurm_cluster/modules/slurm_files?ref=6.2.0""

  project_id         = var.project_id
  slurm_cluster_name = local.slurm_cluster_name
  bucket_dir         = var.bucket_dir
  bucket_name        = local.bucket_name

  slurmdbd_conf_tpl = var.slurmdbd_conf_tpl
  slurm_conf_tpl    = var.slurm_conf_tpl
  cgroup_conf_tpl   = var.cgroup_conf_tpl
  cloud_parameters  = var.cloud_parameters
  # TODO: resolve cyclic dependency and use 
  # try(module.slurm_controller_instance.cloudsql_secret, null)
  cloudsql_secret = null

  controller_startup_scripts         = local.ghpc_startup_script_controller
  controller_startup_scripts_timeout = var.controller_startup_scripts_timeout
  compute_startup_scripts            = local.ghpc_startup_script_compute
  compute_startup_scripts_timeout    = var.compute_startup_scripts_timeout
  login_startup_scripts              = local.ghpc_startup_script_login
  login_startup_scripts_timeout      = var.login_startup_scripts_timeout

  enable_devel         = var.enable_devel
  enable_debug_logging = var.enable_debug_logging
  extra_logging_flags  = var.extra_logging_flags

  enable_bigquery_load = var.enable_bigquery_load
  epilog_scripts       = var.epilog_scripts
  prolog_scripts       = var.prolog_scripts

  disable_default_mounts = var.disable_default_mounts
  network_storage        = var.network_storage
  login_network_storage  = var.network_storage # TODO: reconsider duplication

  partitions  = values(module.slurm_partition)[*]
  nodeset     = values(module.slurm_nodeset)[*]
  nodeset_tpu = values(module.slurm_nodeset_tpu)[*]

  depends_on = [module.bucket]
}
",module,"module ""slurm_files"" {
  source = ""github.com/SchedMD/slurm-gcp.git//terraform/slurm_cluster/modules/slurm_files?ref=6.2.0""

  project_id         = var.project_id
  slurm_cluster_name = local.slurm_cluster_name
  bucket_dir         = var.bucket_dir
  bucket_name        = local.bucket_name

  slurmdbd_conf_tpl = var.slurmdbd_conf_tpl
  slurm_conf_tpl    = var.slurm_conf_tpl
  cgroup_conf_tpl   = var.cgroup_conf_tpl
  cloud_parameters  = var.cloud_parameters
  cloudsql_secret = try(
    one(google_secret_manager_secret_version.cloudsql_version[*].id),
  null)

  controller_startup_scripts         = local.ghpc_startup_script_controller
  controller_startup_scripts_timeout = var.controller_startup_scripts_timeout
  compute_startup_scripts            = local.ghpc_startup_script_compute
  compute_startup_scripts_timeout    = var.compute_startup_scripts_timeout
  login_startup_scripts              = local.ghpc_startup_script_login
  login_startup_scripts_timeout      = var.login_startup_scripts_timeout

  enable_devel         = var.enable_devel
  enable_debug_logging = var.enable_debug_logging
  extra_logging_flags  = var.extra_logging_flags

  enable_bigquery_load = var.enable_bigquery_load
  epilog_scripts       = var.epilog_scripts
  prolog_scripts       = var.prolog_scripts

  disable_default_mounts = var.disable_default_mounts
  network_storage        = var.network_storage
  login_network_storage  = var.login_network_storage

  partitions  = values(module.slurm_partition)[*]
  nodeset     = values(module.slurm_nodeset)[*]
  nodeset_tpu = values(module.slurm_nodeset_tpu)[*]

  depends_on = [module.bucket]
}
",module,101,,82199854e24ac10e7293605c5c4eae45748e8c99,c47d346676eb04eb46385f018a745fb865c081a0,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/82199854e24ac10e7293605c5c4eae45748e8c99/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf#L101,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/c47d346676eb04eb46385f018a745fb865c081a0/community/modules/scheduler/schedmd-slurm-gcp-v6-controller/slurm_files.tf,2023-11-09 19:50:03+00:00,2023-11-15 01:15:19+00:00,3,1,1,1,0,0,0,0,0,0
https://github.com/nasa/cumulus,5,packages/s3-credentials-endpoint/main.tf,packages/s3-credentials-endpoint/main.tf,0,todo,# TODO Remove this stub value,"DISTRIBUTION_REDIRECT_ENDPOINT = ""https://${var.rest_api.id}.execute-api.${var.region}.amazonaws.com/${var.stage_name}/${var.redirect_path}"" 
 # TODO Remove this stub value 
 # public_buckets            = """"","resource ""aws_lambda_function"" ""s3_credentials"" {
  function_name = ""${var.prefix}-s3-credentials-endpoint""
  # TODO Fetch this from ... somewhere. Or package it with the module zip file? Probably the better option
  filename         = ""${path.module}/dist/src.zip""
  source_code_hash = filebase64sha256(""${path.module}/dist/src.zip"")
  handler          = ""index.handler""
  role             = aws_iam_role.s3_credentials_lambda.arn
  runtime          = ""nodejs8.10""
  timeout          = 10
  memory_size      = 320
  environment {
    variables = {
      DISTRIBUTION_REDIRECT_ENDPOINT = ""https://${var.rest_api.id}.execute-api.${var.region}.amazonaws.com/${var.stage_name}/${var.redirect_path}""
      # TODO Remove this stub value
      # public_buckets            = """"
      EARTHDATA_BASE_URL        = var.urs_url
      EARTHDATA_CLIENT_ID       = var.urs_client_id
      EARTHDATA_CLIENT_PASSWORD = var.urs_client_password
      AccessTokensTable         = aws_dynamodb_table.access_tokens.id
      STSCredentialsLambda      = var.sts_credentials_lambda_arn
    }
  }
}
",resource,"resource ""aws_lambda_function"" ""s3_credentials"" {
  function_name    = ""${var.prefix}-s3-credentials-endpoint""
  filename         = ""${local.dist_dir}/index.js""
  source_code_hash = data.archive_file.s3_credentials_endpoint_package.output_base64sha256
  handler          = ""index.handler""
  role             = aws_iam_role.s3_credentials_lambda.arn
  runtime          = ""nodejs8.10""
  timeout          = 10
  memory_size      = 320
  vpc_config {
    subnet_ids = var.subnet_ids
    security_group_ids = var.ngap_sgs
  }
  environment {
    variables = {
      DISTRIBUTION_REDIRECT_ENDPOINT = ""https://${var.rest_api.id}.execute-api.${var.region}.amazonaws.com/${var.stage_name}/${var.redirect_path}""
      public_buckets            = var.public_buckets
      EARTHDATA_BASE_URL        = var.urs_url
      EARTHDATA_CLIENT_ID       = var.urs_client_id
      EARTHDATA_CLIENT_PASSWORD = var.urs_client_password
      AccessTokensTable         = aws_dynamodb_table.access_tokens.id
      STSCredentialsLambda      = var.sts_credentials_lambda_arn
    }
  }
}
",resource,64,,b37630397418a4cb61e428974577b0e21a636fae,6af6a727f927229cc2c252726bf45b1ab093c4be,https://github.com/nasa/cumulus/blob/b37630397418a4cb61e428974577b0e21a636fae/packages/s3-credentials-endpoint/main.tf#L64,https://github.com/nasa/cumulus/blob/6af6a727f927229cc2c252726bf45b1ab093c4be/packages/s3-credentials-endpoint/main.tf,2019-07-02 10:24:14-04:00,2019-07-17 18:01:16-05:00,3,1,1,1,0,0,0,0,0,0
https://github.com/ministryofjustice/cloud-platform-infrastructure,19,terraform/global-resources/elasticsearch.tf,terraform/global-resources/elasticsearch.tf,0,todo,# returns a single list item then leave it as-is and remove this TODO comment.,"      # TF-UPGRADE-TODO: In Terraform v0.10 and earlier, it was sometimes necessary to
      # force an interpolation expression to be interpreted as a list by wrapping it
      # in an extra set of list brackets. That form was supported for compatibility in
      # v0.11, but is no longer supported in Terraform v0.12.
      #
      # If the expression in the following list itself returns a list, remove the
      # brackets to avoid interpretation as a list of lists. If the expression
      # returns a single list item then leave it as-is and remove this TODO comment.","data ""aws_iam_policy_document"" ""live"" {
  statement {
    actions = [
      ""es:*"",
    ]

    resources = [
      ""arn:aws:es:${data.aws_region.moj-dsd.name}:${data.aws_caller_identity.moj-dsd.account_id}:domain/${local.live_domain}/*"",
    ]

    principals {
      type        = ""AWS""
      identifiers = [""*""]
    }

    condition {
      test     = ""IpAddress""
      variable = ""aws:SourceIp""

      # TF-UPGRADE-TODO: In Terraform v0.10 and earlier, it was sometimes necessary to
      # force an interpolation expression to be interpreted as a list by wrapping it
      # in an extra set of list brackets. That form was supported for compatibility in
      # v0.11, but is no longer supported in Terraform v0.12.
      #
      # If the expression in the following list itself returns a list, remove the
      # brackets to avoid interpretation as a list of lists. If the expression
      # returns a single list item then leave it as-is and remove this TODO comment.
      values = [
        keys(local.allowed_live_ips),
      ]
    }
  }
}
",data,"data ""aws_iam_policy_document"" ""live_1"" {
  statement {
    actions = [
      ""es:*"",
    ]

    resources = [
      ""arn:aws:es:${data.aws_region.moj-cp.name}:${data.aws_caller_identity.moj-cp.account_id}:domain/${local.live_domain}/*"",
    ]

    principals {
      type        = ""AWS""
      identifiers = [""*""]
    }

    condition {
      test     = ""IpAddress""
      variable = ""aws:SourceIp""

      # TF-UPGRADE-TODO: In Terraform v0.10 and earlier, it was sometimes necessary to
      # force an interpolation expression to be interpreted as a list by wrapping it
      # in an extra set of list brackets. That form was supported for compatibility in
      # v0.11, but is no longer supported in Terraform v0.12.
      #
      # If the expression in the following list itself returns a list, remove the
      # brackets to avoid interpretation as a list of lists. If the expression
      # returns a single list item then leave it as-is and remove this TODO comment.
      values = keys(local.allowed_live_1_ips)
    }
  }
}
",data,81,73.0,38754ae14bf7b30f08dd5fc4dfd1424c49b6ead4,c60111476d269d642447f8389a62abd932702104,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/38754ae14bf7b30f08dd5fc4dfd1424c49b6ead4/terraform/global-resources/elasticsearch.tf#L81,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/c60111476d269d642447f8389a62abd932702104/terraform/global-resources/elasticsearch.tf#L73,2019-11-21 10:14:42+00:00,2020-05-28 16:02:51+01:00,4,0,0,1,1,1,0,0,0,0
https://github.com/cattle-ops/terraform-aws-gitlab-runner,1,main.tf,main.tf,0,workaround,"# workaround for ""conditional operator cannot be used with list values""","# workaround for ""conditional operator cannot be used with list values""","locals {
  # workaround for ""conditional operator cannot be used with list values""
  runner_ssh_config = {
    enabled  = ""${var.gitlab_runner_ssh_cidr_blocks}""
    disabled = ""${list()}""
  }
}
",locals,"locals {
  // Convert list to a string separated and prepend by a comma
  docker_machine_options_string           = ""${format("",%s"", join("","", formatlist(""%q"", var.docker_machine_options)))}""
  runners_off_peak_periods_string         = ""${var.runners_off_peak_periods == """" ? """" : format(""OffPeakPeriods = %s"", var.runners_off_peak_periods)}""
  secure_parameter_store_runner_token_key = ""${var.environment}-${var.secure_parameter_store_runner_token_key}""
}
",locals,7,,55e74cc467c2d39b7e458496c24eb5d6ca924609,f42a055f92d05224c26848049123e24b89161c16,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/55e74cc467c2d39b7e458496c24eb5d6ca924609/main.tf#L7,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/f42a055f92d05224c26848049123e24b89161c16/main.tf,2019-04-12 18:06:36-07:00,2019-04-12 22:18:20-07:00,2,1,0,1,0,0,0,0,0,0
https://github.com/uyuni-project/sumaform,1354,modules/pts/main.tf,modules/pts/main.tf,0,fix,// FIXME,// FIXME,"module ""locust"" {
  source               = ""../locust""
  name                 = var.locust_name
  base_configuration   = var.base_configuration
  server_configuration = module.server.configuration
  locust_file          = ""modules/libvirt/pts/locustfile.py""
  slave_quantity       = 5


  memory = 1024
  // FIXME
  mac    = var.locust_mac
}
",module,"module ""locust"" {
  source               = ""../locust""
  name                 = var.locust_name
  base_configuration   = var.base_configuration
  server_configuration = module.server.configuration
  locust_file          = ""${path.module}/locustfile.py""
  slave_quantity       = 5
  provider_settings    = var.locust_provider_settings
}
",module,59,,e929516f1e746ca6b74abf40d71b026a56e09d0c,284b756e4cf20114fb5a8919c36fdb838b2ab269,https://github.com/uyuni-project/sumaform/blob/e929516f1e746ca6b74abf40d71b026a56e09d0c/modules/pts/main.tf#L59,https://github.com/uyuni-project/sumaform/blob/284b756e4cf20114fb5a8919c36fdb838b2ab269/modules/pts/main.tf,2020-01-28 10:22:11+00:00,2020-01-28 10:22:11+00:00,2,1,0,1,0,0,1,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,1,main.tf,main.tf,0,#todo,#TODO: add if condition to validate if neither US or EU are supplied,#TODO: add if condition to validate if neither US or EU are supplied,"resource ""google_bigquery_dataset"" ""default"" {
  dataset_id                  = ""${var.dataset_id}""
  friendly_name               = ""${var.dataset_name}""
  description                 = ""${var.description}""
  #TODO: add if condition to validate if neither US or EU are supplied
  location                    = ""${var.region}""
  #TODO: format this ne excluded by default but can optionally be defined if the user wishes
  default_table_expiration_ms = ""${var.expiration}""
  project                     = ""${var.project_id}""

  #TODO: Need to find a way to dynamically assign a dict object(s)
  labels {
    env = ""default""
    foo = ""bar""
    tonyd = ""tonyd""
  }

  //TODO: array of users or groups needs to be added to have access. Need to figure out the best method of customers to allocate users or groups.
  # access {
  #   role   = ""READER""
  #   domain = ""adigangi.com""
  # }
  #
  # access {
  #   role           = ""WRITER""
  #   user_by_email = ""adigangi@adigangi.com""
  # }
  #
  # access {
  #   role           = ""OWNER""
  #   special_group  = ""projectOwners""
  # }
}
",resource,the block associated got renamed or deleted,,26,,d56aa2c9a80343d60eed3e1a7d24962be31ee0b6,7f922f7e9df197df38c9b09dfa6e3614d71f19f5,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/d56aa2c9a80343d60eed3e1a7d24962be31ee0b6/main.tf#L26,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/7f922f7e9df197df38c9b09dfa6e3614d71f19f5/main.tf,2018-11-20 10:30:15-05:00,2019-01-16 18:10:54-05:00,3,1,0,1,0,0,0,0,0,0
https://github.com/nasa/cumulus,265,tf-modules/ingest/sqs-message-remover.tf,tf-modules/ingest/sqs-message-remover.tf,0,# todo,# TODO: Create a local variable for security groups to use,"# TODO: Create a local variable for security groups to use 
 # throughout the ingest modiule","module ""sqs_message_remover_lambda"" {
  source = ""../../lambdas/sqs-message-remover""

  prefix = var.prefix

  cmr_environment = var.cmr_environment

  system_bucket = var.system_bucket

  lambda_subnet_ids = var.lambda_subnet_ids
  # TODO: Create a local variable for security groups to use
  # throughout the ingest modiule
  security_group_ids = [
    aws_security_group.no_ingress_all_egress[0].id
  ]

  tags = var.tags

  # is this necessary or should we move towards least privileges
  # for the lambda?
  lambda_processing_role_arn = var.lambda_processing_role_arn
}
",module,"module ""sqs_message_remover_lambda"" {
  source = ""../../lambdas/sqs-message-remover""

  prefix = var.prefix

  system_bucket = var.system_bucket

  lambda_subnet_ids = var.lambda_subnet_ids
  # TODO: Create a local variable for security groups to use
  # throughout the ingest modiule
  security_group_ids = [
    aws_security_group.no_ingress_all_egress[0].id
  ]

  tags = var.tags

  # is this necessary or should we move towards least privileges
  # for the lambda?
  lambda_processing_role_arn = var.lambda_processing_role_arn
  lambda_timeouts       = var.lambda_timeouts
  lambda_memory_sizes   = var.lambda_memory_sizes
}
",module,11,9.0,4ddf0e2f969ae00b52c0cf63fbaf74be80401915,b3166bf3a9969a4cbff452ab7f6568874f94d06e,https://github.com/nasa/cumulus/blob/4ddf0e2f969ae00b52c0cf63fbaf74be80401915/tf-modules/ingest/sqs-message-remover.tf#L11,https://github.com/nasa/cumulus/blob/b3166bf3a9969a4cbff452ab7f6568874f94d06e/tf-modules/ingest/sqs-message-remover.tf#L9,2020-08-04 11:28:42-04:00,2023-10-13 14:43:14-04:00,4,0,1,1,0,1,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,1,worker_nodes.tf,worker_nodes.tf,0,# todo,"# TODO: create this policy as a data source
","# EKS Worker Nodes Resources 
 #  * IAM role allowing Kubernetes actions to access other AWS services 
 #  * EC2 Security Group to allow networking traffic 
 #  * Data source to fetch latest EKS worker AMI 
 #  * AutoScaling Launch Configuration to configure worker instances 
 #  * AutoScaling Group to launch worker instances 
 #  
 # TODO: create this policy as a data source","resource ""aws_iam_role"" ""demo-node"" {
  name = ""terraform-eks-demo-node""

  assume_role_policy = <<POLICY
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""Service"": ""ec2.amazonaws.com""
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}
POLICY
}
",resource,,,9,0.0,07aba1b7667da1c6df0540564d3bcd011764d4aa,309e7f70832270f7a40bcaf751380242515c2989,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/07aba1b7667da1c6df0540564d3bcd011764d4aa/worker_nodes.tf#L9,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/309e7f70832270f7a40bcaf751380242515c2989/worker_nodes.tf#L0,2018-06-06 20:55:23-07:00,2018-06-06 20:55:44-07:00,2,2,0,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,698,infra/modules/aws-cognito-identity-cli/variables.tf,infra/modules/aws-cognito-identity-cli/variables.tf,0,# todo,"# TODO: expect value format ""${module.cognito-identity-pool.developer_provider_name}=${module.msft-connection[k].connector.application_id}""","# TODO: expect value format ""${module.cognito-identity-pool.developer_provider_name}=${module.msft-connection[k].connector.application_id}"" 
 # --> add validation of that OR split those components in properties of an object??","variable ""login_ids"" {
  type = map(string)
  description = ""Map of connector id => login id for which to create identities in pool""
}
",variable,"variable ""login_ids"" {
  type        = map(string)
  description = ""Map of connector id => login id for which to create identities in pool""
}",variable,16,17.0,03edc740757710dc38e422206367817e234b7a8e,ea4455ba9b9a4fd4b9009750d297f8fc83482ad0,https://github.com/Worklytics/psoxy/blob/03edc740757710dc38e422206367817e234b7a8e/infra/modules/aws-cognito-identity-cli/variables.tf#L16,https://github.com/Worklytics/psoxy/blob/ea4455ba9b9a4fd4b9009750d297f8fc83482ad0/infra/modules/aws-cognito-identity-cli/variables.tf#L17,2023-02-15 12:34:03-08:00,2023-02-16 07:54:57-08:00,3,0,0,1,0,1,0,0,0,0
https://github.com/apache/beam,4,playground/terraform/infrastructure/provider.tf,playground/terraform/infrastructure/provider.tf,0,todo,// TODO may need to run module.setup first independent of this solution and add the terraform service account as a variable,"// TODO may need to run module.setup first independent of this solution and add the terraform service account as a variable 
 // This allows us to use a service account to provision resources without downloading or storing service account keys 
 #  impersonate_service_account = module.setup.terraform_service_account_email","provider ""google-beta"" {
  region = var.region
  // TODO may need to run module.setup first independent of this solution and add the terraform service account as a variable
  // This allows us to use a service account to provision resources without downloading or storing service account keys
  #  impersonate_service_account = module.setup.terraform_service_account_email
}
",provider,,,30,0.0,675c0bc10f813ea593702f5e6a0fd2ce38caf720,ad21d8353c856152346408f1d5029c9af05957c8,https://github.com/apache/beam/blob/675c0bc10f813ea593702f5e6a0fd2ce38caf720/playground/terraform/infrastructure/provider.tf#L30,https://github.com/apache/beam/blob/ad21d8353c856152346408f1d5029c9af05957c8/playground/terraform/infrastructure/provider.tf#L0,2022-02-22 10:04:20-08:00,2022-03-16 14:13:22-07:00,2,2,0,1,0,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,490,examples/istio-mc/locals.tf,examples/istio-mc/locals.tf,0,# todo,# TODO: check when is 15021 required for public,# TODO: check when is 15021 required for public,"locals {

  all_ports = -1

  # Protocols
  # See https://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml
  all_protocols = ""all""
  icmp_protocol = 1
  tcp_protocol  = 6
  udp_protocol  = 17

  anywhere          = ""0.0.0.0/0""
  rule_type_nsg     = ""NETWORK_SECURITY_GROUP""
  rule_type_cidr    = ""CIDR_BLOCK""
  rule_type_service = ""SERVICE_CIDR_BLOCK""

  bastion_ip = one(element([module.c1[*].bastion_public_ip], 0))

  operator_ip = one(element([module.c1[*].operator_private_ip], 0))
  
  # TODO: check when is 15021 required for public
  public_lb_allowed_ports = [80, 443, 15021]

  # ports required to be opened for inter-cluster communication between for Istio
  service_mesh_ports = [15012, 15017, 15021, 15443]

  regions = {
    # Africa
    johannesburg = ""af-johannesburg-1""

    # Asia
    chuncheon = ""ap-chuncheon-1""
    hyderabad = ""ap-hyderabad-1""
    mumbai    = ""ap-mumbai-1""
    osaka     = ""ap-osaka-1""
    seoul     = ""ap-seoul-1""
    singapore = ""ap-singapore-1""
    tokyo     = ""ap-tokyo-1""

    # Europe
    amsterdam = ""eu-amsterdam-1""
    frankfurt = ""eu-frankfurt-1""
    london    = ""uk-london-1""
    madrid    = ""eu-madrid-1""
    marseille = ""eu-marseille-1""
    milan     = ""eu-milan-1""
    newport   = ""uk-cardiff-1""
    paris     = ""eu-paris-1""
    stockholm = ""eu-stockholm-1""
    zurich    = ""eu-zurich-1""

    # Middle East
    abudhabi  = ""me-abudhabi-1""
    dubai     = ""me-dubai-1""
    jeddah    = ""me-jeddah-1""
    jerusalem = ""il-jerusalem-1""

    # Oceania
    melbourne = ""ap-melbourne-1""
    sydney    = ""ap-sydney-1""


    # South America
    bogota     = ""sa-bogota-1""
    santiago   = ""sa-santiago-1""
    saupaulo   = ""sa-saupaulo-1""
    valparaiso = ""sa-valparaiso-1""
    vinhedo    = ""sa-vinhedo-1""

    # North America
    ashburn   = ""us-ashburn-1""
    chicago   = ""us-chicago-1""
    monterrey = ""mx-monterrey-1""
    montreal  = ""ca-montreal-1""
    phoenix   = ""us-phoenix-1""
    queretaro = ""mx-queretaro-1""
    sanjose   = ""us-sanjose-1""
    toronto   = ""ca-toronto-1""

    # US Gov FedRamp
    us-gov-ashburn = ""us-langley-1""
    us-gov-phoenix = ""us-luke-1""

    # US Gov DISA L5
    us-dod-east  = ""us-gov-ashburn-1""
    us-dod-north = ""us-gov-chicago-1""
    us-dod-west  = ""us-gov-phoenix-1""

    # UK Gov
    uk-gov-south = ""uk-gov-london-1""
    uk-gov-west  = ""uk-gov-cardiff-1""

    # Australia Gov
    au-gov-cbr = ""ap-dcc-canberra-1""

  }

  worker_cloud_init = [
    {
      content      = <<-EOT
    runcmd:
    - 'echo ""Kernel module configuration for Istio and worker node initialization""'
    - 'modprobe br_netfilter'
    - 'modprobe nf_nat'
    - 'modprobe xt_REDIRECT'
    - 'modprobe xt_owner'
    - 'modprobe iptable_nat'
    - 'modprobe iptable_mangle'
    - 'modprobe iptable_filter'
    - '/usr/libexec/oci-growfs -y'
    - 'timedatectl set-timezone Australia/Sydney'
    - 'curl --fail -H ""Authorization: Bearer Oracle"" -L0 http://169.254.169.254/opc/v2/instance/metadata/oke_init_script | base64 --decode >/var/run/oke-init.sh'
    - 'bash -x /var/run/oke-init.sh'
    EOT
      content_type = ""text/cloud-config"",
    }
  ]
}
",locals,"locals {

  all_ports = -1

  # Protocols
  # See https://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml
  all_protocols = ""all""
  icmp_protocol = 1
  tcp_protocol  = 6
  udp_protocol  = 17

  anywhere          = ""0.0.0.0/0""
  rule_type_nsg     = ""NETWORK_SECURITY_GROUP""
  rule_type_cidr    = ""CIDR_BLOCK""
  rule_type_service = ""SERVICE_CIDR_BLOCK""

  bastion_ip = one(element([module.c1[*].bastion_public_ip], 0))

  operator_ip = one(element([module.c1[*].operator_private_ip], 0))
  
  # TODO: check when is 15021 required for public
  public_lb_allowed_ports = [80, 443, 15021]

  # ports required to be opened for inter-cluster communication between for Istio
  service_mesh_ports = [15012, 15017, 15021, 15443]

  regions = {
    # Africa
    johannesburg = ""af-johannesburg-1""

    # Asia
    chuncheon = ""ap-chuncheon-1""
    hyderabad = ""ap-hyderabad-1""
    mumbai    = ""ap-mumbai-1""
    osaka     = ""ap-osaka-1""
    seoul     = ""ap-seoul-1""
    singapore = ""ap-singapore-1""
    tokyo     = ""ap-tokyo-1""

    # Europe
    amsterdam = ""eu-amsterdam-1""
    frankfurt = ""eu-frankfurt-1""
    london    = ""uk-london-1""
    madrid    = ""eu-madrid-1""
    marseille = ""eu-marseille-1""
    milan     = ""eu-milan-1""
    newport   = ""uk-cardiff-1""
    paris     = ""eu-paris-1""
    stockholm = ""eu-stockholm-1""
    zurich    = ""eu-zurich-1""

    # Middle East
    abudhabi  = ""me-abudhabi-1""
    dubai     = ""me-dubai-1""
    jeddah    = ""me-jeddah-1""
    jerusalem = ""il-jerusalem-1""

    # Oceania
    melbourne = ""ap-melbourne-1""
    sydney    = ""ap-sydney-1""


    # South America
    bogota     = ""sa-bogota-1""
    santiago   = ""sa-santiago-1""
    saupaulo   = ""sa-saupaulo-1""
    valparaiso = ""sa-valparaiso-1""
    vinhedo    = ""sa-vinhedo-1""

    # North America
    ashburn   = ""us-ashburn-1""
    chicago   = ""us-chicago-1""
    monterrey = ""mx-monterrey-1""
    montreal  = ""ca-montreal-1""
    phoenix   = ""us-phoenix-1""
    queretaro = ""mx-queretaro-1""
    sanjose   = ""us-sanjose-1""
    toronto   = ""ca-toronto-1""

    # US Gov FedRamp
    us-gov-ashburn = ""us-langley-1""
    us-gov-phoenix = ""us-luke-1""

    # US Gov DISA L5
    us-dod-east  = ""us-gov-ashburn-1""
    us-dod-north = ""us-gov-chicago-1""
    us-dod-west  = ""us-gov-phoenix-1""

    # UK Gov
    uk-gov-south = ""uk-gov-london-1""
    uk-gov-west  = ""uk-gov-cardiff-1""

    # Australia Gov
    au-gov-cbr = ""ap-dcc-canberra-1""

  }

  worker_cloud_init = [
    {
      content      = <<-EOT
    runcmd:
    - 'echo ""Kernel module configuration for Istio and worker node initialization""'
    - 'modprobe br_netfilter'
    - 'modprobe nf_nat'
    - 'modprobe xt_REDIRECT'
    - 'modprobe xt_owner'
    - 'modprobe iptable_nat'
    - 'modprobe iptable_mangle'
    - 'modprobe iptable_filter'
    - '/usr/libexec/oci-growfs -y'
    - 'timedatectl set-timezone Australia/Sydney'
    - 'curl --fail -H ""Authorization: Bearer Oracle"" -L0 http://169.254.169.254/opc/v2/instance/metadata/oke_init_script | base64 --decode >/var/run/oke-init.sh'
    - 'bash -x /var/run/oke-init.sh'
    EOT
      content_type = ""text/cloud-config"",
    }
  ]
}
",locals,24,24.0,f4cdb00433193c43fd52f8e0f3093d294e5389ac,f4cdb00433193c43fd52f8e0f3093d294e5389ac,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/f4cdb00433193c43fd52f8e0f3093d294e5389ac/examples/istio-mc/locals.tf#L24,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/f4cdb00433193c43fd52f8e0f3093d294e5389ac/examples/istio-mc/locals.tf#L24,2024-02-23 10:56:04+11:00,2024-02-23 10:56:04+11:00,1,0,0,1,0,0,1,0,0,0
https://github.com/kubernetes/k8s.io,115,infra/gcp/clusters/modules/gke-project/main.tf,infra/gcp/terraform/modules/gke-project/main.tf,1,// todo,// TODO(spiffxp): explicitly not using a data source for this until,"// TODO(spiffxp): explicitly not using a data source for this until 
 // I have a better sense of whether this requires more permissions 
 // than (are / should be) available for k8s-infra-prow-oncall and 
 // k8s-infra-cluster-admins 
 // data google_billing_account { 
 // billing_account = locals.billing_account 
 // }  
 // Create the project in which we're creating the cluster","resource ""google_project"" ""project"" {
  name            = var.project_name
  project_id      = var.project_name
  org_id          = data.google_organization.org.org_id
  billing_account = local.billing_account
}
",resource,"resource ""google_project"" ""project"" {
  name            = var.project_name
  project_id      = var.project_name
  org_id          = data.google_organization.org.org_id
  billing_account = local.billing_account
}
",resource,28,55.0,d09eae6759b7f46c3c63dd96593d4a5d8b7c10a1,84bb4180c27c027b4391af1b3c3afa427935d1b9,https://github.com/kubernetes/k8s.io/blob/d09eae6759b7f46c3c63dd96593d4a5d8b7c10a1/infra/gcp/clusters/modules/gke-project/main.tf#L28,https://github.com/kubernetes/k8s.io/blob/84bb4180c27c027b4391af1b3c3afa427935d1b9/infra/gcp/terraform/modules/gke-project/main.tf#L55,2021-03-03 16:40:35-05:00,2024-02-08 15:43:16-08:00,9,0,0,1,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,216,infra/gcp/terraform/k8s-infra-prow-build/main.tf,infra/gcp/terraform/k8s-infra-prow-build/main.tf,0,# todo,# TODO(spiffxp): remove this once all jobs/pods have fully migrated over to,"# TODO(spiffxp): remove this once all jobs/pods have fully migrated over to 
 #                pool5","module ""prow_build_nodepool_n1_highmem_8_maxiops"" {
  source        = ""../modules/gke-nodepool""
  project_name  = local.project_id
  cluster_name  = module.prow_build_cluster.cluster.name
  location      = module.prow_build_cluster.cluster.location
  name          = ""pool4""
  initial_count = 1
  min_count     = 1
  max_count     = 80
  # kind-ipv6 jobs need an ipv6 stack; COS doesn't provide one, so we need to
  # use an UBUNTU image instead. Keep parity with the existing google.com
  # k8s-prow-builds/prow cluster by using the CONTAINERD variant
  image_type   = ""UBUNTU_CONTAINERD""
  machine_type = ""n1-highmem-8""
  # Use an ssd volume sized to allow the max IOPS supported by n1 instances w/ 8 vCPU
  disk_size_gb    = 500
  disk_type       = ""pd-ssd""
  service_account = module.prow_build_cluster.cluster_node_sa.email
}
",module,the block associated got renamed or deleted,,137,,f403678c4aa40765125e4738a91fbfd99addd89b,8c45f5a5981f205b35ef6474b8a2270069ef939b,https://github.com/kubernetes/k8s.io/blob/f403678c4aa40765125e4738a91fbfd99addd89b/infra/gcp/terraform/k8s-infra-prow-build/main.tf#L137,https://github.com/kubernetes/k8s.io/blob/8c45f5a5981f205b35ef6474b8a2270069ef939b/infra/gcp/terraform/k8s-infra-prow-build/main.tf,2021-09-28 05:47:06-07:00,2021-09-28 10:19:06-07:00,2,1,1,1,0,0,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,49,kubernetes-addons.tf,kubernetes-addons.tf,0,todo,# TODO Upgrade,# TODO Upgrade,"module ""cert_manager"" {
  count  = var.create_eks && var.cert_manager_enable ? 1 : 0
  source = ""./kubernetes-addons/cert-manager""

  private_container_repo_url      = var.private_container_repo_url
  public_docker_repo              = var.public_docker_repo
  cert_manager_helm_chart_version = var.cert_manager_helm_chart_version
  cert_manager_image_tag          = var.cert_manager_image_tag
  cert_manager_install_crds       = var.cert_manager_install_crds
  cert_manager_helm_chart_name    = var.cert_manager_helm_chart_name
  cert_manager_helm_chart_url     = var.cert_manager_helm_chart_url
  cert_manager_image_repo_name    = var.cert_manager_image_repo_name

  depends_on = [module.aws_eks]
}
",module,"module ""opentelemetry_collector"" {
  count  = var.create_eks && var.opentelemetry_enable ? 1 : 0
  source = ""./kubernetes-addons/opentelemetry-collector""

  private_container_repo_url                            = var.private_container_repo_url
  public_docker_repo                                    = var.public_docker_repo
  opentelemetry_command_name                            = var.opentelemetry_command_name
  opentelemetry_helm_chart                              = var.opentelemetry_helm_chart
  opentelemetry_helm_chart_url                          = var.opentelemetry_helm_chart_url
  opentelemetry_image                                   = var.opentelemetry_image
  opentelemetry_image_tag                               = var.opentelemetry_image_tag
  opentelemetry_helm_chart_version                      = var.opentelemetry_helm_chart_version
  opentelemetry_enable_agent_collector                  = var.opentelemetry_enable_agent_collector
  opentelemetry_enable_standalone_collector             = var.opentelemetry_enable_standalone_collector
  opentelemetry_enable_autoscaling_standalone_collector = var.opentelemetry_enable_autoscaling_standalone_collector
  opentelemetry_enable_container_logs                   = var.opentelemetry_enable_container_logs
  opentelemetry_min_standalone_collectors               = var.opentelemetry_min_standalone_collectors
  opentelemetry_max_standalone_collectors               = var.opentelemetry_max_standalone_collectors

  depends_on = [module.aws_eks]
}
",module,132,144.0,074bc8c69e3bab74a8642dfd8f32d8ca5afa9aa9,d9986b66484e6905c7ace374fae2c6fe45fa70c7,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/074bc8c69e3bab74a8642dfd8f32d8ca5afa9aa9/kubernetes-addons.tf#L132,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/d9986b66484e6905c7ace374fae2c6fe45fa70c7/kubernetes-addons.tf#L144,2021-10-13 14:49:03+01:00,2021-10-18 13:22:40-04:00,6,0,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,760,examples/cloud-operations/network-dashboard/main.tf,examples/cloud-operations/network-dashboard/main.tf,0,# todo,"# TODO: How to secure Cloud Function invokation? Not   member = ""allUsers"" but specific Scheduler service Account?","# TODO: How to secure Cloud Function invokation? Not   member = ""allUsers"" but specific Scheduler service Account? 
 # Maybe ""service-YOUR_PROJECT_NUMBER@gcp-sa-cloudscheduler.iam.gserviceaccount.com""? ","resource ""google_cloudfunctions_function_iam_member"" ""invoker"" {
  project        = module.project-monitoring.project_id
  region         = var.region
  cloud_function = google_cloudfunctions_function.function_quotas.name

  role   = ""roles/cloudfunctions.invoker""
  member = ""allUsers""
}
",resource,the block associated got renamed or deleted,,164,,9f3ee4dc2299a7e9034cf25988d9c4b59c0fd70b,65172031f0f525adfe6d56aa050aab606d6ce8e7,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9f3ee4dc2299a7e9034cf25988d9c4b59c0fd70b/examples/cloud-operations/network-dashboard/main.tf#L164,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/65172031f0f525adfe6d56aa050aab606d6ce8e7/examples/cloud-operations/network-dashboard/main.tf,2022-03-08 18:36:02+01:00,2022-03-17 20:08:58+01:00,6,1,1,1,0,1,0,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,18,locals.tf,locals.tf,0,#todo,#TODO Allow this to be specified in TFVARS file,#TODO Allow this to be specified in TFVARS file,"locals {

  tags                = tomap({ ""created-by"" = var.terraform_version })
  private_subnet_tags = merge(tomap({ ""kubernetes.io/role/internal-elb"" = ""1"" }), tomap({ ""created-by"" = var.terraform_version }))
  public_subnet_tags  = merge(tomap({ ""kubernetes.io/role/elb"" = ""1"" }), tomap({ ""created-by"" = var.terraform_version }))

  service_account_amp_ingest_name = format(""%s-%s-%s-%s"", var.tenant, var.environment, var.zone, ""amp-ingest-account"")
  service_account_amp_query_name  = format(""%s-%s-%s-%s"", var.tenant, var.environment, var.zone, ""amp-query-account"")
  #TODO Allow this to be specified in TFVARS file
  amp_workspace_name = format(""%s-%s-%s-%s"", var.tenant, var.environment, var.zone, ""EKS-Metrics-Workspace"")

  image_repo = ""${data.aws_caller_identity.current.account_id}.dkr.ecr.${data.aws_region.current.id}.amazonaws.com/""

  self_managed_node_platform = var.enable_windows_support ? ""windows"" : ""linux""

  # Managed node IAM Roles for aws-auth
  managed_node_group_aws_auth_config_map = var.enable_managed_nodegroups == true ? [
    for key, node in var.managed_node_groups : {
      rolearn : ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/${module.eks.cluster_id}-${node.node_group_name}""
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes""
      ]
    }
  ] : []

  # Self Managed node IAM Roles for aws-auth
  self_managed_node_group_aws_auth_config_map = var.enable_self_managed_nodegroups == true ? [
    for key, node in var.self_managed_node_groups : {
      rolearn : ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/${module.eks.cluster_id}-${node.node_group_name}""
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes""
      ]
    }
  ] : []

  # Fargate node IAM Roles for aws-auth
  fargate_profiles_aws_auth_config_map = var.enable_fargate == true ? [
    for key, node in var.fargate_profiles : {
      rolearn : ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/${module.eks.cluster_id}-${node.fargate_profile_name}""
      username : ""system:node:{{SessionName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes"",
        ""system:node-proxier""
      ]
    }
  ] : []

}",locals,"locals {

  tags                = tomap({ ""created-by"" = var.terraform_version })
  private_subnet_tags = merge(tomap({ ""kubernetes.io/role/internal-elb"" = ""1"" }), tomap({ ""created-by"" = var.terraform_version }))
  public_subnet_tags  = merge(tomap({ ""kubernetes.io/role/elb"" = ""1"" }), tomap({ ""created-by"" = var.terraform_version }))

  ecr_image_repo_url = ""${data.aws_caller_identity.current.account_id}.dkr.ecr.${data.aws_region.current.id}.amazonaws.com""

  # Managed node IAM Roles for aws-auth
  managed_node_group_aws_auth_config_map = var.enable_managed_nodegroups == true ? [
    for key, node in var.managed_node_groups : {
      rolearn : ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/${module.eks.eks_cluster_id}-${node.node_group_name}""
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes""
      ]
    }
  ] : []

  # Self Managed node IAM Roles for aws-auth
  self_managed_node_group_aws_auth_config_map = var.enable_self_managed_nodegroups ? [
    for key, node in var.self_managed_node_groups : {
      rolearn : ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/${module.eks.eks_cluster_id}-${node.node_group_name}""
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes""
      ]
    } if node.custom_ami_type != ""windows""
  ] : []

  # Self Managed Windows node IAM Roles for aws-auth
  windows_node_group_aws_auth_config_map = var.enable_self_managed_nodegroups && var.enable_windows_support ? [
    for key, node in var.self_managed_node_groups : {
      rolearn : ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/${module.eks.eks_cluster_id}-${node.node_group_name}""
      username : ""system:node:{{EC2PrivateDNSName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes"",
        ""eks:kube-proxy-windows""
      ]
    } if node.custom_ami_type == ""windows""
  ] : []

  # Fargate node IAM Roles for aws-auth
  fargate_profiles_aws_auth_config_map = var.enable_fargate == true ? [
    for key, node in var.fargate_profiles : {
      rolearn : ""arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:role/${module.eks.eks_cluster_id}-${node.fargate_profile_name}""
      username : ""system:node:{{SessionName}}""
      groups : [
        ""system:bootstrappers"",
        ""system:nodes"",
        ""system:node-proxier""
      ]
    }
  ] : []

}",locals,27,,125390ed86df57dc9d8064df92b36e14cc8eb3e2,0f757b1a24ba147292e99c66667737dd57bb661e,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/125390ed86df57dc9d8064df92b36e14cc8eb3e2/locals.tf#L27,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/0f757b1a24ba147292e99c66667737dd57bb661e/locals.tf,2021-09-13 14:12:34+01:00,2021-09-24 23:30:52+01:00,6,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,966,modules/organization-policy/experimental.tf,modules/organization-policy/experimental.tf,0,# todo,# TODO: Remove once Terraform 1.3 is released https://github.com/hashicorp/terraform/releases/tag/v1.3.0-alpha20220622,# TODO: Remove once Terraform 1.3 is released https://github.com/hashicorp/terraform/releases/tag/v1.3.0-alpha20220622,"terraform {
  # TODO: Remove once Terraform 1.3 is released https://github.com/hashicorp/terraform/releases/tag/v1.3.0-alpha20220622
  experiments = [module_variable_optional_attrs]  
}
",terraform,,,17,0.0,9c942a68d6511184dae15cb2c9711de398db3867,ac835b6d50f7889b6f71fcaf0089a7ecceff5154,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9c942a68d6511184dae15cb2c9711de398db3867/modules/organization-policy/experimental.tf#L17,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/ac835b6d50f7889b6f71fcaf0089a7ecceff5154/modules/organization-policy/experimental.tf#L0,2022-07-08 15:19:47+02:00,2022-09-28 11:28:05+02:00,3,2,0,1,1,0,0,0,0,0
https://github.com/wireapp/wire-server-deploy,61,terraform/modules/sft/variables.tf,terraform/modules/sft/variables.tf,0,#todo,#TODO: Make this better,#TODO: Make this better,"variable ""server_names_stale"" {
  #TODO: Make this better
  description = ""List of names of stale sft servers. The server will be availables at sft<name>.<environment>.<root_domain>, ideally these shouldn't be touched by terraform""
  type = set(string)
}
",variable,the block associated got renamed or deleted,,15,,8d9b7151e0d68df4553b84e43b905305f02c39ba,d8e12109f7193074bb4c065a6e3c0580a24cbceb,https://github.com/wireapp/wire-server-deploy/blob/8d9b7151e0d68df4553b84e43b905305f02c39ba/terraform/modules/sft/variables.tf#L15,https://github.com/wireapp/wire-server-deploy/blob/d8e12109f7193074bb4c065a6e3c0580a24cbceb/terraform/modules/sft/variables.tf,2020-10-06 18:36:03+02:00,2020-10-15 12:30:35+02:00,2,1,0,1,0,0,0,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,141,modules/oke/cluster.tf,modules/oke/cluster.tf,0,//todo,//TODO: remove this option when the relevant Kubernetes version (v1.25) is no longer supported by OKE,//TODO: remove this option when the relevant Kubernetes version (v1.25) is no longer supported by OKE,"resource ""oci_containerengine_cluster"" ""k8s_cluster"" {
  compartment_id     = var.compartment_id
  kubernetes_version = var.cluster_kubernetes_version
  kms_key_id         = var.use_cluster_encryption == true ? var.cluster_kms_key_id : null
  name               = var.label_prefix == ""none"" ? var.cluster_name : ""${var.label_prefix}-${var.cluster_name}""

  depends_on = [time_sleep.wait_30_seconds]

  cluster_pod_network_options {
    cni_type = var.cni_type == ""flannel"" ? ""FLANNEL_OVERLAY"" : ""OCI_VCN_IP_NATIVE""
  }

  endpoint_config {
    is_public_ip_enabled = var.control_plane_type == ""public"" ? true : false
    nsg_ids              = var.control_plane_nsgs
    subnet_id            = var.cluster_subnets[""cp""]
  }

  dynamic ""image_policy_config"" {
    for_each = var.use_signed_images == true ? [1] : []

    content {
      is_policy_enabled = true

      dynamic ""key_details"" {
        iterator = signing_keys_iterator
        for_each = var.image_signing_keys

        content {
          kms_key_id = signing_keys_iterator.value
        }
      }
    }
  }

  freeform_tags = lookup(var.freeform_tags, ""cluster"", {})
  defined_tags  = lookup(var.defined_tags, ""cluster"", {})

  options {
    add_ons {
      is_kubernetes_dashboard_enabled = var.cluster_options_add_ons_is_kubernetes_dashboard_enabled
      is_tiller_enabled               = false
    }

    //TODO: remove this option when the relevant Kubernetes version (v1.25) is no longer supported by OKE
    admission_controller_options {
      is_pod_security_policy_enabled = lookup(var.admission_controller_options, ""PodSecurityPolicy"", false)
    }

    kubernetes_network_config {
      pods_cidr     = var.cluster_options_kubernetes_network_config_pods_cidr
      services_cidr = var.cluster_options_kubernetes_network_config_services_cidr
    }

    persistent_volume_config {
      freeform_tags = lookup(var.freeform_tags, ""persistent_volume"", {})
      defined_tags  = lookup(var.defined_tags, ""persistent_volume"", {})
    }

    service_lb_config {
      freeform_tags = lookup(var.freeform_tags, ""service_lb"", {})
      defined_tags  = lookup(var.defined_tags, ""service_lb"", {})
    }

    service_lb_subnet_ids = [var.cluster_subnets[local.lb_subnet]]
  }

  lifecycle {
    ignore_changes = [defined_tags, freeform_tags, cluster_pod_network_options]

    precondition {
      condition     = var.cluster_subnets[local.lb_subnet] != """"
      error_message = ""Preferred load balancer references unexisting load balancer. Please check variables preferred_load_balancer and load_balancers.""
    }
  }

  vcn_id = var.vcn_id

}
",resource,,,55,0.0,23a6c53575badeea9f8bf2fd815fca59f843e677,6c867cd8e9cbf559742f56658989bcded0d1fd89,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/23a6c53575badeea9f8bf2fd815fca59f843e677/modules/oke/cluster.tf#L55,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/6c867cd8e9cbf559742f56658989bcded0d1fd89/modules/oke/cluster.tf#L0,2023-03-23 09:58:36+11:00,2023-10-25 16:40:02+11:00,5,2,1,1,1,0,0,0,0,0
https://github.com/Worklytics/psoxy,421,infra/examples/msft-365/main.tf,infra/examples/msft-365/main.tf,0,# todo,"# TODO: CLIENT_ID, REFRESH_ENDPOINT better as env variables","# TODO: CLIENT_ID, REFRESH_ENDPOINT better as env variables","resource ""local_file"" ""configure_client_id"" {
  for_each = module.worklytics_connector_specs.enabled_msft_365_connectors


  # TODO: CLIENT_ID, REFRESH_ENDPOINT better as env variables
  filename = ""TODO 1 - setup ${each.key} secrets in AWS (SENSITIVE).md""
  content  = <<EOT

  1. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_CLIENT_ID` as an AWS SSM Parameter to value
     `${module.msft-connection[each.key].connector.application_id}`
  2. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_REFRESH_ENDPOINT` as an AWS SSM Parameter to
     `https://login.microsoftonline.com/${var.msft_tenant_id}/oauth2/v2.0/token`
  3. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_PRIVATE_KEY_ID` as an AWS SSM Parameter to
     `module.msft-connection-auth[each.key].private_key_id`
  4. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_PRIVATE_KEY` as an AWS SSM Parameter to
     `module.msft-connection-auth[each.key].private_key`

EOT
}
",resource,"resource ""local_file"" ""configure_client_id"" {
  for_each = module.worklytics_connector_specs.enabled_msft_365_connectors


  # TODO: CLIENT_ID, REFRESH_ENDPOINT better as env variables
  filename = ""TODO 1 - setup ${each.key} secrets in AWS (SENSITIVE).md""
  content  = <<EOT

  1. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_CLIENT_ID` as an AWS SSM Parameter to value
     `${module.msft-connection[each.key].connector.application_id}`
  2. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_REFRESH_ENDPOINT` as an AWS SSM Parameter to
     `https://login.microsoftonline.com/${var.msft_tenant_id}/oauth2/v2.0/token`
  3. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_PRIVATE_KEY_ID` as an AWS SSM Parameter to
     `module.msft-connection-auth[each.key].private_key_id`
  4. Set `PSOXY_${upper(replace(each.key, ""-"", ""_""))}_PRIVATE_KEY` as an AWS SSM Parameter to
     `module.msft-connection-auth[each.key].private_key`

EOT
}
",resource,78,84.0,a63a56baa7f445fb28653b850d4c6c7c53c97019,8f6786b90a7f0fcb6120cc8f822fd07cab49697f,https://github.com/Worklytics/psoxy/blob/a63a56baa7f445fb28653b850d4c6c7c53c97019/infra/examples/msft-365/main.tf#L78,https://github.com/Worklytics/psoxy/blob/8f6786b90a7f0fcb6120cc8f822fd07cab49697f/infra/examples/msft-365/main.tf#L84,2022-10-07 20:39:52-07:00,2024-04-23 19:32:11-07:00,84,0,0,1,0,0,0,0,0,0
https://github.com/aws-observability/terraform-aws-observability-accelerator,5,examples/workloads.tf,examples/workloads.tf,0,# todo,"# TODO: create also a cluster, VPC -- check if enough VPCs","# TODO: create also a cluster, VPC -- check if enough VPCs  
 # deploys AWS Distro for OpenTelemetry operator into the cluster","module ""eks_observability_accelerator"" {
  #source = ""aws-ia/terrarom-aws-observability-accelerator""
  source = ""../""

  aws_region     = var.aws_region
  eks_cluster_id = var.eks_cluster_id

  # TODO: create also a cluster, VPC -- check if enough VPCs

  # deploys AWS Distro for OpenTelemetry operator into the cluster
  enable_amazon_eks_adot = false

  # reusing existing certificate manager? defaults to true
  enable_cert_manager = false

  # # -- or enable opentelemetry operator
  enable_opentelemetry_operator = false #-- true doesn't work for me, needs fix
  #open_telemetry_operator_config = map() // custom config

  # creates a new AMP workspace, defaults to true
  enable_managed_prometheus = false

  # reusing existing AMP -- needs data source for alerting rules
  managed_prometheus_id     = var.managed_prometheus_workspace_id
  managed_prometheus_region = null # defaults to the current region, useful for cross region scenarios (same account)

  # sets up the AMP alert manager at the workspace level
  enable_alertmanager = true

  # create a new Grafana workspace
  enable_managed_grafana = true
  #managed_grafana_workspace_id = ""g-9790a4306b""



  enable_java                 = true
  enable_java_recording_rules = true


  # enable_haproxy = true
  # haproxy_config = {
  #   amp_endpoint     = module / amp.endpoint
  #   grafana_endpoint = module.grafana.endpoint
  # }


  # java_config = {
  #   amp_endpoint     = """"
  #   grafana_endpoint = """"
  # }

  # # -- or provide custom alerts definition
  # prometheus_custom_alert_rule = var.prometheus_custom_alert_rule


  # # create grafana workspace, and customer to deal with authentication later
  # create_managed_grafana_workspace = true
  # grafana_auth_provider            = var.grafana_auth_provider       //SAML or AWS_SSO
  # grafana_account_access_type      = var.grafana_account_access_type // CURRENT_ACCOUNT or ORGANIZATION
  # grafana_permission_type          = var.grafana_permission_type     // SERVICE_MANAGED or CUSTOMER_MANAGED
  # grafana_permission_role_arn      = var.grafana_permission_role_arn // if CUSTOMER_MANAGED

  # # -- or using existing amg workspace. so we can use API for keys


  tags = local.tags


}
",module,"module ""eks_observability_accelerator"" {
  #source = ""aws-ia/terrarom-aws-observability-accelerator""
  source = ""../""

  aws_region     = var.aws_region
  eks_cluster_id = var.eks_cluster_id

  # deploys AWS Distro for OpenTelemetry operator into the cluster
  enable_amazon_eks_adot = true

  # reusing existing certificate manager? defaults to true
  enable_cert_manager = true

  # # -- or enable opentelemetry operator
  enable_opentelemetry_operator = false
  #open_telemetry_operator_config = map() // custom config

  # creates a new AMP workspace, defaults to true
  enable_managed_prometheus = false

  # reusing existing AMP -- needs data source for alerting rules
  managed_prometheus_id     = var.managed_prometheus_workspace_id
  managed_prometheus_region = null # defaults to the current region, useful for cross region scenarios (same account)

  # sets up the AMP alert manager at the workspace level
  enable_alertmanager = true

  # create a new Grafana workspace - TODO review design
  enable_managed_grafana       = false
  managed_grafana_workspace_id = var.managed_grafana_workspace_id
  grafana_api_key              = var.grafana_api_key

  # enable workload-specific collector, metrics, alerts and dashboards
  enable_java                 = false
  enable_java_recording_rules = false

  # enable_haproxy = true
  # haproxy_config = {
  #   amp_endpoint     = module / amp.endpoint
  #   grafana_endpoint = module.grafana.endpoint
  # }

  enable_infra_metrics = true
  #infra_metrics_config = {}

  tags = local.tags
}
",module,10,,2a5564607491389ad1a87c16add9542d44d9ac20,a8fc12f7b7d7f4ed814d48061cf3e0eb4a646c9e,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/2a5564607491389ad1a87c16add9542d44d9ac20/examples/workloads.tf#L10,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/a8fc12f7b7d7f4ed814d48061cf3e0eb4a646c9e/examples/workloads.tf,2022-08-26 17:30:03+02:00,2022-08-26 17:30:03+02:00,3,1,1,1,0,0,1,0,0,0
https://github.com/oracle-terraform-modules/terraform-oci-oke,291,variables-network.tf,variables-network.tf,0,todo,"// TODO Align with subnets declaration, never/auto/always","variable ""create_nsgs_always"" { // TODO Align with subnets declaration, never/auto/always","variable ""create_nsgs_always"" { // TODO Align with subnets declaration, never/auto/always
  default     = false
  description = ""Whether to create standard network security groups when associated components will not be.""
  type        = bool
}
",variable,the block associated got renamed or deleted,,16,,cf7f4da8a0c56d0350bd86c2ef1011e3f6c2f3b2,32cf3b275cc82bf2119d9d27231c8bc86b6e0ed1,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/cf7f4da8a0c56d0350bd86c2ef1011e3f6c2f3b2/variables-network.tf#L16,https://github.com/oracle-terraform-modules/terraform-oci-oke/blob/32cf3b275cc82bf2119d9d27231c8bc86b6e0ed1/variables-network.tf,2023-10-25 16:40:02+11:00,2023-10-25 16:40:02+11:00,5,1,0,1,0,1,1,0,0,0
https://github.com/terraform-google-modules/terraform-google-iam,23,test/fixtures/additive/outputs.tf,test/fixtures/additive/outputs.tf,0,workaround,# workaround InSpec lack of support for integer,# workaround InSpec lack of support for integer,"output ""roles"" {
  # workaround InSpec lack of support for integer
  value       = tostring(var.roles)
  description = ""Amount of roles assigned. Useful for testing how the module behaves on updates.""
}
",output,"output ""roles"" {
  # workaround InSpec lack of support for integer
  value       = tostring(var.roles)
  description = ""Amount of roles assigned. Useful for testing how the module behaves on updates.""
}
",output,119,131.0,2529fe6173f7540994a346066d5ab141144861e9,4f6e19d1e561853dd55106bcb2bcc1c4edc96d45,https://github.com/terraform-google-modules/terraform-google-iam/blob/2529fe6173f7540994a346066d5ab141144861e9/test/fixtures/additive/outputs.tf#L119,https://github.com/terraform-google-modules/terraform-google-iam/blob/4f6e19d1e561853dd55106bcb2bcc1c4edc96d45/test/fixtures/additive/outputs.tf#L131,2019-10-23 19:31:30+03:00,2023-02-15 13:45:50-06:00,4,0,0,1,0,0,0,0,0,1
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,25,modules/on-prem-in-a-box/main.tf,modules/on-prem-in-a-box/main.tf,0,# todo,# TODO: use a narrower firewall rule and tie it to the service account,# TODO: use a narrower firewall rule and tie it to the service account,"resource ""google_compute_firewall"" ""allow-vpn"" {
  name          = ""onprem-in-a-box-allow-vpn""
  description   = ""Allow VPN traffic to the onprem instance""
  network       = var.network
  project       = var.project_id
  source_ranges = [format(""%s/32"", var.vpn_config.peer_ip)]
  target_tags   = [""onprem""]
  allow {
    protocol = ""tcp""
  }
  allow {
    protocol = ""udp""
  }
  allow {
    protocol = ""icmp""
  }
}
",resource,,,99,0.0,c486bfc66f9814e33b410602cb557a5e4d532912,409407ae7d02e9b7eedf63bd9815c1c789bc45bc,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c486bfc66f9814e33b410602cb557a5e4d532912/modules/on-prem-in-a-box/main.tf#L99,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/409407ae7d02e9b7eedf63bd9815c1c789bc45bc/modules/on-prem-in-a-box/main.tf#L0,2020-04-03 14:06:48+02:00,2020-04-06 16:27:13+02:00,2,2,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1699,fast/stages/1-resman/branch-teams.tf,fast/stages/1-resman/branch-teams.tf,0,# todo,# TODO: move into team's own IaC project,# TODO: move into team's own IaC project,"module ""branch-teams-team-sa"" {
  source       = ""../../../modules/iam-service-account""
  for_each     = var.fast_features.teams ? coalesce(var.team_folders, {}) : {}
  project_id   = var.automation.project_id
  name         = ""prod-teams-${each.key}-0""
  display_name = ""Terraform team ${each.key} service account.""
  prefix       = var.prefix
  iam = {
    ""roles/iam.serviceAccountTokenCreator"" = concat(
      compact([try(module.branch-teams-team-sa-cicd[each.key].iam_email, null)]),
      (
        each.value.impersonation_groups == null
        ? []
        : [for g in each.value.impersonation_groups : ""group:${g}""]
      )
    )
  }
}
",module,,,88,0.0,e7e188818a633c1b6a47ec318eb3513b50b7437a,7a5dd4e6db197daa52da8a8d877ce86b5c93182e,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/e7e188818a633c1b6a47ec318eb3513b50b7437a/fast/stages/1-resman/branch-teams.tf#L88,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/7a5dd4e6db197daa52da8a8d877ce86b5c93182e/fast/stages/1-resman/branch-teams.tf#L0,2023-10-18 12:18:31+00:00,2024-05-15 09:17:13+00:00,8,2,0,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,22,main.tf,main.tf,0,# todo,# TODO: revisit terraform 0.12 if arrays are available,"# TODO: revisit terraform 0.12 if arrays are available 
 # access { 
 #   role   = ""READER"" 
 #   domain = ""adigangi.com"" 
 # }","resource ""google_bigquery_dataset"" ""main"" {
  dataset_id    = ""${var.dataset_id}""
  friendly_name = ""${var.dataset_name}""
  description   = ""${var.description}""
  location      = ""${local.location}""

  #TODO: terraform 0.12 will enable ""expiration_mode ? a_table_expiration : null"" (https://github.com/hashicorp/terraform/issues/17968)
  default_table_expiration_ms = ""${var.expiration}""
  project                     = ""${var.project_id}""
  labels                      = ""${var.dataset_labels}""

  # TODO: revisit terraform 0.12 if arrays are available
  # access {
  #   role   = ""READER""
  #   domain = ""adigangi.com""
  # }
}
",resource,"resource ""google_bigquery_dataset"" ""main"" {
  dataset_id    = ""${var.dataset_id}""
  friendly_name = ""${var.dataset_name}""
  description   = ""${var.description}""
  location      = ""${local.location}""

  default_table_expiration_ms = ""${var.expiration}""
  project                     = ""${var.project_id}""
  labels                      = ""${var.dataset_labels}""
}
",resource,35,,7f922f7e9df197df38c9b09dfa6e3614d71f19f5,a7966ae4afc7bb73842fb9fd6f0f708b359cf36e,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/7f922f7e9df197df38c9b09dfa6e3614d71f19f5/main.tf#L35,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/a7966ae4afc7bb73842fb9fd6f0f708b359cf36e/main.tf,2019-01-16 18:10:54-05:00,2019-01-28 12:41:47-05:00,2,1,1,1,1,1,0,0,0,0
https://github.com/CDCgov/prime-simplereport,40,ops/services/postgres_db/monitor.tf,ops/services/postgres_db/monitor.tf,0,// todo,// TODO: delete the current target_resource_id line and uncomment the line below it,"// TODO: delete the current target_resource_id line and uncomment the line below it 
 // when removing old DB configuration.","resource ""azurerm_monitor_diagnostic_setting"" ""postgres"" {
  name               = ""simple-report-${var.env}-db-diag""
  target_resource_id = azurerm_postgresql_server.db.id
  //target_resource_id         = azurerm_postgresql_flexible_server.db.id
  log_analytics_workspace_id = var.log_workspace_id

  log {
    category = ""PostgreSQLLogs""
    enabled  = true

    retention_policy {
      enabled = false
    }
  }

  metric {
    category = ""AllMetrics""
    enabled  = true

    retention_policy {
      enabled = false
    }
  }

  log {
    category = ""QueryStoreRuntimeStatistics""
    enabled  = false

    retention_policy {
      enabled = false
    }
  }

  log {
    category = ""QueryStoreWaitStatistics""
    enabled  = false

    retention_policy {
      enabled = false
    }
  }
}
",resource,"resource ""azurerm_monitor_diagnostic_setting"" ""postgres"" {
  name                       = ""simple-report-${var.env}-db-diag""
  target_resource_id         = azurerm_postgresql_flexible_server.db.id
  log_analytics_workspace_id = var.log_workspace_id

  log {
    category = ""PostgreSQLLogs""
    enabled  = true

    retention_policy {
      enabled = false
    }
  }

  metric {
    category = ""AllMetrics""
    enabled  = true

    retention_policy {
      enabled = false
    }
  }
}
",resource,1,,3e4185fa549937cff9213c325bc0fee780e41eb0,fb9a18eb71f31044ca7a58958a25dea489263ca7,https://github.com/CDCgov/prime-simplereport/blob/3e4185fa549937cff9213c325bc0fee780e41eb0/ops/services/postgres_db/monitor.tf#L1,https://github.com/CDCgov/prime-simplereport/blob/fb9a18eb71f31044ca7a58958a25dea489263ca7/ops/services/postgres_db/monitor.tf,2022-02-16 12:59:39-05:00,2022-04-28 23:27:57-04:00,2,1,1,1,0,0,0,0,1,0
https://github.com/terraform-google-modules/terraform-google-bigquery,68,modules/data_warehouse/outputs.tf,modules/data_warehouse/outputs.tf,0,#todo,#TODO Create new Looker Studio Template,#TODO Create new Looker Studio Template,"output ""lookerstudio_report_url"" {
  value       = ""https://lookerstudio.google.com/reporting/create?c.reportId=8a6517b8-8fcd-47a2-a953-9d4fb9ae4794&ds.ds_profit.datasourceName=lookerstudio_report_profit&ds.ds_profit.projectId=${module.project-services.project_id}&ds.ds_profit.type=TABLE&ds.ds_profit.datasetId=${google_bigquery_dataset.ds_edw.dataset_id}&ds.ds_profit.tableId=lookerstudio_report_profit&ds.ds_dc.datasourceName=lookerstudio_report_distribution_centers&ds.ds_dc.projectId=${module.project-services.project_id}&ds.ds_dc.type=TABLE&ds.ds_dc.datasetId=${google_bigquery_dataset.ds_edw.dataset_id}&ds.ds_dc.tableId=lookerstudio_report_distribution_centers""
  description = ""The URL to create a new Looker Studio report displays a sample dashboard for the e-commerce data analysis""
}
",output,"output ""lookerstudio_report_url"" {
  value       = ""https://lookerstudio.google.com/reporting/create?c.reportId=8a6517b8-8fcd-47a2-a953-9d4fb9ae4794&ds.ds_profit.datasourceName=lookerstudio_report_profit&ds.ds_profit.projectId=${module.project-services.project_id}&ds.ds_profit.type=TABLE&ds.ds_profit.datasetId=${google_bigquery_dataset.ds_edw.dataset_id}&ds.ds_profit.tableId=lookerstudio_report_profit&ds.ds_dc.datasourceName=lookerstudio_report_distribution_centers&ds.ds_dc.projectId=${module.project-services.project_id}&ds.ds_dc.type=TABLE&ds.ds_dc.datasetId=${google_bigquery_dataset.ds_edw.dataset_id}&ds.ds_dc.tableId=lookerstudio_report_distribution_centers""
  description = ""The URL to create a new Looker Studio report displays a sample dashboard for the e-commerce data analysis""
}
",output,27,,e97adfb1984592a6f9e4eea8f8bdc3d2969e3d2d,f88f4b53ea5ed8416ed01e7285fa6018ddb8bd0b,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/e97adfb1984592a6f9e4eea8f8bdc3d2969e3d2d/modules/data_warehouse/outputs.tf#L27,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/f88f4b53ea5ed8416ed01e7285fa6018ddb8bd0b/modules/data_warehouse/outputs.tf,2023-10-09 13:27:46-06:00,2023-10-23 13:10:22-06:00,2,1,0,1,0,0,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,613,modules/karpenter/main.tf,modules/karpenter/main.tf,0,todo,# TODO - this will be replaced in v20.0 with the scoped policy provided by Karpenter,"  # TODO - this will be replaced in v20.0 with the scoped policy provided by Karpenter
  # https://github.com/aws/karpenter/blob/main/website/content/en/docs/upgrading/v1beta1-controller-policy.json","data ""aws_iam_policy_document"" ""irsa"" {
  count = local.create_irsa ? 1 : 0

  statement {
    actions = [
      ""ec2:CreateLaunchTemplate"",
      ""ec2:CreateFleet"",
      ""ec2:CreateTags"",
      ""ec2:DescribeLaunchTemplates"",
      ""ec2:DescribeImages"",
      ""ec2:DescribeInstances"",
      ""ec2:DescribeSecurityGroups"",
      ""ec2:DescribeSubnets"",
      ""ec2:DescribeInstanceTypes"",
      ""ec2:DescribeInstanceTypeOfferings"",
      ""ec2:DescribeAvailabilityZones"",
      ""ec2:DescribeSpotPriceHistory"",
      ""pricing:GetProducts"",
    ]

    resources = [""*""]
  }

  statement {
    actions = [
      ""ec2:TerminateInstances"",
      ""ec2:DeleteLaunchTemplate"",
    ]

    resources = [""*""]

    condition {
      test     = ""StringEquals""
      variable = ""ec2:ResourceTag/${var.irsa_tag_key}""
      values   = local.irsa_tag_values
    }
  }

  statement {
    actions = [""ec2:RunInstances""]
    resources = [
      ""arn:${local.partition}:ec2:*:${local.account_id}:launch-template/*"",
    ]

    condition {
      test     = ""StringEquals""
      variable = ""ec2:ResourceTag/${var.irsa_tag_key}""
      values   = local.irsa_tag_values
    }
  }

  statement {
    actions = [""ec2:RunInstances""]
    resources = [
      ""arn:${local.partition}:ec2:*::image/*"",
      ""arn:${local.partition}:ec2:*::snapshot/*"",
      ""arn:${local.partition}:ec2:*:${local.account_id}:instance/*"",
      ""arn:${local.partition}:ec2:*:${local.account_id}:spot-instances-request/*"",
      ""arn:${local.partition}:ec2:*:${local.account_id}:security-group/*"",
      ""arn:${local.partition}:ec2:*:${local.account_id}:volume/*"",
      ""arn:${local.partition}:ec2:*:${local.account_id}:network-interface/*"",
      ""arn:${local.partition}:ec2:*:${coalesce(var.irsa_subnet_account_id, local.account_id)}:subnet/*"",
    ]
  }

  statement {
    actions   = [""ssm:GetParameter""]
    resources = var.irsa_ssm_parameter_arns
  }

  statement {
    actions   = [""eks:DescribeCluster""]
    resources = [""arn:${local.partition}:eks:*:${local.account_id}:cluster/${var.cluster_name}""]
  }

  statement {
    actions   = [""iam:PassRole""]
    resources = [var.create_iam_role ? aws_iam_role.this[0].arn : var.iam_role_arn]
  }

  dynamic ""statement"" {
    for_each = local.enable_spot_termination ? [1] : []

    content {
      actions = [
        ""sqs:DeleteMessage"",
        ""sqs:GetQueueUrl"",
        ""sqs:GetQueueAttributes"",
        ""sqs:ReceiveMessage"",
      ]
      resources = [aws_sqs_queue.this[0].arn]
    }
  }

  # TODO - this will be replaced in v20.0 with the scoped policy provided by Karpenter
  # https://github.com/aws/karpenter/blob/main/website/content/en/docs/upgrading/v1beta1-controller-policy.json
  dynamic ""statement"" {
    for_each = var.enable_karpenter_instance_profile_creation ? [1] : []

    content {
      actions = [
        ""iam:AddRoleToInstanceProfile"",
        ""iam:CreateInstanceProfile"",
        ""iam:DeleteInstanceProfile"",
        ""iam:GetInstanceProfile"",
        ""iam:RemoveRoleFromInstanceProfile"",
        ""iam:TagInstanceProfile"",
      ]
      resources = [""*""]
    }
  }
}
",data,the block associated got renamed or deleted,,164,,aec2bab1d8da89b65b84d11fef77cbc969fccc91,6b40bdbb1d283d9259f43b03d24dca99cc1eceff,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/aec2bab1d8da89b65b84d11fef77cbc969fccc91/modules/karpenter/main.tf#L164,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/6b40bdbb1d283d9259f43b03d24dca99cc1eceff/modules/karpenter/main.tf,2023-11-01 11:33:07-04:00,2024-02-02 09:36:25-05:00,4,1,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1562,blueprints/data-solutions/data-platform-minimal/03-curated.tf,blueprints/data-solutions/data-platform-minimal/03-curated.tf,0,fix,# Remove once bug is fixed. https://github.com/apache/airflow/issues/32106,"module.processing-sa-0.iam_email, # Remove once bug is fixed. https://github.com/apache/airflow/issues/32106","locals {
  cur_iam = {
    ""roles/bigquery.dataOwner"" = [module.processing-sa-0.iam_email]
    ""roles/bigquery.dataViewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/bigquery.jobUser"" = [
      module.processing-sa-0.iam_email, # Remove once bug is fixed. https://github.com/apache/airflow/issues/32106
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/datacatalog.tagTemplateViewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/datacatalog.viewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/storage.objectViewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/storage.objectAdmin"" = [module.processing-sa-0.iam_email]
  }
  cur_services = [
    ""bigquery.googleapis.com"",
    ""bigqueryreservation.googleapis.com"",
    ""bigquerystorage.googleapis.com"",
    ""cloudkms.googleapis.com"",
    ""cloudresourcemanager.googleapis.com"",
    ""compute.googleapis.com"",
    ""iam.googleapis.com"",
    ""servicenetworking.googleapis.com"",
    ""serviceusage.googleapis.com"",
    ""stackdriver.googleapis.com"",
    ""storage.googleapis.com"",
    ""storage-component.googleapis.com""
  ]
}
",locals,"locals {
  cur_services = [
    ""bigquery.googleapis.com"",
    ""bigqueryreservation.googleapis.com"",
    ""bigquerystorage.googleapis.com"",
    ""cloudkms.googleapis.com"",
    ""cloudresourcemanager.googleapis.com"",
    ""compute.googleapis.com"",
    ""datalineage.googleapis.com"",
    ""iam.googleapis.com"",
    ""servicenetworking.googleapis.com"",
    ""serviceusage.googleapis.com"",
    ""stackdriver.googleapis.com"",
    ""storage.googleapis.com"",
    ""storage-component.googleapis.com""
  ]
  iam_cur = {
    ""roles/bigquery.dataOwner"" = [
      module.processing-sa-0.iam_email
    ]
    ""roles/bigquery.dataViewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/bigquery.jobUser"" = [
      # Remove once bug is fixed. https://github.com/apache/airflow/issues/32106
      module.processing-sa-0.iam_email,
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/datacatalog.tagTemplateViewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/datacatalog.viewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts,
      local.groups_iam.data-engineers
    ]
    ""roles/storage.objectViewer"" = [
      module.cur-sa-0.iam_email,
      local.groups_iam.data-analysts
    ]
    ""roles/storage.admin"" = [
      module.processing-sa-0.iam_email,
      local.groups_iam.data-engineers
    ]
  }
  # this only works because the service account module uses a static output
  iam_cur_additive = {
    for k in flatten([
      for role, members in local.iam_cur : [
        for member in members : {
          role   = role
          member = member
        }
      ]
    ]) : ""${k.member}-${k.role}"" => k
  }
}
",locals,26,43.0,099ad03910542c546e50d16676b45fc9ce25ef91,a0ae43fc6f4dfddcd4bd1c99d03a8a7714ace9bb,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/099ad03910542c546e50d16676b45fc9ce25ef91/blueprints/data-solutions/data-platform-minimal/03-curated.tf#L26,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/a0ae43fc6f4dfddcd4bd1c99d03a8a7714ace9bb/blueprints/data-solutions/data-platform-minimal/03-curated.tf#L43,2023-06-28 09:05:48+02:00,2023-11-01 17:53:06+01:00,5,0,0,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,959,tests/modules/organization-policy/fixture/variables.tf,tests/modules/organization-policy/fixture/variables.tf,0,# todo,"# TODO: convert to a proper data structure map(map(object({...}))) once tf1.3 is released and optional object keys are avaliable,","# TODO: convert to a proper data structure map(map(object({...}))) once tf1.3 is released and optional object keys are avaliable, 
 # for now it will cause multiple keys to be set to null for every policy definition 
 # https://github.com/hashicorp/terraform/releases/tag/v1.3.0-alpha20220622","variable ""organization_policies"" {
  description = ""Organization policies keyed by parent in format `projects/project-id`, `folders/1234567890` or `organizations/1234567890`.""
  type        = any
  default     = {}
}
",variable,the block associated got renamed or deleted,,24,,a34983b2e960ec5d2e1539f6c781248068d22ef3,b8fae0fbf064d881e70b7958685006c3c084ff10,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/a34983b2e960ec5d2e1539f6c781248068d22ef3/tests/modules/organization-policy/fixture/variables.tf#L24,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/b8fae0fbf064d881e70b7958685006c3c084ff10/tests/modules/organization-policy/fixture/variables.tf,2022-07-06 19:41:18+02:00,2022-07-08 14:55:28+02:00,3,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd,4,modules/secure-cd/main.tf,modules/secure-cd/main.tf,0,// todo,// TODO: is this necessary or can we just know the branch inherently,_ENVIRONMENT       = each.value // TODO: is this necessary or can we just know the branch inherently,"resource ""google_cloudbuild_trigger"" ""deploy_trigger"" {
  for_each = toset(var.deploy_branches)
  project = var.project_id
  name    = ""deploy-trigger-${each.value}""
  trigger_template {
    branch_name = each.value
    repo_name   = var.manifest_wet_repo
  }
  substitutions = merge(
    {
      _GAR_REPOSITORY    = local.gar_name
      _DEFAULT_REGION    = var.primary_location
      _MANIFEST_WET_REPO = var.manifest_wet_repo
      _ENVIRONMENT       = each.value // TODO: is this necessary or can we just know the branch inherently
    },
    var.additional_substitutions
  )
  filename   = var.app_deploy_trigger_yaml
  depends_on = [google_sourcerepo_repository.repos]
}
",resource,"resource ""google_cloudbuild_trigger"" ""deploy_trigger"" {
  for_each = var.deploy_branch_clusters
  project  = var.project_id
  name     = ""deploy-trigger-${each.key}""

  trigger_template {
    branch_name = each.key
    repo_name   = var.manifest_wet_repo
  }
  substitutions = merge(
    {
      _GAR_REPOSITORY      = var.gar_repo_name
      _DEFAULT_REGION      = each.value.location
      _MANIFEST_WET_REPO   = var.manifest_wet_repo
      _CLUSTER_NAME        = each.value.cluster
      _CLUSTER_PROJECT     = each.value.project_id
      _CLOUDBUILD_FILENAME = var.app_deploy_trigger_yaml
      _CACHE_BUCKET_NAME   = var.cache_bucket_name
      _NEXT_ENV            = each.value.next_env
      _ATTESTOR_NAME       = var.deploy_branch_clusters[""${each.value.next_env}""].attestations[0]
    }
  )
  filename = var.app_deploy_trigger_yaml

}
",resource,38,,75fd6e2deaa66330698ccc35ac59bd65b827cb93,8c54ab472287c4a642c961dcdf44f71626f956af,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/75fd6e2deaa66330698ccc35ac59bd65b827cb93/modules/secure-cd/main.tf#L38,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/8c54ab472287c4a642c961dcdf44f71626f956af/modules/secure-cd/main.tf,2021-10-14 17:52:10-05:00,2021-11-12 15:01:06-06:00,9,1,0,1,0,0,0,1,0,0
https://github.com/chanzuckerberg/cztack,1,aws-default-vpc-security/main.tf,aws-default-vpc-security/main.tf,0,hack,# count hack here based on https://github.com/hashicorp/terraform/issues/11574#issuecomment-365690226,"# count hack here based on https://github.com/hashicorp/terraform/issues/11574#issuecomment-365690226 
 # :(","resource ""aws_default_subnet"" ""default"" {
  count = ""${length(split("","", join("","", flatten(data.aws_availability_zones.available.*.names))))}""

  availability_zone = ""${data.aws_availability_zones.available.names[count.index]}""
}
",resource,"resource ""aws_default_subnet"" ""default"" {
  count = length(split("","", join("","", flatten(data.aws_availability_zones.available.*.names))))

  availability_zone = data.aws_availability_zones.available.names[count.index]
}
",resource,40,40.0,038021ff34e8b2c5bce7a32400dca526838790df,b71a885f73472340541c8cb9f7bc1f5e279538a4,https://github.com/chanzuckerberg/cztack/blob/038021ff34e8b2c5bce7a32400dca526838790df/aws-default-vpc-security/main.tf#L40,https://github.com/chanzuckerberg/cztack/blob/b71a885f73472340541c8cb9f7bc1f5e279538a4/aws-default-vpc-security/main.tf#L40,2018-07-27 14:32:44-07:00,2021-06-25 11:33:29-04:00,4,0,0,1,1,0,1,0,0,0
https://github.com/kubernetes/k8s.io,353,infra/gcp/terraform/k8s-infra-oci-proxy/main.tf,infra/gcp/terraform/k8s-infra-oci-proxy/main.tf,0,// todo,// TODO: create a dedicated service account for auto-deployment,"// Currently we only do this for staging, prod is manual deployed by admins 
 // 
 // Ensure gcb-builder can auto-deploy registry-sandbox.k8s.io 
 // 
 // TODO: create a dedicated service account for auto-deployment","data ""google_project"" ""k8s_infra_staging_tools"" {
  project_id = ""k8s-staging-infra-tools""
}
",data,the block associated got renamed or deleted,,45,,585608eaa86847071620860d476b7fb781794e33,f4c3b4d855050bb779bce449c60c07d599e8e2aa,https://github.com/kubernetes/k8s.io/blob/585608eaa86847071620860d476b7fb781794e33/infra/gcp/terraform/k8s-infra-oci-proxy/main.tf#L45,https://github.com/kubernetes/k8s.io/blob/f4c3b4d855050bb779bce449c60c07d599e8e2aa/infra/gcp/terraform/k8s-infra-oci-proxy/main.tf,2023-04-04 15:39:35-07:00,2023-04-05 19:28:57-07:00,2,1,0,1,0,1,0,0,0,0
https://github.com/kbst/terraform-kubestack,54,aws/cluster/elb-dns/ingress.tf,aws/cluster/elb-dns/ingress.tf,0,workaround,# this is a workaround as aws_elb_hosted_zone_id doesn't support NLBs,"# this is a workaround as aws_elb_hosted_zone_id doesn't support NLBs 
 # ref: https://github.com/hashicorp/terraform-provider-aws/issues/7988","data ""aws_lb"" ""current"" {
  name = split(""-"", data.kubernetes_service.current.status[0].load_balancer[0].ingress[0].hostname).0
}
",data,"data ""aws_lb"" ""current"" {
  count = var.using_nlb ? 1 : 0

  name = split(""-"", data.kubernetes_service.current.status[0].load_balancer[0].ingress[0].hostname).0
}
",data,15,16.0,dc6dff74e9f4b68bf9ef5d0b8366f664a52cbe55,5594ec823bd41d2d4b09ca8fa83e57eb9a536eb0,https://github.com/kbst/terraform-kubestack/blob/dc6dff74e9f4b68bf9ef5d0b8366f664a52cbe55/aws/cluster/elb-dns/ingress.tf#L15,https://github.com/kbst/terraform-kubestack/blob/5594ec823bd41d2d4b09ca8fa83e57eb9a536eb0/aws/cluster/elb-dns/ingress.tf#L16,2021-11-05 09:56:54+09:00,2021-11-08 19:12:28+01:00,2,0,0,0,1,0,1,0,0,0
https://github.com/SUSE/ha-sap-terraform-deployments,40,gcp/terraform/instances.tf,gcp/terraform/instances.tf,0,xxx,# XXX: The join() is a workaround for https://github.com/hashicorp/terraform/issues/11566,# XXX: The join() is a workaround for https://github.com/hashicorp/terraform/issues/11566,"resource ""google_compute_instance"" ""clusternodes"" {
  machine_type            = ""${var.machine_type}""
  metadata_startup_script = ""${file(""startup.sh"")}""
  count                   = ""2""
  name                    = ""${terraform.workspace}-${var.name}-node-${count.index}""
  zone                    = ""${element(data.google_compute_zones.available.names, count.index)}""

  can_ip_forward = true

  network_interface {
    subnetwork = ""${google_compute_subnetwork.ha_subnet.name}""
    network_ip = ""${cidrhost(var.ip_cidr_range, count.index+2)}""

    access_config {
      nat_ip = """"
    }
  }

  scheduling {
    automatic_restart   = true
    on_host_maintenance = ""MIGRATE""
    preemptible         = false
  }

  boot_disk {
    initialize_params {
      # XXX: The join() is a workaround for https://github.com/hashicorp/terraform/issues/11566
      image = ""${var.use_custom_image == ""true"" ? ""${join("""", google_compute_image.sles4sap_bootable_image.*.self_link)}"" : ""suse-sap-cloud/${var.sles4sap_os_image}""}""
    }

    auto_delete = true
  }

  attached_disk {
    source      = ""${element(google_compute_disk.node_data.*.self_link, count.index)}""
    device_name = ""${element(google_compute_disk.node_data.*.name, count.index)}""
    mode        = ""READ_WRITE""
  }

  attached_disk {
    source      = ""${element(google_compute_disk.backup.*.self_link, count.index)}""
    device_name = ""${element(google_compute_disk.backup.*.name, count.index)}""
    mode        = ""READ_WRITE""
  }

  metadata {
    sshKeys = ""root:${file(var.public_key_location)}""

    # For a description of these:
    # https://storage.googleapis.com/sapdeploy/dm-templates/sap_hana_ha/template.yaml

    post_deployment_script     = ""${var.post_deployment_script}""
    sap_deployment_debug       = ""${var.sap_deployment_debug}""
    sap_hana_backup_bucket     = """"
    sap_hana_deployment_bucket = ""${var.sap_hana_deployment_bucket}""
    sap_hana_instance_number   = ""${var.sap_hana_instance_number}""
    sap_hana_sapsys_gid        = ""${var.sap_hana_sapsys_gid}""
    sap_hana_scaleout_nodes    = ""0""
    sap_hana_sid               = ""${var.sap_hana_sid}""
    sap_hana_sidadm_password   = ""${var.sap_hana_sidadm_password}""
    sap_hana_sidadm_uid        = ""${var.sap_hana_sidadm_uid}""
    sap_hana_standby_nodes     = """"
    sap_hana_system_password   = ""${var.sap_hana_system_password}""
    sap_primary_instance       = ""${terraform.workspace}-${var.name}-node-0""
    sap_primary_zone           = ""${data.google_compute_zones.available.names[0]}""
    sap_secondary_instance     = ""${terraform.workspace}-${var.name}-node-1""
    sap_secondary_zone         = ""${data.google_compute_zones.available.names[1]}""
    sap_vip                    = ""${var.sap_vip}""
    sap_vip_secondary_range    = """"
    suse_regcode               = ""${var.suse_regcode}""
    init_type                  = ""${var.init_type}""
    iscsi_ip                   = ""${var.iscsi_ip}""
    use_gcp_stonith            = ""${var.use_gcp_stonith}""
  }

  service_account {
    scopes = [""compute-rw"", ""storage-rw"", ""logging-write"", ""monitoring-write"", ""service-control"", ""service-management""]
  }

  provisioner ""file"" {
    source      = ""./provision/""
    destination = ""/root/provision/""

    connection {
      type        = ""ssh""
      user        = ""root""
      private_key = ""${file(var.private_key_location)}""
    }
  }
}
",resource,,,74,0.0,35dd6a5cc3e10c7feae7d5f80b69906d73057cae,4873066a74f41c7ff5286f386c32c8400b429675,https://github.com/SUSE/ha-sap-terraform-deployments/blob/35dd6a5cc3e10c7feae7d5f80b69906d73057cae/gcp/terraform/instances.tf#L74,https://github.com/SUSE/ha-sap-terraform-deployments/blob/4873066a74f41c7ff5286f386c32c8400b429675/gcp/terraform/instances.tf#L0,2019-05-06 13:04:28+02:00,2019-08-14 11:31:57+02:00,2,2,1,1,1,0,0,0,0,0
https://github.com/zenml-io/mlstacks,3,gcp-kubeflow-kserve/kserve-module/istio.tf,gcp-kubeflow-kserve/kserve-module/istio.tf,0,hack,# repeating the same command as a hack to prevent errors in subsequent commands,"# repeating the same command as a hack to prevent errors in subsequent commands 
 # running it only once would have caused the next command to fail with no envoyfilter CRDs.","resource ""null_resource"" ""create-istio-kserve"" {
  provisioner ""local-exec"" {
    command = ""kubectl apply -l knative.dev/crd-install=true -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  # repeating the same command as a hack to prevent errors in subsequent commands
  # running it only once would have caused the next command to fail with no envoyfilter CRDs.
  provisioner ""local-exec"" {
    command = ""kubectl apply -l knative.dev/crd-install=true -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    command = ""kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    command = ""kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/net-istio.yaml""
  }

  # destroy-time provisioners
  provisioner ""local-exec"" {
    when    = destroy
    command = ""kubectl delete -l knative.dev/crd-install=true -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    when    = destroy
    command = ""kubectl delete -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    when    = destroy
    command = ""kubectl delete -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/net-istio.yaml""
  }

  depends_on = [
    null_resource.create-knative-serving,
  ]
}",resource,"resource ""null_resource"" ""create-istio-kserve"" {
  provisioner ""local-exec"" {
    command = ""kubectl apply -l knative.dev/crd-install=true -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  # repeating the same command as a hack to prevent errors in subsequent commands
  # running it only once would have caused the next command to fail with no envoyfilter CRDs.
  provisioner ""local-exec"" {
    command = ""kubectl apply -l knative.dev/crd-install=true -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    command = ""kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    command = ""kubectl apply -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/net-istio.yaml""
  }

  # destroy-time provisioners
  provisioner ""local-exec"" {
    when    = destroy
    command = ""kubectl delete -l knative.dev/crd-install=true -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    when    = destroy
    command = ""kubectl delete -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/istio.yaml""
  }
  provisioner ""local-exec"" {
    when    = destroy
    command = ""kubectl delete -f https://github.com/knative/net-istio/releases/download/knative-v1.6.0/net-istio.yaml""
  }

  depends_on = [
    null_resource.create-knative-serving,
  ]
}",resource,6,6.0,e6fee7c554adb678d7c35f29cefcfb37e37ed0c0,e6fee7c554adb678d7c35f29cefcfb37e37ed0c0,https://github.com/zenml-io/mlstacks/blob/e6fee7c554adb678d7c35f29cefcfb37e37ed0c0/gcp-kubeflow-kserve/kserve-module/istio.tf#L6,https://github.com/zenml-io/mlstacks/blob/e6fee7c554adb678d7c35f29cefcfb37e37ed0c0/gcp-kubeflow-kserve/kserve-module/istio.tf#L6,2022-08-15 22:40:55+05:30,2022-08-15 22:40:55+05:30,1,0,0,1,0,0,0,1,0,0
https://github.com/nasa/cumulus,267,lambdas/data-migration/main.tf,lambdas/data-migration1/main.tf,1,# todo,# TODO: add RDS perms,# TODO: add RDS perms,"data ""aws_iam_policy_document"" ""data_migration"" {
  statement {
    actions = [
      ""ec2:CreateNetworkInterface"",
      ""ec2:DeleteNetworkInterface"",
      ""ec2:DescribeNetworkInterfaces"",
      ""logs:CreateLogGroup"",
      ""logs:CreateLogStream"",
      ""logs:DescribeLogStreams"",
      ""logs:PutLogEvents""
    ]
    resources = [""*""]
  }

  statement {
    actions = [
      ""dynamodb:Scan"",
    ]
    resources = [for k, table in var.dynamo_tables : table.arn]
  }

  # TODO: add RDS perms
}
",data,the block associated got renamed or deleted,,44,,8480e4969011afe1186f85d493806bcf48ea9698,42876df16a4c014fb7750bb8f8f9fd1ca55ae5d4,https://github.com/nasa/cumulus/blob/8480e4969011afe1186f85d493806bcf48ea9698/lambdas/data-migration/main.tf#L44,https://github.com/nasa/cumulus/blob/42876df16a4c014fb7750bb8f8f9fd1ca55ae5d4/lambdas/data-migration1/main.tf,2020-08-21 17:47:34-04:00,2020-08-28 17:34:02-04:00,6,1,1,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,534,fast/stages/02-security/core-dev.tf,fast/stages/02-security/core-dev.tf,0,# todo,# TODO(ludo): grant delegated role at key instead of project level,"# TODO(ludo): add support for conditions to Fabric modules 
 # TODO(ludo): grant delegated role at key instead of project level ","resource ""google_project_iam_member"" ""dev_key_admin_delegated"" {
  for_each = toset(try(var.kms_restricted_admins.dev, []))
  project  = module.dev-sec-project.project_id
  role     = ""roles/cloudkms.admin""
  member   = each.key
  condition {
    title       = ""kms_sa_delegated_grants""
    description = ""Automation service account delegated grants""
    expression = format(
      ""api.getAttribute('iam.googleapis.com/modifiedGrantsByRole', []).hasOnly([%s])"",
      join("","", formatlist(""'%s'"", [
        ""roles/cloudkms.cryptoKeyEncrypterDecrypter"",
        ""roles/cloudkms.cryptoKeyEncrypterDecrypterViaDelegation""
      ]))
    )
  }
  depends_on = [module.dev-sec-project]
}
",resource,"resource ""google_project_iam_member"" ""dev_key_admin_delegated"" {
  for_each = toset(local.dev_kms_restricted_admins)
  project  = module.dev-sec-project.project_id
  role     = ""roles/cloudkms.admin""
  member   = each.key
  condition {
    title       = ""kms_sa_delegated_grants""
    description = ""Automation service account delegated grants.""
    expression = format(
      ""api.getAttribute('iam.googleapis.com/modifiedGrantsByRole', []).hasOnly([%s]) && resource.type == 'cloudkms.googleapis.com/CryptoKey'"",
      join("","", formatlist(""'%s'"", [
        ""roles/cloudkms.cryptoKeyEncrypterDecrypter"",
        ""roles/cloudkms.cryptoKeyEncrypterDecrypterViaDelegation""
      ]))
    )
  }
  depends_on = [module.dev-sec-project]
}
",resource,46,,4e02f4475a6140e0baab53867d847345a28d4b3b,ceb611bb819cced9f35ee19031584197caafddd1,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/4e02f4475a6140e0baab53867d847345a28d4b3b/fast/stages/02-security/core-dev.tf#L46,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/ceb611bb819cced9f35ee19031584197caafddd1/fast/stages/02-security/core-dev.tf,2022-01-19 17:03:58+01:00,2022-06-23 07:04:35+02:00,6,1,0,1,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,277,infra/gcp/terraform/k8s-infra-oci-proxy/oci-proxy-sandbox.tf,infra/gcp/terraform/k8s-infra-oci-proxy/oci-proxy-sandbox.tf,0,//todo,//TODO: create a dedicated service account for auto-deployment,"//Ensure gcb-builder can auto-deploy registry-sandbox.k8s.io 
 //TODO: create a dedicated service account for auto-deployment","data ""google_project"" ""k8s_infra_staging_tools"" {
  project_id = ""k8s-staging-infra-tools""
}
",data,,,94,0.0,622302ef9a54357cd041f080e16bc7da4682829f,585608eaa86847071620860d476b7fb781794e33,https://github.com/kubernetes/k8s.io/blob/622302ef9a54357cd041f080e16bc7da4682829f/infra/gcp/terraform/k8s-infra-oci-proxy/oci-proxy-sandbox.tf#L94,https://github.com/kubernetes/k8s.io/blob/585608eaa86847071620860d476b7fb781794e33/infra/gcp/terraform/k8s-infra-oci-proxy/oci-proxy-sandbox.tf#L0,2022-04-21 17:50:25+02:00,2023-04-04 15:39:35-07:00,7,2,0,1,0,1,0,0,0,0
https://github.com/chanzuckerberg/cztack,128,aws-iam-role/main.tf,aws-iam-role/main.tf,0,hack,# Slight hack so Terraform can get the size statically during the plan.,"# Slight hack so Terraform can get the size statically during the plan. 
 # Simply passing the list to `for_each` throws an Invalid for_each argument","locals {
  # Slight hack so Terraform can get the size statically during the plan. 
  # Simply passing the list to `for_each` throws an Invalid for_each argument
  attached_policies_names_arns = zipmap(var.attached_policies_arns, var.attached_policies_arns)

  tags = {
    project   = var.project
    env       = var.env
    service   = var.service
    owner     = var.owner
    managedBy = ""terraform""
  }
}
",locals,"locals {
  tags = {
    project   = var.project
    env       = var.env
    service   = var.service
    owner     = var.owner
    managedBy = ""terraform""
  }
}
",locals,2,,4008493ab394e50f6def8dc22d9297f29556e638,216fe141b5aebe1d4eb594f10016eed34716d6fc,https://github.com/chanzuckerberg/cztack/blob/4008493ab394e50f6def8dc22d9297f29556e638/aws-iam-role/main.tf#L2,https://github.com/chanzuckerberg/cztack/blob/216fe141b5aebe1d4eb594f10016eed34716d6fc/aws-iam-role/main.tf,2020-12-16 10:47:02+08:00,2020-12-16 23:03:48+08:00,2,1,0,1,1,0,0,0,0,0
https://github.com/nasa/cumulus,28,example/cumulus-tf/main.tf,example/cumulus-tf/main.tf,0,todo,# TODO This should be coming from the ingest module,# TODO This should be coming from the ingest module,"module ""cumulus"" {
  source = ""../../tf-modules/cumulus""

  prefix = var.prefix

  vpc_id            = var.vpc_id
  lambda_subnet_ids = var.subnet_ids

  ecs_cluster_instance_subnet_ids = var.subnet_ids
  ecs_cluster_min_size            = 1
  ecs_cluster_desired_size        = 1
  ecs_cluster_max_size            = 2

  urs_url             = ""https://uat.urs.earthdata.nasa.gov""
  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password

  cmr_client_id   = var.cmr_client_id
  cmr_environment = ""UAT""
  cmr_username    = var.cmr_username
  cmr_password    = var.cmr_password
  cmr_provider    = var.cmr_provider

  permissions_boundary_arn = var.permissions_boundary_arn

  system_bucket     = var.system_bucket
  public_buckets    = var.public_buckets
  protected_buckets = var.protected_buckets
  private_buckets   = var.private_buckets

  elasticsearch_domain_arn        = data.terraform_remote_state.data_persistence.outputs.elasticsearch_domain_arn
  elasticsearch_hostname          = data.terraform_remote_state.data_persistence.outputs.elasticsearch_hostname
  elasticsearch_security_group_id = data.terraform_remote_state.data_persistence.outputs.elasticsearch_security_group_id

  dynamo_tables = data.terraform_remote_state.data_persistence.outputs.dynamo_tables

  archive_api_port = 8000

  token_secret = var.token_secret

  archive_api_users = [
    ""jennyhliu"",
    ""jmcampbell"",
    ""jnorton1"",
    ""kbaynes"",
    ""kkelly"",
    ""kovarik"",
    ""lfrederick"",
    ""matthewsavoie"",
    ""mboyd"",
    ""menno.vandiermen"",
    ""mhuffnagle2"",
    ""pquinn1""
  ]

  # TODO This should be coming from the ingest module
  kinesis_inbound_event_logger = ""${var.prefix}-KinesisInboundEventLogger""

  distribution_url = var.distribution_url
}
",module,"module ""cumulus"" {
  source = ""../../tf-modules/cumulus""

  prefix = var.prefix

  vpc_id            = var.vpc_id
  lambda_subnet_ids = var.subnet_ids

  ecs_cluster_instance_subnet_ids = var.subnet_ids
  ecs_cluster_min_size            = 1
  ecs_cluster_desired_size        = 1
  ecs_cluster_max_size            = 2

  urs_url             = ""https://uat.urs.earthdata.nasa.gov""
  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password

  cmr_client_id   = var.cmr_client_id
  cmr_environment = ""UAT""
  cmr_username    = var.cmr_username
  cmr_password    = var.cmr_password
  cmr_provider    = var.cmr_provider

  permissions_boundary_arn = var.permissions_boundary_arn

  system_bucket     = var.system_bucket
  public_buckets    = var.public_buckets
  protected_buckets = var.protected_buckets
  private_buckets   = var.private_buckets

  elasticsearch_domain_arn        = data.terraform_remote_state.data_persistence.outputs.elasticsearch_domain_arn
  elasticsearch_hostname          = data.terraform_remote_state.data_persistence.outputs.elasticsearch_hostname
  elasticsearch_security_group_id = data.terraform_remote_state.data_persistence.outputs.elasticsearch_security_group_id

  dynamo_tables = data.terraform_remote_state.data_persistence.outputs.dynamo_tables

  archive_api_port = 8000

  token_secret = var.token_secret

  archive_api_users = [
    ""jennyhliu"",
    ""jmcampbell"",
    ""jnorton1"",
    ""kbaynes"",
    ""kkelly"",
    ""kovarik"",
    ""lfrederick"",
    ""matthewsavoie"",
    ""mboyd"",
    ""menno.vandiermen"",
    ""mhuffnagle2"",
    ""pquinn1""
  ]

  distribution_url = var.distribution_url
}
",module,66,,ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7,59283cd10c3891258816b76160848a494ffc35b7,https://github.com/nasa/cumulus/blob/ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7/example/cumulus-tf/main.tf#L66,https://github.com/nasa/cumulus/blob/59283cd10c3891258816b76160848a494ffc35b7/example/cumulus-tf/main.tf,2019-08-14 14:23:38-04:00,2019-08-16 13:47:55-04:00,2,1,1,1,0,0,1,0,0,0
https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner,306,init.tf,init.tf,0,implementation,# This method is a stub which could be replaced by a more practical helm implementation,"# Upload the calico patch config, for the kustomization of the calico manifest 
 # This method is a stub which could be replaced by a more practical helm implementation","resource ""null_resource"" ""kustomization"" {
  connection {
    user           = ""root""
    private_key    = var.ssh_private_key
    agent_identity = local.ssh_agent_identity
    host           = module.control_planes[keys(module.control_planes)[0]].ipv4_address
    port           = var.ssh_port
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content = yamlencode({
      apiVersion = ""kustomize.config.k8s.io/v1beta1""
      kind       = ""Kustomization""

      resources = concat(
        [
          ""https://github.com/hetznercloud/hcloud-cloud-controller-manager/releases/download/${local.ccm_version}/ccm-networks.yaml"",
          ""https://github.com/weaveworks/kured/releases/download/${local.kured_version}/kured-${local.kured_version}-dockerhub.yaml"",
          ""https://raw.githubusercontent.com/rancher/system-upgrade-controller/master/manifests/system-upgrade-controller.yaml"",
        ],
        var.disable_hetzner_csi ? [] : [
          ""hcloud-csi.yml""
        ],
        lookup(local.ingress_controller_install_resources, local.ingress_controller, []),
        lookup(local.cni_install_resources, var.cni_plugin, []),
        var.enable_longhorn ? [""longhorn.yaml""] : [],
        var.enable_cert_manager || var.enable_rancher ? [""cert_manager.yaml""] : [],
        var.enable_rancher ? [""rancher.yaml""] : [],
        var.rancher_registration_manifest_url != """" ? [var.rancher_registration_manifest_url] : []
      ),
      patchesStrategicMerge = concat(
        [
          file(""${path.module}/kustomize/system-upgrade-controller.yaml""),
          ""kured.yaml"",
          ""ccm.yaml"",
        ],
        lookup(local.cni_install_resource_patches, var.cni_plugin, [])
      )
    })
    destination = ""/var/post_install/kustomization.yaml""
  }

  # Upload traefik ingress controller config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/traefik_ingress.yaml.tpl"",
      {
        values = indent(4, trimspace(local.traefik_values))
    })
    destination = ""/var/post_install/traefik_ingress.yaml""
  }

  # Upload nginx ingress controller config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/nginx_ingress.yaml.tpl"",
      {
        values = indent(4, trimspace(local.nginx_values))
    })
    destination = ""/var/post_install/nginx_ingress.yaml""
  }

  # Upload the CCM patch config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/ccm.yaml.tpl"",
      {
        cluster_cidr_ipv4   = local.cluster_cidr_ipv4
        default_lb_location = var.load_balancer_location
        using_klipper_lb    = local.using_klipper_lb
    })
    destination = ""/var/post_install/ccm.yaml""
  }

  # Upload the calico patch config, for the kustomization of the calico manifest
  # This method is a stub which could be replaced by a more practical helm implementation
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/calico.yaml.tpl"",
      {
        values = trimspace(local.calico_values)
    })
    destination = ""/var/post_install/calico.yaml""
  }

  # Upload the cilium install file
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cilium.yaml.tpl"",
      {
        values = indent(4, trimspace(local.cilium_values))
    })
    destination = ""/var/post_install/cilium.yaml""
  }

  # Upload the system upgrade controller plans config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/plans.yaml.tpl"",
      {
        channel = var.initial_k3s_channel
    })
    destination = ""/var/post_install/plans.yaml""
  }

  # Upload the Longhorn config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/longhorn.yaml.tpl"",
      {
        longhorn_namespace  = var.longhorn_namespace
        longhorn_repository = var.longhorn_repository
        values              = indent(4, trimspace(local.longhorn_values))
    })
    destination = ""/var/post_install/longhorn.yaml""
  }

  # Upload the cert-manager config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cert_manager.yaml.tpl"",
      {
        values = indent(4, trimspace(local.cert_manager_values))
    })
    destination = ""/var/post_install/cert_manager.yaml""
  }

  # Upload the Rancher config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/rancher.yaml.tpl"",
      {
        rancher_install_channel = var.rancher_install_channel
        values                  = indent(4, trimspace(local.rancher_values))
    })
    destination = ""/var/post_install/rancher.yaml""
  }

  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/kured.yaml.tpl"",
      {
        options = local.kured_options
      }
    )
    destination = ""/var/post_install/kured.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""set -ex"",
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${hcloud_network.k3s.name} --dry-run=client -o yaml | kubectl apply -f -"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token} --dry-run=client -o yaml | kubectl apply -f -"",
      ""curl https://raw.githubusercontent.com/hetznercloud/csi-driver/${local.csi_version}/deploy/kubernetes/hcloud-csi.yml | sed -e 's|k8s.gcr.io|registry.k8s.io|g' > /var/post_install/hcloud-csi.yml""
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = concat([
      ""set -ex"",

      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /var/post_install/kustomization.yaml"",

      # Wait for k3s to become ready (we check one more time) because in some edge cases,
      # the cluster had become unvailable for a few seconds, at this very instant.
      <<-EOT
      timeout 180 bash <<EOF
        until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do
          echo ""Waiting for the cluster to become ready...""
          sleep 2
        done
      EOF
      EOT
      ]
      ,

      [
        # Ready, set, go for the kustomization
        ""kubectl apply -k /var/post_install"",
        ""echo 'Waiting for the system-upgrade-controller deployment to become available...'"",
        ""kubectl -n system-upgrade wait --for=condition=available --timeout=180s deployment/system-upgrade-controller"",
        ""sleep 5"", # important as the system upgrade controller CRDs sometimes don't get ready right away, especially with Cilium.
        ""kubectl -n system-upgrade apply -f /var/post_install/plans.yaml""
      ],
      local.has_external_load_balancer ? [] : [
        <<-EOT
      timeout 180 bash <<EOF
      until [ -n ""\$(kubectl get -n ${lookup(local.ingress_controller_namespace_names, local.ingress_controller)} service/${lookup(local.ingress_controller_service_names, local.ingress_controller)} --output=jsonpath='{.status.loadBalancer.ingress[0].${var.lb_hostname != """" ? ""hostname"" : ""ip""}}' 2> /dev/null)"" ]; do
          echo ""Waiting for load-balancer to get an IP...""
          sleep 2
      done
      EOF
      EOT
    ])
  }

  depends_on = [
    null_resource.first_control_plane,
    random_password.rancher_bootstrap,
    hcloud_volume.longhorn_volume
  ]
}
",resource,"resource ""null_resource"" ""kustomization"" {
  triggers = {
    # Redeploy helm charts when the underlying values change
    helm_values_yaml = join(""---\n"", [
      local.traefik_values,
      local.nginx_values,
      local.calico_values,
      local.cilium_values,
      local.longhorn_values,
      local.csi_driver_smb_values,
      local.cert_manager_values,
      local.rancher_values
    ])
    # Redeploy when versions of addons need to be updated
    versions = join(""\n"", [
      coalesce(var.initial_k3s_channel, ""N/A""),
      coalesce(var.cluster_autoscaler_version, ""N/A""),
      coalesce(var.hetzner_ccm_version, ""N/A""),
      coalesce(var.hetzner_csi_version, ""N/A""),
      coalesce(var.kured_version, ""N/A""),
      coalesce(var.calico_version, ""N/A""),
      coalesce(var.cilium_version, ""N/A""),
      coalesce(var.traefik_version, ""N/A""),
      coalesce(var.nginx_version, ""N/A""),
    ])
    options = join(""\n"", [
      for option, value in local.kured_options : ""${option}=${value}""
    ])
  }

  connection {
    user           = ""root""
    private_key    = var.ssh_private_key
    agent_identity = local.ssh_agent_identity
    host           = module.control_planes[keys(module.control_planes)[0]].ipv4_address
    port           = var.ssh_port
  }

  # Upload kustomization.yaml, containing Hetzner CSI & CSM, as well as kured.
  provisioner ""file"" {
    content     = local.kustomization_backup_yaml
    destination = ""/var/post_install/kustomization.yaml""
  }

  # Upload traefik ingress controller config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/traefik_ingress.yaml.tpl"",
      {
        version          = var.traefik_version
        values           = indent(4, trimspace(local.traefik_values))
        target_namespace = local.ingress_controller_namespace
    })
    destination = ""/var/post_install/traefik_ingress.yaml""
  }

  # Upload nginx ingress controller config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/nginx_ingress.yaml.tpl"",
      {
        version          = var.nginx_version
        values           = indent(4, trimspace(local.nginx_values))
        target_namespace = local.ingress_controller_namespace
    })
    destination = ""/var/post_install/nginx_ingress.yaml""
  }

  # Upload the CCM patch config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/ccm.yaml.tpl"",
      {
        cluster_cidr_ipv4   = var.cluster_ipv4_cidr
        default_lb_location = var.load_balancer_location
        using_klipper_lb    = local.using_klipper_lb
    })
    destination = ""/var/post_install/ccm.yaml""
  }

  # Upload the calico patch config, for the kustomization of the calico manifest
  # This method is a stub which could be replaced by a more practical helm implementation
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/calico.yaml.tpl"",
      {
        values = trimspace(local.calico_values)
    })
    destination = ""/var/post_install/calico.yaml""
  }

  # Upload the cilium install file
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cilium.yaml.tpl"",
      {
        values  = indent(4, trimspace(local.cilium_values))
        version = var.cilium_version
    })
    destination = ""/var/post_install/cilium.yaml""
  }

  # Upload the system upgrade controller plans config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/plans.yaml.tpl"",
      {
        channel          = var.initial_k3s_channel
        disable_eviction = !var.system_upgrade_enable_eviction
    })
    destination = ""/var/post_install/plans.yaml""
  }

  # Upload the Longhorn config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/longhorn.yaml.tpl"",
      {
        longhorn_namespace  = var.longhorn_namespace
        longhorn_repository = var.longhorn_repository
        values              = indent(4, trimspace(local.longhorn_values))
    })
    destination = ""/var/post_install/longhorn.yaml""
  }

  # Upload the csi-driver-smb config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/csi-driver-smb.yaml.tpl"",
      {
        values = indent(4, trimspace(local.csi_driver_smb_values))
    })
    destination = ""/var/post_install/csi-driver-smb.yaml""
  }

  # Upload the cert-manager config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/cert_manager.yaml.tpl"",
      {
        values = indent(4, trimspace(local.cert_manager_values))
    })
    destination = ""/var/post_install/cert_manager.yaml""
  }

  # Upload the Rancher config
  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/rancher.yaml.tpl"",
      {
        rancher_install_channel = var.rancher_install_channel
        values                  = indent(4, trimspace(local.rancher_values))
    })
    destination = ""/var/post_install/rancher.yaml""
  }

  provisioner ""file"" {
    content = templatefile(
      ""${path.module}/templates/kured.yaml.tpl"",
      {
        options = local.kured_options
      }
    )
    destination = ""/var/post_install/kured.yaml""
  }

  # Deploy secrets, logging is automatically disabled due to sensitive variables
  provisioner ""remote-exec"" {
    inline = [
      ""set -ex"",
      ""kubectl -n kube-system create secret generic hcloud --from-literal=token=${var.hcloud_token} --from-literal=network=${data.hcloud_network.k3s.name} --dry-run=client -o yaml | kubectl apply -f -"",
      ""kubectl -n kube-system create secret generic hcloud-csi --from-literal=token=${var.hcloud_token} --dry-run=client -o yaml | kubectl apply -f -"",
      local.csi_version != null ? ""curl https://raw.githubusercontent.com/hetznercloud/csi-driver/${coalesce(local.csi_version, ""v2.4.0"")}/deploy/kubernetes/hcloud-csi.yml -o /var/post_install/hcloud-csi.yml"" : ""echo 'Skipping hetzner csi.'""
    ]
  }

  # Deploy our post-installation kustomization
  provisioner ""remote-exec"" {
    inline = concat([
      ""set -ex"",

      # This ugly hack is here, because terraform serializes the
      # embedded yaml files with ""- |2"", when there is more than
      # one yamldocument in the embedded file. Kustomize does not understand
      # that syntax and tries to parse the blocks content as a file, resulting
      # in weird errors. so gnu sed with funny escaping is used to
      # replace lines like ""- |3"" by ""- |"" (yaml block syntax).
      # due to indendation this should not changes the embedded
      # manifests themselves
      ""sed -i 's/^- |[0-9]\\+$/- |/g' /var/post_install/kustomization.yaml"",

      # Wait for k3s to become ready (we check one more time) because in some edge cases,
      # the cluster had become unvailable for a few seconds, at this very instant.
      <<-EOT
      timeout 360 bash <<EOF
        until [[ ""\$(kubectl get --raw='/readyz' 2> /dev/null)"" == ""ok"" ]]; do
          echo ""Waiting for the cluster to become ready...""
          sleep 2
        done
      EOF
      EOT
      ]
      ,

      [
        # Ready, set, go for the kustomization
        ""kubectl apply -k /var/post_install"",
        ""echo 'Waiting for the system-upgrade-controller deployment to become available...'"",
        ""kubectl -n system-upgrade wait --for=condition=available --timeout=360s deployment/system-upgrade-controller"",
        ""sleep 7"", # important as the system upgrade controller CRDs sometimes don't get ready right away, especially with Cilium.
        ""kubectl -n system-upgrade apply -f /var/post_install/plans.yaml""
      ],
      local.has_external_load_balancer ? [] : [
        <<-EOT
      timeout 360 bash <<EOF
      until [ -n ""\$(kubectl get -n ${local.ingress_controller_namespace} service/${lookup(local.ingress_controller_service_names, var.ingress_controller)} --output=jsonpath='{.status.loadBalancer.ingress[0].${var.lb_hostname != """" ? ""hostname"" : ""ip""}}' 2> /dev/null)"" ]; do
          echo ""Waiting for load-balancer to get an IP...""
          sleep 2
      done
      EOF
      EOT
    ])
  }

  depends_on = [
    hcloud_load_balancer.cluster,
    null_resource.control_planes,
    random_password.rancher_bootstrap,
    hcloud_volume.longhorn_volume
  ]
}
",resource,163,195.0,c6c30138e6871a8fb690eda917d1f81f8d0e09e0,7b82c726a6d9bc3da643eefbee1b587e0126d889,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/c6c30138e6871a8fb690eda917d1f81f8d0e09e0/init.tf#L163,https://github.com/kube-hetzner/terraform-hcloud-kube-hetzner/blob/7b82c726a6d9bc3da643eefbee1b587e0126d889/init.tf#L195,2023-01-29 04:20:09+01:00,2024-05-06 13:56:24+02:00,41,0,1,1,0,0,0,1,0,0
https://github.com/Worklytics/psoxy,203,infra/modules/psoxy-package/main.tf,infra/modules/psoxy-package/main.tf,0,# todo,# TODO: solve weirdness with AWS case: AWS lambda is deployed directly by Terraform from a code,"# packages psoxy for deployment  
 # TODO: solve weirdness with AWS case: AWS lambda is deployed directly by Terraform from a code 
 # package built by terraform, but terraform doesn't understand this dependency in its plan. we make 
 # it work via explicit 'depends_on', but Terraform still gives 'error inconsistent plan' as its 
 # original plan presumed the sha-256 of the package","locals {
  path_to_core_module    = ""${var.path_to_psoxy_java}/core""
  path_to_impl_module    = ""${var.path_to_psoxy_java}/impl/${var.implementation}""
  path_to_deployment_jar = ""${local.path_to_impl_module}/target/psoxy-${var.implementation}-1.0-SNAPSHOT.jar""
}
",locals,"data ""external"" ""deployment_package"" {
  count = var.deployment_bundle == null ? 1 : 0

  program = [
    ""${path.module}/build.sh"",
    var.path_to_psoxy_java,
    var.implementation,
    var.force_bundle ? ""--force_bundle"" : """"
  ]
}
",data,3,3.0,450629d23c2f1ec4eb34e057d6d2c3ae5215abf2,02ae30df5ec3b392705780a5f58ae90ca40f01d7,https://github.com/Worklytics/psoxy/blob/450629d23c2f1ec4eb34e057d6d2c3ae5215abf2/infra/modules/psoxy-package/main.tf#L3,https://github.com/Worklytics/psoxy/blob/02ae30df5ec3b392705780a5f58ae90ca40f01d7/infra/modules/psoxy-package/main.tf#L3,2022-01-19 08:22:58-08:00,2023-06-21 21:02:46+00:00,16,0,1,1,1,0,0,0,0,0
https://github.com/uyuni-project/sumaform,318,modules/aws/host/main.tf,backend_modules/aws/host/main.tf,1,hack,# HACK: ephemeral block devices are defined in any case,"# HACK: ephemeral block devices are defined in any case 
 # they will only be used for instance types that provide them","resource ""aws_instance"" ""instance"" {
  ami = ""${var.ami}""
  instance_type = ""${var.instance_type}""
  count = ""${var.count}""
  availability_zone = ""${var.availability_zone}""
  key_name = ""${var.key_name}""
  subnet_id = ""${var.private_subnet_id}""
  vpc_security_group_ids = [""${var.private_security_group_id}""]

  root_block_device {
    volume_size = ""${var.volume_size}""
  }

  # HACK: ephemeral block devices are defined in any case
  # they will only be used for instance types that provide them
  ephemeral_block_device {
    device_name = ""xvda""
    virtual_name = ""ephemeral0""
  }

  ephemeral_block_device {
    device_name = ""xvdb""
    virtual_name = ""ephemeral1""
  }

  tags {
    Name = ""${var.name_prefix}-${var.name}-${count.index}""
  }
}
",resource,,,18,0.0,4c84d7d4843e3689fe1bfe2935361dc81a0118d1,05d52dd67d43ae0332c9d3b41cf9b2d08a0248f8,https://github.com/uyuni-project/sumaform/blob/4c84d7d4843e3689fe1bfe2935361dc81a0118d1/modules/aws/host/main.tf#L18,https://github.com/uyuni-project/sumaform/blob/05d52dd67d43ae0332c9d3b41cf9b2d08a0248f8/backend_modules/aws/host/main.tf#L0,2017-10-18 14:30:01+02:00,2020-05-04 19:34:32+01:00,21,2,1,0,0,0,0,0,0,0
https://github.com/uyuni-project/sumaform,181,modules/libvirt/package_mirror/main.tf,modules/libvirt/package_mirror/main.tf,0,hack,"// HACK: evaluates to ""terraform-network"" if bridge is empty, """" otherwise","// HACK: evaluates to ""terraform-network"" if bridge is empty, """" otherwise","resource ""libvirt_domain"" ""domain"" {
  name = ""package-mirror""
  memory = 512
  vcpu = 1
  running = ""${var.running}""

  disk {
    volume_id = ""${libvirt_volume.main_disk.id}""
  }
  disk {
    volume_id = ""${libvirt_volume.data_disk.id}""
  }

  network_interface {
    wait_for_lease = true
    // HACK: evaluates to ""terraform-network"" if bridge is empty, """" otherwise
    network_name = ""${element(list(""terraform-network"", """"), replace(replace(var.bridge, ""/.+/"", ""1""), ""/^$/"", ""0""))}""
    bridge = ""${var.bridge}""
  }

  connection {
    user = ""root""
    password = ""linux""
  }

  provisioner ""file"" {
    source = ""salt""
    destination = ""/srv""
  }

  provisioner ""file"" {
    content = <<EOF

hostname: package-mirror
domain: ${var.domain}
use-avahi: True
role: package-mirror
cc_username: ${var.cc_username}
cc_password: ${var.cc_password}
data_disk_device: vdb

EOF

    destination = ""/etc/salt/grains""
  }

  provisioner ""remote-exec"" {
    inline = [
      ""salt-call --local state.sls terraform-resource"",
      ""salt-call --local state.highstate""
    ]
  }
}
",resource,"resource ""libvirt_domain"" ""domain"" {
  name = ""package-mirror""
  memory = 512
  vcpu = 1
  running = ""${var.running}""

  disk {
    volume_id = ""${libvirt_volume.main_disk.id}""
  }
  disk {
    volume_id = ""${libvirt_volume.data_disk.id}""
  }

  network_interface {
    wait_for_lease = true
    // HACK: evaluates to ""nat_network"" if bridge is empty, """" otherwise
    network_name = ""${element(list(""${var.name_prefix}nat_network"", """"), replace(replace(var.bridge, ""/.+/"", ""1""), ""/^$/"", ""0""))}""
    bridge = ""${var.bridge}""
    mac = ""${var.mac}""
  }

  connection {
    user = ""root""
    password = ""linux""
  }

  provisioner ""file"" {
    source = ""salt""
    destination = ""/srv""
  }

  provisioner ""file"" {
    content = <<EOF

hostname: package-mirror
domain: ${var.domain}
use-avahi: True
role: package-mirror
cc_username: ${var.cc_username}
cc_password: ${var.cc_password}
data_disk_device: vdb

EOF

    destination = ""/etc/salt/grains""
  }

  provisioner ""remote-exec"" {
    inline = [
      ""salt-call --local state.sls terraform-resource"",
      ""salt-call --local state.highstate""
    ]
  }
}
",resource,31,,6654b3387922337448a757823b9cd540396f8e17,2caf37c92febd157d60a29f7c1151bee01524c62,https://github.com/uyuni-project/sumaform/blob/6654b3387922337448a757823b9cd540396f8e17/modules/libvirt/package_mirror/main.tf#L31,https://github.com/uyuni-project/sumaform/blob/2caf37c92febd157d60a29f7c1151bee01524c62/modules/libvirt/package_mirror/main.tf,2016-11-04 16:33:43+01:00,2016-11-04 16:33:43+01:00,4,1,1,1,0,0,1,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,57,modules/emr-on-eks/main.tf,modules/emr-on-eks/main.tf,0,todo,# TODO Replace this resource once the provider is available for aws emr-containers,"# Update trust relationship for job execution role 
 # Use the below command in shell script to assume a different role 
 #   $(aws sts assume-role --role-arn ${local.pass_local_deployment_role} --role-session-name terraform_run_instance_refresh --query 'Credentials.[`export#AWS_ACCESS_KEY_ID=`,AccessKeyId,`#AWS_SECRET_ACCESS_KEY=`,SecretAccessKey,`#AWS_SESSION_TOKEN=`,SessionToken]' --output text | sed $'s/\t//g' | sed 's/#/ /g') 
 # TODO Replace this resource once the provider is available for aws emr-containers","resource ""null_resource"" ""update_trust_policy"" {
  provisioner ""local-exec"" {
    interpreter = [""/bin/sh"", ""-c""]
    environment = {
      AWS_DEFAULT_REGION = data.aws_region.current.id
    }
    command = <<EOF
set -e

aws emr-containers update-role-trust-policy \
--cluster-name ${var.eks_cluster_id} \
--namespace ${kubernetes_namespace.spark.id} \
--role-name ${aws_iam_role.emr_on_eks_execution.id}

EOF
  }
  //  triggers = {
  //    always_run = timestamp()
  //  }
  depends_on = [kubernetes_namespace.spark, aws_iam_role.emr_on_eks_execution]
}
",resource,,,146,0.0,634c8579408be9f42cae5c089a8845a193e12c33,19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/634c8579408be9f42cae5c089a8845a193e12c33/modules/emr-on-eks/main.tf#L146,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/19c7cfd40c4b83ec534b3ffcd70c3a3efc42ffd1/modules/emr-on-eks/main.tf#L0,2021-10-15 14:39:05+01:00,2023-06-05 10:07:47-04:00,14,2,1,0,1,0,0,1,0,0
https://github.com/uyuni-project/sumaform,204,modules/libvirt/package_mirror/main.tf,modules/libvirt/package_mirror/main.tf,0,hack,"// HACK: evaluates to ""nat_network"" if bridge is empty, """" otherwise","// HACK: evaluates to ""nat_network"" if bridge is empty, """" otherwise","resource ""libvirt_domain"" ""domain"" {
  name = ""package-mirror""
  memory = 512
  vcpu = 1
  running = ""${var.running}""

  disk {
    volume_id = ""${libvirt_volume.main_disk.id}""
  }
  disk {
    volume_id = ""${libvirt_volume.data_disk.id}""
  }

  network_interface {
    wait_for_lease = true
    // HACK: evaluates to ""nat_network"" if bridge is empty, """" otherwise
    network_name = ""${element(list(""${var.name_prefix}nat_network"", """"), replace(replace(var.bridge, ""/.+/"", ""1""), ""/^$/"", ""0""))}""
    bridge = ""${var.bridge}""
    mac = ""${var.mac}""
  }

  connection {
    user = ""root""
    password = ""linux""
  }

  provisioner ""file"" {
    source = ""salt""
    destination = ""/srv""
  }

  provisioner ""file"" {
    content = <<EOF

hostname: package-mirror
domain: ${var.domain}
use-avahi: True
role: package-mirror
cc_username: ${var.cc_username}
cc_password: ${var.cc_password}
data_disk_device: vdb

EOF

    destination = ""/etc/salt/grains""
  }

  provisioner ""remote-exec"" {
    inline = [
      ""salt-call --local state.sls terraform-resource"",
      ""salt-call --local state.highstate""
    ]
  }
}
",resource,the block associated got renamed or deleted,,31,,2caf37c92febd157d60a29f7c1151bee01524c62,0d3a83d22f57360dec522bcf1e5dd7adb3b9f9c3,https://github.com/uyuni-project/sumaform/blob/2caf37c92febd157d60a29f7c1151bee01524c62/modules/libvirt/package_mirror/main.tf#L31,https://github.com/uyuni-project/sumaform/blob/0d3a83d22f57360dec522bcf1e5dd7adb3b9f9c3/modules/libvirt/package_mirror/main.tf,2016-11-04 16:33:43+01:00,2016-11-04 17:34:25+01:00,2,1,1,1,0,0,1,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1595,blueprints/factories/project-factory/outputs.tf,modules/project-factory/outputs.tf,1,# todo,# TODO: group by project,# TODO: group by project,"output ""service_accounts"" {
  description = ""Service account emails.""
  # TODO: group by project
  value = {
    for k, v in module.service-accounts : k => v.email
  }
}
",output,"output ""service_accounts"" {
  description = ""Service account emails.""
  value = {
    for k, v in module.service-accounts : k => v.email
  }
}
",output,24,,819894d2bab4b440f1b52b1ac8035912fb107004,39139e2fa19df5e42df781979c7c60bc24bb156c,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/819894d2bab4b440f1b52b1ac8035912fb107004/blueprints/factories/project-factory/outputs.tf#L24,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/39139e2fa19df5e42df781979c7c60bc24bb156c/modules/project-factory/outputs.tf,2023-08-20 09:44:20+02:00,2024-03-05 13:13:02+01:00,2,1,0,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,52,infra/modules/gcp-oauth-refresh-strategy/main.tf,infra/modules/gcp-oauth-refresh-strategy/main.tf,0,implementation,"# composing them is complex and exposes implementation details (eg, that refresh token strategy","# not 'proper' terraform style to invoke modules hierarchically rather than via composition; but 
 # composing them is complex and exposes implementation details (eg, that refresh token strategy 
 # requires two secrets)""","module ""client_secret_grant"" {
  source = ""../gcp-secret-user-version-adder""

  project_id     = var.project_id
  secret_id      = google_secret_manager_secret.client_secret.secret_id
  grant_duration = var.token_adder_grant_duration
  user_emails    = var.token_adder_user_emails
}
",module,,,29,0.0,1fd1678613e7c2ef7cf06efe921c58ec6e0e4a9a,f6f2f7e067e314e015df718052bc6b672925bfd4,https://github.com/Worklytics/psoxy/blob/1fd1678613e7c2ef7cf06efe921c58ec6e0e4a9a/infra/modules/gcp-oauth-refresh-strategy/main.tf#L29,https://github.com/Worklytics/psoxy/blob/f6f2f7e067e314e015df718052bc6b672925bfd4/infra/modules/gcp-oauth-refresh-strategy/main.tf#L0,2021-10-26 11:04:53-07:00,2022-11-10 23:38:02+01:00,3,2,0,1,0,1,0,0,0,0
https://github.com/nasa/cumulus,31,tf-modules/cumulus/archive.tf,tf-modules/cumulus/archive.tf,0,todo,# TODO This should eventually come from the ingest module,# TODO This should eventually come from the ingest module,"module ""archive"" {
  source = ""../archive""

  prefix = var.prefix

  permissions_boundary_arn = var.permissions_boundary_arn

  ecs_cluster_name = aws_ecs_cluster.default.name

  elasticsearch_domain_arn        = var.elasticsearch_domain_arn
  elasticsearch_hostname          = var.elasticsearch_hostname
  elasticsearch_security_group_id = var.elasticsearch_security_group_id

  ems_host = ""change-ems-host""

  system_bucket     = var.system_bucket
  public_buckets    = var.public_buckets
  protected_buckets = var.protected_buckets
  private_buckets   = var.private_buckets

  vpc_id            = var.vpc_id
  lambda_subnet_ids = var.lambda_subnet_ids

  cmr_client_id   = var.cmr_client_id
  cmr_environment = var.cmr_environment
  cmr_provider    = var.cmr_provider
  cmr_username    = var.cmr_username
  cmr_password    = var.cmr_password

  urs_url             = ""https://uat.urs.earthdata.nasa.gov""
  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password

  token_secret = var.token_secret

  dynamo_tables = var.dynamo_tables

  api_port = var.archive_api_port

  schedule_sf_function_arn      = data.aws_lambda_function.schedule_sf.arn
  message_consumer_function_arn = data.aws_lambda_function.message_consumer.arn
  # TODO This should eventually come from the ingest module
  kinesis_inbound_event_logger = var.kinesis_inbound_event_logger

  # TODO We need to figure out how to make this dynamic
  background_queue_name = ""backgroundProcessing""

  distribution_api_id = module.distribution.rest_api_id
  distribution_url    = module.distribution.distribution_url

  users = var.archive_api_users
}
",module,"module ""archive"" {
  source = ""../archive""

  prefix = var.prefix

  permissions_boundary_arn = var.permissions_boundary_arn

  lambda_processing_role_arn = aws_iam_role.lambda_processing.arn

  ecs_cluster_name = aws_ecs_cluster.default.name

  elasticsearch_domain_arn        = var.elasticsearch_domain_arn
  elasticsearch_hostname          = var.elasticsearch_hostname
  elasticsearch_security_group_id = var.elasticsearch_security_group_id

  ems_host = ""change-ems-host""

  system_bucket     = var.system_bucket
  public_buckets    = var.public_buckets
  protected_buckets = var.protected_buckets
  private_buckets   = var.private_buckets

  vpc_id            = var.vpc_id
  lambda_subnet_ids = var.lambda_subnet_ids

  cmr_client_id   = var.cmr_client_id
  cmr_environment = var.cmr_environment
  cmr_provider    = var.cmr_provider
  cmr_username    = var.cmr_username
  cmr_password    = var.cmr_password

  urs_url             = ""https://uat.urs.earthdata.nasa.gov""
  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password

  token_secret = var.token_secret

  dynamo_tables = var.dynamo_tables

  api_port = var.archive_api_port

  schedule_sf_function_arn                   = data.aws_lambda_function.schedule_sf.arn
  message_consumer_function_arn              = data.aws_lambda_function.message_consumer.arn
  kinesis_inbound_event_logger_function_name = module.ingest.kinesis_inbound_event_logger_function_name

  # TODO We need to figure out how to make this dynamic
  background_queue_name = ""backgroundProcessing""

  distribution_api_id = module.distribution.rest_api_id
  distribution_url    = module.distribution.distribution_url

  users = var.archive_api_users
}
",module,52,,ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7,59283cd10c3891258816b76160848a494ffc35b7,https://github.com/nasa/cumulus/blob/ad90977e68ab8e08ed1dda6f0cc022f825b5c3b7/tf-modules/cumulus/archive.tf#L52,https://github.com/nasa/cumulus/blob/59283cd10c3891258816b76160848a494ffc35b7/tf-modules/cumulus/archive.tf,2019-08-14 14:23:38-04:00,2019-08-16 13:47:55-04:00,2,1,0,1,0,0,1,0,0,0
https://github.com/returntocorp/semgrep-rules,4,terraform/aws/security/aws-ssm-document-logging-issues.tf,terraform/aws/security/aws-ssm-document-logging-issues.tf,0,# todo,# todoruleid: aws-ssm-document-logging-issues,# todoruleid: aws-ssm-document-logging-issues,"resource ""aws_ssm_document"" ""cw_enabled_not_encrypted_yaml"" {
  name          = ""SSM-SessionManagerRunShell""
  document_type = ""Session""

  document_format = ""YAML""

  # todoruleid: aws-ssm-document-logging-issues
  content = <<DOC
  schemaVersion: '1.0'
  description: Document to hold regional settings for Session Manager
  sessionType: Standard_Stream
  inputs:
    s3BucketName: ''
    s3KeyPrefix: ''
    s3EncryptionEnabled: false
    cloudWatchLogGroupName: 'example'
    cloudWatchEncryptionEnabled: false
    cloudWatchStreamingEnabled: true
    kmsKeyId: ''
    runAsEnabled: true
    runAsDefaultUser: ''
    idleSessionTimeout: '20'
    shellProfile:
      windows: ''
      linux: ''
DOC
}
",resource,"resource ""aws_ssm_document"" ""cw_enabled_not_encrypted_yaml"" {
  name          = ""SSM-SessionManagerRunShell""
  document_type = ""Session""

  document_format = ""YAML""

  # todoruleid: aws-ssm-document-logging-issues
  content = <<DOC
  schemaVersion: '1.0'
  description: Document to hold regional settings for Session Manager
  sessionType: Standard_Stream
  inputs:
    s3BucketName: ''
    s3KeyPrefix: ''
    s3EncryptionEnabled: false
    cloudWatchLogGroupName: 'example'
    cloudWatchEncryptionEnabled: false
    cloudWatchStreamingEnabled: true
    kmsKeyId: ''
    runAsEnabled: true
    runAsDefaultUser: ''
    idleSessionTimeout: '20'
    shellProfile:
      windows: ''
      linux: ''
DOC
}
",resource,267,267.0,eefcc40d66ea97ca14eb496031ad9a84388214e2,eefcc40d66ea97ca14eb496031ad9a84388214e2,https://github.com/returntocorp/semgrep-rules/blob/eefcc40d66ea97ca14eb496031ad9a84388214e2/terraform/aws/security/aws-ssm-document-logging-issues.tf#L267,https://github.com/returntocorp/semgrep-rules/blob/eefcc40d66ea97ca14eb496031ad9a84388214e2/terraform/aws/security/aws-ssm-document-logging-issues.tf#L267,2022-02-02 16:53:31-06:00,2022-02-02 16:53:31-06:00,1,0,1,1,0,0,0,0,1,0
https://github.com/cookpad/terraform-aws-eks,1,modules/asg_node_group/variables.tf,modules/asg_node_group/variables.tf,0,# todo,# TODO: add custom validation rule once the feature is stable https://www.terraform.io/docs/configuration/variables.html#custom-validation-rules,# TODO: add custom validation rule once the feature is stable https://www.terraform.io/docs/configuration/variables.html#custom-validation-rules,"variable ""labels"" {
  type        = map(string)
  default     = {}
  description = ""Labels that will be added to the kubernetes node. A qualified name must consist of alphanumeric characters, '-', '_' or '.', and must start and end with an alphanumeric character (e.g. 'MyName',  or 'my.name',  or '123-abc', regex used for validation is '([A-Za-z0-9][-A-Za-z0-9_.]*)?[A-Za-z0-9]') with an optional DNS subdomain prefix and '/' (e.g. 'example.com/MyName')""
  # TODO: add custom validation rule once the feature is stable https://www.terraform.io/docs/configuration/variables.html#custom-validation-rules
}
",variable,,,70,0.0,e8c595caffa0772127b635ed600e65e4fac97800,f5918bacf2ee295f233c047cb71488a47c248e52,https://github.com/cookpad/terraform-aws-eks/blob/e8c595caffa0772127b635ed600e65e4fac97800/modules/asg_node_group/variables.tf#L70,https://github.com/cookpad/terraform-aws-eks/blob/f5918bacf2ee295f233c047cb71488a47c248e52/modules/asg_node_group/variables.tf#L0,2020-02-21 15:19:07+00:00,2023-06-21 09:58:23+02:00,43,2,0,1,1,0,0,0,0,0
https://github.com/nasa/cumulus,22,tf-modules/cumulus/ecs_cluster.tf,tf-modules/cumulus/ecs_cluster.tf,0,todo,# TODO I don't like the fact that we're making an assumption here about the names of our tables,# TODO I don't like the fact that we're making an assumption here about the names of our tables,"data ""aws_iam_policy_document"" ""ecs_cluster_instance_policy"" {
  statement {
    actions   = [""dynamodb:UpdateItem""]
    resources = [data.aws_dynamodb_table.async_operations.arn]
  }

  statement {
    actions = [
      ""autoscaling:CompleteLifecycleAction"",
      ""autoscaling:DescribeAutoScalingInstances"",
      ""autoscaling:DescribeLifecycleHooks"",
      ""autoscaling:RecordLifecycleActionHeartbeat"",
      ""cloudwatch:GetMetricStatistics"",
      ""ec2:DescribeInstances"",
      ""ecr:BatchCheckLayerAvailability"",
      ""ecr:BatchGetImage"",
      ""ecr:GetAuthorizationToken"",
      ""ecr:GetDownloadUrlForLayer"",
      ""ecs:DeregisterContainerInstance"",
      ""ecs:DescribeClusters"",
      ""ecs:DescribeContainerInstances"",
      ""ecs:DescribeServices"",
      ""ecs:DiscoverPollEndpoint"",
      ""ecs:ListContainerInstances"",
      ""ecs:ListServices"",
      ""ecs:ListTaskDefinitions"",
      ""ecs:ListTasks"",
      ""ecs:Poll"",
      ""ecs:RegisterContainerInstance"",
      ""ecs:RunTask"",
      ""ecs:StartTelemetrySession"",
      ""ecs:Submit*"",
      ""ecs:UpdateContainerInstancesState"",
      ""lambda:GetFunction"",
      ""lambda:invokeFunction"",
      ""logs:CreateLogGroup"",
      ""logs:CreateLogStream"",
      ""logs:DescribeLogStreams"",
      ""logs:PutLogEvents"",
      ""ssm:GetParameter""
    ]
    resources = [""*""]
  }

  statement {
    actions = [
      ""states:DescribeActivity"",
      ""states:GetActivityTask"",
      ""states:GetExecutionHistory"",
      ""states:SendTaskFailure"",
      ""states:SendTaskSuccess""
    ]
    resources = [""arn:aws:states:*:*:*""]
  }

  statement {
    actions = [
      ""s3:GetAccelerateConfiguration"",
      ""s3:GetBucket*"",
      ""s3:GetLifecycleConfiguration"",
      ""s3:GetReplicationConfiguration"",
      ""s3:ListBucket*"",
      ""s3:PutAccelerateConfiguration"",
      ""s3:PutBucket*"",
      ""s3:PutLifecycleConfiguration"",
      ""s3:PutReplicationConfiguration""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}""]
  }

  statement {
    actions = [
      ""s3:AbortMultipartUpload"",
      ""s3:DeleteObject"",
      ""s3:DeleteObjectVersion"",
      ""s3:GetObject*"",
      ""s3:ListMultipartUploadParts"",
      ""s3:PutObject*""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}/*""]
  }

  statement {
    actions = [""dynamodb:Scan""]
    # TODO I don't like the fact that we're making an assumption here about the names of our tables
    resources = [""arn:aws:dynamodb:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:table/${var.prefix}-*""]
  }

  statement {
    actions = [
      ""es:ESHttpDelete"",
      ""es:ESHttpGet"",
      ""es:ESHttpHead"",
      ""es:ESHttpPost"",
      ""es:ESHttpPut""
    ]
    # TODO Get this value dynamically instead of from a variable
    resources = [var.elasticsearch_arn]
  }
}
",data,"data ""aws_iam_policy_document"" ""ecs_cluster_instance_policy"" {
  statement {
    actions   = [""dynamodb:UpdateItem""]
    resources = [var.dynamo_tables.async_operations.arn]
  }

  statement {
    actions = [
      ""autoscaling:CompleteLifecycleAction"",
      ""autoscaling:DescribeAutoScalingInstances"",
      ""autoscaling:DescribeLifecycleHooks"",
      ""autoscaling:RecordLifecycleActionHeartbeat"",
      ""cloudwatch:GetMetricStatistics"",
      ""ec2:DescribeInstances"",
      ""ecr:BatchCheckLayerAvailability"",
      ""ecr:BatchGetImage"",
      ""ecr:GetAuthorizationToken"",
      ""ecr:GetDownloadUrlForLayer"",
      ""ecs:DeregisterContainerInstance"",
      ""ecs:DescribeClusters"",
      ""ecs:DescribeContainerInstances"",
      ""ecs:DescribeServices"",
      ""ecs:DiscoverPollEndpoint"",
      ""ecs:ListContainerInstances"",
      ""ecs:ListServices"",
      ""ecs:ListTaskDefinitions"",
      ""ecs:ListTasks"",
      ""ecs:Poll"",
      ""ecs:RegisterContainerInstance"",
      ""ecs:RunTask"",
      ""ecs:StartTelemetrySession"",
      ""ecs:Submit*"",
      ""ecs:UpdateContainerInstancesState"",
      ""lambda:GetFunction"",
      ""lambda:invokeFunction"",
      ""logs:CreateLogGroup"",
      ""logs:CreateLogStream"",
      ""logs:DescribeLogStreams"",
      ""logs:PutLogEvents"",
      ""ssm:GetParameter""
    ]
    resources = [""*""]
  }

  statement {
    actions = [
      ""states:DescribeActivity"",
      ""states:GetActivityTask"",
      ""states:GetExecutionHistory"",
      ""states:SendTaskFailure"",
      ""states:SendTaskSuccess""
    ]
    resources = [""arn:aws:states:*:*:*""]
  }

  statement {
    actions = [
      ""s3:GetAccelerateConfiguration"",
      ""s3:GetBucket*"",
      ""s3:GetLifecycleConfiguration"",
      ""s3:GetReplicationConfiguration"",
      ""s3:ListBucket*"",
      ""s3:PutAccelerateConfiguration"",
      ""s3:PutBucket*"",
      ""s3:PutLifecycleConfiguration"",
      ""s3:PutReplicationConfiguration""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}""]
  }

  statement {
    actions = [
      ""s3:AbortMultipartUpload"",
      ""s3:DeleteObject"",
      ""s3:DeleteObjectVersion"",
      ""s3:GetObject*"",
      ""s3:ListMultipartUploadParts"",
      ""s3:PutObject*""
    ]
    resources = [for b in flatten([var.public_buckets, var.protected_buckets, var.private_buckets, var.system_bucket]) : ""arn:aws:s3:::${b}/*""]
  }

  statement {
    actions   = [""dynamodb:Scan""]
    resources = [for k, v in var.dynamo_tables : v.arn]
  }

  statement {
    actions = [
      ""es:ESHttpDelete"",
      ""es:ESHttpGet"",
      ""es:ESHttpHead"",
      ""es:ESHttpPost"",
      ""es:ESHttpPut""
    ]
    resources = [var.elasticsearch_domain_arn]
  }
}
",data,90,,1da53282470313085da6e713a94458500df71f6c,ff8e3e11f6c76726b63759d970e3289c743980ad,https://github.com/nasa/cumulus/blob/1da53282470313085da6e713a94458500df71f6c/tf-modules/cumulus/ecs_cluster.tf#L90,https://github.com/nasa/cumulus/blob/ff8e3e11f6c76726b63759d970e3289c743980ad/tf-modules/cumulus/ecs_cluster.tf,2019-08-02 16:32:51-04:00,2019-09-06 13:28:48-04:00,6,1,1,1,0,1,0,0,0,0
https://github.com/Worklytics/psoxy,2496,infra/modules/psoxy-constants/main.tf,infra/modules/psoxy-constants/main.tf,0,# todo,# TODO: confirm that this is indeed the same list (believe it is),"# TODO: add list of permissions, which customer could use to create custom role as alternative   
 # TODO: confirm that this is indeed the same list (believe it is)","locals {

  # AWS Managed polices
  # see: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#aws-managed-policies
  required_aws_roles_to_provision_host = {
    ""arn:aws:iam::aws:policy/IAMFullAccess""        = ""IAMFullAccess""
    ""arn:aws:iam::aws:policy/AmazonS3FullAccess""   = ""AmazonS3FullAccess""
    ""arn:aws:iam::aws:policy/CloudWatchFullAccess"" = ""CloudWatchFullAccess""
    ""arn:aws:iam::aws:policy/AmazonSSMFullAccess""  = ""AmazonSSMFullAccess""
    ""arn:aws:iam::aws:policy/AWSLambda_FullAccess"" = ""AWSLambda_FullAccess""
  }
  # AWS managed policy required to consume Microsoft 365 data
  # (in addition to above)
  required_aws_managed_policies_to_consume_msft_365_source = {
    ""arn:aws:iam::aws:policy/AmazonCognitoPowerUser"" = ""AmazonCognitoPowerUser""
  }

  # TODO: create IAM policy document, which installer could use to create their own policy as
  # alternative to using AWS Managed policies

  # initial GCP APIs that must be enabled in projects that will host the proxy.
  # (Terraform apply will enabled additional ones)
  required_gcp_apis_to_host = {
    # https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com
    ""iamcredentials.googleapis.com"" = ""IAM Service Account Credentials API"",
    # https://console.cloud.google.com/apis/library/serviceusage.googleapis.com
    ""serviceusage.googleapis.com""   = ""Service Usage API"",
  }

  required_gcp_roles_to_provision_host = {
    ""roles/storage.admin"" = {
      display_name    = ""Storage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#storage.admin""
    },
    ""roles/iam.roleAdmin"" = {
      display_name    = ""IAM Role Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.roleAdmin""
    },
    ""roles/secretmanager.admin"" = {
      display_name    = ""Secret Manager Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#secretmanager.admin""
    },
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    },
    ""roles/cloudfunctions.admin"" = {
      display_name    = ""Cloud Functions Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#cloudfunctions.admin""
    },
  }
  # TODO: add list of permissions, which customer could use to create custom role as alternative


  # TODO: confirm that this is indeed the same list (believe it is)
  required_gcp_apis_to_provision_google_workspace_source = local.required_gcp_apis_to_host

  required_gcp_roles_to_provision_google_workspace_source = {
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    }
  }
  # TODO: add list of permissions, which customer could use to create custom role as alternative

  required_azuread_roles_to_provision_msft_365_source = {
    ""7ab1d382-f21e-4acd-a863-ba3e13f7da61"" = ""Cloud Application Administrator"",
  }
}
",locals,"locals {

  # AWS Managed polices
  # see: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_managed-vs-inline.html#aws-managed-policies
  required_aws_roles_to_provision_host = {
    ""arn:aws:iam::aws:policy/IAMFullAccess""        = ""IAMFullAccess""
    ""arn:aws:iam::aws:policy/AmazonS3FullAccess""   = ""AmazonS3FullAccess"" # only if using bulk sources, although 95% do
    ""arn:aws:iam::aws:policy/CloudWatchFullAccess"" = ""CloudWatchFullAccess""
    ""arn:aws:iam::aws:policy/AmazonSSMFullAccess""  = ""AmazonSSMFullAccess""
    ""arn:aws:iam::aws:policy/AWSLambda_FullAccess"" = ""AWSLambda_FullAccess""
  }
  # AWS managed policy required to consume Microsoft 365 data
  # (in addition to above)
  required_aws_managed_policies_to_consume_msft_365_source = {
    ""arn:aws:iam::aws:policy/AmazonCognitoPowerUser"" = ""AmazonCognitoPowerUser""
  }

  # subset of https://docs.aws.amazon.com/aws-managed-policy/latest/reference/SecretsManagerReadWrite.html
  # as that seems like overkill
  #  - if you're going to use KMS to encrypt the secrets, then you'll need to add the KMS permissions
  #    on the key you intend to use.
  #  - you can/should modify the Resource part of this to limit to a subset of secrets, if this
  #    is being deployed to an AWS account that's used for purposes beyond this proxy deployment
  required_aws_policy_to_use_secrets_manager = {
    ""Version"" : ""2012-10-17"",
    ""Statement"" : [
      {
        ""Effect"" : ""Allow"",
        ""Action"" : [
          ""secretsmanager:*"",
          ""tag:GetResources""
        ],
        ""Resource"" : ""*""
      }
    ]
  }


  # TODO: create IAM policy document, which installer could use to create their own policy as
  # alternative to using AWS Managed policies

  # initial GCP APIs that must be enabled in projects that will host the proxy.
  # (Terraform apply will enabled additional ones)
  required_gcp_apis_to_host = {
    # https://console.cloud.google.com/apis/library/iamcredentials.googleapis.com
    ""iamcredentials.googleapis.com"" = ""IAM Service Account Credentials API"",
    # https://console.cloud.google.com/apis/library/serviceusage.googleapis.com
    ""serviceusage.googleapis.com"" = ""Service Usage API"",
  }

  required_gcp_roles_to_provision_host = {
    ""roles/storage.admin"" = {
      display_name    = ""Storage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#storage.admin""
    },
    ""roles/iam.roleAdmin"" = {
      display_name    = ""IAM Role Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.roleAdmin""
    },
    ""roles/secretmanager.admin"" = {
      display_name    = ""Secret Manager Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#secretmanager.admin""
    },
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    },
    ""roles/cloudfunctions.admin"" = {
      display_name    = ""Cloud Functions Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#cloudfunctions.admin""
    },
  }
  # TODO: add list of permissions, which customer could use to create custom role as alternative


  # TODO: confirm that this is indeed the same list (believe it is)
  required_gcp_apis_to_provision_google_workspace_source = local.required_gcp_apis_to_host

  required_gcp_roles_to_provision_google_workspace_source = {
    ""roles/iam.serviceAccountAdmin"" = {
      display_name    = ""Service Account Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#iam.serviceAccountAdmin""
    },
    ""roles/serviceusage.serviceUsageAdmin"" = {
      display_name    = ""Service Usage Admin"",
      description_url = ""https://cloud.google.com/iam/docs/understanding-roles#serviceusage.serviceUsageAdmin""
    }
  }
  # TODO: add list of permissions, which customer could use to create custom role as alternative

  required_azuread_roles_to_provision_msft_365_source = {
    ""7ab1d382-f21e-4acd-a863-ba3e13f7da61"" = ""Cloud Application Administrator"",
  }
}
",locals,59,80.0,bcddaabc129dfb7cce0bc825f95b49ce878a0d55,005e1fed5f46b4310d81d41d29862bb1c4f360b0,https://github.com/Worklytics/psoxy/blob/bcddaabc129dfb7cce0bc825f95b49ce878a0d55/infra/modules/psoxy-constants/main.tf#L59,https://github.com/Worklytics/psoxy/blob/005e1fed5f46b4310d81d41d29862bb1c4f360b0/infra/modules/psoxy-constants/main.tf#L80,2023-07-21 10:14:50-07:00,2024-02-06 19:07:07+00:00,3,0,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1607,modules/net-vpc/subnets.tf,modules/net-vpc/subnets.tf,0,# todo,# TODO: merge factory subnet IAM members,# TODO: merge factory subnet IAM members,"resource ""google_compute_subnetwork_iam_member"" ""bindings"" {
  for_each   = var.subnet_iam_bindings_additive
  project    = var.project_id
  subnetwork = google_compute_subnetwork.subnetwork[each.value.subnet].name
  region     = google_compute_subnetwork.subnetwork[each.value.subnet].region
  role       = each.value.role
  member     = each.value.member
  dynamic ""condition"" {
    for_each = each.value.condition == null ? [] : [""""]
    content {
      expression  = each.value.condition.expression
      title       = each.value.condition.title
      description = each.value.condition.description
    }
  }
}
",resource,"resource ""google_compute_subnetwork_iam_member"" ""bindings"" {
  for_each   = local.subnet_iam_bindings_additive
  project    = var.project_id
  subnetwork = local.all_subnets[each.value.subnet].name
  region     = local.all_subnets[each.value.subnet].region
  role       = each.value.role
  member     = each.value.member
  dynamic ""condition"" {
    for_each = each.value.condition == null ? [] : [""""]
    content {
      expression  = each.value.condition.expression
      title       = each.value.condition.title
      description = each.value.condition.description
    }
  }
}
",resource,187,,819894d2bab4b440f1b52b1ac8035912fb107004,f19ab4872f816cecb866e0b635e302236a444eef,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/819894d2bab4b440f1b52b1ac8035912fb107004/modules/net-vpc/subnets.tf#L187,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/f19ab4872f816cecb866e0b635e302236a444eef/modules/net-vpc/subnets.tf,2023-08-20 09:44:20+02:00,2023-09-15 00:27:55+02:00,11,1,0,1,0,1,1,0,0,0
https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd,131,modules/secure-cd/main.tf,modules/secure-cd/main.tf,0,#todo,#TODO parameterize,"description = ""Pipeline for application"" #TODO parameterize","resource ""google_clouddeploy_delivery_pipeline"" ""pipeline"" {
  name        = var.clouddeploy_pipeline_name
  description = ""Pipeline for application"" #TODO parameterize
  project     = var.project_id
  location    = var.primary_location

  serial_pipeline {
    dynamic ""stages"" {
      for_each = var.deploy_branch_clusters
      content {
        target_id = google_clouddeploy_target.deploy_target[stages.key].name
      }
    }
  }
}
",resource,"resource ""google_clouddeploy_delivery_pipeline"" ""pipeline"" {
  name        = var.clouddeploy_pipeline_name
  description = ""Pipeline for application"" #TODO parameterize
  project     = var.project_id
  location    = var.primary_location

  serial_pipeline {
    dynamic ""stages"" {
      for_each = var.deploy_branch_clusters
      content {
        target_id = google_clouddeploy_target.deploy_target[stages.key].name
      }
    }
  }
}
",resource,57,76.0,49402b46010b20e3afcaeec200cc2b64db409d01,771f462ca788d02a284fbaf58dc2cd4072b355a2,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/49402b46010b20e3afcaeec200cc2b64db409d01/modules/secure-cd/main.tf#L57,https://github.com/GoogleCloudPlatform/terraform-google-secure-cicd/blob/771f462ca788d02a284fbaf58dc2cd4072b355a2/modules/secure-cd/main.tf#L76,2022-08-23 16:20:45-04:00,2023-03-28 11:20:32-05:00,3,0,0,1,0,0,0,1,0,0
https://github.com/Worklytics/psoxy,86,infra/examples/aws-dev/main.tf,infra/examples/aws-google-workspace/main.tf,1,# todo,"# TODO: loop over sources to 1) provision client in data source, 2) provision proxy instance in AWS","# TODO: loop over sources to 1) provision client in data source, 2) provision proxy instance in AWS ","locals {
  # Google Workspace Sources; add/remove as you wish
  google_workspace_sources = {
    # GDirectory connections are a PRE-REQ for gmail, gdrive, and gcal connections. remove only
    # if you plan to directly connect Directory to worklytics (without proxy). such a scenario is
    # used for customers who care primarily about pseudonymizing PII of external subjects with whom
    # they collaborate in GMail/GCal/Gdrive. the Directory does not contain PII of subjects external
    # to the Google Workspace, so may be directly connected in such scenarios.
    ""gdirectory"": {
      display_name: ""Google Directory""
      apis_consumed: [
        ""admin.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/admin.directory.user.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.user.alias.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.domain.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.group.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.group.member.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.orgunit.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.rolemanagement.readonly""
      ],
      worklytics_connector_name: ""Google Workspace Directory via Psoxy""
    }
    ""gcal"": {
      display_name: ""Google Calendar""
      apis_consumed: [
        ""calendar-json.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/calendar.readonly""
      ]
    }
    ""gmail"": {
      display_name: ""GMail""
      apis_consumed: [
        ""gmail.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/gmail.metadata""
      ]
    }
    ""google-chat"": {
      display_name: ""Google Chat""
      apis_consumed: [
        ""admin.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/admin.reports.audit.readonly""
      ]
    }
    ""gdrive"": {
      display_name: ""Google Drive""
      apis_consumed: [
        ""drive.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/drive.metadata.readonly""
      ]
    }
    ""google-meet"": {
      display_name: ""Google Meet""
      apis_consumed: [
        ""admin.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/admin.reports.audit.readonly""
      ]
    }
  }
}
",locals,"locals {
  # Google Workspace Sources; add/remove as you wish
  google_workspace_sources = {
    # GDirectory connections are a PRE-REQ for gmail, gdrive, and gcal connections. remove only
    # if you plan to directly connect Directory to worklytics (without proxy). such a scenario is
    # used for customers who care primarily about pseudonymizing PII of external subjects with whom
    # they collaborate in GMail/GCal/Gdrive. the Directory does not contain PII of subjects external
    # to the Google Workspace, so may be directly connected in such scenarios.
    ""gdirectory"": {
      display_name: ""Google Directory""
      apis_consumed: [
        ""admin.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/admin.directory.user.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.user.alias.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.domain.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.group.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.group.member.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.orgunit.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.rolemanagement.readonly""
      ],
      worklytics_connector_name: ""Google Workspace Directory via Psoxy""
    }
    ""gcal"": {
      display_name: ""Google Calendar""
      apis_consumed: [
        ""calendar-json.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/calendar.readonly""
      ]
    }
    ""gmail"": {
      display_name: ""GMail""
      apis_consumed: [
        ""gmail.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/gmail.metadata""
      ]
    }
    ""google-chat"": {
      display_name: ""Google Chat""
      apis_consumed: [
        ""admin.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/admin.reports.audit.readonly""
      ]
    }
    ""gdrive"": {
      display_name: ""Google Drive""
      apis_consumed: [
        ""drive.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/drive.metadata.readonly""
      ]
    }
    ""google-meet"": {
      display_name: ""Google Meet""
      apis_consumed: [
        ""admin.googleapis.com""
      ]
      oauth_scopes_needed: [
        ""https://www.googleapis.com/auth/admin.reports.audit.readonly""
      ]
    }
  }
}
",locals,55,,2f81e3da70338154c1ff8ee7d1220a5b5602fcab,1af1e4a9aac1a576d4b6e57a4f259c5d2520b19d,https://github.com/Worklytics/psoxy/blob/2f81e3da70338154c1ff8ee7d1220a5b5602fcab/infra/examples/aws-dev/main.tf#L55,https://github.com/Worklytics/psoxy/blob/1af1e4a9aac1a576d4b6e57a4f259c5d2520b19d/infra/examples/aws-google-workspace/main.tf,2022-01-05 16:14:33-08:00,2022-01-17 12:41:53-08:00,16,1,1,1,0,0,0,0,0,0
https://github.com/Worklytics/psoxy,832,infra/modules/google-workspace-dwd-connection/main.tf,infra/modules/google-workspace-dwd-connection/main.tf,0,# todo,"# TODO: md5 here is 32 chars of hex, so some risk of collision by truncating, while could use","# TODO: md5 here is 32 chars of hex, so some risk of collision by truncating, while could use","locals {
  # sa_account_ids must be 6-30 chars, and must start with a letter, use only lowercase letters,
  # numbers and - (inside)
  # see https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_service_account

  trimmed_id = trim(var.connector_service_account_id, "" "")

  # TODO: md5 here is 32 chars of hex, so some risk of collision by truncating, while could use
  sa_account_id = local.trimmed_id < 31 ? lower(replace(local.trimmed_id , "" "", ""-"")) : substr(md5(local.trimmed_id), 0, 30)
}
",locals,"locals {
  # sa_account_ids must be 6-30 chars, and must start with a letter, use only lowercase letters,
  # numbers and - (inside)
  # see https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/google_service_account

  # trim trailing replaces, replace enclosed spaces with dashes, and everything as lower case
  trimmed_id = lower(replace(trim(var.connector_service_account_id, "" ""), "" "", ""-""))

  # pad if too short
  padded_id = length(local.trimmed_id) < 6 ? ""psoxy-${local.trimmed_id}"" : local.trimmed_id

  # hash if too long
  # TODO: md5 here is 32 chars of hex, so some risk of collision by truncating
  sa_account_id = length(local.padded_id) < 31 ? local.padded_id : substr(md5(local.padded_id), 0, 30)

  instance_id = coalesce(var.instance_id, var.display_name)
}
",locals,14,,674bb3cc40b9675953558a8fe9f432c2298ba3a0,2f240146ca64d11927aac88550c4df4b094c45ee,https://github.com/Worklytics/psoxy/blob/674bb3cc40b9675953558a8fe9f432c2298ba3a0/infra/modules/google-workspace-dwd-connection/main.tf#L14,https://github.com/Worklytics/psoxy/blob/2f240146ca64d11927aac88550c4df4b094c45ee/infra/modules/google-workspace-dwd-connection/main.tf,2023-03-22 16:22:58-07:00,2023-06-20 14:50:05+00:00,5,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,485,tests/fast/stages/s03_project_factory/fixture/variables.tf,tests/fast/stages/s03_project_factory/fixture/variables.tf,0,#todo,#TODO(sruffilli): is this really required?,#TODO(sruffilli): is this really required?,"variable ""environment"" {
  description = ""Environment where projects will be created (e.g. prod, dev, ...).""
  type        = string
  default     = ""prod""
}
",variable,,,44,0.0,cee207b4544cfe2bc2eb517fd91c79952e3052b3,dc3a2ad7be032c093ae4d82d3abfeb628c16dfe6,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/cee207b4544cfe2bc2eb517fd91c79952e3052b3/tests/fast/stages/s03_project_factory/fixture/variables.tf#L44,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/dc3a2ad7be032c093ae4d82d3abfeb628c16dfe6/tests/fast/stages/s03_project_factory/fixture/variables.tf#L0,2022-01-17 10:36:38+01:00,2022-02-24 15:05:18+01:00,5,2,0,1,0,0,0,0,0,1
https://github.com/uyuni-project/sumaform,1385,backend_modules/aws/host/main.tf,backend_modules/aws/host/main.tf,0,workaround,# WORKAROUND,"# WORKAROUND 
 # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner""). 
 # After the first `apply`, terraform removes those tags. The following block avoids this behavior. 
 # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider 
 # See github:terraform-providers/terraform-provider-aws#10689","resource ""aws_ebs_volume"" ""data_disk"" {
  count = var.additional_disk_size == null ? 0 : var.additional_disk_size > 0 ? var.quantity : 0

  availability_zone = local.availability_zone
  size              = var.additional_disk_size == null ? 0 : var.additional_disk_size
  type              = lookup(var.volume_provider_settings, ""type"", ""sc1"")
  snapshot_id       = lookup(var.volume_provider_settings, ""volume_snapshot_id"", null)
  tags = {
    Name = ""${local.resource_name_prefix}-data-volume${var.quantity > 1 ? ""-${count.index + 1}"" : """"}""
  }
  # WORKAROUND
  # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner"").
  # After the first `apply`, terraform removes those tags. The following block avoids this behavior.
  # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider
  # See github:terraform-providers/terraform-provider-aws#10689
  lifecycle {
    ignore_changes = [tags]
  }
}
",resource,"resource ""aws_ebs_volume"" ""data_disk"" {
  count = var.additional_disk_size == null ? 0 : var.additional_disk_size > 0 ? var.quantity : 0

  availability_zone = local.availability_zone
  size              = var.additional_disk_size == null ? 0 : var.additional_disk_size
  type              = lookup(var.volume_provider_settings, ""type"", ""sc1"")
  snapshot_id       = lookup(var.volume_provider_settings, ""volume_snapshot_id"", null)
  tags = {
    Name = ""${local.resource_name_prefix}-data-volume${var.quantity > 1 ? ""-${count.index + 1}"" : """"}""
  }
  # WORKAROUND
  # SUSE internal openbare AWS accounts add special tags to identify the instance owner (""PrincipalId"", ""Owner"").
  # After the first `apply`, terraform removes those tags. The following block avoids this behavior.
  # The correct way to do it would be by ignoring those tags, which is not supported yet by the AWS terraform provider
  # See github:terraform-providers/terraform-provider-aws#10689
  lifecycle {
    ignore_changes = [tags]
  }
}
",resource,114,160.0,12fc857978857ebd94f9b0906480004ca9b88c22,3a05fda9be0240b065bb2031b5812c595b77f973,https://github.com/uyuni-project/sumaform/blob/12fc857978857ebd94f9b0906480004ca9b88c22/backend_modules/aws/host/main.tf#L114,https://github.com/uyuni-project/sumaform/blob/3a05fda9be0240b065bb2031b5812c595b77f973/backend_modules/aws/host/main.tf#L160,2021-01-26 15:58:29+01:00,2024-04-12 13:39:31+12:00,35,0,1,1,1,1,0,0,0,0
https://github.com/cookpad/terraform-aws-eks,64,modules/cluster/addons.tf,modules/cluster/addons.tf,0,fix,# The kube-proxy EKS addon introduced regressions of #124 and #209. We will move to the EKS addon when these are fixed.,# The kube-proxy EKS addon introduced regressions of #124 and #209. We will move to the EKS addon when these are fixed.,"module ""kube_proxy"" {
  source = ""./kubectl""
  config = local.config
  manifest = templatefile(
    ""${path.module}/addons/kube-proxy.yaml"",
    { aws_region = data.aws_region.current.name },
  )
}
",module,the block associated got renamed or deleted,,28,,84a41b51db99fb6468a7b371b198925eb286ebda,f2e040bdc100be73d949adda0582df2f2322f183,https://github.com/cookpad/terraform-aws-eks/blob/84a41b51db99fb6468a7b371b198925eb286ebda/modules/cluster/addons.tf#L28,https://github.com/cookpad/terraform-aws-eks/blob/f2e040bdc100be73d949adda0582df2f2322f183/modules/cluster/addons.tf,2021-12-22 17:09:59+00:00,2022-05-23 14:13:41+02:00,3,1,1,1,0,1,0,0,0,0
https://github.com/kbst/terraform-kubestack,29,google/cluster-local/configuration.tf,google/cluster-local/configuration.tf,0,implementation,# to align with the local implementations,"# while we have the real region for GKE 
 # we still hash and prefix it with gke- 
 # to align with the local implementations 
 # for AKS end EKS","locals {
  # current workspace config
  cfg = module.configuration.merged[terraform.workspace]

  name_prefix = local.cfg[""name_prefix""]

  base_domain = local.cfg[""base_domain""]

  # while we have the real region for GKE
  # we still hash and prefix it with gke-
  # to align with the local implementations
  # for AKS end EKS
  fake_region_hash = substr(sha256(local.cfg[""region""]), 0, 7)
  fake_region      = ""gke-${local.fake_region_hash}""

  http_port_default = terraform.workspace == ""apps"" ? 80 : 8080
  http_port         = lookup(local.cfg, ""http_port"", local.http_port_default)

  https_port_default = terraform.workspace == ""apps"" ? 443 : 8443
  https_port         = lookup(local.cfg, ""https_port"", local.https_port_default)

  manifest_path_default = ""manifests/overlays/${terraform.workspace}""
  manifest_path         = var.manifest_path != null ? var.manifest_path : local.manifest_path_default

  disable_default_ingress = lookup(local.cfg, ""disable_default_ingress"", false)

  node_image = lookup(local.cfg, ""node_image"", ""kindest/node:v1.18.0"")

  # technically it should be min_node_count times number of AZs
  # but it seems better to keep node count low in the dev env
  node_count = lookup(local.cfg, ""cluster_min_node_count"", 1)
  nodes = [
    for node, _ in range(local.node_count) :
    ""worker""
  ]
  extra_nodes = join("","", local.nodes)

}
",locals,"locals {
  # current workspace config
  cfg = module.configuration.merged[terraform.workspace]

  name_prefix = local.cfg[""name_prefix""]

  base_domain = local.cfg[""base_domain""]

  # while we have the real region for GKE
  # we still hash and prefix it with gke-
  # to align with the local implementations
  # for AKS end EKS
  fake_region_hash = substr(sha256(local.cfg[""region""]), 0, 7)
  fake_region      = ""gke-${local.fake_region_hash}""

  http_port_default = terraform.workspace == ""apps"" ? 80 : 8080
  http_port         = lookup(local.cfg, ""http_port"", local.http_port_default)

  https_port_default = terraform.workspace == ""apps"" ? 443 : 8443
  https_port         = lookup(local.cfg, ""https_port"", local.https_port_default)

  disable_default_ingress = lookup(local.cfg, ""disable_default_ingress"", false)

  node_image = lookup(local.cfg, ""node_image"", null)

  # technically it should be min_node_count times number of AZs
  # but it seems better to keep node count low in the dev env
  node_count = lookup(local.cfg, ""cluster_min_node_count"", 1)
  nodes = [
    for node, _ in range(local.node_count) :
    ""worker""
  ]
  extra_nodes = join("","", local.nodes)

}
",locals,18,18.0,40af925aa6e6543373a298870aadf27d1672f58d,67b7dfce00f2dc67a9293d74ef5ac80879de5a2b,https://github.com/kbst/terraform-kubestack/blob/40af925aa6e6543373a298870aadf27d1672f58d/google/cluster-local/configuration.tf#L18,https://github.com/kbst/terraform-kubestack/blob/67b7dfce00f2dc67a9293d74ef5ac80879de5a2b/google/cluster-local/configuration.tf#L18,2020-09-29 15:18:43+02:00,2021-05-25 21:22:58+02:00,3,0,1,1,0,0,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-eks,126,node_groups.tf,node_groups.tf,0,hack,# Hack to ensure ordering of resource creation. Do not create node_groups,"# Hack to ensure ordering of resource creation. Do not create node_groups 
 # before other resources are ready. Removes race conditions","data ""null_data_source"" ""node_groups"" {
  count = var.create_eks ? 1 : 0

  inputs = {
    cluster_name = var.cluster_name

    # Ensure these resources are created before ""unlocking"" the data source.
    # `depends_on` causes a refresh on every run so is useless here.
    # [Re]creating or removing these resources will trigger recreation of Node Group resources
    aws_auth         = coalescelist(kubernetes_config_map.aws_auth[*].id, [""""])[0]
    role_NodePolicy  = coalescelist(aws_iam_role_policy_attachment.workers_AmazonEKSWorkerNodePolicy[*].id, [""""])[0]
    role_CNI_Policy  = coalescelist(aws_iam_role_policy_attachment.workers_AmazonEKS_CNI_Policy[*].id, [""""])[0]
    role_Container   = coalescelist(aws_iam_role_policy_attachment.workers_AmazonEC2ContainerRegistryReadOnly[*].id, [""""])[0]
    role_autoscaling = coalescelist(aws_iam_role_policy_attachment.workers_autoscaling[*].id, [""""])[0]
  }
}
",data,the block associated got renamed or deleted,,1,,11147e9af34054c4c4576aa00938a2c65198ca5f,616d30ec674ff1d125710755f5073b1665bbd1af,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/11147e9af34054c4c4576aa00938a2c65198ca5f/node_groups.tf#L1,https://github.com/terraform-aws-modules/terraform-aws-eks/blob/616d30ec674ff1d125710755f5073b1665bbd1af/node_groups.tf,2020-01-09 12:53:08+01:00,2020-06-28 02:31:23+02:00,5,1,1,1,0,0,0,0,0,0
https://github.com/pingcap/tidb-operator,8,deploy/alicloud/main.tf,deploy/aliyun/main.tf,1,fix,// TODO: use STS when upstream get this fixed,"// Workaround: ACK does not support customize node RAM role, access key is the only way get local volume provisioner working 
 // TODO: use STS when upstream get this fixed","resource ""local_file"" ""local-volume-provisioner"" {
  depends_on = [""data.template_file.local-volume-provisioner""]
  filename   = ""${local.local_volume_provisioner_path}""
  content    = ""${data.template_file.local-volume-provisioner.rendered}""
}
",resource,the block associated got renamed or deleted,,86,,eebd686956c0b2adf67a10e1a376659c95c571a3,042b1a97fbbdf342297002990564828e6644a3f0,https://github.com/pingcap/tidb-operator/blob/eebd686956c0b2adf67a10e1a376659c95c571a3/deploy/alicloud/main.tf#L86,https://github.com/pingcap/tidb-operator/blob/042b1a97fbbdf342297002990564828e6644a3f0/deploy/aliyun/main.tf,2019-05-06 19:59:43+08:00,2019-07-23 19:44:58+08:00,4,1,0,1,1,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,589,examples/data-solutions/dp-foundation/07-exposure.tf,examples/data-solutions/data-platform-foundations/07-exposure.tf,1,#todo,#TODO add role => service account mapping to assign roles on exposure project,#TODO add role => service account mapping to assign roles on exposure project,"locals {
  group_iam_exp = {
    #TODO add group => role mapping to asign on exposure project
  }
  iam_exp = {
    #TODO add role => service account mapping to assign roles on exposure project
  }
  prefix_exp = ""${var.prefix}-exp""
}
",locals,the block associated got renamed or deleted,,22,,3c99074b3ff652827d277217e4f84b48a713b224,4f4a9cd7ac2cee3a1b2e413cd19f6b6b07c35622,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3c99074b3ff652827d277217e4f84b48a713b224/examples/data-solutions/dp-foundation/07-exposure.tf#L22,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/4f4a9cd7ac2cee3a1b2e413cd19f6b6b07c35622/examples/data-solutions/data-platform-foundations/07-exposure.tf,2022-02-02 15:31:54+01:00,2022-02-09 17:01:25+01:00,3,1,0,1,0,1,0,0,0,0
https://github.com/nasa/cumulus,3,example/main.tf,example/main.tf,0,todo,# TODO Don't hard-code,# TODO Don't hard-code,"module ""s3_credentials_endpoint"" {
  source = ""../packages/s3-credentials-endpoint""

  prefix               = ""mth-2""
  permissions_boundary = var.permissions_boundary

  rest_api   = module.thin_egress_app.rest_api
  stage_name = module.thin_egress_app.rest_api_stage_name

  sts_credentials_lambda_arn = ""asdf""

  # TODO Don't hard-code
  urs_client_id = ""asdf""
  # TODO Don't hard-code
  urs_client_password = ""asdf""
  # TODO Don't hard-code
  urs_url = ""https://uat.urs.earthdata.nasa.gov""
}
",module,"module ""s3_credentials_endpoint"" {
  source = ""../packages/s3-credentials-endpoint""

  prefix               = var.prefix
  permissions_boundary = var.permissions_boundary
  subnet_ids           = var.tea_subnet_ids
  ngap_sgs             = var.ngap_sgs

  rest_api   = module.thin_egress_app.rest_api
  stage_name = module.thin_egress_app.rest_api_stage_name

  sts_credentials_lambda_arn = var.sts_credentials_lambda_arn

  urs_client_id       = var.urs_client_id
  urs_client_password = var.urs_client_password
  urs_url             = var.urs_url
}
",module,43,,b37630397418a4cb61e428974577b0e21a636fae,3342da9920c9b6d6b98498b5edda515c84c59a24,https://github.com/nasa/cumulus/blob/b37630397418a4cb61e428974577b0e21a636fae/example/main.tf#L43,https://github.com/nasa/cumulus/blob/3342da9920c9b6d6b98498b5edda515c84c59a24/example/main.tf,2019-07-02 10:24:14-04:00,2019-07-07 22:31:31-05:00,2,1,1,1,0,0,0,0,0,0
https://github.com/kubernetes/k8s.io,45,infra/gcp/clusters/projects/k8s-infra-prow-build/prow-build/main.tf,infra/gcp/clusters/projects/k8s-infra-prow-build/prow-build/main.tf,0,# todo,# TODO: use data resource to get org role name instead of hardcode,# TODO: use data resource to get org role name instead of hardcode,"resource ""google_project_iam_member"" ""k8s_infra_prow_viewers"" {
  project = local.project_id
  # TODO: use data resource to get org role name instead of hardcode
  role    = ""organizations/758905017065/roles/prow.viewer""
  member  = ""group:k8s-infra-prow-viewers@kubernetes.io""
}
",resource,"resource ""google_project_iam_member"" ""k8s_infra_prow_viewers"" {
  project = local.project_id
  role    = data.google_iam_role.prow_viewer.name
  member  = ""group:k8s-infra-prow-viewers@kubernetes.io""
}
",resource,52,,ad776a80065d3a90e2328b4a8d31fba8feb58bb2,d09eae6759b7f46c3c63dd96593d4a5d8b7c10a1,https://github.com/kubernetes/k8s.io/blob/ad776a80065d3a90e2328b4a8d31fba8feb58bb2/infra/gcp/clusters/projects/k8s-infra-prow-build/prow-build/main.tf#L52,https://github.com/kubernetes/k8s.io/blob/d09eae6759b7f46c3c63dd96593d4a5d8b7c10a1/infra/gcp/clusters/projects/k8s-infra-prow-build/prow-build/main.tf,2020-07-24 13:52:36-07:00,2021-03-03 16:40:35-05:00,17,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,52,modules/compute/vm-instance/variables.tf,modules/compute/vm-instance/variables.tf,0,workaround,# It's a workaround of lack of `optional` in Terraform 1.2,type    = any # It's a workaround of lack of `optional` in Terraform 1.2,"variable ""placement_policy"" {
  description = <<-EOT
  Control where your VM instances are physically located relative to each other within a zone.
  See https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_resource_policy#nested_group_placement_policy
  EOT

  type    = any # It's a workaround of lack of `optional` in Terraform 1.2
  default = null
  validation {
    condition     = var.placement_policy == null ? true : try(keys(var.placement_policy), null) != null
    error_message = <<-EOT
    The var.placement_policy should be either unset/null or be a map/object with 
    fields: vm_count (number), availability_domain_count (number), collocation (string), max_distance (number).
    EOT
  }

  validation {
    condition = alltrue([
      for k in try(keys(var.placement_policy), []) : contains([
      ""vm_count"", ""availability_domain_count"", ""collocation"", ""max_distance""], k)
    ])
    error_message = <<-EOT
    The supported fields for var.placement_policy are:
    vm_count (number), availability_domain_count (number), collocation (string), max_distance (number).
    EOT
  }
}
",variable,"variable ""placement_policy"" {
  description = <<-EOT
  Control where your VM instances are physically located relative to each other within a zone.
  See https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/compute_resource_policy#nested_group_placement_policy
  EOT

  type    = any # It's a workaround of lack of `optional` in Terraform 1.2
  default = null
  validation {
    condition     = var.placement_policy == null ? true : try(keys(var.placement_policy), null) != null
    error_message = <<-EOT
    The var.placement_policy should be either unset/null or be a map/object with 
    fields: vm_count (number), availability_domain_count (number), collocation (string), max_distance (number).
    EOT
  }

  validation {
    condition = alltrue([
      for k in try(keys(var.placement_policy), []) : contains([
      ""vm_count"", ""availability_domain_count"", ""collocation"", ""max_distance""], k)
    ])
    error_message = <<-EOT
    The supported fields for var.placement_policy are:
    vm_count (number), availability_domain_count (number), collocation (string), max_distance (number).
    EOT
  }
}
",variable,286,308.0,99573023b3653e2d7aca52c3f41de5c788b9a342,99abb3b2a16d20128fe85ba43c8c1356ebd9858f,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/99573023b3653e2d7aca52c3f41de5c788b9a342/modules/compute/vm-instance/variables.tf#L286,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/99abb3b2a16d20128fe85ba43c8c1356ebd9858f/modules/compute/vm-instance/variables.tf#L308,2023-07-12 12:33:12-07:00,2024-03-07 14:31:49-06:00,8,0,0,0,1,0,0,0,0,0
https://github.com/Worklytics/psoxy,6,infra/modules/google-workspace-dwd-connection/main.tf,infra/modules/google-workspace-dwd-connection/main.tf,0,# todo,# TODO: specify the following step in in terraform once https://github.com/hashicorp/terraform-provider-google/issues/1959 solved,"# enable domain-wide-delegation via GCP console 
 # NOTE: side effect of enabling domain-wide-delegation is that an ""OAuth 2.0 Client ID"" will be 
 # created for the service account and listed in GCP Console 
 # TODO: specify the following step in in terraform once https://github.com/hashicorp/terraform-provider-google/issues/1959 solved","resource ""local_file"" ""todo"" {
  filename = ""TODO - ${var.display_name} setup.md""
  content  = <<EOT
Complete the following steps via GCP console:
  1. Visit https://console.cloud.google.com/apis/credentials?project=${var.project_id}
  2. Find the `${var.connector_service_account_id}` service account.
  3. Enable 'Domain-wide Delegation' and 'Save'. This provisions an 'Oauth 2.0 client' for the
     service account. Copy the Client ID of that client.

Complete the following steps via the Google Workspace Admin console:
   1. Visit https://admin.google.com/ and navigate to Security --> API Controls, then find ""Manage
      Domain Wide Delegation"". Click ""Add new""
   2. Copy and paste the client ID from the prior section into the ""Client ID"" input in the popup.
   3. Copy and paste the scopes for your data source (see `java/configs/`, and find the `SCOPE`
      variable in the yaml file that matches your source) into the ""Scopes"" input.
   4. Authorize it.

With this, your psoxy instance should be able to authenticate with Google as `${var.connector_service_account_id}`
and request data from Google as authorized by the OAuth scopes you granted.
EOT

}
",resource,the block associated got renamed or deleted,,28,,1259c535e4d315fea708946a07b95f255b249721,58a702bcda40d675a44440f97ae00acbd68d0694,https://github.com/Worklytics/psoxy/blob/1259c535e4d315fea708946a07b95f255b249721/infra/modules/google-workspace-dwd-connection/main.tf#L28,https://github.com/Worklytics/psoxy/blob/58a702bcda40d675a44440f97ae00acbd68d0694/infra/modules/google-workspace-dwd-connection/main.tf,2021-10-06 09:58:36-07:00,2021-11-16 16:56:51-08:00,3,1,0,0,1,1,0,0,0,0
https://github.com/Worklytics/psoxy,415,infra/modules/aws-psoxy-lambda/main.tf,infra/modules/aws-psoxy-lambda/main.tf,0,# todo,"# TODO: this for 'writable' case requires TWO applies to show up. Eg, on first run, the ACCESS_TOKEN","# q: creates implicit dependency on parameters being created, which may not be case in first run??  
 # TODO: this for 'writable' case requires TWO applies to show up. Eg, on first run, the ACCESS_TOKEN 
 # params aren't there, so not visible via 'data'; and not added to write. 
 # ""PSOXY_${upper(replace(each.value.connector_name, ""-"", ""_""))}_${upper(each.value.secret_name)}""","data ""aws_ssm_parameters_by_path"" ""psoxy_parameters"" {
  path            = ""/""
  with_decryption = false # we just want the arns, not the values
}
",data,the block associated got renamed or deleted,,82,,5d8f4928978c96878368db88bc908047217bcda8,b4d53a1c6321a4b354b8703f9105a67cc872c310,https://github.com/Worklytics/psoxy/blob/5d8f4928978c96878368db88bc908047217bcda8/infra/modules/aws-psoxy-lambda/main.tf#L82,https://github.com/Worklytics/psoxy/blob/b4d53a1c6321a4b354b8703f9105a67cc872c310/infra/modules/aws-psoxy-lambda/main.tf,2022-10-06 13:05:00-07:00,2022-10-06 13:06:47-07:00,2,1,0,0,0,1,0,0,0,0
https://github.com/CDCgov/prime-simplereport,24,ops/dev/persistent/main.tf,ops/dev/persistent/main.tf,0,// todo,// TODO: delete old_subnet_id when removing the old DB configuration,// TODO: delete old_subnet_id when removing the old DB configuration,"module ""db"" {
  source      = ""../../services/postgres_db""
  env         = local.env
  rg_location = local.rg_location
  rg_name     = local.rg_name

  global_vault_id = data.azurerm_key_vault.global.id
  db_vault_id     = data.azurerm_key_vault.db_keys.id
  // TODO: delete old_subnet_id when removing the old DB configuration
  old_subnet_id = module.vnet.subnet_vm_id
  subnet_id     = module.vnet.subnet_db_id
  // TODO: remove this when removing old DB config
  dns_zone_id      = module.vnet.private_dns_zone_id
  log_workspace_id = module.monitoring.log_analytics_workspace_id
  // TODO: remove this when removing old DB config
  nophi_user_password = random_password.random_nophi_password.result

  tags = local.management_tags
}
",module,"module ""db"" {
  source      = ""../../services/postgres_db""
  env         = local.env
  rg_location = local.rg_location
  rg_name     = local.rg_name

  global_vault_id  = data.azurerm_key_vault.global.id
  db_vault_id      = data.azurerm_key_vault.db_keys.id
  subnet_id        = module.vnet.subnet_db_id
  log_workspace_id = module.monitoring.log_analytics_workspace_id

  nophi_user_password = random_password.random_nophi_password.result

  tags = local.management_tags
}
",module,54,,3e4185fa549937cff9213c325bc0fee780e41eb0,fb9a18eb71f31044ca7a58958a25dea489263ca7,https://github.com/CDCgov/prime-simplereport/blob/3e4185fa549937cff9213c325bc0fee780e41eb0/ops/dev/persistent/main.tf#L54,https://github.com/CDCgov/prime-simplereport/blob/fb9a18eb71f31044ca7a58958a25dea489263ca7/ops/dev/persistent/main.tf,2022-02-16 12:59:39-05:00,2022-04-28 23:27:57-04:00,2,1,1,1,0,0,1,0,0,0
https://github.com/aws-ia/terraform-aws-eks-blueprints,29,modules/aws-eks-managed-node-groups/iam.tf,modules/aws-eks-managed-node-groups/iam.tf,0,#todo,#TODO Allow IAM policies can be passed from tfvars file,#TODO Allow IAM policies can be passed from tfvars file,"resource ""aws_iam_role_policy_attachment"" ""managed_ng_AmazonEKSWorkerNodePolicy"" {
  policy_arn = ""${local.policy_arn_prefix}/AmazonEKSWorkerNodePolicy""
  role       = aws_iam_role.managed_ng.name
}
",resource,"resource ""aws_iam_role_policy_attachment"" ""managed_ng_AmazonEKSWorkerNodePolicy"" {
  policy_arn = ""${local.policy_arn_prefix}/AmazonEKSWorkerNodePolicy""
  role       = aws_iam_role.managed_ng.name
}
",resource,21,,836f6ff008667e880f7591aa5c3d5d755d8da8e6,b80759c98848fb1b79fffc9db9d9c6606fafd320,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/836f6ff008667e880f7591aa5c3d5d755d8da8e6/modules/aws-eks-managed-node-groups/iam.tf#L21,https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/b80759c98848fb1b79fffc9db9d9c6606fafd320/modules/aws-eks-managed-node-groups/iam.tf,2021-09-30 15:35:25+01:00,2021-11-24 18:47:05+00:00,3,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,5,infrastructure/hub-and-spoke-vpns/main.tf,infrastructure/hub-and-spoke-vpns/main.tf,0,todo,# TODO Provide resolver addresses in the output once https://github.com/terraform-providers/terraform-provider-google/issues/3753 resolved.,"############################################################## 
 #                   Inbount DNS Forwarding                   # 
 ##############################################################  
 # TODO Provide resolver addresses in the output once https://github.com/terraform-providers/terraform-provider-google/issues/3753 resolved. 
 # For now please refer to the Documentation on how to get the compute addresses for the DNS Resolver https://cloud.google.com/dns/zones/#creating_a_dns_policy_that_enables_inbound_dns_forwarding","resource ""google_dns_policy"" ""google_dns_policy"" {
  provider = ""google-beta""

  project = var.hub_project_id
  name = ""inbound-dns-forwarding-policy""
  enable_inbound_forwarding = true

  networks {
    network_url = module.vpc-hub.network_self_link
  }
}
",resource,,,332,0.0,cb3260b3979cdc72a3920b385c0f997256bcf34a,c486bfc66f9814e33b410602cb557a5e4d532912,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/cb3260b3979cdc72a3920b385c0f997256bcf34a/infrastructure/hub-and-spoke-vpns/main.tf#L332,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c486bfc66f9814e33b410602cb557a5e4d532912/infrastructure/hub-and-spoke-vpns/main.tf#L0,2019-10-30 16:38:26+01:00,2020-04-03 14:06:48+02:00,3,2,0,1,1,0,1,0,0,0
https://github.com/ministryofjustice/aws-root-account,9,terraform/organizations-accounts-hmpps-delius.tf,terraform/organizations-accounts-hmpps-delius.tf,0,# todo,# TODO: Move this into AWS Secrets Manager,"email     = local.account_emails[""Alfresco non-prod""][0] # TODO: Move this into AWS Secrets Manager","resource ""aws_organizations_account"" ""alfresco-non-prod"" {
  name      = ""Alfresco non-prod""
  email     = local.account_emails[""Alfresco non-prod""][0] # TODO: Move this into AWS Secrets Manager
  parent_id = aws_organizations_organizational_unit.hmpps-delius.id

  lifecycle {
    # If any of these attributes are changed, it attempts to destroy and recreate the account,
    # so we should ignore the changes to prevent this from happening.
    ignore_changes = [
      name,
      email,
      iam_user_access_to_billing,
      role_name
    ]
  }
}
",resource,,,4,0.0,b2b66fbf2222c0f21bb889002f2aa294eb1fe375,d7b325af5b38460b9d14f63e6bdc9716285854f3,https://github.com/ministryofjustice/aws-root-account/blob/b2b66fbf2222c0f21bb889002f2aa294eb1fe375/terraform/organizations-accounts-hmpps-delius.tf#L4,https://github.com/ministryofjustice/aws-root-account/blob/d7b325af5b38460b9d14f63e6bdc9716285854f3/terraform/organizations-accounts-hmpps-delius.tf#L0,2020-11-26 14:00:11+00:00,2022-03-01 19:30:50+00:00,5,2,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,88,community/modules/compute/schedmd-slurm-gcp-v6-nodeset/source_image_logic.tf,community/modules/compute/schedmd-slurm-gcp-v6-nodeset/source_image_logic.tf,0,hack,"# This approach to ""hacking"" the project name allows a chain of Terraform","# This approach to ""hacking"" the project name allows a chain of Terraform 
 # calls to set the instance source_image (boot disk) with a ""relative 
 # resource name"" that passes muster with VPC Service Control rules 
 # 
 # https://github.com/terraform-google-modules/terraform-google-vm/blob/735bd415fc5f034d46aa0de7922e8fada2327c0c/modules/instance_template/main.tf#L28 
 # https://cloud.google.com/apis/design/resource_names#relative_resource_name","locals {
  # Currently supported images and projects
  known_project_families = {
    schedmd-slurm-public = [
      ""slurm-gcp-6-1-debian-11"",
      ""slurm-gcp-6-1-hpc-rocky-linux-8"",
      ""slurm-gcp-6-1-ubuntu-2004-lts"",
      ""slurm-gcp-6-1-ubuntu-2204-lts-arm64"",
      ""slurm-gcp-6-1-hpc-centos-7-k80"",
      ""slurm-gcp-6-1-hpc-centos-7""
    ]
  }

  # This approach to ""hacking"" the project name allows a chain of Terraform
  # calls to set the instance source_image (boot disk) with a ""relative
  # resource name"" that passes muster with VPC Service Control rules
  #
  # https://github.com/terraform-google-modules/terraform-google-vm/blob/735bd415fc5f034d46aa0de7922e8fada2327c0c/modules/instance_template/main.tf#L28
  # https://cloud.google.com/apis/design/resource_names#relative_resource_name
  source_image_project_normalized = (can(var.instance_image.family) ?
    ""projects/${data.google_compute_image.slurm.project}/global/images/family"" :
    ""projects/${data.google_compute_image.slurm.project}/global/images""
  )
  source_image_family = can(var.instance_image.family) ? data.google_compute_image.slurm.family : """"
  source_image        = can(var.instance_image.name) ? data.google_compute_image.slurm.name : """"
}
",locals,"locals {
  # Currently supported images and projects
  known_project_families = {
    schedmd-slurm-public = [
      ""slurm-gcp-6-4-debian-11"",
      ""slurm-gcp-6-4-hpc-rocky-linux-8"",
      ""slurm-gcp-6-4-ubuntu-2004-lts"",
      ""slurm-gcp-6-4-ubuntu-2204-lts-arm64"",
      ""slurm-gcp-6-4-hpc-centos-7-k80"",
      ""slurm-gcp-6-4-hpc-centos-7""
    ]
  }

  # This approach to ""hacking"" the project name allows a chain of Terraform
  # calls to set the instance source_image (boot disk) with a ""relative
  # resource name"" that passes muster with VPC Service Control rules
  #
  # https://github.com/terraform-google-modules/terraform-google-vm/blob/735bd415fc5f034d46aa0de7922e8fada2327c0c/modules/instance_template/main.tf#L28
  # https://cloud.google.com/apis/design/resource_names#relative_resource_name
  source_image_project_normalized = (can(var.instance_image.family) ?
    ""projects/${data.google_compute_image.slurm.project}/global/images/family"" :
    ""projects/${data.google_compute_image.slurm.project}/global/images""
  )
  source_image_family = can(var.instance_image.family) ? data.google_compute_image.slurm.family : """"
  source_image        = can(var.instance_image.name) ? data.google_compute_image.slurm.name : """"
}
",locals,30,30.0,33bf402eaa82607a027754c6048fb0dce6d7668c,e56c284617fd5295bec771512fef0496b4994195,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/33bf402eaa82607a027754c6048fb0dce6d7668c/community/modules/compute/schedmd-slurm-gcp-v6-nodeset/source_image_logic.tf#L30,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/e56c284617fd5295bec771512fef0496b4994195/community/modules/compute/schedmd-slurm-gcp-v6-nodeset/source_image_logic.tf#L30,2023-10-26 09:59:26-07:00,2024-02-21 19:44:32-06:00,5,0,1,1,1,0,0,0,0,0
https://github.com/terraform-aws-modules/terraform-aws-iam,4,modules/iam-role-for-service-accounts-eks/policies.tf,modules/iam-role-for-service-accounts-eks/policies.tf,0,todo,# TODO - remove *_ids at next breaking change,# TODO - remove *_ids at next breaking change,"data ""aws_iam_policy_document"" ""cluster_autoscaler"" {
  count = var.create_role && var.attach_cluster_autoscaler_policy ? 1 : 0

  statement {
    actions = [
      ""autoscaling:DescribeAutoScalingGroups"",
      ""autoscaling:DescribeAutoScalingInstances"",
      ""autoscaling:DescribeLaunchConfigurations"",
      ""autoscaling:DescribeScalingActivities"",
      ""autoscaling:DescribeTags"",
      ""ec2:DescribeLaunchTemplateVersions"",
      ""ec2:DescribeInstanceTypes"",
      ""eks:DescribeNodegroup"",
      ""ec2:DescribeImages"",
      ""ec2:GetInstanceTypesFromInstanceRequirements""
    ]

    resources = [""*""]
  }

  dynamic ""statement"" {
    # TODO - remove *_ids at next breaking change
    for_each = toset(coalescelist(var.cluster_autoscaler_cluster_ids, var.cluster_autoscaler_cluster_names))
    content {
      actions = [
        ""autoscaling:SetDesiredCapacity"",
        ""autoscaling:TerminateInstanceInAutoScalingGroup""
      ]

      resources = [""*""]

      condition {
        test     = ""StringEquals""
        variable = ""autoscaling:ResourceTag/kubernetes.io/cluster/${statement.value}""
        values   = [""owned""]
      }
    }
  }
}
",data,"data ""aws_iam_policy_document"" ""cluster_autoscaler"" {
  count = var.create_role && var.attach_cluster_autoscaler_policy ? 1 : 0

  statement {
    actions = [
      ""autoscaling:DescribeAutoScalingGroups"",
      ""autoscaling:DescribeAutoScalingInstances"",
      ""autoscaling:DescribeLaunchConfigurations"",
      ""autoscaling:DescribeScalingActivities"",
      ""autoscaling:DescribeTags"",
      ""ec2:DescribeLaunchTemplateVersions"",
      ""ec2:DescribeInstanceTypes"",
      ""eks:DescribeNodegroup"",
      ""ec2:DescribeImages"",
      ""ec2:GetInstanceTypesFromInstanceRequirements""
    ]

    resources = [""*""]
  }

  dynamic ""statement"" {
    # TODO - remove *_ids at next breaking change
    for_each = toset(coalescelist(var.cluster_autoscaler_cluster_ids, var.cluster_autoscaler_cluster_names))
    content {
      actions = [
        ""autoscaling:SetDesiredCapacity"",
        ""autoscaling:TerminateInstanceInAutoScalingGroup""
      ]

      resources = [""*""]

      condition {
        test     = ""StringEquals""
        variable = ""autoscaling:ResourceTag/kubernetes.io/cluster/${statement.value}""
        values   = [""owned""]
      }
    }
  }
}
",data,111,111.0,fdee003477c5f86c4236be08ef6a69dffbcc39fd,f9d5e28996ca282af4c09cb97f6291cf77ac03ea,https://github.com/terraform-aws-modules/terraform-aws-iam/blob/fdee003477c5f86c4236be08ef6a69dffbcc39fd/modules/iam-role-for-service-accounts-eks/policies.tf#L111,https://github.com/terraform-aws-modules/terraform-aws-iam/blob/f9d5e28996ca282af4c09cb97f6291cf77ac03ea/modules/iam-role-for-service-accounts-eks/policies.tf#L111,2023-05-22 15:55:09-04:00,2024-04-08 18:43:26-04:00,13,0,0,1,0,1,0,0,0,0
https://github.com/wireapp/wire-server-deploy,3,terraform/modules/aws_vpc_security_groups/main.tf,terraform/modules/aws-vpc-security-groups/main.tf,1,fix,# FIXME: tighten this up. need UDP for flannel.,# FIXME: tighten this up. need UDP for flannel.,"resource ""aws_security_group"" ""k8s_node"" {
  name        = ""k8s_node""
  description = ""hosts that have kubernetes.""
  vpc_id      = var.vpc_id

  # incoming from the admin node (kubectl)
  ingress {
    from_port   = 6443
    to_port     = 6443
    protocol    = ""tcp""
    security_groups = [""${aws_security_group.talk_to_k8s.id}""]
  }

  # FIXME: tighten this up.
  ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = ""tcp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # FIXME: tighten this up. need UDP for flannel.
  ingress {
    from_port   = 0
    to_port     = 65535
    protocol    = ""udp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  tags = {
    Name = ""k8s_node""
  }
}
",resource,"resource ""aws_security_group"" ""k8s_node"" {
  name        = ""k8s_node""
  description = ""hosts that have kubernetes.""
  vpc_id      = var.vpc_id

  # incoming from the admin node (kubectl)
  ingress {
    description     = """"
    from_port       = 6443
    to_port         = 6443
    protocol        = ""tcp""
    security_groups = [""${aws_security_group.talk_to_k8s.id}""]
  }

  # FIXME: tighten this up.
  ingress {
    description     = """"
    from_port       = 0
    to_port         = 65535
    protocol        = ""tcp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # FIXME: tighten this up. need UDP for flannel.
  ingress {
    description     = """"
    from_port       = 0
    to_port         = 65535
    protocol        = ""udp""
    security_groups = [""${aws_security_group.k8s_private.id}""]
  }

  # incoming traffic to the application.
  ingress {
    from_port   = 31772
    to_port     = 31773
    protocol    = ""tcp""
    # NOTE: NLBs dont allow security groups to be set on them, which is why
    # we go with the CIDR for now, which is hard-coded and needs fixing
    cidr_blocks = [""172.17.0.0/20""]
  }

  tags = {
    Name = ""k8s_node""
  }
}
",resource,221,234.0,cb61b733457d5a1ccdb5c9dcb5fde4cab9e5d7e0,e7f081ca744de60bf9d10cb037828b995883d2aa,https://github.com/wireapp/wire-server-deploy/blob/cb61b733457d5a1ccdb5c9dcb5fde4cab9e5d7e0/terraform/modules/aws_vpc_security_groups/main.tf#L221,https://github.com/wireapp/wire-server-deploy/blob/e7f081ca744de60bf9d10cb037828b995883d2aa/terraform/modules/aws-vpc-security-groups/main.tf#L234,2020-04-23 17:54:17+01:00,2020-08-26 16:29:39+02:00,5,0,0,0,0,1,0,0,0,0
https://github.com/aws-observability/terraform-aws-observability-accelerator,33,examples/existing-cluster-with-base-and-infra/main.tf,examples/existing-cluster-with-base-and-infra/main.tf,0,todo,# TODO remove when Kevin's PR is live,# TODO remove when Kevin's PR is live,"module ""workloads_infra"" {
  source = ""../../workloads/infra""
  # source = ""aws-ia/terrarom-aws-observability-accelerator/workloads/infra""

  eks_cluster_id = module.eks_observability_accelerator.eks_cluster_id

  dashboards_folder_id            = module.eks_observability_accelerator.grafana_dashboards_folder_id
  managed_prometheus_workspace_id = module.eks_observability_accelerator.managed_prometheus_workspace_id

  # TODO remove when Kevin's PR is live
  managed_prometheus_workspace_endpoint = module.eks_observability_accelerator.managed_prometheus_workspace_endpoint
  managed_prometheus_workspace_region   = module.eks_observability_accelerator.managed_prometheus_workspace_region


  managed_grafana_workspace_endpoint = module.eks_observability_accelerator.managed_grafana_workspace_endpoint

  # TODO: manage with Secrets manager
  grafana_api_key = var.grafana_api_key

  # module custom configuration, check module Documentation
  # config               = {}

  # depends_on = [
  #   module.eks_observability_accelerator
  # ]
}
",module,"module ""workloads_infra"" {
  source = ""../../modules/workloads/infra""
  # source = ""aws-observability/terrarom-aws-observability-accelerator/workloads/infra""

  eks_cluster_id = module.eks_observability_accelerator.eks_cluster_id

  dashboards_folder_id            = module.eks_observability_accelerator.grafana_dashboards_folder_id
  managed_prometheus_workspace_id = module.eks_observability_accelerator.managed_prometheus_workspace_id

  managed_prometheus_workspace_endpoint = module.eks_observability_accelerator.managed_prometheus_workspace_endpoint
  managed_prometheus_workspace_region   = module.eks_observability_accelerator.managed_prometheus_workspace_region

  enable_alerting_rules = false

  tags = local.tags

  depends_on = [
    module.eks_observability_accelerator
  ]
}
",module,90,,6dca51b135de9dae7aa62a61e7eebc9895c27412,05511992e9cc5b9fdd5c3ba59e30704d78fadda3,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/6dca51b135de9dae7aa62a61e7eebc9895c27412/examples/existing-cluster-with-base-and-infra/main.tf#L90,https://github.com/aws-observability/terraform-aws-observability-accelerator/blob/05511992e9cc5b9fdd5c3ba59e30704d78fadda3/examples/existing-cluster-with-base-and-infra/main.tf,2022-08-26 17:30:03+02:00,2022-08-30 19:59:34+02:00,7,1,0,1,0,0,0,0,1,0
https://github.com/Worklytics/psoxy,744,infra/modules/worklytics-connector-specs/main.tf,infra/modules/worklytics-connector-specs/main.tf,0,# todo,"# TODO: arguably it does make sense to have these in yaml, and read them from there; bc YAML gives","# TODO: arguably it does make sense to have these in yaml, and read them from there; bc YAML gives 
 # more interoperability than in a .tf file  ","locals {

  google_workspace_sources = {
    # GDirectory connections are a PRE-REQ for gmail, gdrive, and gcal connections. remove only
    # if you plan to directly connect Directory to worklytics (without proxy). such a scenario is
    # used for customers who care primarily about pseudonymizing PII of external subjects with whom
    # they collaborate in GMail/GCal/GDrive. the Directory does not contain PII of subjects external
    # to the Google Workspace, so may be directly connected in such scenarios.
    ""gdirectory"" : {
      source_kind : ""gdirectory"",
      worklytics_connector_id : ""gdirectory-psoxy"",
      display_name : ""Google Directory""
      identifier_scope_id : ""gapps""
      apis_consumed : [
        ""admin.googleapis.com""
      ]
      oauth_scopes_needed : [
        ""https://www.googleapis.com/auth/admin.directory.user.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.user.alias.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.domain.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.group.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.group.member.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.orgunit.readonly"",
        ""https://www.googleapis.com/auth/admin.directory.rolemanagement.readonly""
      ]
      source_auth_strategy : ""gcp_service_account_key""
      target_host : ""admin.googleapis.com""
      environment_variables : {}
      example_api_calls : [
        ""/admin/directory/v1/users?customer=my_customer&maxResults=10"",
        ""/admin/directory/v1/groups?customer=my_customer&maxResults=10"",
        ""/admin/directory/v1/customer/my_customer/domains"",
        ""/admin/directory/v1/customer/my_customer/roles?maxResults=10"",
        ""/admin/directory/v1/customer/my_customer/roleassignments?maxResults=10""
      ]
      example_api_calls_user_to_impersonate : var.google_workspace_example_admin
    },
    ""gcal"" : {
      source_kind : ""gcal"",
      worklytics_connector_id : ""gcal-psoxy"",
      display_name : ""Google Calendar""
      identifier_scope_id : ""gapps""
      apis_consumed : [
        ""calendar-json.googleapis.com""
      ]
      source_auth_strategy : ""gcp_service_account_key""
      target_host : ""www.googleapis.com""
      oauth_scopes_needed : [
        ""https://www.googleapis.com/auth/calendar.readonly""
      ],
      environment_variables : {}
      example_api_calls : [
        ""/calendar/v3/calendars/primary"",
        ""/calendar/v3/users/me/settings"",
        ""/calendar/v3/calendars/primary/events?maxResults=10""
      ]
      example_api_calls_user_to_impersonate : var.google_workspace_example_user
    },
    ""gmail"" : {
      source_kind : ""gmail"",
      worklytics_connector_id : ""gmail-meta-psoxy"",
      display_name : ""GMail""
      identifier_scope_id : ""gapps""
      apis_consumed : [
        ""gmail.googleapis.com""
      ]
      source_auth_strategy : ""gcp_service_account_key""
      target_host : ""www.googleapis.com""
      oauth_scopes_needed : [
        ""https://www.googleapis.com/auth/gmail.metadata""
      ],
      environment_variables : {},
      example_api_calls : [
        ""/gmail/v1/users/me/messages?maxResults=10""
      ]
      example_api_calls_user_to_impersonate : var.google_workspace_example_user
    },
    ""google-chat"" : {
      source_kind : ""google-chat"",
      worklytics_connector_id : ""google-chat-psoxy"",
      display_name : ""Google Chat""
      identifier_scope_id : ""gapps""
      apis_consumed : [
        ""admin.googleapis.com""
      ]
      source_auth_strategy : ""gcp_service_account_key""
      target_host : ""admin.googleapis.com""
      oauth_scopes_needed : [
        ""https://www.googleapis.com/auth/admin.reports.audit.readonly""
      ]
      environment_variables : {}
      example_api_calls : [
        ""/admin/reports/v1/activity/users/all/applications/chat?maxResults=10""
      ]
      example_api_calls_user_to_impersonate : var.google_workspace_example_user
    },
    ""google-meet"" : {
      source_kind : ""google-meet""
      worklytics_connector_id : ""google-meet-psoxy""
      display_name : ""Google Meet""
      identifier_scope_id : ""gapps""
      apis_consumed : [
        ""admin.googleapis.com""
      ]
      source_auth_strategy : ""gcp_service_account_key""
      target_host : ""admin.googleapis.com""
      oauth_scopes_needed : [
        ""https://www.googleapis.com/auth/admin.reports.audit.readonly""
      ]
      environment_variables : {}
      example_api_calls : [
        ""/admin/reports/v1/activity/users/all/applications/meet?maxResults=10""
      ]
      example_api_calls_user_to_impersonate : var.google_workspace_example_user
    },
    ""gdrive"" : {
      source_kind : ""gdrive"",
      worklytics_connector_id : ""gdrive-psoxy"",
      display_name : ""Google Drive""
      identifier_scope_id : ""gapps""
      apis_consumed : [
        ""drive.googleapis.com""
      ]
      source_auth_strategy : ""gcp_service_account_key""
      target_host : ""www.googleapis.com""
      oauth_scopes_needed : [
        ""https://www.googleapis.com/auth/drive.metadata.readonly""
      ],
      environment_variables : {}
      example_api_calls : [
        ""/drive/v2/files"",
        ""/drive/v3/files""
      ],
      example_api_calls_user_to_impersonate : var.google_workspace_example_user
    }
  }

  # Microsoft 365 sources; add/remove as you wish
  # See https://docs.microsoft.com/en-us/graph/permissions-reference for all the permissions available in AAD Graph API
  msft_365_connectors = {
    ""azure-ad"" : {
      enabled : true,
      worklytics_connector_id : ""azure-ad-psoxy"",
      source_kind : ""azure-ad"",
      display_name : ""Azure Directory""
      identifier_scope_id : ""azure-ad""
      source_auth_strategy : ""oauth2_refresh_token""
      target_host : ""graph.microsoft.com""
      required_oauth2_permission_scopes : [],
      # Delegated permissions (from `az ad sp list --query ""[?appDisplayName=='Microsoft Graph'].oauth2Permissions"" --all`)
      required_app_roles : [
        # Application permissions (form az ad sp list --query ""[?appDisplayName=='Microsoft Graph'].appRoles"" --all
        ""User.Read.All"",
        ""Group.Read.All""
      ]
      environment_variables : {
        GRANT_TYPE : ""workload_identity_federation"" # by default, assumed to be of type 'urn:ietf:params:oauth:client-assertion-type:jwt-bearer'
        TOKEN_SCOPE : ""https://graph.microsoft.com/.default""
        REFRESH_ENDPOINT = ""https://login.microsoftonline.com/${var.msft_tenant_id}/oauth2/v2.0/token""
      },
      example_api_calls : [
        ""/v1.0/users"",
        ""/v1.0/users/${var.example_msft_user_guid}"",
        ""/v1.0/groups"",
        ""/v1.0/groups/{group-id}/members""
      ]
    },
    ""outlook-cal"" : {
      enabled : true,
      source_kind : ""outlook-cal"",
      worklytics_connector_id : ""outlook-cal-psoxy"",
      display_name : ""Outlook Calendar""
      identifier_scope_id : ""azure-ad""
      source_auth_strategy : ""oauth2_refresh_token""
      target_host : ""graph.microsoft.com""
      required_oauth2_permission_scopes : [],
      required_app_roles : [
        ""OnlineMeetings.Read.All"",
        ""Calendars.Read"",
        ""MailboxSettings.Read"",
        ""Group.Read.All"",
        ""User.Read.All""
      ],
      environment_variables : {
        GRANT_TYPE : ""workload_identity_federation"" # by default, assumed to be of type 'urn:ietf:params:oauth:client-assertion-type:jwt-bearer'
        TOKEN_SCOPE : ""https://graph.microsoft.com/.default""
        REFRESH_ENDPOINT = ""https://login.microsoftonline.com/${var.msft_tenant_id}/oauth2/v2.0/token""
      },
      example_api_calls : [
        ""/v1.0/users"",
        ""/v1.0/users/${var.example_msft_user_guid}/events"",
        ""/v1.0/users/${var.example_msft_user_guid}/calendarView?startDateTime=2022-10-01T00:00:00Z&endDateTime=${timestamp()}"",
        ""/v1.0/users/${var.example_msft_user_guid}/mailboxSettings"",
        ""/v1.0/groups"",
        ""/v1.0/groups/{group-id}/members""
      ]
    },
    ""outlook-mail"" : {
      enabled : true,
      source_kind : ""outlook-mail""
      worklytics_connector_id : ""outlook-mail-psoxy"",
      display_name : ""Outlook Mail""
      identifier_scope_id : ""azure-ad""
      source_auth_strategy : ""oauth2_refresh_token""
      target_host : ""graph.microsoft.com""
      required_oauth2_permission_scopes : [],
      required_app_roles : [
        ""Mail.ReadBasic.All"",
        ""MailboxSettings.Read"",
        ""Group.Read.All"",
        ""User.Read.All""
      ],
      environment_variables : {
        GRANT_TYPE : ""workload_identity_federation"" # by default, assumed to be of type 'urn:ietf:params:oauth:client-assertion-type:jwt-bearer'
        TOKEN_SCOPE : ""https://graph.microsoft.com/.default""
        REFRESH_ENDPOINT : ""https://login.microsoftonline.com/${var.msft_tenant_id}/oauth2/v2.0/token""
      }
      example_api_calls : [
        ""/beta/users"",
        ""/beta/users/${var.example_msft_user_guid}/mailboxSettings"",
        ""/beta/users/${var.example_msft_user_guid}/mailFolders/SentItems/messages"",
        ""/v1.0/groups"",
        ""/v1.0/groups/{group-id}/members""
      ]
    }
  }
  oauth_long_access_connectors = {
    asana = {
      source_kind : ""asana"",
      worklytics_connector_id : ""asana-psoxy""
      display_name : ""Asana""
      identifier_scope_id : ""asana""
      worklytics_connector_name : ""Asana via Psoxy""
      target_host : ""app.asana.com""
      source_auth_strategy : ""oauth2_access_token""
      environment_variables : {}
      secured_variables : [
        { name : ""ACCESS_TOKEN"", writable : false },
      ],
      reserved_concurrent_executions : null
      example_api_calls_user_to_impersonate : null
      example_api_calls : [
        ""/api/1.0/workspaces"",
        ""/api/1.0/users?workspace={ANY_WORKSPACE_GID}&limit=10"",
        ""/api/1.0/workspaces/{ANY_WORKSPACE_GID}/teams&limit=10"",
        ""/api/1.0/teams/{ANY_TEAM_GID}/projects?limit=20"",
        ""/api/1.0/tasks?project={ANY_PROJECT_GID}"",
        ""/api/1.0/tasks/{ANY_TASK_GID}"",
        ""/api/1.0/tasks/{ANY_TASK_GID}/stories"",
      ],
      secured_variables : [
        { name : ""ACCESS_TOKEN"", writable : false },
      ],
      external_token_todo : <<EOT
  1. Create a [Service Account User + token](https://asana.com/guide/help/premium/service-accounts)
     or a sufficiently [Personal Access Token]() for a sufficiently privileged user (who can see all
     the workspaces/teams/projects/tasks you wish to import to Worklytics via this connection).
  2. Update the content of PSOXY_ASANA_ACCESS_TOKEN variable with the previous token value obtained
EOT
    }
    slack-discovery-api = {
      source_kind : ""slack""
      identifier_scope_id : ""slack""
      worklytics_connector_id : ""slack-discovery-api-psoxy"",
      worklytics_connector_name : ""Slack via Psoxy"",
      display_name : ""Slack Discovery API""
      target_host : ""www.slack.com""
      source_auth_strategy : ""oauth2_access_token""
      oauth_scopes_needed : [
        ""discovery:read"",
      ]
      environment_variables : {}
      secured_variables : [
        { name : ""ACCESS_TOKEN"", writable : false },
      ]
      reserved_concurrent_executions : null
      example_api_calls_user_to_impersonate : null
      example_api_calls : [
        ""/api/discovery.enterprise.info"",
        ""/api/discovery.conversations.list"",
        ""/api/discovery.conversations.history?channel={CHANNEL_ID}&limit=10"",
        ""/api/discovery.users.list"",
      ]
      external_token_todo : <<EOT
## Slack Discovery Setup
For enabling Slack Discovery with the Psoxy you must first setup an app on your Slack Enterprise
instance.
  1. Go to https://api.slack.com/apps and create an app, select name a development workspace
  2. Take note of your App ID and contact your Slack rep and ask them to enable `discovery:read` scope for the app.
If they also enable `discovery:write` then delete it for safety, the app just needs read access.
3. Generate the following URL replacing the placeholders for *YOUR_CLIENT_ID* and *YOUR_APP_SECRET* and save it for later
`https://api.slack.com/api/oauth.v2.access?client_id=YOUR_CLIENT_ID&client_secret=YOUR_APP_SECRET`
4. Go to OAuth & Permissions > Redirect URLs and add the previous URL there
The next step depends on your installation approach you might need to change slightly
### Org wide install
Use this step if you want to install in the whole org, across multiple workspaces.
  1. Add a bot scope (not really used, but Slack doesn't allow org-wide without a bot scope requested).
     Just add `users:read`, something that is read-only and we already have access through discovery.
  2. Go to *Org Level Apps* and Opt-in to the program
  3. Go to Settings > Install App
  4. Install into *organization*
  5. Copy the User OAuth Token and store it in secret manager.
  Otherwise, share the token with the AWS/GCP administrator completing the implementation.
### Workspace install
Use this steps if you intend to install in just one workspace within your org.
  1. Go to Settings > Install App
  2. Install into *workspace*
  3. Copy the User OAuth Token and store it in the secret manager (or share with the administrator completing the implementation)
EOT
    }
    zoom = {
      source_kind : ""zoom""
      worklytics_connector_id : ""zoom-psoxy""
      display_name : ""Zoom""
      worklytics_connector_name : ""Zoom via Psoxy""
      identifier_scope_id : ""zoom""
      source_auth_strategy: ""oauth2_refresh_token""
      target_host: ""api.zoom.us""
      environment_variables : {
        GRANT_TYPE: ""account_credentials""
        REFRESH_ENDPOINT: ""https://zoom.us/oauth/token""
      }
      secured_variables : [
        { name : ""CLIENT_SECRET"", writable : false },
        { name : ""CLIENT_ID"", writable : false },
        { name : ""ACCOUNT_ID"", writable : false },
        { name : ""ACCESS_TOKEN"", writable : true },
      ],
      reserved_concurrent_executions : null # 1
      example_api_calls_user_to_impersonate : null
      example_api_calls : [
        ""/v2/users"",
        ""/v2/users/{USER_ID}/meetings"",
        ""/v2/past_meetings/{MEETING_ID}"",
        ""/v2/past_meetings/{MEETING_ID}/instances"",
        ""/v2/past_meetings/{MEETING_ID}/participants"",
        ""/v2/report/users/{userId}/meetings"",
        ""/v2/report/meetings/{meetingId}"",
        ""/v2/report/meetings/{meetingId}/participants""
      ],
      external_token_todo : <<EOT
## Zoom Setup
Zoom connector through Psoxy requires a custom managed app on the Zoom Marketplace (in development
mode, no need to publish).
1. Go to https://marketplace.zoom.us/develop/create and create an app of type ""Server to Server OAuth""
2. After creation it will show the App Credentials. Share them with the AWS/GCP administrator, the
following secret values must be filled in the Secret Manager for the Proxy with the appropriate values:
- `PSOXY_ZOOM_CLIENT_ID`
- `PSOXY_ZOOM_ACCOUNT_ID`
- `PSOXY_ZOOM_CLIENT_SECRET`
Anytime the *client secret* is regenerated it needs to be updated in the Proxy too.
3. Fill the information section
4. Fill the scopes section, enabling the following:
- Users / View all user information /user:read:admin
  - To be able to gather information about the zoom users
- Meetings / View all user meetings /meeting:read:admin
  - Allows us to list all user meeting
- Report / View report data /report:read:admin
  - Last 6 months view for user meetings
5. Activate the app
EOT
    },
    dropbox-business = {
      source_kind : ""dropbox-business""
      worklytics_connector_id : ""dropbox-business-log-psoxy""
      target_host: ""api.dropboxapi.com""
      source_auth_strategy: ""oauth2_refresh_token""
      display_name : ""Dropbox Business""
      identifier_scope_id : ""dropbox-business""
      worklytics_connector_name : ""Dropbox Business via Psoxy""
      secured_variables : [
        { name : ""REFRESH_TOKEN"", writable : false },
        { name : ""CLIENT_ID"", writable : false },
        { name : ""CLIENT_SECRET"", writable : false },
      ],
      environment_variables : {
        GRANT_TYPE : ""refresh_token""
        REFRESH_ENDPOINT : ""https://api.dropboxapi.com/oauth2/token""
      }
      reserved_concurrent_executions : null
      example_api_calls_user_to_impersonate : null
      example_api_calls : [
        ""/2/team/members/list_v2"",
        ""/2/team/groups/list"",
        ""/2/team_log/get_events"",
      ],
      external_token_todo : <<EOT
Dropbox connector through Psoxy requires a Dropbox Application created in Dropbox Console. The application
does not require to be public, and it needs to have the following scopes to support
all the operations for the connector:

- members.read: member listing
- events.read: event listing
- groups.read: group listing

1. Go to https://www.dropbox.com/apps and Build an App
2. Then go https://www.dropbox.com/developers to enter in `App Console` to configure your app
3. Now you are in the app, go to `Permissions` and mark all the scopes described before. NOTE: Probably in the UI will mark you more required permissions automatically (like *account_info_read*.) Just mark the ones
   described and the UI will ask you to include required.
4. On settings, you could access to `App key` and `App secret`. You can create an access token here, but with limited
   expiration. We need to create a long-lived token, so edit the following URL with your `App key` and paste it into the
   browser:

   `https://www.dropbox.com/oauth2/authorize?client_id=<APP_KEY>&token_access_type=offline&response_type=code`

   That will return an `Authorization Code` that you have to paste.
   **NOTE** This `Authorization Code` if for a one single use; if expired or used you will need to get it again pasting
   the
   URL in the browser.
5. Now, replace the values in following URL and run it from command line in your terminal. Replace `Authorization Code`
   , `App key`
   and `App secret` in the placeholders:

   `curl https://api.dropbox.com/oauth2/token -d code=<AUTHORIZATION_CODE> -d grant_type=authorization_code -u <APP_KEY>:<APP_SECRET>`
6. After running that command, if successful you will see a [JSON response](https://www.dropbox.com/developers/Documentation/http/Documentation#oauth2-authorize) like this:

```json
{
  ""access_token"": ""some short live access token"",
  ""token_type"": ""bearer"",
  ""expires_in"": 14399,
  ""refresh_token"": ""some long live token we are going to use"",
  ""scope"": ""account_info.read events.read files.metadata.read groups.read members.read team_data.governance.read team_data.governance.write team_data.member"",
  ""uid"": """",
  ""team_id"": ""some team id""
}
```
7. Finally set following variables in AWS System Manager parameters store / GCP Cloud Secrets (if default implementation):
  - `PSOXY_DROPBOX_BUSINESS_REFRESH_TOKEN` secret variable with value of `refresh_token` received in previous response
  - `PSOXY_DROPBOX_BUSINESS_CLIENT_ID` with `App key` value.
  - `PSOXY_DROPBOX_BUSINESS_CLIENT_SECRET` with `App secret` value.

EOT
    }
  }

  bulk_connectors = {
    ""badge"" = {
      source_kind               = ""badge""
      worklytics_connector_id   = ""bulk-import-psoxy"",
      worklytics_connector_name = ""Bulk Data Import via Psoxy""
      rules = {
        columnsToRedact = []
        columnsToPseudonymize = [
          ""EMPLOYEE_ID"", # primary key
          # ""employee_email"", # if exists
        ]
      }
      settings_to_provide = {
        ""Data Source Processing"" = ""badge""
      }
    }
    ""hris"" = {
      source_kind               = ""hris""
      worklytics_connector_id   = ""bulk-import-psoxy""
      worklytics_connector_name = ""HRIS Data Import via Psoxy""
      rules = {
        columnsToRedact = []
        columnsToPseudonymize = [
          ""EMPLOYEE_ID"",    # primary key
          ""EMPLOYEE_EMAIL"", # for matching
          ""MANAGER_ID"",     # should match to employee_id
          # ""MANAGER_EMAIL""      # if exists
        ]
      }
      settings_to_provide = {
        ""Parser"" = ""EMPLOYEE_SNAPSHOT""
      }
    }
    ""survey"" = {
      worklytics_connector_id   = ""survey-import-psoxy""
      source_kind               = ""survey""
      worklytics_connector_name = ""Survey Data Import via Psoxy""
      rules = {
        columnsToRedact = []
        columnsToPseudonymize = [
          ""EMPLOYEE_ID"", # primary key
          # ""EMPLOYEE_EMAIL"", # if exists
        ]
      }
    }
    ""qualtrics"" = {
      source_kind               = ""qualtrics""
      worklytics_connector_id   = ""survey-import-psoxy""
      worklytics_connector_name = ""Survey Data Import via Psoxy""
      rules = {
        columnsToRedact = []
        columnsToPseudonymize = [
          ""EMPLOYEE_ID"", # primary key
          # ""employee_email"", # if exists
        ]
      }
    }
  }
}
",locals,"resource ""time_static"" ""deployment"" {

}
",resource,2,2.0,312b106f15389e35c0bf553b361f4c21c20cd607,3687e70bceaa637086ceb913629b9462ec220d0c,https://github.com/Worklytics/psoxy/blob/312b106f15389e35c0bf553b361f4c21c20cd607/infra/modules/worklytics-connector-specs/main.tf#L2,https://github.com/Worklytics/psoxy/blob/3687e70bceaa637086ceb913629b9462ec220d0c/infra/modules/worklytics-connector-specs/main.tf#L2,2023-03-01 10:56:50-08:00,2024-05-21 13:49:30-07:00,70,0,0,1,0,0,0,0,0,0
https://github.com/kubernetes/k8s.io,22,infra/gcp/clusters/k8s-infra-prow-build-trusted/prow-build-trusted/main.tf,infra/gcp/clusters/projects/k8s-infra-prow-build-trusted/prow-build-trusted/main.tf,1,// todo,// TODO: I think more people than me should have owner/edit access to this project,// TODO: I think more people than me should have owner/edit access to this project,"module ""project"" {
  source = ""../../modules/k8s-infra-gke-project""
  project_id            = local.project_id
  project_name          = local.project_id
}
",module,"module ""project"" {
  source = ""../../../modules/gke-project""
  project_id            = local.project_id
  project_name          = local.project_id
}
",module,34,,f9cdbcda7e1c4ec32148ade91be7cbec6bf5f2fc,e8ea80d6c56a1b2bf601b509470e6834a837d646,https://github.com/kubernetes/k8s.io/blob/f9cdbcda7e1c4ec32148ade91be7cbec6bf5f2fc/infra/gcp/clusters/k8s-infra-prow-build-trusted/prow-build-trusted/main.tf#L34,https://github.com/kubernetes/k8s.io/blob/e8ea80d6c56a1b2bf601b509470e6834a837d646/infra/gcp/clusters/projects/k8s-infra-prow-build-trusted/prow-build-trusted/main.tf,2020-05-06 14:16:39-07:00,2020-05-15 18:23:44-07:00,4,1,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1481,blueprints/data-solutions/bq-ml/vpc.tf,blueprints/data-solutions/bq-ml/vpc.tf,0,fix,#TODO Remove and rely on 'ssh' tag once terraform-provider-google/issues/9273 is fixed,#TODO Remove and rely on 'ssh' tag once terraform-provider-google/issues/9273 is fixed,"module ""vpc-firewall"" {
  source     = ""../../../modules/net-vpc-firewall""
  count      = local.use_shared_vpc ? 0 : 1
  project_id = module.project.project_id
  network    = module.vpc.0.name
  default_rules_config = {
    admin_ranges = [""10.0.0.0/20""]
  }
  ingress_rules = {
    #TODO Remove and rely on 'ssh' tag once terraform-provider-google/issues/9273 is fixed
    (""${var.prefix}-iap"") = {
      description   = ""Enable SSH from IAP on Notebooks.""
      source_ranges = [""35.235.240.0/20""]
      targets       = [""notebook-instance""]
      rules         = [{ protocol = ""tcp"", ports = [22] }]
    }
  }
}
",module,"module ""vpc-firewall"" {
  source     = ""../../../modules/net-vpc-firewall""
  count      = local.use_shared_vpc ? 0 : 1
  project_id = module.project.project_id
  network    = module.vpc[0].name
  default_rules_config = {
    admin_ranges = [""10.0.0.0/20""]
  }
  ingress_rules = {
    #TODO Remove and rely on 'ssh' tag once terraform-provider-google/issues/9273 is fixed
    (""${var.prefix}-iap"") = {
      description   = ""Enable SSH from IAP on Notebooks.""
      source_ranges = [""35.235.240.0/20""]
      targets       = [""notebook-instance""]
      rules         = [{ protocol = ""tcp"", ports = [22] }]
    }
  }
}
",module,40,40.0,9e19f8960861fe61830801eab27111422f1d7a4e,3af7e257d21f889ffaf7b32a3bab974fdbfda6e4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9e19f8960861fe61830801eab27111422f1d7a4e/blueprints/data-solutions/bq-ml/vpc.tf#L40,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3af7e257d21f889ffaf7b32a3bab974fdbfda6e4/blueprints/data-solutions/bq-ml/vpc.tf#L40,2023-03-05 22:02:41+01:00,2024-04-17 10:23:48+02:00,2,0,0,1,1,1,0,0,0,0
https://github.com/cattle-ops/terraform-aws-gitlab-runner,112,main.tf,main.tf,0,todo,# TODO Please explain how `agent_enable_asg_recreation` works,# TODO Please explain how `agent_enable_asg_recreation` works,"resource ""aws_autoscaling_group"" ""gitlab_runner_instance"" {
  # TODO Please explain how `agent_enable_asg_recreation` works
  name                      = var.runner_enable_asg_recreation ? ""${aws_launch_template.gitlab_runner_instance.name}-asg"" : ""${var.environment}-as-group""
  vpc_zone_identifier       = length(var.runner_worker_docker_machine_instance.subnet_ids) > 0 ? var.runner_worker_docker_machine_instance.subnet_ids : [var.subnet_id]
  min_size                  = ""1""
  max_size                  = ""1""
  desired_capacity          = ""1""
  health_check_grace_period = 0
  max_instance_lifetime     = var.runner_instance.max_lifetime_seconds
  enabled_metrics           = var.runner_instance.collect_autoscaling_metrics

  dynamic ""tag"" {
    for_each = local.agent_tags

    content {
      key                 = tag.key
      value               = tag.value
      propagate_at_launch = true
    }
  }

  launch_template {
    id      = aws_launch_template.gitlab_runner_instance.id
    version = aws_launch_template.gitlab_runner_instance.latest_version
  }

  instance_refresh {
    strategy = ""Rolling""
    preferences {
      min_healthy_percentage = 0
    }
    triggers = [""tag""]
  }

  timeouts {
    delete = var.runner_terraform_timeout_delete_asg
  }
  lifecycle {
    ignore_changes = [min_size, max_size, desired_capacity]
  }
}
",resource,"resource ""aws_autoscaling_group"" ""gitlab_runner_instance"" {
  # TODO Please explain how `agent_enable_asg_recreation` works
  name                      = var.runner_enable_asg_recreation ? ""${aws_launch_template.gitlab_runner_instance.name}-asg"" : ""${var.environment}-as-group""
  vpc_zone_identifier       = length(var.runner_worker_docker_machine_instance.subnet_ids) > 0 ? var.runner_worker_docker_machine_instance.subnet_ids : [var.subnet_id]
  min_size                  = ""1""
  max_size                  = ""1""
  desired_capacity          = ""1""
  health_check_grace_period = 0
  max_instance_lifetime     = var.runner_instance.max_lifetime_seconds
  enabled_metrics           = var.runner_instance.collect_autoscaling_metrics

  dynamic ""tag"" {
    for_each = local.agent_tags

    content {
      key                 = tag.key
      value               = tag.value
      propagate_at_launch = true
    }
  }

  launch_template {
    id      = aws_launch_template.gitlab_runner_instance.id
    version = aws_launch_template.gitlab_runner_instance.latest_version
  }

  instance_refresh {
    strategy = ""Rolling""
    preferences {
      min_healthy_percentage = 0
    }
    triggers = [""tag""]
  }

  timeouts {
    delete = var.runner_terraform_timeout_delete_asg
  }
  lifecycle {
    ignore_changes = [min_size, max_size, desired_capacity]
  }
}
",resource,174,152.0,c8a3b89c46f749214461bade8e1e6d161d0ef860,fec8c8a8d729f8d6076a38d8b063f1e14f4aa518,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/c8a3b89c46f749214461bade8e1e6d161d0ef860/main.tf#L174,https://github.com/cattle-ops/terraform-aws-gitlab-runner/blob/fec8c8a8d729f8d6076a38d8b063f1e14f4aa518/main.tf#L152,2023-09-07 17:14:21+02:00,2024-05-10 11:02:08+02:00,10,0,1,0,0,0,0,0,0,0
https://github.com/uyuni-project/sumaform,1593,modules/server_containerized/main.tf,modules/server_containerized/main.tf,0,// todo,"// TODO: This module is a copy/paste of the server module and some variables are not yet implemented, some could also be dropped, work in-progress.","// TODO: This module is a copy/paste of the server module and some variables are not yet implemented, some could also be dropped, work in-progress. ","variable ""images"" {
  default = {
    ""head""           = ""sles15sp4o""
    ""uyuni-master""   = ""opensuse154o""
    ""uyuni-released"" = ""opensuse154o""
    ""uyuni-pr""       = ""opensuse154o""
  }
}
",variable,"variable ""images"" {
  default = {
    ""head""           = ""slemicro55o""
    ""uyuni-master""   = ""leapmicro55o""
    ""uyuni-released"" = ""leapmicro55o""
    ""uyuni-pr""       = ""leapmicro55o""
  }
}
",variable,1,1.0,d102072b9d7d2bdbb735f2ab16d98fef44f031dd,951f2ff6b900c938712001c5ce9daf9a1119209b,https://github.com/uyuni-project/sumaform/blob/d102072b9d7d2bdbb735f2ab16d98fef44f031dd/modules/server_containerized/main.tf#L1,https://github.com/uyuni-project/sumaform/blob/951f2ff6b900c938712001c5ce9daf9a1119209b/modules/server_containerized/main.tf#L1,2023-07-31 09:35:18+02:00,2024-04-10 13:06:55+02:00,19,0,0,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1982,modules/organization/variables-logging.tf,modules/organization/variables-logging.tf,0,# todo,# TODO: add support for CMEK,# TODO: add support for CMEK,"variable ""logging_settings"" {
  description = ""Default settings for logging resources.""
  type = object({
    # TODO: add support for CMEK
    disable_default_sink = optional(bool)
    storage_location     = optional(string)
  })
  default = null
}
",variable,"variable ""logging_settings"" {
  description = ""Default settings for logging resources.""
  type = object({
    # TODO: add support for CMEK
    disable_default_sink = optional(bool)
    storage_location     = optional(string)
  })
  default = null
}
",variable,42,42.0,604920dec9ceb39e20b9768f482807f2675092ff,604920dec9ceb39e20b9768f482807f2675092ff,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/604920dec9ceb39e20b9768f482807f2675092ff/modules/organization/variables-logging.tf#L42,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/604920dec9ceb39e20b9768f482807f2675092ff/modules/organization/variables-logging.tf#L42,2024-05-13 09:24:17+02:00,2024-05-13 09:24:17+02:00,1,0,0,1,0,1,0,0,1,0
https://github.com/GoogleCloudPlatform/hpc-toolkit,240,community/modules/scheduler/htcondor-service-accounts/main.tf,community/modules/scheduler/htcondor-service-accounts/main.tf,0,implementation,"# underlying implementation changes, this module should declare explicit","/** 
 * Copyright 2024 Google LLC 
 * 
 * Licensed under the Apache License, Version 2.0 (the ""License""); 
 * you may not use this file except in compliance with the License. 
 * You may obtain a copy of the License at 
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, software 
 * distributed under the License is distributed on an ""AS IS"" BASIS, 
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 * See the License for the specific language governing permissions and 
 * limitations under the License. 
 */  
 # NB: the community/modules/project/service-account module will not output the 
 # service account e-mail address until all IAM bindings have been created; if 
 # underlying implementation changes, this module should declare explicit 
 # depends_on the IAM bindings to prevent race conditions for services that 
 # require them ","module ""access_point_service_account"" {
  source = ""github.com/GoogleCloudPlatform/hpc-toolkit//community/modules/project/service-account?ref=v1.28.1&depth=1""

  project_id      = var.project_id
  display_name    = ""HTCondor Access Point""
  deployment_name = var.deployment_name
  name            = ""access""
  project_roles   = var.access_point_roles
}
",module,"module ""access_point_service_account"" {
  source = ""github.com/GoogleCloudPlatform/hpc-toolkit//community/modules/project/service-account?ref=v1.32.1&depth=1""

  project_id      = var.project_id
  display_name    = ""HTCondor Access Point""
  deployment_name = var.deployment_name
  name            = ""access""
  project_roles   = var.access_point_roles
}
",module,19,19.0,1983ad58ecdb0d56ab724713d6ae8ee4058242b3,0aec7fb77c813bd747de536bc927542877d69de6,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/1983ad58ecdb0d56ab724713d6ae8ee4058242b3/community/modules/scheduler/htcondor-service-accounts/main.tf#L19,https://github.com/GoogleCloudPlatform/hpc-toolkit/blob/0aec7fb77c813bd747de536bc927542877d69de6/community/modules/scheduler/htcondor-service-accounts/main.tf#L19,2024-02-21 09:21:11-06:00,2024-04-19 19:04:24+00:00,5,0,0,1,0,1,0,0,0,0
https://github.com/terraform-google-modules/terraform-google-bigquery,39,modules/data_warehouse/main.tf,modules/data_warehouse/main.tf,0,# todo,# TODO: scope this down,# TODO: scope this down,"resource ""google_project_iam_member"" ""cloud_function_service_account_editor_role"" {
  project = var.project_id
  role    = ""roles/editor""
  member  = ""serviceAccount:${google_service_account.cloud_function_service_account.email}""

  depends_on = [
    google_service_account.cloud_function_service_account
  ]
}
",resource,the block associated got renamed or deleted,,64,,ad3c3472b644fe79c37ae1416b28faf5e0cbe271,ef343096883bf6daaaea016ae561f089fab4539c,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/ad3c3472b644fe79c37ae1416b28faf5e0cbe271/modules/data_warehouse/main.tf#L64,https://github.com/terraform-google-modules/terraform-google-bigquery/blob/ef343096883bf6daaaea016ae561f089fab4539c/modules/data_warehouse/main.tf,2023-02-17 11:54:10-06:00,2023-03-15 18:25:56-05:00,5,1,0,0,0,1,0,0,0,0
https://github.com/kubernetes/k8s.io,215,infra/gcp/terraform/k8s-infra-prow-build/main.tf,infra/gcp/terraform/k8s-infra-prow-build/main.tf,0,# todo,# TODO(spiffxp): intent is to replace pool4 with this after scheduling a few select,"# TODO(spiffxp): intent is to replace pool4 with this after scheduling a few select 
 #                jobs to this pool to confirm things work as expected; this will 
 #                involve dropping the taints, raising the max_count","module ""prow_build_nodepool_n1_highmem_8_localssd"" {
  source        = ""../modules/gke-nodepool""
  project_name  = local.project_id
  cluster_name  = module.prow_build_cluster.cluster.name
  location      = module.prow_build_cluster.cluster.location
  name          = ""pool5""
  # NOTE: taints are only applied during creation and ignored after that, see module docs
  taints        = [{ key = ""ephemeral-ssd-experiment"", value = ""true"", effect = ""NO_SCHEDULE"" }]
  initial_count = 1
  min_count     = 1
  max_count     = 5
  # kind-ipv6 jobs need an ipv6 stack; COS doesn't provide one, so we need to
  # use an UBUNTU image instead. Keep parity with the existing google.com
  # k8s-prow-builds/prow cluster by using the CONTAINERD variant
  image_type   = ""UBUNTU_CONTAINERD""
  machine_type = ""n1-highmem-8""
  disk_size_gb    = 100
  disk_type       = ""pd-standard""
  # Use local SSDs for ephemeral storage; each SSD is 375GB
  ephemeral_local_ssd_count =  2
  service_account = module.prow_build_cluster.cluster_node_sa.email
}
",module,"module ""prow_build_nodepool_n1_highmem_8_localssd"" {
  source        = ""../modules/gke-nodepool""
  project_name  = local.project_id
  cluster_name  = module.prow_build_cluster.cluster.name
  location      = module.prow_build_cluster.cluster.location
  name          = ""pool5""
  initial_count = 1
  min_count     = 1
  max_count     = 80
  # kind-ipv6 jobs need an ipv6 stack; COS doesn't provide one, so we need to
  # use an UBUNTU image instead. Keep parity with the existing google.com
  # k8s-prow-builds/prow cluster by using the CONTAINERD variant
  image_type   = ""UBUNTU_CONTAINERD""
  machine_type = ""n1-highmem-8""
  disk_size_gb    = 100
  disk_type       = ""pd-standard""
  # Use local SSDs for ephemeral storage; each SSD is 375GB
  ephemeral_local_ssd_count =  2
  service_account = module.prow_build_cluster.cluster_node_sa.email
}
",module,157,,963ed6bb4beab18030eea9899916522f22f31524,f403678c4aa40765125e4738a91fbfd99addd89b,https://github.com/kubernetes/k8s.io/blob/963ed6bb4beab18030eea9899916522f22f31524/infra/gcp/terraform/k8s-infra-prow-build/main.tf#L157,https://github.com/kubernetes/k8s.io/blob/f403678c4aa40765125e4738a91fbfd99addd89b/infra/gcp/terraform/k8s-infra-prow-build/main.tf,2021-09-27 13:07:57-07:00,2021-09-28 05:47:06-07:00,2,1,1,1,0,0,0,0,0,1
https://github.com/Worklytics/psoxy,536,infra/modules/worklytics-psoxy-connection-generic/variables.tf,infra/modules/worklytics-psoxy-connection-generic/variables.tf,0,fix,"# TODO: fix these validations; logically correct, but Terraform doesn't allow validation conditions","# TODO: fix these validations; logically correct, but Terraform doesn't allow validation conditions 
 # to depend on values of other variables 
 #  validation { 
 #    condition     = var.psoxy_host_platform_id != ""AWS"" || contains(keys(var.settings_to_provide), ""AWS Psoxy Region"") 
 #    error_message = ""For connections to AWS deployments, must provide 'AWS Psoxy Region' in settings_to_provide."" 
 #  } 
 # 
 #  validation { 
 #    condition     = var.psoxy_host_platform_id != ""AWS"" || contains(keys(var.settings_to_provide), ""AWS Psoxy Role ARN"") 
 #    error_message = ""For connections to AWS deployments, must provide 'AWS Psoxy Role ARN' in settings_to_provide."" 
 #  }","variable ""settings_to_provide"" {
  type        = map(string)
  description = ""map of values for installer to copy-paste into connection settings in Worklytics UX""
  default     = {}

  # TODO: fix these validations; logically correct, but Terraform doesn't allow validation conditions
  # to depend on values of other variables
  #  validation {
  #    condition     = var.psoxy_host_platform_id != ""AWS"" || contains(keys(var.settings_to_provide), ""AWS Psoxy Region"")
  #    error_message = ""For connections to AWS deployments, must provide 'AWS Psoxy Region' in settings_to_provide.""
  #  }
  #
  #  validation {
  #    condition     = var.psoxy_host_platform_id != ""AWS"" || contains(keys(var.settings_to_provide), ""AWS Psoxy Role ARN"")
  #    error_message = ""For connections to AWS deployments, must provide 'AWS Psoxy Role ARN' in settings_to_provide.""
  #  }
}
",variable,"variable ""settings_to_provide"" {
  type        = map(string)
  description = ""map of values for installer to copy-paste into connection settings in Worklytics UX""
  default     = {}

  # TODO: fix these validations; logically correct, but Terraform doesn't allow validation conditions
  # to depend on values of other variables
  #  validation {
  #    condition     = var.psoxy_host_platform_id != ""AWS"" || contains(keys(var.settings_to_provide), ""AWS Psoxy Region"")
  #    error_message = ""For connections to AWS deployments, must provide 'AWS Psoxy Region' in settings_to_provide.""
  #  }
  #
  #  validation {
  #    condition     = var.psoxy_host_platform_id != ""AWS"" || contains(keys(var.settings_to_provide), ""AWS Psoxy Role ARN"")
  #    error_message = ""For connections to AWS deployments, must provide 'AWS Psoxy Role ARN' in settings_to_provide.""
  #  }
}
",variable,34,48.0,fc966f9714e332add194d2aacc9c7e328b714c1c,5937e6a45055a94ff2b493f96f21863d91699825,https://github.com/Worklytics/psoxy/blob/fc966f9714e332add194d2aacc9c7e328b714c1c/infra/modules/worklytics-psoxy-connection-generic/variables.tf#L34,https://github.com/Worklytics/psoxy/blob/5937e6a45055a94ff2b493f96f21863d91699825/infra/modules/worklytics-psoxy-connection-generic/variables.tf#L48,2022-12-21 09:04:05-08:00,2024-03-06 18:11:21+00:00,4,0,0,1,1,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,980,fast/stages/03-gke-multitenant/module/main.tf,fast/stages/03-gke-multitenant/module/main.tf,0,# todo,# TODO: depend GKE hub services on GKE hub variable/activation,# TODO: depend GKE hub services on GKE hub variable/activation,"module ""gke-project-0"" {
  source          = ""../../../../modules/project""
  billing_account = var.billing_account.id
  name            = ""dev-gke-clusters-0""
  parent          = var.folder_ids.gke-dev
  prefix          = var.prefix
  group_iam       = var.group_iam
  labels          = local.labels
  # TODO: depend GKE hub services on GKE hub variable/activation
  services = [
    # ""anthosconfigmanagement.googleapis.com"",
    # ""anthos.googleapis.com"",
    ""cloudresourcemanager.googleapis.com"",
    ""container.googleapis.com"",
    ""dns.googleapis.com"",
    # ""gkeconnect.googleapis.com"",
    # ""gkehub.googleapis.com"",
    ""iam.googleapis.com"",
    # ""multiclusteringress.googleapis.com"",
    # ""multiclusterservicediscovery.googleapis.com"",
    ""stackdriver.googleapis.com"",
    # ""trafficdirector.googleapis.com""
  ]
  shared_vpc_service_config = {
    attach       = true
    host_project = var.host_project_ids.dev-spoke-0
    service_identity_iam = {
      ""roles/compute.networkUser"" = [
        ""cloudservices"", ""container-engine""
      ]
      ""roles/container.hostServiceAgentUser"" = [
        ""container-engine""
      ]
    }
  }
  # specify project-level org policies here if you need them
  # policy_boolean = {
  #   ""constraints/compute.disableGuestAttributesAccess"" = true
  # }
  # policy_list = {
  #   ""constraints/compute.trustedImageProjects"" = {
  #     inherit_from_parent = null
  #     suggested_value     = null
  #     status              = true
  #     values              = [""projects/fl01-prod-iac-core-0""]
  #   }
  # }
}
",module,"module ""gke-project-0"" {
  source          = ""../../../../modules/project""
  billing_account = var.billing_account.id
  name            = ""dev-gke-clusters-0""
  parent          = var.folder_ids.gke-dev
  prefix          = var.prefix
  group_iam       = var.group_iam
  labels          = local.labels
  services = concat(
    [
      ""cloudresourcemanager.googleapis.com"",
      ""container.googleapis.com"",
      ""dns.googleapis.com"",
      ""iam.googleapis.com"",
      ""stackdriver.googleapis.com"",
    ],
    !local.fleet_enabled ? [] : [
      ""anthosconfigmanagement.googleapis.com"",
      ""anthos.googleapis.com"",
      ""gkeconnect.googleapis.com"",
      ""gkehub.googleapis.com"",
      ""multiclusteringress.googleapis.com"",
      ""multiclusterservicediscovery.googleapis.com"",
      ""trafficdirector.googleapis.com""
    ]
  )
  shared_vpc_service_config = {
    attach       = true
    host_project = var.host_project_ids.dev-spoke-0
    service_identity_iam = merge({
      ""roles/compute.networkUser"" = [
        ""cloudservices"", ""container-engine""
      ]
      ""roles/container.hostServiceAgentUser"" = [
        ""container-engine""
      ]
      },
      !local.fleet_mcs_enabled ? {} : {
        ""roles/multiclusterservicediscovery.serviceAgent"" = [""gke-mcs""]
        ""roles/compute.networkViewer""                     = [""gke-mcs-importer""]
    })
  }
  # specify project-level org policies here if you need them
  # policy_boolean = {
  #   ""constraints/compute.disableGuestAttributesAccess"" = true
  # }
  # policy_list = {
  #   ""constraints/compute.trustedImageProjects"" = {
  #     inherit_from_parent = null
  #     suggested_value     = null
  #     status              = true
  #     values              = [""projects/fl01-prod-iac-core-0""]
  #   }
  # }
}
",module,29,,1260db923e8179de72cf63b79f1381791b30885f,c24e66138339bb5c599e52cd88236267acac4611,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/1260db923e8179de72cf63b79f1381791b30885f/fast/stages/03-gke-multitenant/module/main.tf#L29,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/c24e66138339bb5c599e52cd88236267acac4611/fast/stages/03-gke-multitenant/module/main.tf,2022-07-29 10:49:50+02:00,2022-07-29 14:01:35+02:00,3,1,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,1480,blueprints/data-solutions/bq-ml/vertex.tf,blueprints/data-solutions/bq-ml/vertex.tf,0,fix,#TODO Uncomment once terraform-provider-google/issues/9273 is fixed,"#TODO Uncomment once terraform-provider-google/issues/9273 is fixed 
 # tags = [""ssh""]","resource ""google_notebooks_instance"" ""playground"" {
  name         = ""${var.prefix}-notebook""
  location     = format(""%s-%s"", var.region, ""b"")
  machine_type = ""e2-medium""
  project      = module.project.project_id

  container_image {
    repository = ""gcr.io/deeplearning-platform-release/base-cpu""
    tag        = ""latest""
  }

  install_gpu_driver = true
  boot_disk_type     = ""PD_SSD""
  boot_disk_size_gb  = 110
  disk_encryption    = try(local.service_encryption_keys.compute != null, false) ? ""CMEK"" : null
  kms_key            = try(local.service_encryption_keys.compute, null)

  no_public_ip    = true
  no_proxy_access = false

  network = local.vpc
  subnet  = local.subnet

  service_account = module.service-account-notebook.email

  # Enable Secure Boot 
  shielded_instance_config {
    enable_secure_boot = true
  }

  # Remove once terraform-provider-google/issues/9164 is fixed
  lifecycle {
    ignore_changes = [disk_encryption, kms_key]
  }

  #TODO Uncomment once terraform-provider-google/issues/9273 is fixed
  # tags = [""ssh""]
  depends_on = [
    google_project_iam_member.shared_vpc,
  ]
}
",resource,"resource ""google_notebooks_instance"" ""playground"" {
  name         = ""${var.prefix}-notebook""
  location     = format(""%s-%s"", var.region, ""b"")
  machine_type = ""e2-medium""
  project      = module.project.project_id

  container_image {
    repository = ""gcr.io/deeplearning-platform-release/base-cpu""
    tag        = ""latest""
  }

  install_gpu_driver = true
  boot_disk_type     = ""PD_SSD""
  boot_disk_size_gb  = 110
  disk_encryption    = try(local.service_encryption_keys.compute != null, false) ? ""CMEK"" : null
  kms_key            = try(local.service_encryption_keys.compute, null)

  no_public_ip    = true
  no_proxy_access = false

  network = local.vpc
  subnet  = local.subnet

  service_account = module.service-account-notebook.email

  # Enable Secure Boot 
  shielded_instance_config {
    enable_secure_boot = true
  }

  # Remove once terraform-provider-google/issues/9164 is fixed
  lifecycle {
    ignore_changes = [disk_encryption, kms_key]
  }

  #TODO Uncomment once terraform-provider-google/issues/9273 is fixed
  # tags = [""ssh""]
  depends_on = [
    google_project_iam_member.shared_vpc,
  ]
}
",resource,99,106.0,9e19f8960861fe61830801eab27111422f1d7a4e,3f9bbc2e5c57ccad4b0532107eef8dd9245d1627,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/9e19f8960861fe61830801eab27111422f1d7a4e/blueprints/data-solutions/bq-ml/vertex.tf#L99,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3f9bbc2e5c57ccad4b0532107eef8dd9245d1627/blueprints/data-solutions/bq-ml/vertex.tf#L106,2023-03-05 22:02:41+01:00,2023-03-09 09:13:21+01:00,2,0,1,1,1,0,0,0,0,0
https://github.com/kubernetes/k8s.io,226,infra/gcp/terraform/k8s-infra-prow-build-trusted/main.tf,infra/gcp/terraform/k8s-infra-prow-build-trusted/main.tf,0,// todo,// TODO: this role belongs in k8s-infra-prow-build,// TODO: this role belongs in k8s-infra-prow-build,"resource ""google_project_iam_member"" ""prow_deployer_for_prow_build"" {
  project = ""k8s-infra-prow-build""
  role    = ""roles/container.admin""
  member  = ""serviceAccount:prow-deployer@k8s-infra-prow-build-trusted.iam.gserviceaccount.com""
}
",resource,the block associated got renamed or deleted,,101,,a3ad1141d5391ea466f6e15dce112d7d2be2c89b,e48cc4ae5c901edac148bb40e05077612b80ac64,https://github.com/kubernetes/k8s.io/blob/a3ad1141d5391ea466f6e15dce112d7d2be2c89b/infra/gcp/terraform/k8s-infra-prow-build-trusted/main.tf#L101,https://github.com/kubernetes/k8s.io/blob/e48cc4ae5c901edac148bb40e05077612b80ac64/infra/gcp/terraform/k8s-infra-prow-build-trusted/main.tf,2021-09-28 20:31:14-07:00,2021-09-29 06:31:05-07:00,3,1,0,0,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,288,modules/compute-vm/main.tf,modules/compute-vm/main.tf,0,# todo,# TODO: check if self link works here,# TODO: check if self link works here,"resource ""google_compute_region_disk"" ""disks"" {
  provider = google-beta
  for_each = var.create_template ? {} : {
    for k, v in local.attached_disks_regional :
    k => v if v.source_type != ""attach""
  }
  project       = var.project_id
  region        = local.region
  replica_zones = [var.zone, each.value.options.replica_zone]
  name          = ""${var.name}-${each.key}""
  type          = each.value.options.type
  size          = each.value.size
  # image         = each.value.source_type == ""image"" ? each.value.source : null
  snapshot = each.value.source_type == ""snapshot"" ? each.value.source : null
  labels = merge(var.labels, {
    disk_name = each.value.name
    disk_type = each.value.options.type
  })
  dynamic ""disk_encryption_key"" {
    for_each = var.encryption != null ? [""""] : []
    content {
      raw_key = var.encryption.disk_encryption_key_raw
      # TODO: check if self link works here
      kms_key_name = var.encryption.kms_key_self_link
    }
  }
}
",resource,"resource ""google_compute_region_disk"" ""disks"" {
  provider = google-beta
  for_each = var.create_template ? {} : {
    for k, v in local.attached_disks_regional :
    k => v if v.source_type != ""attach""
  }
  project       = var.project_id
  region        = local.region
  replica_zones = [var.zone, each.value.options.replica_zone]
  name          = ""${var.name}-${each.key}""
  type          = each.value.options.type
  size          = each.value.size
  # image         = each.value.source_type == ""image"" ? each.value.source : null
  snapshot = each.value.source_type == ""snapshot"" ? each.value.source : null
  labels = merge(var.labels, {
    disk_name = each.value.name
    disk_type = each.value.options.type
  })
  dynamic ""disk_encryption_key"" {
    for_each = var.encryption != null ? [""""] : []
    content {
      raw_key = var.encryption.disk_encryption_key_raw
      # TODO: check if self link works here
      kms_key_name = var.encryption.kms_key_self_link
    }
  }
}
",resource,112,142.0,262f823464462dfd7c75f8917835e829e2f88bf7,3af7e257d21f889ffaf7b32a3bab974fdbfda6e4,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/262f823464462dfd7c75f8917835e829e2f88bf7/modules/compute-vm/main.tf#L112,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3af7e257d21f889ffaf7b32a3bab974fdbfda6e4/modules/compute-vm/main.tf#L142,2021-10-04 10:46:44+02:00,2024-04-17 10:23:48+02:00,24,0,0,1,0,1,0,0,0,1
https://github.com/compiler-explorer/infra,224,terraform/cloudfront.tf,terraform/cloudfront.tf,0,todo,# TODO change,"  name  = ""RateLimitCompilerExplorer"" # TODO change","resource ""aws_wafv2_web_acl"" ""rate_limit"" {
  name  = ""RateLimitCompilerExplorer"" # TODO change
  scope = ""CLOUDFRONT""
  default_action {
    allow {}
  }

  rule {
    name     = ""deny-ipv4""
    priority = 0
    action {
      block {}
    }
    statement {
      ip_set_reference_statement {
        arn = aws_wafv2_ip_set.banned-ipv4.arn
        ip_set_forwarded_ip_config {
          fallback_behavior = ""MATCH""
          header_name       = ""X-Forwarded-For""
          position          = ""ANY""
        }
      }
    }
    visibility_config {
      cloudwatch_metrics_enabled = true
      metric_name                = ""deny-ipv4""
      sampled_requests_enabled   = true
    }
  }

  rule {
    name     = ""deny-ipv6""
    priority = 1
    action {
      block {}
    }
    statement {
      ip_set_reference_statement {
        arn = aws_wafv2_ip_set.banned-ipv6.arn
        ip_set_forwarded_ip_config {
          fallback_behavior = ""MATCH""
          header_name       = ""X-Forwarded-For""
          position          = ""ANY""
        }
      }
    }
    visibility_config {
      cloudwatch_metrics_enabled = true
      metric_name                = ""deny-ipv6""
      sampled_requests_enabled   = true
    }
  }

  rule {
    name     = ""RateLimitPost""
    priority = 2
    action {
      block {}
    }
    statement {
      rate_based_statement {
        // Limit to this many per 5 minutes (300 seconds)
        limit              = 300
        aggregate_key_type = ""IP""
        scope_down_statement {
          byte_match_statement {
            positional_constraint = ""EXACTLY""
            search_string         = ""POST""
            field_to_match {
              method {}
            }
            text_transformation {
              priority = 0
              type     = ""NONE""
            }
          }
        }
      }
    }
    visibility_config {
      cloudwatch_metrics_enabled = true
      metric_name                = ""ce_rate_limited_blocked""
      sampled_requests_enabled   = true
    }
  }

  visibility_config {
    cloudwatch_metrics_enabled = true
    metric_name                = ""request_ok""
    sampled_requests_enabled   = true
  }
}
",resource,the block associated got renamed or deleted,,502,,5984967d8fe56d0296be70c38a7c15dd8e16615d,d9f55d45f1463a6f60c5e16029a41c538165536d,https://github.com/compiler-explorer/infra/blob/5984967d8fe56d0296be70c38a7c15dd8e16615d/terraform/cloudfront.tf#L502,https://github.com/compiler-explorer/infra/blob/d9f55d45f1463a6f60c5e16029a41c538165536d/terraform/cloudfront.tf,2022-10-06 19:03:41-05:00,2022-10-06 19:56:07-05:00,3,1,0,1,0,0,0,0,0,0
https://github.com/kubernetes/k8s.io,119,infra/gcp/clusters/projects/k8s-infra-ii-sandbox/provider.tf,infra/gcp/terraform/k8s-infra-ii-sandbox/provider.tf,1,// todo,"// TODO(spiffxp): the names not matching weirds me out a bit, it would be","// TODO(spiffxp): the names not matching weirds me out a bit, it would be 
 //                nice to rename the project at some point","terraform {

  backend ""gcs"" {
    bucket = ""k8s-infra-tf-sandbox-ii""
    // TODO(spiffxp): the names not matching weirds me out a bit, it would be
    //                nice to rename the project at some point
    prefix = ""k8s-infra-ii-sandbox""
  }


  required_providers {
    google = {
      source  = ""hashicorp/google""
      version = ""~> 3.46.0""
    }
    google-beta = {
      source  = ""hashicorp/google-beta""
      version = ""~> 3.46.0""
    }
  }
}
",terraform,"terraform {

  backend ""gcs"" {
    bucket = ""k8s-infra-tf-sandbox-ii""
    // TODO(spiffxp): the names not matching weirds me out a bit, it would be
    //                nice to rename the project at some point
    prefix = ""k8s-infra-ii-sandbox""
  }


  required_providers {
    google = {
      source  = ""hashicorp/google""
      version = ""~> 4.73.2""
    }
    google-beta = {
      source  = ""hashicorp/google-beta""
      version = ""~> 4.73.2""
    }
  }
}
",terraform,11,27.0,54c58f67e60fd834cbd42c91d6347711a0be1def,e1d6f748424d52ef72ce49ad744f7c10854717db,https://github.com/kubernetes/k8s.io/blob/54c58f67e60fd834cbd42c91d6347711a0be1def/infra/gcp/clusters/projects/k8s-infra-ii-sandbox/provider.tf#L11,https://github.com/kubernetes/k8s.io/blob/e1d6f748424d52ef72ce49ad744f7c10854717db/infra/gcp/terraform/k8s-infra-ii-sandbox/provider.tf#L27,2021-05-05 01:50:41-04:00,2023-07-18 00:00:05+02:00,9,0,1,1,0,0,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,406,modules/vpc-sc/service_perimeters_bridge.tf,modules/vpc-sc/service-perimeters-bridge.tf,1,implement,"# this code implements ""additive"" service perimeters, if ""authoritative""","/** 
 * Copyright 2021 Google LLC 
 * 
 * Licensed under the Apache License, Version 2.0 (the ""License""); 
 * you may not use this file except in compliance with the License. 
 * You may obtain a copy of the License at 
 * 
 *      http://www.apache.org/licenses/LICENSE-2.0 
 * 
 * Unless required by applicable law or agreed to in writing, software 
 * distributed under the License is distributed on an ""AS IS"" BASIS, 
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. 
 * See the License for the specific language governing permissions and 
 * limitations under the License. 
 */  
 # this code implements ""additive"" service perimeters, if ""authoritative"" 
 # service perimeters are needed, switch to the 
 # google_access_context_manager_service_perimeters resource ","resource ""google_access_context_manager_service_perimeter"" ""bridge"" {
  for_each                  = var.service_perimeters_bridge
  parent                    = ""accessPolicies/${local.access_policy}""
  name                      = ""accessPolicies/${local.access_policy}/servicePerimeters/${each.key}""
  title                     = each.key
  perimeter_type            = ""PERIMETER_TYPE_BRIDGE""
  use_explicit_dry_run_spec = each.value.use_explicit_dry_run_spec
  spec {
    resources = each.value.spec_resources
  }
  status {
    resources = each.value.status_resources
  }
  lifecycle {
    ignore_changes = [spec[0].resources, status[0].resources]
  }
  depends_on = [
    google_access_context_manager_access_policy.default,
    google_access_context_manager_access_level.basic
  ]
}
",resource,"resource ""google_access_context_manager_service_perimeter"" ""bridge"" {
  for_each                  = var.service_perimeters_bridge
  parent                    = ""accessPolicies/${local.access_policy}""
  name                      = ""accessPolicies/${local.access_policy}/servicePerimeters/${each.key}""
  title                     = each.key
  perimeter_type            = ""PERIMETER_TYPE_BRIDGE""
  use_explicit_dry_run_spec = each.value.use_explicit_dry_run_spec

  dynamic ""spec"" {
    for_each = each.value.spec_resources == null ? [] : [""""]
    content {
      resources = each.value.spec_resources
    }
  }

  status {
    resources = each.value.status_resources == null ? [] : each.value.status_resources
  }

  # lifecycle {
  #   ignore_changes = [spec[0].resources, status[0].resources]
  # }

  depends_on = [
    google_access_context_manager_access_policy.default,
    google_access_context_manager_access_level.basic,
    google_access_context_manager_service_perimeter.regular
  ]
}
",resource,17,19.0,2c7dab3bb2b572c4d60bc91ba48eb9e4ffbb8913,aecb6fd543f1f2e9cc852a19996aef2922ecd145,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/2c7dab3bb2b572c4d60bc91ba48eb9e4ffbb8913/modules/vpc-sc/service_perimeters_bridge.tf#L17,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/aecb6fd543f1f2e9cc852a19996aef2922ecd145/modules/vpc-sc/service-perimeters-bridge.tf#L19,2021-12-31 13:29:22+01:00,2023-02-25 16:04:19+00:00,7,0,0,1,0,1,0,0,0,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,852,fast/stages/03-gke-multitenant/prod/gke-nodepools.tf,fast/stages/03-gke-multitenant/_module/gke-nodepools.tf,1,# todo,# TODO(jccb): can we use spot instances here?,# TODO(jccb): can we use spot instances here?,"module ""gke_1_nodepool"" {
  source             = ""../../../../modules/gke-nodepool""
  for_each           = local.nodepools
  name               = each.value.name
  project_id         = module.gke-project-0.project_id
  cluster_name       = module.gke-cluster[each.value.cluster].name
  location           = module.gke-cluster[each.value.cluster].location
  initial_node_count = each.value.node_count
  node_machine_type  = each.value.node_type
  # TODO(jccb): can we use spot instances here?
  node_preemptible = each.value.preemptible

  node_count = each.value.node_count
  # node_count = (
  #   each.value.autoscaling_config == null ? each.value.node_count : null
  # )
  # dynamic ""autoscaling_config"" {
  #   for_each = each.value.autoscaling_config == null ? {} : { 1 = 1 }
  #   content {
  #     min_node_count = each.value.autoscaling_config.min_node_count
  #     max_node_count = each.value.autoscaling_config.max_node_count
  #   }
  # }

  # overrides
  node_locations    = each.value.overrides.node_locations
  max_pods_per_node = each.value.overrides.max_pods_per_node
  node_image_type   = each.value.overrides.image_type
  node_tags         = each.value.overrides.node_tags
  node_taints       = each.value.overrides.node_taints

  management_config = {
    auto_repair  = true
    auto_upgrade = true
  }

  node_service_account_create = true
}
",module,the block associated got renamed or deleted,,39,,f3f9a4a88cedd64f6fc91b64666046aa6726a2f3,3745b2885ef1a43b46c663221e77aba0f2eee817,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/f3f9a4a88cedd64f6fc91b64666046aa6726a2f3/fast/stages/03-gke-multitenant/prod/gke-nodepools.tf#L39,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/3745b2885ef1a43b46c663221e77aba0f2eee817/fast/stages/03-gke-multitenant/_module/gke-nodepools.tf,2022-06-08 11:41:50+02:00,2022-08-06 11:00:46+02:00,4,1,1,1,0,0,0,0,0,1
https://github.com/ministryofjustice/cloud-platform-infrastructure,497,terraform/aws-accounts/cloud-platform-aws/account/cloudfront-waf.tf,terraform/aws-accounts/cloud-platform-aws/account/cloudfront-waf.tf,0,implement,# This is a very specific AWS WAF implemention for use with CloudFront only,"# This is a very specific AWS WAF implemention for use with CloudFront only 
 # Users with an ingress must only utilise ModSecurity or the WAF-enabled centralised ingress 
 # This WAF IPSet, rule, and corresponding WAF ACL is very specific to Prisoner Content Hub 
 # who stream videos from an S3 bucket which is limited to access within a prison estate 
 # Using AWS WAF is/was the only way to IP allowlist with CloudFront (as at 2023-10-12). 
 # We should explore alternatives as they become available.  
 # resource ""aws_ssm_parameter"" ""prisoner_content_hub_production"" { 
 #   name  = ""/prisoner-content-hub-production/ip-allow-list"" 
 #   type  = ""String"" 
 #   value = ""bar"" 
 # }  
 # resource ""aws_ssm_parameter"" ""prisoner_content_hub_staging"" { 
 #   name  = ""/prisoner-content-hub-staging/ip-allow-list"" 
 #   type  = ""String"" 
 #   value = ""bar"" 
 # }  
 # resource ""aws_ssm_parameter"" ""prisoner_content_hub_development"" { 
 #   name  = ""/prisoner-content-hub-development/ip-allow-list"" 
 #   type  = ""String"" 
 #   value = ""bar"" 
 # } ","data ""aws_ssm_parameter"" ""test"" {
  name = ""/prisoner-content-hub-test/ip-allow-list""
}
",data,"locals {
  prisoner_content_hub_environments = toset([""production"", ""staging"", ""development""])
}
",locals,1,1.0,1ae9a83fb9a5ccf6da6b53b29bb6ef8ecdc36d0d,33ffa7decc6224b56c3095105529593748aa8707,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/1ae9a83fb9a5ccf6da6b53b29bb6ef8ecdc36d0d/terraform/aws-accounts/cloud-platform-aws/account/cloudfront-waf.tf#L1,https://github.com/ministryofjustice/cloud-platform-infrastructure/blob/33ffa7decc6224b56c3095105529593748aa8707/terraform/aws-accounts/cloud-platform-aws/account/cloudfront-waf.tf#L1,2023-10-12 11:54:37+01:00,2023-11-07 10:40:55+00:00,5,0,1,1,0,0,1,0,0,0
https://github.com/chanzuckerberg/cztack,9,aws-s3-private-bucket/main.tf,aws-s3-private-bucket/main.tf,0,# todo,# TODO,"# TODO 
 #   logging { 
 #   target_bucket = """" 
 #   target_prefix = """" 
 # } ","resource ""aws_s3_bucket"" ""bucket"" {
  bucket = ""${var.bucket_name}""
  acl    = ""private""

  policy = ""${data.aws_iam_policy_document.bucket_policy.json}""

  versioning {
    enabled = true
  }

  # TODO
  #   logging {
  #   target_bucket = """"
  #   target_prefix = """"
  # }

  server_side_encryption_configuration {
    rule {
      apply_server_side_encryption_by_default {
        sse_algorithm = ""AES256""
      }
    }
  }
  tags = ""${local.tags}""
}
",resource,"resource ""aws_s3_bucket"" ""bucket"" {
  bucket = var.bucket_name
  # `grant` and `acl` conflict with each other - https://www.terraform.io/docs/providers/aws/r/s3_bucket.html#acl

  # Using canned ACL will conflict with using grant ACL
  acl = local.acl

  dynamic ""grant"" {
    for_each = local.valid_grants

    content {
      id          = lookup(grant.value, ""canonical_user_id"", null)
      uri         = lookup(grant.value, ""uri"", null)
      permissions = grant.value.permissions
      type        = lookup(grant.value, ""canonical_user_id"", null) == null ? ""Group"" : ""CanonicalUser""
    }
  }

  versioning {
    enabled = var.enable_versioning
  }

  dynamic ""cors_rule"" {
    for_each = var.cors_rules

    content {
      allowed_headers = lookup(cors_rule.value, ""allowed_headers"", null)
      allowed_methods = lookup(cors_rule.value, ""allowed_methods"", null)
      allowed_origins = lookup(cors_rule.value, ""allowed_origins"", null)
      expose_headers  = lookup(cors_rule.value, ""expose_headers"", null)
      max_age_seconds = lookup(cors_rule.value, ""max_age_seconds"", null)
    }
  }

  acceleration_status = var.transfer_acceleration ? ""Enabled"" : ""Suspended""

  # dynamic block used instead of simply assigning a variable b/c lifecycle_rule is configuration block
  dynamic ""lifecycle_rule"" {
    for_each = var.lifecycle_rules

    content {
      id      = lookup(lifecycle_rule.value, ""id"", null) #lookup() provides default value in case it does not exist in var.lifecycle_rules input
      prefix  = lookup(lifecycle_rule.value, ""prefix"", null)
      tags    = lookup(lifecycle_rule.value, ""tags"", null)
      enabled = lookup(lifecycle_rule.value, ""enabled"", false)
      # var.abort_incomplete_multipart_upload_days is 14 by default
      abort_incomplete_multipart_upload_days = lookup(lifecycle_rule.value, ""abort_incomplete_multipart_upload_days"", var.abort_incomplete_multipart_upload_days)

      dynamic ""expiration"" {
        for_each = length(keys(lookup(lifecycle_rule.value, ""expiration"", {}))) == 0 ? [] : [lookup(lifecycle_rule.value, ""expiration"", {})]

        content {
          date                         = lookup(expiration.value, ""date"", null)
          days                         = lookup(expiration.value, ""days"", null)
          expired_object_delete_marker = lookup(expiration.value, ""expired_object_delete_marker"", null)
        }
      }

      dynamic ""transition"" {
        for_each = length(keys(lookup(lifecycle_rule.value, ""transition"", {}))) == 0 ? [] : [lookup(lifecycle_rule.value, ""transition"", {})]

        content {
          date          = lookup(transition.value, ""date"", null)
          days          = lookup(transition.value, ""days"", null)
          storage_class = lookup(transition.value, ""storage_class"", null)
        }
      }

      dynamic ""noncurrent_version_expiration"" {
        for_each = length(keys(lookup(lifecycle_rule.value, ""noncurrent_version_expiration"", {}))) == 0 ? [] : [lookup(lifecycle_rule.value, ""noncurrent_version_expiration"", {})]

        content {
          days = lookup(noncurrent_version_expiration.value, ""days"", null)
        }
      }

      dynamic ""noncurrent_version_transition"" {
        for_each = length(keys(lookup(lifecycle_rule.value, ""noncurrent_version_transition"", {}))) == 0 ? [] : [lookup(lifecycle_rule.value, ""noncurrent_version_transition"", {})]

        content {
          days          = lookup(lifecycle_rule.value.noncurrent_version_transition, ""days"", null)
          storage_class = lookup(lifecycle_rule.value.noncurrent_version_transition, ""storage_class"", null)
        }
      }
    }
  }

  dynamic ""logging"" {
    for_each = var.logging_bucket == null ? [] : [var.logging_bucket]
    content {
      target_bucket = var.logging_bucket.name
      target_prefix = var.logging_bucket.prefix
    }
  }

  server_side_encryption_configuration {
    rule {
      apply_server_side_encryption_by_default {
        sse_algorithm = ""AES256""
      }
    }
  }
  tags = local.tags
}
",resource,21,,22e21f1ad3ed47710fa56060efc74a5c531c574f,c0c5731e3901a2e1bb1fde5c2b3a5337afdb8ac1,https://github.com/chanzuckerberg/cztack/blob/22e21f1ad3ed47710fa56060efc74a5c531c574f/aws-s3-private-bucket/main.tf#L21,https://github.com/chanzuckerberg/cztack/blob/c0c5731e3901a2e1bb1fde5c2b3a5337afdb8ac1/aws-s3-private-bucket/main.tf,2019-08-20 10:49:00-07:00,2021-04-15 16:45:51-07:00,17,1,1,1,0,0,0,0,1,0
https://github.com/GoogleCloudPlatform/cloud-foundation-fabric,65,modules/gke-cluster/main.tf,modules/gke-cluster-standard/main.tf,1,// todo,// TODO: support GPUs too,// TODO: support GPUs too,"resource ""google_container_cluster"" ""cluster"" {
  provider                    = google-beta
  project                     = var.project_id
  name                        = var.name
  description                 = var.description
  location                    = var.location
  node_locations              = length(var.node_locations) == 0 ? null : var.node_locations
  min_master_version          = var.min_master_version
  network                     = var.network
  subnetwork                  = var.subnetwork
  logging_service             = var.logging_service
  monitoring_service          = var.monitoring_service
  resource_labels             = var.labels
  default_max_pods_per_node   = var.default_max_pods_per_node
  enable_binary_authorization = var.enable_binary_authorization
  enable_intranode_visibility = var.enable_intranode_visibility
  enable_shielded_nodes       = var.enable_shielded_nodes
  enable_tpu                  = var.enable_tpu
  initial_node_count          = 1
  remove_default_node_pool    = true

  # node_config {}
  # NOTE: Default node_pool is deleted, so node_config (here) is extranneous.
  # Specify that node_config as an parameter to gke-nodepool module instead.

  # TODO(ludomagno): compute addons map in locals and use a single dynamic block
  addons_config {
    dns_cache_config {
      enabled = var.addons.dns_cache_config
    }
    http_load_balancing {
      disabled = ! var.addons.http_load_balancing
    }
    horizontal_pod_autoscaling {
      disabled = ! var.addons.horizontal_pod_autoscaling
    }
    network_policy_config {
      disabled = ! var.addons.network_policy_config
    }
    # beta addons
    # cloudrun is dynamic as it tends to trigger cluster recreation on change
    dynamic cloudrun_config {
      for_each = var.addons.istio_config.enabled && var.addons.cloudrun_config ? [""""] : []
      content {
        disabled = false
      }
    }
    istio_config {
      disabled = ! var.addons.istio_config.enabled
      auth     = var.addons.istio_config.tls ? ""AUTH_MUTUAL_TLS"" : ""AUTH_NONE""
    }
  }

  # TODO(ludomagno): support setting address ranges instead of range names
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#cluster_ipv4_cidr_block
  ip_allocation_policy {
    cluster_secondary_range_name  = var.secondary_range_pods
    services_secondary_range_name = var.secondary_range_services
  }

  # TODO(ludomagno): make optional, and support beta feature
  # https://www.terraform.io/docs/providers/google/r/container_cluster.html#daily_maintenance_window
  maintenance_policy {
    daily_maintenance_window {
      start_time = var.maintenance_start_time
    }
  }

  master_auth {
    client_certificate_config {
      issue_client_certificate = false
    }
  }

  dynamic master_authorized_networks_config {
    for_each = length(var.master_authorized_ranges) == 0 ? [] : list(var.master_authorized_ranges)
    iterator = ranges
    content {
      dynamic cidr_blocks {
        for_each = ranges.value
        iterator = range
        content {
          cidr_block   = range.value
          display_name = range.key
        }
      }
    }
  }

  dynamic network_policy {
    for_each = var.addons.network_policy_config ? [""""] : []
    content {
      enabled  = true
      provider = ""CALICO""
    }
  }

  dynamic private_cluster_config {
    for_each = local.is_private ? [var.private_cluster_config] : []
    iterator = config
    content {
      enable_private_nodes    = config.value.enable_private_nodes
      enable_private_endpoint = config.value.enable_private_endpoint
      master_ipv4_cidr_block  = config.value.master_ipv4_cidr_block
    }
  }

  # beta features

  dynamic authenticator_groups_config {
    for_each = var.authenticator_security_group == null ? [] : [""""]
    content {
      security_group = var.authenticator_security_group
    }
  }

  dynamic cluster_autoscaling {
    for_each = var.cluster_autoscaling.enabled ? [var.cluster_autoscaling] : []
    iterator = config
    content {
      enabled = true
      resource_limits {
        resource_type = ""cpu""
        minimum       = config.value.cpu_min
        maximum       = config.value.cpu_max
      }
      resource_limits {
        resource_type = ""memory""
        minimum       = config.value.memory_min
        maximum       = config.value.memory_max
      }
      // TODO: support GPUs too
    }
  }

  dynamic database_encryption {
    for_each = var.database_encryption.enabled ? [var.database_encryption] : []
    iterator = config
    content {
      state    = config.value.state
      key_name = config.value.key_name
    }
  }

  dynamic pod_security_policy_config {
    for_each = var.pod_security_policy != null ? [""""] : []
    content {
      enabled = var.pod_security_policy
    }
  }

  dynamic release_channel {
    for_each = var.release_channel != null ? [""""] : []
    content {
      channel = var.release_channel
    }
  }

  dynamic resource_usage_export_config {
    for_each = (
      var.resource_usage_export_config.enabled != null
      &&
      var.resource_usage_export_config.dataset != null
      ? [""""] : []
    )
    content {
      enable_network_egress_metering = var.resource_usage_export_config.enabled
      bigquery_destination {
        dataset_id = var.resource_usage_export_config.dataset
      }
    }
  }

  dynamic vertical_pod_autoscaling {
    for_each = var.vertical_pod_autoscaling == null ? [] : [""""]
    content {
      enabled = var.vertical_pod_autoscaling
    }
  }

  dynamic workload_identity_config {
    for_each = var.workload_identity ? [""""] : []
    content {
      identity_namespace = ""${var.project_id}.svc.id.goog""
    }
  }

}
",resource,"resource ""google_container_cluster"" ""cluster"" {
  provider    = google-beta
  project     = var.project_id
  name        = var.name
  description = var.description
  location    = var.location
  node_locations = (
    length(var.node_locations) == 0 ? null : var.node_locations
  )
  min_master_version          = var.min_master_version
  network                     = var.vpc_config.network
  subnetwork                  = var.vpc_config.subnetwork
  resource_labels             = var.labels
  default_max_pods_per_node   = var.max_pods_per_node
  enable_intranode_visibility = var.enable_features.intranode_visibility
  enable_l4_ilb_subsetting    = var.enable_features.l4_ilb_subsetting
  enable_shielded_nodes       = var.enable_features.shielded_nodes
  enable_fqdn_network_policy  = var.enable_features.fqdn_network_policy
  enable_tpu                  = var.enable_features.tpu
  initial_node_count          = 1
  remove_default_node_pool    = true
  deletion_protection         = var.deletion_protection
  datapath_provider = (
    var.enable_features.dataplane_v2
    ? ""ADVANCED_DATAPATH""
    : ""DATAPATH_PROVIDER_UNSPECIFIED""
  )

  # the default node pool is deleted here, use the gke-nodepool module instead.
  # the default node pool configuration is based on a shielded_nodes variable.
  node_config {
    service_account = var.service_account
    dynamic ""shielded_instance_config"" {
      for_each = var.enable_features.shielded_nodes ? [""""] : []
      content {
        enable_secure_boot          = true
        enable_integrity_monitoring = true
      }
    }
    tags = var.tags
  }

  addons_config {
    dns_cache_config {
      enabled = var.enable_addons.dns_cache
    }
    http_load_balancing {
      disabled = !var.enable_addons.http_load_balancing
    }
    horizontal_pod_autoscaling {
      disabled = !var.enable_addons.horizontal_pod_autoscaling
    }
    network_policy_config {
      disabled = !var.enable_addons.network_policy
    }
    cloudrun_config {
      disabled = !var.enable_addons.cloudrun
    }
    istio_config {
      disabled = var.enable_addons.istio == null
      auth = (
        try(var.enable_addons.istio.enable_tls, false) ? ""AUTH_MUTUAL_TLS"" : ""AUTH_NONE""
      )
    }
    gce_persistent_disk_csi_driver_config {
      enabled = var.enable_addons.gce_persistent_disk_csi_driver
    }
    gcp_filestore_csi_driver_config {
      enabled = var.enable_addons.gcp_filestore_csi_driver
    }
    gcs_fuse_csi_driver_config {
      enabled = var.enable_addons.gcs_fuse_csi_driver
    }
    kalm_config {
      enabled = var.enable_addons.kalm
    }
    config_connector_config {
      enabled = var.enable_addons.config_connector
    }
    gke_backup_agent_config {
      enabled = var.backup_configs.enable_backup_agent
    }
  }

  dynamic ""authenticator_groups_config"" {
    for_each = var.enable_features.groups_for_rbac != null ? [""""] : []
    content {
      security_group = var.enable_features.groups_for_rbac
    }
  }

  dynamic ""binary_authorization"" {
    for_each = var.enable_features.binary_authorization ? [""""] : []
    content {
      evaluation_mode = ""PROJECT_SINGLETON_POLICY_ENFORCE""
    }
  }

  dynamic ""cost_management_config"" {
    for_each = var.enable_features.cost_management == true ? [""""] : []
    content {
      enabled = true
    }
  }

  dynamic ""cluster_autoscaling"" {
    for_each = var.cluster_autoscaling == null ? [] : [""""]
    content {
      enabled = true

      autoscaling_profile = var.cluster_autoscaling.autoscaling_profile

      dynamic ""auto_provisioning_defaults"" {
        for_each = var.cluster_autoscaling.auto_provisioning_defaults != null ? [""""] : []
        content {
          boot_disk_kms_key = var.cluster_autoscaling.auto_provisioning_defaults.boot_disk_kms_key
          disk_size         = var.cluster_autoscaling.auto_provisioning_defaults.disk_size
          disk_type         = var.cluster_autoscaling.auto_provisioning_defaults.disk_type
          image_type        = var.cluster_autoscaling.auto_provisioning_defaults.image_type
          oauth_scopes      = var.cluster_autoscaling.auto_provisioning_defaults.oauth_scopes
          service_account   = var.cluster_autoscaling.auto_provisioning_defaults.service_account
          dynamic ""management"" {
            for_each = var.cluster_autoscaling.auto_provisioning_defaults.management != null ? [""""] : []
            content {
              auto_repair  = var.cluster_autoscaling.auto_provisioning_defaults.management.auto_repair
              auto_upgrade = var.cluster_autoscaling.auto_provisioning_defaults.management.auto_upgrade
            }
          }
          dynamic ""shielded_instance_config"" {
            for_each = var.cluster_autoscaling.auto_provisioning_defaults.shielded_instance_config != null ? [""""] : []
            content {
              enable_integrity_monitoring = var.cluster_autoscaling.auto_provisioning_defaults.shielded_instance_config.integrity_monitoring
              enable_secure_boot          = var.cluster_autoscaling.auto_provisioning_defaults.shielded_instance_config.secure_boot
            }
          }
        }
      }
      dynamic ""resource_limits"" {
        for_each = var.cluster_autoscaling.cpu_limits != null ? [""""] : []
        content {
          resource_type = ""cpu""
          minimum       = var.cluster_autoscaling.cpu_limits.min
          maximum       = var.cluster_autoscaling.cpu_limits.max
        }
      }
      dynamic ""resource_limits"" {
        for_each = var.cluster_autoscaling.mem_limits != null ? [""""] : []
        content {
          resource_type = ""memory""
          minimum       = var.cluster_autoscaling.mem_limits.min
          maximum       = var.cluster_autoscaling.mem_limits.max
        }
      }
      dynamic ""resource_limits"" {
        for_each = (
          try(var.cluster_autoscaling.gpu_resources, null) == null
          ? []
          : var.cluster_autoscaling.gpu_resources
        )
        iterator = gpu_resources
        content {
          resource_type = gpu_resources.value.resource_type
          minimum       = gpu_resources.value.min
          maximum       = gpu_resources.value.max
        }
      }
    }
  }

  dynamic ""database_encryption"" {
    for_each = var.enable_features.database_encryption != null ? [""""] : []
    content {
      state    = var.enable_features.database_encryption.state
      key_name = var.enable_features.database_encryption.key_name
    }
  }

  dynamic ""dns_config"" {
    for_each = var.enable_features.dns != null ? [""""] : []
    content {
      cluster_dns        = var.enable_features.dns.provider
      cluster_dns_scope  = var.enable_features.dns.scope
      cluster_dns_domain = var.enable_features.dns.domain
    }
  }

  dynamic ""gateway_api_config"" {
    for_each = var.enable_features.gateway_api ? [""""] : []
    content {
      channel = ""CHANNEL_STANDARD""
    }
  }

  dynamic ""ip_allocation_policy"" {
    for_each = var.vpc_config.secondary_range_blocks != null ? [""""] : []
    content {
      cluster_ipv4_cidr_block  = var.vpc_config.secondary_range_blocks.pods
      services_ipv4_cidr_block = var.vpc_config.secondary_range_blocks.services
      stack_type               = var.vpc_config.stack_type
    }
  }
  dynamic ""ip_allocation_policy"" {
    for_each = var.vpc_config.secondary_range_names != null ? [""""] : []
    content {
      cluster_secondary_range_name  = var.vpc_config.secondary_range_names.pods
      services_secondary_range_name = var.vpc_config.secondary_range_names.services
      stack_type                    = var.vpc_config.stack_type
    }
  }

  # Send GKE cluster logs from chosen sources to Cloud Logging.
  # System logs must be enabled if any other source is enabled.
  # This is validated by input variable validation rules.
  dynamic ""logging_config"" {
    for_each = var.logging_config.enable_system_logs ? [""""] : []
    content {
      enable_components = toset(compact([
        var.logging_config.enable_api_server_logs ? ""APISERVER"" : null,
        var.logging_config.enable_controller_manager_logs ? ""CONTROLLER_MANAGER"" : null,
        var.logging_config.enable_scheduler_logs ? ""SCHEDULER"" : null,
        ""SYSTEM_COMPONENTS"",
        var.logging_config.enable_workloads_logs ? ""WORKLOADS"" : null,
      ]))
    }
  }
  # Don't send any GKE cluster logs to Cloud Logging. Input variable validation
  # makes sure every other log source is false when enable_system_logs is false.
  dynamic ""logging_config"" {
    for_each = var.logging_config.enable_system_logs == false ? [""""] : []
    content {
      enable_components = []
    }
  }

  maintenance_policy {
    dynamic ""daily_maintenance_window"" {
      for_each = (
        try(var.maintenance_config.daily_window_start_time, null) != null
        ? [""""]
        : []
      )
      content {
        start_time = var.maintenance_config.daily_window_start_time
      }
    }
    dynamic ""recurring_window"" {
      for_each = (
        try(var.maintenance_config.recurring_window, null) != null
        ? [""""]
        : []
      )
      content {
        start_time = var.maintenance_config.recurring_window.start_time
        end_time   = var.maintenance_config.recurring_window.end_time
        recurrence = var.maintenance_config.recurring_window.recurrence
      }
    }
    dynamic ""maintenance_exclusion"" {
      for_each = (
        try(var.maintenance_config.maintenance_exclusions, null) == null
        ? []
        : var.maintenance_config.maintenance_exclusions
      )
      iterator = exclusion
      content {
        exclusion_name = exclusion.value.name
        start_time     = exclusion.value.start_time
        end_time       = exclusion.value.end_time
      }
    }
  }

  master_auth {
    client_certificate_config {
      issue_client_certificate = var.issue_client_certificate
    }
  }

  dynamic ""master_authorized_networks_config"" {
    for_each = var.vpc_config.master_authorized_ranges != null ? [""""] : []
    content {
      dynamic ""cidr_blocks"" {
        for_each = var.vpc_config.master_authorized_ranges
        iterator = range
        content {
          cidr_block   = range.value
          display_name = range.key
        }
      }
    }
  }

  dynamic ""mesh_certificates"" {
    for_each = var.enable_features.mesh_certificates != null ? [""""] : []
    content {
      enable_certificates = var.enable_features.mesh_certificates
    }
  }

  monitoring_config {
    enable_components = toset(compact([
      # System metrics is the minimum requirement if any other metrics are enabled. This is checked by input var validation.
      var.monitoring_config.enable_system_metrics ? ""SYSTEM_COMPONENTS"" : null,
      # Control plane metrics
      var.monitoring_config.enable_api_server_metrics ? ""APISERVER"" : null,
      var.monitoring_config.enable_controller_manager_metrics ? ""CONTROLLER_MANAGER"" : null,
      var.monitoring_config.enable_scheduler_metrics ? ""SCHEDULER"" : null,
      # Kube state metrics
      var.monitoring_config.enable_daemonset_metrics ? ""DAEMONSET"" : null,
      var.monitoring_config.enable_deployment_metrics ? ""DEPLOYMENT"" : null,
      var.monitoring_config.enable_hpa_metrics ? ""HPA"" : null,
      var.monitoring_config.enable_pod_metrics ? ""POD"" : null,
      var.monitoring_config.enable_statefulset_metrics ? ""STATEFULSET"" : null,
      var.monitoring_config.enable_storage_metrics ? ""STORAGE"" : null,
    ]))
    managed_prometheus {
      enabled = var.monitoring_config.enable_managed_prometheus
    }
  }

  # Dataplane V2 has built-in network policies
  dynamic ""network_policy"" {
    for_each = (
      var.enable_addons.network_policy && !var.enable_features.dataplane_v2
      ? [""""]
      : []
    )
    content {
      enabled  = true
      provider = ""CALICO""
    }
  }

  dynamic ""notification_config"" {
    for_each = var.enable_features.upgrade_notifications != null ? [""""] : []
    content {
      pubsub {
        enabled = true
        topic = (
          try(var.enable_features.upgrade_notifications.topic_id, null) != null
          ? var.enable_features.upgrade_notifications.topic_id
          : google_pubsub_topic.notifications[0].id
        )
      }
    }
  }

  dynamic ""private_cluster_config"" {
    for_each = (
      var.private_cluster_config != null ? [""""] : []
    )
    content {
      enable_private_nodes    = true
      enable_private_endpoint = var.private_cluster_config.enable_private_endpoint
      master_ipv4_cidr_block  = try(var.vpc_config.master_ipv4_cidr_block, null)
      master_global_access_config {
        enabled = var.private_cluster_config.master_global_access
      }
    }
  }

  dynamic ""pod_security_policy_config"" {
    for_each = var.enable_features.pod_security_policy ? [""""] : []
    content {
      enabled = var.enable_features.pod_security_policy
    }
  }

  dynamic ""release_channel"" {
    for_each = var.release_channel != null ? [""""] : []
    content {
      channel = var.release_channel
    }
  }

  dynamic ""resource_usage_export_config"" {
    for_each = (
      try(var.enable_features.resource_usage_export.dataset, null) != null
      ? [""""]
      : []
    )
    content {
      enable_network_egress_metering = (
        var.enable_features.resource_usage_export.enable_network_egress_metering
      )
      enable_resource_consumption_metering = (
        var.enable_features.resource_usage_export.enable_resource_consumption_metering
      )
      bigquery_destination {
        dataset_id = var.enable_features.resource_usage_export.dataset
      }
    }
  }

  dynamic ""vertical_pod_autoscaling"" {
    for_each = var.enable_features.vertical_pod_autoscaling ? [""""] : []
    content {
      enabled = var.enable_features.vertical_pod_autoscaling
    }
  }

  dynamic ""workload_identity_config"" {
    for_each = var.enable_features.workload_identity ? [""""] : []
    content {
      workload_pool = ""${var.project_id}.svc.id.goog""
    }
  }
  lifecycle {
    ignore_changes = [node_config]
  }
}
",resource,158,,de9825310c11ec5d1bc3cc6e9736253e8ced1021,0f446e89d49a9a801dc5ff932295c6ba476119b3,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/de9825310c11ec5d1bc3cc6e9736253e8ced1021/modules/gke-cluster/main.tf#L158,https://github.com/GoogleCloudPlatform/cloud-foundation-fabric/blob/0f446e89d49a9a801dc5ff932295c6ba476119b3/modules/gke-cluster-standard/main.tf,2020-05-12 18:46:50+02:00,2023-11-10 12:39:50+01:00,69,1,1,1,0,0,0,0,0,0
https://github.com/uyuni-project/sumaform,488,modules/openstack/host/main.tf,modules/openstack/host/main.tf,0,hack,# HACK: it should be possible to use 0 in case var.floating_ips has a value,"# HACK: it should be possible to use 0 in case var.floating_ips has a value 
 # this is not possible though, because 1) ?: is not short-circuiting 
 # 2) ?: can't return a list 3) element() does not operate on empty lists 
 # and 4) expressions like list.*.attribute will fail if list is empty","resource ""openstack_networking_floatingip_v2"" ""floating_ip"" {
  pool = ""floating""
  # HACK: it should be possible to use 0 in case var.floating_ips has a value
  # this is not possible though, because 1) ?: is not short-circuiting
  # 2) ?: can't return a list 3) element() does not operate on empty lists
  # and 4) expressions like list.*.attribute will fail if list is empty
  count = ""${var.count}""
}
",resource,,,61,0.0,519968238b16c7e6c42f0d516d2c590f0a788b9c,1c35cccee18a9803abaf1dc824c6ea13e068d344,https://github.com/uyuni-project/sumaform/blob/519968238b16c7e6c42f0d516d2c590f0a788b9c/modules/openstack/host/main.tf#L61,https://github.com/uyuni-project/sumaform/blob/1c35cccee18a9803abaf1dc824c6ea13e068d344/modules/openstack/host/main.tf#L0,2018-02-23 16:14:43+01:00,2019-11-08 15:50:33+01:00,18,2,0,1,0,0,1,0,0,0
https://github.com/Worklytics/psoxy,979,infra/modular-examples/gcp/main.tf,infra/modular-examples/gcp/main.tf,0,# todo,"# TODO: this would be cleaner as env var, but creates a cycle:","# TODO: this would be cleaner as env var, but creates a cycle: 
 # Error: Cycle: module.psoxy.module.psoxy-bulk.local_file.todo-gcp-psoxy-bulk-test, module.psoxy.module.lookup_output.var.function_service_account_email (expand), module.psoxy.module.lookup_output.google_storage_bucket_iam_member.write_to_output_bucket, module.psoxy.module.lookup_output.output.bucket_name (expand), module.psoxy.module.lookup_output.var.bucket_name_prefix (expand), module.psoxy.module.lookup_output.google_storage_bucket.bucket, module.psoxy.module.lookup_output.google_storage_bucket_iam_member.accessors, module.psoxy.module.lookup_output (close), module.psoxy.module.psoxy-bulk.var.environment_variables (expand), module.psoxy.module.psoxy-bulk.google_cloudfunctions_function.function, module.psoxy.module.psoxy-bulk (close)","resource ""google_secret_manager_secret"" ""additional_transforms"" {
  for_each = local.inputs_to_build_lookups_for

  project   = var.gcp_project_id
  secret_id = ""${local.config_parameter_prefix}${upper(replace(each.key, ""-"", ""_""))}_ADDITIONAL_TRANSFORMS""

  replication {
    automatic = true
  }
}
",resource,,,343,0.0,4f41d480721ffa94079bbfb0cdcc181e22423300,f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e,https://github.com/Worklytics/psoxy/blob/4f41d480721ffa94079bbfb0cdcc181e22423300/infra/modular-examples/gcp/main.tf#L343,https://github.com/Worklytics/psoxy/blob/f6f60a1c3c6fa3e2898c9b0c26a8430f0000bd7e/infra/modular-examples/gcp/main.tf#L0,2023-04-18 16:24:15-07:00,2023-06-16 14:08:45-07:00,13,2,0,1,0,1,0,0,0,0
https://github.com/uyuni-project/sumaform,1180,modules/libvirt/suse_manager/main.tf,modules/libvirt/suse_manager/main.tf,0,hack,"# HACK: work around ""conditional operator cannot be used with list values""","# HACK: work around ""conditional operator cannot be used with list values""","module ""suse_manager"" {
  source = ""../host""

  base_configuration = ""${var.base_configuration}""
  name = ""${var.name}""
  count = 1
  use_os_released_updates = ""${var.use_os_released_updates}""
  use_os_unreleased_updates = ""${var.use_os_unreleased_updates}""
  additional_repos = ""${var.additional_repos}""
  additional_repos_only = ""${var.additional_repos_only}""
  additional_certs = ""${var.additional_certs}""
  additional_packages = ""${var.additional_packages}""
  swap_file_size = ""${var.swap_file_size}""
  ssh_key_path = ""${var.ssh_key_path}""
  gpg_keys = ""${var.gpg_keys}""
  ipv6 = ""${var.ipv6}""
  connect_to_base_network = true
  connect_to_additional_network = false
  # HACK: work around ""conditional operator cannot be used with list values""
  roles = ""${split("","", var.register_to_server == ""null"" ? ""suse_manager_server"" : ""suse_manager_server,minion"")}""
  grains = <<EOF

product_version: ${var.product_version}
cc_username: ${var.base_configuration[""cc_username""]}
cc_password: ${var.base_configuration[""cc_password""]}
channels: [${join("","", var.channels)}]
cloned_channels: ${var.cloned_channels}
mirror: ${var.base_configuration[""mirror""]}
iss_master: ${var.iss_master}
iss_slave: ${var.iss_slave}
server: ${var.register_to_server}
auto_connect_to_master: ${var.auto_register}
susemanager:
  activation_key: ${var.activation_key}
smt: ${var.smt}
server_username: ${var.server_username}
server_password: ${var.server_password}
disable_firewall: ${var.disable_firewall}
allow_postgres_connections: ${var.allow_postgres_connections}
unsafe_postgres: ${var.unsafe_postgres}
java_debugging: ${var.java_debugging}
skip_changelog_import: ${var.skip_changelog_import}
browser_side_less: ${var.browser_side_less}
create_first_user: ${var.create_first_user}
mgr_sync_autologin: ${var.mgr_sync_autologin}
create_sample_channel: ${var.create_sample_channel}
create_sample_activation_key: ${var.create_sample_activation_key}
create_sample_bootstrap_script: ${var.create_sample_bootstrap_script}
publish_private_ssl_key: ${var.publish_private_ssl_key}
auto_accept: ${var.auto_accept}
monitored: ${var.monitored}
pts: ${var.pts}
pts_minion: ${var.pts_minion}
pts_locust: ${var.pts_locust}
pts_system_count: ${var.pts_system_count}
pts_system_prefix: ${var.pts_system_prefix}
apparmor: ${var.apparmor}
from_email: ${var.from_email}
traceback_email: ${var.traceback_email}
saltapi_tcpdump: ${var.saltapi_tcpdump}

EOF

  // Provider-specific variables
  image = ""${var.image == ""default"" ? lookup(var.images, var.product_version) : var.image}""
  memory = ""${var.memory}""
  vcpu = ""${var.vcpu}""
  running = ""${var.running}""
  mac = ""${var.mac}""
  additional_disk = ""${var.additional_disk}""
}
",module,"module ""suse_manager"" {
  source = ""../host""

  base_configuration            = var.base_configuration
  name                          = var.name
  use_os_released_updates       = var.use_os_released_updates
  use_os_unreleased_updates     = var.use_os_unreleased_updates
  additional_repos              = var.additional_repos
  additional_repos_only         = var.additional_repos_only
  additional_certs              = var.additional_certs
  additional_packages           = var.additional_packages
  swap_file_size                = var.swap_file_size
  ssh_key_path                  = var.ssh_key_path
  gpg_keys                      = var.gpg_keys
  ipv6                          = var.ipv6
  connect_to_base_network       = true
  connect_to_additional_network = false
  roles = var.register_to_server == null ? [""suse_manager_server""] : [""suse_manager_server"", ""minion""]

  grains = {
    product_version        = var.product_version
    cc_username            = var.base_configuration[""cc_username""]
    cc_password            = var.base_configuration[""cc_password""]
    channels               = var.channels
    wait_for_reposync      = var.wait_for_reposync
    cloned_channels        = var.cloned_channels
    mirror                 = var.base_configuration[""mirror""]
    iss_master             = var.iss_master
    iss_slave              = var.iss_slave
    server                 = var.register_to_server
    auto_connect_to_master = var.auto_register
    susemanager = {
      activation_key = var.activation_key
    }
    smt                            = var.smt
    server_username                = var.server_username
    server_password                = var.server_password
    disable_firewall               = var.disable_firewall
    allow_postgres_connections     = var.allow_postgres_connections
    unsafe_postgres                = var.unsafe_postgres
    java_debugging                 = var.java_debugging
    skip_changelog_import          = var.skip_changelog_import
    browser_side_less              = var.browser_side_less
    create_first_user              = var.create_first_user
    mgr_sync_autologin             = var.mgr_sync_autologin
    create_sample_channel          = var.create_sample_channel
    create_sample_activation_key   = var.create_sample_activation_key
    create_sample_bootstrap_script = var.create_sample_bootstrap_script
    publish_private_ssl_key        = var.publish_private_ssl_key
    disable_download_tokens        = var.disable_download_tokens
    auto_accept                    = var.auto_accept
    monitored                      = var.monitored
    pts                            = var.pts
    pts_minion                     = var.pts_minion
    pts_locust                     = var.pts_locust
    pts_system_count               = var.pts_system_count
    pts_system_prefix              = var.pts_system_prefix
    apparmor                       = var.apparmor
    from_email                     = var.from_email
    traceback_email                = var.traceback_email
    saltapi_tcpdump                = var.saltapi_tcpdump
    repository_disk_size           = var.repository_disk_size
    repository_disk_device         = ""vdb""
  }


  // Provider-specific variables
  image           = var.image == ""default"" || var.product_version == ""head"" ? var.images[var.product_version] : var.image
  memory          = var.memory
  vcpu            = var.vcpu
  running         = var.running
  mac             = var.mac
  additional_disk = var.repository_disk_size > 0 ? [{ volume_id = libvirt_volume.server_data_disk[0].id }] : []
}
",module,32,,281c7982a20f0d4ae89b7de0744ec51f16e53203,ba7df33db5944093ff582de14f377399f43cd9b1,https://github.com/uyuni-project/sumaform/blob/281c7982a20f0d4ae89b7de0744ec51f16e53203/modules/libvirt/suse_manager/main.tf#L32,https://github.com/uyuni-project/sumaform/blob/ba7df33db5944093ff582de14f377399f43cd9b1/modules/libvirt/suse_manager/main.tf,2019-09-27 08:59:18+02:00,2019-12-03 17:04:10+01:00,14,1,0,1,0,0,0,0,0,0
